:PROPERTIES:
:ID:       758c8d96-3776-4254-bc91-dbc6680456ed
:END:
#+TITLE: EM for Gaussian mixtures (latent variables)
#+FILETAGS: :literature:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

We now consider the application of this latent variable view of EM to the specific case of a Gaussian mixture model. Recall that our goal is to maximize the log likelihood function (9.14), which is computed using the observed data set \(\mathbf{X}\), and we saw that this was more difficult than for the case of a single Gaussian distribution due to the presence of the summation over \(k\) that occurs inside the logarithm. Suppose then that in addition to the observed data set \(\mathbf{X}\), we were also given the values of the corresponding discrete variables Z . Recall that Figure 9.5(a) shows a 'complete' data set (i.e., one that includes labels showing which component generated each data point) while Figure 9.5(b) shows the corresponding 'incomplete' data set. The graphical model for the complete data is shown in Figure 9.9. Figure 9.9 This shows the same graph as in Figure 9.6 except that we now suppose that the discrete variables \(\mathbf{z}_{n}\) are observed, as well as the data variables \(\mathbf{x}_{n}\).

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_27_4c5a80c0a42382702197g-458}
\end{center}

Now consider the problem of maximizing the likelihood for the complete data set \(\{\mathbf{X}, \mathbf{Z}\}\). From (9.10) and (9.11), this likelihood function takes the form

\[
p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})=\prod_{n=1}^{N} \prod_{k=1}^{K} \pi_{k}^{z_{n k}} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)^{z_{n k}}
\]

where \(z_{n k}\) denotes the \(k^{\text {th }}\) component of \(\mathbf{z}_{n}\). Taking the logarithm, we obtain

\[
\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k}\left\{\ln \pi_{k}+\ln \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\}
\]

Comparison with the log likelihood function (9.14) for the incomplete data shows that the summation over \(k\) and the logarithm have been interchanged. The logarithm now acts directly on the Gaussian distribution, which itself is a member of the exponential family. Not surprisingly, this leads to a much simpler solution to the maximum likelihood problem, as we now show. Consider first the maximization with respect to the means and covariances. Because \(\mathbf{z}_{n}\) is a \(K\)-dimensional vector with all elements equal to 0 except for a single element having the value 1 , the complete-data log likelihood function is simply a sum of \(K\) independent contributions, one for each mixture component. Thus the maximization with respect to a mean or a covariance is exactly as for a single Gaussian, except that it involves only the subset of data points that are 'assigned' to that component. For the maximization with respect to the mixing coefficients, we note that these are coupled for different values of \(k\) by virtue of the summation constraint (9.9). Again, this can be enforced using a Lagrange multiplier as before, and leads to the result

\[
\pi_{k}=\frac{1}{N} \sum_{n=1}^{N} z_{n k}
\]

so that the mixing coefficients are equal to the fractions of data points assigned to the corresponding components.

Thus we see that the complete-data log likelihood function can be maximized trivially in closed form. In practice, however, we do not have values for the latent variables so, as discussed earlier, we consider the expectation, with respect to the posterior distribution of the latent variables, of the complete-data log likelihood. Exercise 9.5

Section 8.2
Exercise 9.8
Using (9.10) and (9.11) together with Bayes' theorem, we see that this posterior distribution takes the form

\[
p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) \propto \prod_{n=1}^{N} \prod_{k=1}^{K}\left[\pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right]^{z_{n k}} .
\]

and hence factorizes over \(n\) so that under the posterior distribution the \(\left\{\mathbf{z}_{n}\right\}\) are independent. This is easily verified by inspection of the directed graph in Figure 9.6 and making use of the d-separation criterion. The expected value of the indicator variable \(z_{n k}\) under this posterior distribution is then given by

\begin{align*}
\mathbb{E}\left[z_{n k}\right] & =\frac{\sum_{z_{n k}} z_{n k}\left[\pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right]^{z_{n k}}}{\sum_{z_{n j}}\left[\pi_{j} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)\right]^{z_{n j}}} \\
= & \frac{\pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)}=\gamma\left(z_{n k}\right)
\end{align*}

which is just the responsibility of component \(k\) for data point \(\mathbf{x}_{n}\). The expected value of the complete-data log likelihood function is therefore given by

\[
\mathbb{E}_{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]=\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left\{\ln \pi_{k}+\ln \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\}
\]

We can now proceed as follows. First we choose some initial values for the parameters \(\boldsymbol{\mu}^{\text {old }}, \boldsymbol{\Sigma}^{\text {old }}\) and \(\boldsymbol{\pi}^{\text {old }}\), and use these to evaluate the responsibilities (the E step). We then keep the responsibilities fixed and maximize (9.40) with respect to \(\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\) and \(\pi_{k}\) (the M step). This leads to closed form solutions for \(\boldsymbol{\mu}^{\text {new }}, \boldsymbol{\Sigma}^{\text {new }}\) and \(\boldsymbol{\pi}^{\text {new }}\) given by (9.17), (9.19), and (9.22) as before. This is precisely the EM algorithm for Gaussian mixtures as derived earlier. We shall gain more insight into the role of the expected complete-data log likelihood function when we give a proof of convergence of the EM algorithm in Section 9.4.
