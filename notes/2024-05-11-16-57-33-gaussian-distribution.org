:PROPERTIES:
:ID:       99d348be-0ce8-4de4-a9f9-ae9636f32887
:END:
#+TITLE: Gaussian distribution
#+FILETAGS: :literature:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

The Gaussian, also known as the normal distribution, is a widely used model for the distribution of continuous variables. In the case of a single variable \(x\), the Gaussian distribution can be written in the form

\[\mathcal{N}\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}\]

where \(\mu\) is the mean and \(\sigma^{2}\) is the variance. For a \(D\)-dimensional vector \(\mathbf{x}\), the multivariate Gaussian distribution takes the form

\[\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}} \exp \left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}\]

where \(\boldsymbol{\mu}\) is a \(D\)-dimensional mean vector, \(\boldsymbol{\Sigma}\) is a \(D \times D\) covariance matrix, and \(|\boldsymbol{\Sigma}|\) denotes the determinant of \(\boldsymbol{\Sigma}\).

The Gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives. For example, we have already seen that for a single real variable, the distribution that maximizes the entropy is the Gaussian. This property applies also to the multivariate Gaussian.

Another situation in which the Gaussian distribution arises is when we consider the sum of multiple random variables. The central limit theorem (due to Laplace) tells us that, subject to certain mild conditions, the sum of a set of random variables, which is of course itself a random variable, has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases (Walker, 1969). We can [[file:2023_08_27_4c5a80c0a42382702197g-097(1)]]

Figure 2.6 Histogram plots of the mean of \(N\) uniformly distributed numbers for various values of \(N\). We observe that as \(N\) increases, the distribution tends towards a Gaussian.

illustrate this by considering \(N\) variables \(x_{1}, \ldots, x_{N}\) each of which has a uniform distribution over the interval \([0,1]\) and then considering the distribution of the mean \(\left(x_{1}+\cdots+x_{N}\right) / N\). For large \(N\), this distribution tends to a Gaussian, as illustrated in Figure 2.6. In practice, the convergence to a Gaussian as \(N\) increases can be very rapid. One consequence of this result is that the binomial distribution (2.9), which is a distribution over \(m\) defined by the sum of \(N\) observations of the random binary variable \(x\), will tend to a Gaussian as \(N \rightarrow \infty\) (see Figure 2.1 for the case of \(N=10)\).

The Gaussian distribution has many important analytical properties, and we shall consider several of these in detail. As a result, this section will be rather more technically involved than some of the earlier sections, and will require familiarity with Appendix C various matrix identities. However, we strongly encourage the reader to become proficient in manipulating Gaussian distributions using the techniques presented here as this will prove invaluable in understanding the more complex models presented in later chapters.

We begin by considering the geometrical form of the Gaussian distribution. The functional dependence of the Gaussian on \(\mathbf{x}\) is through the quadratic form

\[\Delta^{2}=(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\]

which appears in the exponent. The quantity \(\Delta\) is called the Mahalanobis distance from \(\boldsymbol{\mu}\) to \(\mathrm{x}\) and reduces to the Euclidean distance when \(\boldsymbol{\Sigma}\) is the identity matrix. The Gaussian distribution will be constant on surfaces in \(\mathrm{x}\)-space for which this quadratic form is constant.

First of all, we note that the matrix \(\boldsymbol{\Sigma}\) can be taken to be symmetric, without loss of generality, because any antisymmetric component would disappear from the exponent. Now consider the eigenvector equation for the covariance matrix

\[\Sigma \mathbf{u}_{i}=\lambda_{i} \mathbf{u}_{i}\]

where \(i=1, \ldots, D\). Because \(\boldsymbol{\Sigma}\) is a real, symmetric matrix its eigenvalues will be real, and its eigenvectors can be chosen to form an orthonormal set, so that

\[\mathbf{u}_{i}^{\mathrm{T}} \mathbf{u}_{j}=I_{i j}\]

where \(I_{i j}\) is the \(i, j\) element of the identity matrix and satisfies

\[I_{i j}= \begin{cases}1, & \text { if } i=j \\ 0, & \text { otherwise. }\end{cases}\]

The covariance matrix \(\boldsymbol{\Sigma}\) can be expressed as an expansion in terms of its eigenvectors in the form

\[\boldsymbol{\Sigma}=\sum_{i=1}^{D} \lambda_{i} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm{T}}\]

and similarly the inverse covariance matrix \(\boldsymbol{\Sigma}^{-1}\) can be expressed as

\[\boldsymbol{\Sigma}^{-1}=\sum_{i=1}^{D} \frac{1}{\lambda_{i}} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm{T}} .\]

Substituting (2.49) into (2.44), the quadratic form becomes

\[\Delta^{2}=\sum_{i=1}^{D} \frac{y_{i}^{2}}{\lambda_{i}}\]

where we have defined

\[y_{i}=\mathbf{u}_{i}^{\mathrm{T}}(\mathbf{x}-\boldsymbol{\mu}) .\]

We can interpret \(\left\{y_{i}\right\}\) as a new coordinate system defined by the orthonormal vectors \(\mathbf{u}_{i}\) that are shifted and rotated with respect to the original \(x_{i}\) coordinates. Forming the vector \(\mathbf{y}=\left(y_{1}, \ldots, y_{D}\right)^{\mathrm{T}}\), we have

\[\mathbf{y}=\mathbf{U}(\mathbf{x}-\boldsymbol{\mu})\]

Figure 2.7 The red curve shows the ellip- \(x_{2}\) tical surface of constant probability density for a Gaussian in a two-dimensional space \(\mathrm{x}=\) \(\left(x_{1}, x_{2}\right)\) on which the density is \(\exp (-1 / 2)\) of its value at \(\mathrm{x}=\boldsymbol{\mu}\). The major axes of the ellipse are defined by the eigenvectors \(\mathbf{u}_{i}\) of the covariance matrix, with corresponding eigenvalues \(\lambda_{i}\). Appendix C

where \(\mathbf{U}\) is a matrix whose rows are given by \(\mathbf{u}_{i}^{\mathrm{T}}\). From (2.46) it follows that \(\mathbf{U}\) is an orthogonal matrix, i.e., it satisfies \(\mathbf{U U}^{\mathrm{T}}=\mathbf{I}\), and hence also \(\mathbf{U}^{\mathrm{T}} \mathbf{U}=\mathbf{I}\), where \(\mathbf{I}\) is the identity matrix.

The quadratic form, and hence the Gaussian density, will be constant on surfaces for which (2.51) is constant. If all of the eigenvalues \(\lambda_{i}\) are positive, then these surfaces represent ellipsoids, with their centres at \(\boldsymbol{\mu}\) and their axes oriented along \(\mathbf{u}_{i}\), and with scaling factors in the directions of the axes given by \(\lambda_{i}^{1 / 2}\), as illustrated in Figure 2.7.

For the Gaussian distribution to be well defined, it is necessary for all of the eigenvalues \(\lambda_{i}\) of the covariance matrix to be strictly positive, otherwise the distribution cannot be properly normalized. A matrix whose eigenvalues are strictly positive is said to be positive definite. In Chapter 12, we will encounter Gaussian distributions for which one or more of the eigenvalues are zero, in which case the distribution is singular and is confined to a subspace of lower dimensionality. If all of the eigenvalues are nonnegative, then the covariance matrix is said to be positive semidefinite.

Now consider the form of the Gaussian distribution in the new coordinate system defined by the \(y_{i}\). In going from the \(\mathbf{x}\) to the \(\mathbf{y}\) coordinate system, we have a Jacobian matrix \(\mathbf{J}\) with elements given by

\[J_{i j}=\frac{\partial x_{i}}{\partial y_{j}}=U_{j i}\]

where \(U_{j i}\) are the elements of the matrix \(\mathbf{U}^{\mathrm{T}}\). Using the orthonormality property of the matrix \(\mathbf{U}\), we see that the square of the determinant of the Jacobian matrix is

\[|\mathbf{J}|^{2}=\left|\mathbf{U}^{\mathrm{T}}\right|^{2}=\left|\mathbf{U}^{\mathrm{T}}\right||\mathbf{U}|=\left|\mathbf{U}^{\mathrm{T}} \mathbf{U}\right|=|\mathbf{I}|=1\]

and hence \(|\mathbf{J}|=1\). Also, the determinant \(|\boldsymbol{\Sigma}|\) of the covariance matrix can be written as the product of its eigenvalues, and hence

\[|\boldsymbol{\Sigma}|^{1 / 2}=\prod_{j=1}^{D} \lambda_{j}^{1 / 2}\]

Thus in the \(y_{j}\) coordinate system, the Gaussian distribution takes the form

\[p(\mathbf{y})=p(\mathbf{x})|\mathbf{J}|=\prod_{j=1}^{D} \frac{1}{\left(2 \pi \lambda_{j}\right)^{1 / 2}} \exp \left\{-\frac{y_{j}^{2}}{2 \lambda_{j}}\right\}\]

which is the product of \(D\) independent univariate Gaussian distributions. The eigenvectors therefore define a new set of shifted and rotated coordinates with respect to which the joint probability distribution factorizes into a product of independent distributions. The integral of the distribution in the \(\mathbf{y}\) coordinate system is then

\[\int p(\mathbf{y}) \mathrm{d} \mathbf{y}=\prod_{j=1}^{D} \int_{-\infty}^{\infty} \frac{1}{\left(2 \pi \lambda_{j}\right)^{1 / 2}} \exp \left\{-\frac{y_{j}^{2}}{2 \lambda_{j}}\right\} \mathrm{d} y_{j}=1\]

where we have used the result (1.48) for the normalization of the univariate Gaussian. This confirms that the multivariate Gaussian (2.43) is indeed normalized.

We now look at the moments of the Gaussian distribution and thereby provide an interpretation of the parameters \(\boldsymbol{\mu}\) and \(\boldsymbol{\Sigma}\). The expectation of \(\mathbf{x}\) under the Gaussian distribution is given by

\[\begin{align*}
\mathbb{E}[\mathbf{x}] & =\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}} \int \exp \left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\} \mathbf{x} \mathrm{d} \mathbf{x} \\
& =\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}} \int \exp \left\{-\frac{1}{2} \mathbf{z}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{z}\right\}(\mathbf{z}+\boldsymbol{\mu}) \mathrm{d} \mathbf{z}
\end{align*}\]

where we have changed variables using \(\mathbf{z}=\mathbf{x}-\boldsymbol{\mu}\). We now note that the exponent is an even function of the components of \(\mathbf{z}\) and, because the integrals over these are taken over the range \((-\infty, \infty)\), the term in \(\mathbf{z}\) in the factor \((\mathbf{z}+\boldsymbol{\mu})\) will vanish by symmetry. Thus

\[\mathbb{E}[\mathbf{x}]=\boldsymbol{\mu}\]

and so we refer to \(\boldsymbol{\mu}\) as the mean of the Gaussian distribution.

We now consider second order moments of the Gaussian. In the univariate case, we considered the second order moment given by \(\mathbb{E}\left[x^{2}\right]\). For the multivariate Gaussian, there are \(D^{2}\) second order moments given by \(\mathbb{E}\left[x_{i} x_{j}\right]\), which we can group together to form the matrix \(\mathbb{E}\left[\mathbf{x x}^{\mathrm{T}}\right]\). This matrix can be written as

\[\begin{align*}
\mathbb{E} & {\left[\mathbf{x x}^{\mathrm{T}}\right]=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}} \int \exp \left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\} \mathbf{x x}^{\mathrm{T}} \mathrm{d} \mathbf{x} } \\
& =\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}} \int \exp \left\{-\frac{1}{2} \mathbf{z}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{z}\right\}(\mathbf{z}+\boldsymbol{\mu})(\mathbf{z}+\boldsymbol{\mu})^{\mathrm{T}} \mathrm{d} \mathbf{z}
\end{align*}\]

where again we have changed variables using \(\mathbf{z}=\mathbf{x}-\boldsymbol{\mu}\). Note that the cross-terms involving \(\boldsymbol{\mu} \mathbf{z}^{\mathrm{T}}\) and \(\boldsymbol{\mu}^{\mathrm{T}} \mathbf{z}\) will again vanish by symmetry. The term \(\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}\) is constant and can be taken outside the integral, which itself is unity because the Gaussian distribution is normalized. Consider the term involving \(\mathbf{z z}^{\mathrm{T}}\). Again, we can make use of the eigenvector expansion of the covariance matrix given by (2.45), together with the completeness of the set of eigenvectors, to write

\[\mathbf{z}=\sum_{j=1}^{D} y_{j} \mathbf{u}_{j}\]

where \(y_{j}=\mathbf{u}_{j}^{\mathrm{T}} \mathbf{z}\), which gives

\begin{align*}
& \frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}} \int \exp \left\{-\frac{1}{2} \mathbf{z}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{z}\right\} \mathbf{z z}^{\mathrm{T}} \mathrm{d} \mathbf{z} \\
& =\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}} \sum_{i=1}^{D} \sum_{j=1}^{D} \mathbf{u}_{i} \mathbf{u}_{j}^{\mathrm{T}} \int \exp \left\{-\sum_{k=1}^{D} \frac{y_{k}^{2}}{2 \lambda_{k}}\right\} y_{i} y_{j} \mathrm{~d} \mathbf{y} \\
& =\sum_{i=1}^{D} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm{T}} \lambda_{i}=\mathbf{\Sigma}
\end{align*}

where we have made use of the eigenvector equation (2.45), together with the fact that the integral on the right-hand side of the middle line vanishes by symmetry unless \(i=j\), and in the final line we have made use of the results (1.50) and (2.55), together with (2.48). Thus we have

\[\mathbb{E}\left[\mathbf{x x}^{\mathrm{T}}\right]=\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+\boldsymbol{\Sigma} .\]

For single random variables, we subtracted the mean before taking second moments in order to define a variance. Similarly, in the multivariate case it is again convenient to subtract off the mean, giving rise to the covariance of a random vector \(\mathrm{x}\) defined by

\[\operatorname{cov}[\mathbf{x}]=\mathbb{E}\left[(\mathbf{x}-\mathbb{E}[\mathbf{x}])(\mathbf{x}-\mathbb{E}[\mathbf{x}])^{\mathrm{T}}\right] .\]

For the specific case of a Gaussian distribution, we can make use of \(\mathbb{E}[\mathbf{x}]=\boldsymbol{\mu}\), together with the result (2.62), to give

\[\operatorname{cov}[\mathbf{x}]=\mathbf{\Sigma}\]

Because the parameter matrix \(\boldsymbol{\Sigma}\) governs the covariance of \(\mathbf{x}\) under the Gaussian distribution, it is called the covariance matrix.

Although the Gaussian distribution (2.43) is widely used as a density model, it suffers from some significant limitations. Consider the number of free parameters in the distribution. A general symmetric covariance matrix \(\boldsymbol{\Sigma}\) will have \(D(D+1) / 2\) independent parameters, and there are another \(D\) independent parameters in \(\mu\), giving \(D(D+3) / 2\) parameters in total. For large \(D\), the total number of parameters Figure 2.8 Contours of constant probability density for a Gaussian distribution in two dimensions in which the covariance matrix is (a) of general form, (b) diagonal, in which the elliptical contours are aligned with the coordinate axes, and (c) proportional to the identity matrix, in which the contours are concentric circles.

therefore grows quadratically with \(D\), and the computational task of manipulating and inverting large matrices can become prohibitive. One way to address this problem is to use restricted forms of the covariance matrix. If we consider covariance matrices that are diagonal, so that \(\boldsymbol{\Sigma}=\operatorname{diag}\left(\sigma_{i}^{2}\right)\), we then have a total of \(2 D\) independent parameters in the density model. The corresponding contours of constant density are given by axis-aligned ellipsoids. We could further restrict the covariance matrix to be proportional to the identity matrix, \(\boldsymbol{\Sigma}=\sigma^{2} \mathbf{I}\), known as an isotropic covariance, giving \(D+1\) independent parameters in the model and spherical surfaces of constant density. The three possibilities of general, diagonal, and isotropic covariance matrices are illustrated in Figure 2.8. Unfortunately, whereas such approaches limit the number of degrees of freedom in the distribution and make inversion of the covariance matrix a much faster operation, they also greatly restrict the form of the probability density and limit its ability to capture interesting correlations in the data.

A further limitation of the Gaussian distribution is that it is intrinsically unimodal (i.e., has a single maximum) and so is unable to provide a good approximation to multimodal distributions. Thus the Gaussian distribution can be both too flexible, in the sense of having too many parameters, while also being too limited in the range of distributions that it can adequately represent. We will see later that the introduction of latent variables, also called hidden variables or unobserved variables, allows both of these problems to be addressed. In particular, a rich family of multimodal distributions is obtained by introducing discrete latent variables leading to mixtures of Gaussians, as discussed in Section 2.3.9. Similarly, the introduction of continuous latent variables, as described in Chapter 12, leads to models in which the number of free parameters can be controlled independently of the dimensionality \(D\) of the data space while still allowing the model to capture the dominant correlations in the data set. Indeed, these two approaches can be combined and further extended to derive a very rich set of hierarchical models that can be adapted to a broad range of practical applications. For instance, the Gaussian version of the Markov random field, which is widely used as a probabilistic model of images, is a Gaussian distribution over the joint space of pixel intensities but rendered tractable through the imposition of considerable structure reflecting the spatial organization of the pixels. Similarly, the linear dynamical system, used to model time series data for applications such as tracking, is also a joint Gaussian distribution over a potentially large number of observed and latent variables and again is tractable due to the structure imposed on the distribution. A powerful framework for expressing the form and properties of such complex distributions is that of probabilistic graphical models, which will form the subject of Chapter 8 .
