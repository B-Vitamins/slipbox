:PROPERTIES:
:ID:       b9405cc2-6709-406a-a516-44231ed5bb5f
:END:
#+TITLE: Maximum likelihood (logistic regression)
#+FILETAGS: :literature:prml:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

The [[id:8d6d1914-142b-43aa-b9fd-4ea7404a40ed][logistic regression]] model is defined as

\begin{align*}
p\left(\mathcal{C}_{1} \mid \boldsymbol{\phi}\right)=y(\boldsymbol{\phi})=\sigma (\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}) \tag{1}
\end{align*}

with \(p\left(\mathcal{C}_{2} \mid \phi\right)=1-p\left(\mathcal{C}_{1} \mid \phi\right)\). Here \(\sigma(\cdot)\) is the [[id:d1230eba-905b-4765-84ef-daa0b21fd3d9][logistic sigmoid function]]. For an \(M\)-dimensional feature space \(\phi\), this model has \(M\) adjustable parameters. We can use [[id:adca5f2b-1056-4cb6-b5d4-02be3ccc6e54][maximum likelihood estimation]] to determine \( \mathbf{w} \).

* Likelihood function
For a data set \(\left\{\phi_{n}, t_{n}\right\}\), where \(t_{n} \in\{0,1\}\) and \(\phi_{n}=\boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\), with \(n=\) \(1, \ldots, N\), the [[id:82831310-4909-4f8b-8db0-ff0d8bef8b0b][likelihood function]] can be written as

\begin{align*}
p(\mathbf{t} \mid \mathbf{w})=\prod_{n=1}^{N} y_{n}^{t_{n}}\left\{1-y_{n}\right\}^{1-t_{n}} \tag{2}
\end{align*}

where \(\mathbf{t}=\left(t_{1}, \ldots, t_{N}\right)^{\mathrm{T}}\) and \(y_{n}=p\left(\mathcal{C}_{1} \mid \boldsymbol{\phi}_{n}\right)\). 

* Error function (cross-entropy error function)
As usual, we can define an error function by taking the negative logarithm of the likelihood, which gives the *cross-entropy error function* given by

\begin{align*}
E(\mathbf{w})=-\ln p(\mathbf{t} \mid \mathbf{w})=-\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\} \tag{3}
\end{align*}

where \(y_{n}=\sigma\left(a_{n}\right)\) and \(a_{n}=\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}_{n}\). Taking the gradient of the error function with respect to \(\mathbf{w}\), we obtain

\begin{align*}
\nabla E(\mathbf{w})=\sum_{n=1}^{N}\left(y_{n}-t_{n}\right) \phi_{n} \tag{5}
\end{align*}

where we have made use of (2). The contribution to the gradient from data point \(n\) is given by the 'error' \(y_{n}-t_{n}\) between the target value and the prediction of the model, times the basis function vector \(\phi_{n}\). Furthermore, comparison with [[id:2967ed4f-6501-4d36-8446-50e536e66a2c][maximum likelihood (linear regression)]] shows that this takes precisely the same form as the gradient of the [[id:2967ed4f-6501-4d36-8446-50e536e66a2c][sum of squares error function]] 

\[
E_D(\mathbf{w}) = - \frac{1}{2} \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\}^2. \tag{6}
\]

for the [[id:2967ed4f-6501-4d36-8446-50e536e66a2c][linear regression]] model.
* Sequential learning (LMS algorithm)
If desired, we could make use of the result (5) to give a [[id:9e1d9048-8030-48d1-bc9d-9316e92e47e9][stochastic gradient descent]] based [[id:2a73f861-7650-4a41-84f0-81b975111c4f][sequential algorithm]] in which patterns are presented one at a time and the weight vectors is updated using

\begin{align*}
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta \nabla E_{n} \tag{7}
\end{align*}

similar to the [[id:8581366d-5371-4713-993f-69a202ffe6fe][LMS algorithm]] used for [[id:2967ed4f-6501-4d36-8446-50e536e66a2c][linear regression]] in which \(\nabla E_{n}\) is the \(n^{\text {th }}\) term in (5).
* Limitation of maximum likelihood (logistic regression)
It is worth noting that maximum likelihood can exhibit severe over-fitting for data sets that are [[id:46d7eeb7-d01e-46a8-a4ea-6b27285b265f][linearly separable]]. This arises because the maximum likelihood solution occurs when the hyperplane corresponding to \(\sigma=0.5\), equivalent to \(\mathbf{w}^{\mathrm{T}} \phi=\) 0, separates the two classes and the magnitude of \(\mathbf{w}\) goes to infinity. In this case, the [[id:d1230eba-905b-4765-84ef-daa0b21fd3d9][logistic sigmoid function]] becomes infinitely steep in feature space, corresponding to a step function, so that every training point from each class \(k\) is assigned a posterior probability \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)=1\). The problem will arise even if the number of data points is large compared with the number of parameters in the model, so long as the training data set is linearly separable. The singularity can be avoided by inclusion of a prior and finding a MAP solution for \(\mathbf{w}\), or equivalently by adding a [[id:621dd698-e6fb-485b-95b7-2116cdc47223][regularization]] term to the cross entropy error function.