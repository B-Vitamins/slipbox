:PROPERTIES:
:ID:       fbf51bbb-48c4-4b60-9714-841bb3c046c4
:END:
#+TITLE: Examples of D-separation
#+FILETAGS: :literature:prml:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

Here we collect some examples of the concept of [[id:971e6cb8-1177-4d9a-90d7-43d22c22fb61][D-separation]]:

* Example 1
#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-8-22a.png]]

The path from \(a\) to \(b\) is not blocked by node \(f\) because it is a tail-to-tail node for this path and is not observed, nor is it blocked by node \(e\) because, although the latter is a head-to-head node, it has a descendant \(c\) because is in the conditioning set. Thus the [[id:c6de868e-1284-4ce9-a2e7-31f71c25a270][conditional independence]] statement \(a \perp\!\!\!\perp b \mid c\) does not follow from this graph. 

* Example 2
#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-8-22b.png]]

The path from \(a\) to \(b\) is blocked by node \(f\) because this is a tail-to-tail node that is observed, and so the conditional independence property \(a \perp\!\!\!\perp b \mid f\) will be satisfied by any distribution that factorizes according to this graph. Note that this path is also blocked by node \(e\) because \(e\) is a head-to-head node and neither it nor its descendant are in the conditioning set.

* Example 3
Consider the problem of finding the posterior distribution for the mean of a uni-variate Gaussian distribution. This can be represented by the directed graph shown below (note the use of /plate/ notation, see [[id:6905c200-b885-4c57-aaae-1fc86ca1c025][Polynomial regression (as a DAG)]])

#+ATTR_HTML: :width 200px
[[file:~/.local/images/prml-8-23b.png]]

The [[id:8c833811-3657-4af4-8be5-191d8788b779][joint probability distribution]] is defined by a prior \(p(\mu)\) together with a set of [[id:391465bc-1399-40f0-b049-738c1a64d6fb][conditional probability distribution]] \(p(x_{n} \mid \mu)\) for \(n=1, \ldots, N\). In practice, we observe \(\mathcal{D}=\left\{x_{1}, \ldots, x_{N}\right\}\) and our goal is to infer \(\mu\). Suppose, for a moment, that we condition on \(\mu\) and consider the joint distribution of the observations. Using d-separation, we note that there is a unique path from any \(x_{i}\) to any other \(x_{j \neq i}\) and that this path is tail-to-tail with respect to the observed node \(\mu\). Every such path is blocked and so the observations \(D=\left\{x_{1}, \ldots, x_{N}\right\}\) are independent given \(\mu\), so that

\[
p(\mathcal{D} \mid \mu)=\prod_{n=1}^{N} p\left(x_{n} \mid \mu\right) .
\]

However, if we integrate over \(\mu\), the observations are in general no longer [[id:2e4b8dd4-f9ed-402f-bdf6-9b7a5079411a][independent]]

\[
p(\mathcal{D})=\int_{0}^{\infty} p(\mathcal{D} \mid \mu) p(\mu) \mathrm{d} \mu \neq \prod_{n=1}^{N} p\left(x_{n}\right) .
\]

Here \(\mu\) is a /latent variable/, because its value is not observed.
* Example 4
A model representing [[id:463c36d5-7beb-429d-bf27-98e290353583][independent and identically distributed random variables]] is the graph below

#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-8-7.png]]

which corresponds to [[id:352a4bd7-442d-4e94-bb47-c36c12dd99f0][Bayesian curve fitting]] (see also [[id:6905c200-b885-4c57-aaae-1fc86ca1c025][Polynomial regression (as a DAG)]]). Here the stochastic nodes correspond to \(\left\{t_{n}\right\}\), \( \mathbf{w} \) and \(\widehat{t}\). We see that the node for \(\mathbf{w}\) is tail-to-tail with respect to the path from \(\widehat{t}\) to any one of the nodes \(t_{n}\) and so we have the following conditional independence property

\[
\hat{t} \perp\!\!\!\perp t_{n} \mid \mathbf{w} .
\]

Thus, conditioned on the polynomial coefficients \(\mathbf{w}\), the predictive distribution for \(\hat{t}\) is independent of the training data \(\left\{t_{1}, \ldots, t_{N}\right\}\). We can therefore first use the training data to determine the [[id:e166b400-40c8-410b-bb5b-0e0decca5f4c][posterior distribution]] over the coefficients \(\mathbf{w}\) and then we can discard the training data and use the posterior distribution for \(\mathbf{w}\) to make predictions of \(\hat{t}\) for new input observations \(\hat{x}\).