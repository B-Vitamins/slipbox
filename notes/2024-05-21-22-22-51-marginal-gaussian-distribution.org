:PROPERTIES:
:ID:       19cbd318-ee86-4c5a-9d45-530e8c4bb421
:END:
#+TITLE: Marginal Gaussian distribution
#+FILETAGS: :literature:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org
We have seen that if a joint distribution \(p\left(\mathbf{x}_{a}, \mathbf{x}_{b}\right)\) is Gaussian, then the conditional distribution \(p\left(\mathbf{x}_{a} \mid \mathbf{x}_{b}\right)\) will again be Gaussian. Now we turn to a discussion of the marginal distribution given by

\[p\left(\mathbf{x}_{a}\right)=\int p\left(\mathbf{x}_{a}, \mathbf{x}_{b}\right) \mathrm{d} \mathbf{x}_{b}\]

which, as we shall see, is also Gaussian. Once again, our strategy for evaluating this distribution efficiently will be to focus on the quadratic form in the exponent of the joint distribution and thereby to identify the mean and covariance of the marginal distribution \(p\left(\mathbf{x}_{a}\right)\).

The quadratic form for the joint distribution can be expressed, using the partitioned precision matrix, in the form (2.70). Because our goal is to integrate out \(\mathbf{x}_{b}\), this is most easily achieved by first considering the terms involving \(\mathbf{x}_{b}\) and then completing the square in order to facilitate integration. Picking out just those terms that involve \(\mathbf{x}_{b}\), we have

\[-\frac{1}{2} \mathbf{x}_{b}^{\mathrm{T}} \boldsymbol{\Lambda}_{b b} \mathbf{x}_{b}+\mathbf{x}_{b}^{T} \mathbf{m}=-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\right)+\frac{1}{2} \mathbf{m}^{\mathrm{T}} \boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\]

where we have defined

\[\mathbf{m}=\boldsymbol{\Lambda}_{b b} \boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right) .\]

We see that the dependence on \(\mathbf{x}_{b}\) has been cast into the standard quadratic form of a Gaussian distribution corresponding to the first term on the right-hand side of (2.84), plus a term that does not depend on \(\mathbf{x}_{b}\) (but that does depend on \(\mathbf{x}_{a}\) ). Thus, when we take the exponential of this quadratic form, we see that the integration over \(\mathbf{x}_{b}\) required by (2.83) will take the form

\[\int \exp \left\{-\frac{1}{2}\left(\mathbf{x}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_{b}-\boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\right)\right\} \mathrm{d} \mathbf{x}_{b} .\]

This integration is easily performed by noting that it is the integral over an unnormalized Gaussian, and so the result will be the reciprocal of the normalization coefficient. We know from the form of the normalized Gaussian given by (2.43), that this coefficient is independent of the mean and depends only on the determinant of the covariance matrix. Thus, by completing the square with respect to \(\mathbf{x}_{b}\), we can integrate out \(\mathrm{x}_{b}\) and the only term remaining from the contributions on the left-hand side of (2.84) that depends on \(\mathbf{x}_{a}\) is the last term on the right-hand side of (2.84) in which \(\mathbf{m}\) is given by (2.85). Combining this term with the remaining terms from (2.70) that depend on \(\mathbf{x}_{a}\), we obtain

\[\begin{gathered}
\frac{1}{2}\left[\boldsymbol{\Lambda}_{b b} \boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)\right]^{\mathrm{T}} \boldsymbol{\Lambda}_{b b}^{-1}\left[\boldsymbol{\Lambda}_{b b} \boldsymbol{\mu}_{b}-\boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_{a}-\boldsymbol{\mu}_{a}\right)\right] \\
\quad-\frac{1}{2} \mathbf{x}_{a}^{\mathrm{T}} \boldsymbol{\Lambda}_{a a} \mathbf{x}_{a}+\mathbf{x}_{a}^{\mathrm{T}}\left(\boldsymbol{\Lambda}_{a a} \boldsymbol{\mu}_{a}+\boldsymbol{\Lambda}_{a b} \boldsymbol{\mu}_{b}\right)+\mathrm{const} \\
=\quad-\frac{1}{2} \mathbf{x}_{a}^{\mathrm{T}}\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right) \mathbf{x}_{a} \\
\quad+\mathbf{x}_{a}^{\mathrm{T}}\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right)^{-1} \boldsymbol{\mu}_{a}+\text { const }
\end{gathered}\]

where 'const' denotes quantities independent of \(\mathbf{x}_{a}\). Again, by comparison with (2.71), we see that the covariance of the marginal distribution of \(p\left(\mathbf{x}_{a}\right)\) is given by

\[\boldsymbol{\Sigma}_{a}=\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right)^{-1} .\]

Similarly, the mean is given by

\[\boldsymbol{\Sigma}_{a}\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right) \boldsymbol{\mu}_{a}=\boldsymbol{\mu}_{a}\]

where we have used (2.88). The covariance in (2.88) is expressed in terms of the partitioned precision matrix given by (2.69). We can rewrite this in terms of the corresponding partitioning of the covariance matrix given by (2.67), as we did for the conditional distribution. These partitioned matrices are related by

\[\left(\begin{array}{cc}
\boldsymbol{\Lambda}_{a a} & \boldsymbol{\Lambda}_{a b} \\
\boldsymbol{\Lambda}_{b a} & \boldsymbol{\Lambda}_{b b}
\end{array}\right)^{-1}=\left(\begin{array}{cc}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right)\]

Making use of (2.76), we then have

\[\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right)^{-1}=\boldsymbol{\Sigma}_{a a} .\]

Thus we obtain the intuitively satisfying result that the marginal distribution \(p\left(\mathbf{x}_{a}\right)\) has mean and covariance given by

\[\begin{align*}
\mathbb{E}\left[\mathbf{x}_{a}\right] & =\boldsymbol{\mu}_{a} \\
\operatorname{cov}\left[\mathbf{x}_{a}\right] & =\boldsymbol{\Sigma}_{a a} .
\end{align*}\]

We see that for a marginal distribution, the mean and covariance are most simply expressed in terms of the partitioned covariance matrix, in contrast to the conditional distribution for which the partitioned precision matrix gives rise to simpler expressions.

Our results for the marginal and conditional distributions of a partitioned Gaussian are summarized below.

Partitioned Gaussians

Given a joint Gaussian distribution \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\) with \(\Lambda \equiv \boldsymbol{\Sigma}^{-1}\) and

\[\mathbf{x}=\left(\begin{array}{c}
\mathbf{x}_{a} \\
\mathbf{x}_{b}
\end{array}\right), \quad \boldsymbol{\mu}=\left(\begin{array}{c}
\boldsymbol{\mu}_{a} \\
\boldsymbol{\mu}_{b}
\end{array}\right)\]

Figure 2.9 The plot on the left shows the contours of a Gaussian distribution \(p\left(x_{a}, x_{b}\right)\) over two variables, and the plot on the right shows the marginal distribution \(p\left(x_{a}\right)\) (blue curve) and the conditional distribution \(p\left(x_{a} \mid x_{b}\right)\) for \(x_{b}=0.7\) (red curve).

\[\boldsymbol{\Sigma}=\left(\begin{array}{cc}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right), \quad \boldsymbol{\Lambda}=\left(\begin{array}{cc}
\boldsymbol{\Lambda}_{a a} & \boldsymbol{\Lambda}_{a b} \\
\boldsymbol{\Lambda}_{b a} & \boldsymbol{\Lambda}_{b b}
\end{array}\right) .\]

Conditional distribution:

\[\begin{align*}
p\left(\mathbf{x}_{a} \mid \mathbf{x}_{b}\right) & =\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{a \mid b}, \boldsymbol{\Lambda}_{a a}^{-1}\right) \\
\boldsymbol{\mu}_{a \mid b} & =\boldsymbol{\mu}_{a}-\boldsymbol{\Lambda}_{a a}^{-1} \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) .
\end{align*}\]

Marginal distribution:

\[p\left(\mathbf{x}_{a}\right)=\mathcal{N}\left(\mathbf{x}_{a} \mid \boldsymbol{\mu}_{a}, \boldsymbol{\Sigma}_{a a}\right)\]

We illustrate the idea of conditional and marginal distributions associated with a multivariate Gaussian using an example involving two variables in Figure 2.9.

