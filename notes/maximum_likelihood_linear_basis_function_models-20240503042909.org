:PROPERTIES:
:ID:       2967ed4f-6501-4d36-8446-50e536e66a2c
:END:
#+TITLE: Maximum likelihood (linear regression)
#+FILETAGS: :literature:prml:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

The aim of *regression* is to predict the value of continuous target variables \( t \) given a \( D \)-dimensional input vector \( \mathbf{x} \) (see [[id:a3b84f9d-c03a-4579-9c82-47bba805c09b][Linear basis function models]] and [[id:35e57234-01f9-418c-a9fa-f94040015281][Linear models for regression]]). Assume the target variable \( t \) is given by

\[
t = y(\mathbf{x}, \mathbf{w}) + \epsilon \tag{1}
\]

where \(\epsilon\) is zero mean Gaussian noise with precision \(\beta\). See [[id:c1f28016-d867-4b60-a6c0-f3a7907d13b6][Regularization (linear basis function models)]] for a maximum likelihood solution with *regularization* for linear basis function models. 

* Single input-target pair

The [[id:82831310-4909-4f8b-8db0-ff0d8bef8b0b][Likelihood function]] is

\[
p(t \mid \mathbf{x}, \mathbf{w}, \beta) = \mathcal{N}(t \mid y(\mathbf{x}, \mathbf{w}), \beta^{-1}) \tag{2}
\]

where

\[
\mathcal{N}(x \mid \mu, \sigma^2)=\frac{1}{(2 \pi \sigma^2)^{1 / 2}} \exp \bigg(-\frac{(x-\mu)^2}{2 \sigma^2}\bigg).
\]

For squared loss, the optimal prediction is the conditional mean (see [[id:c0201acc-994e-4ba6-88f9-b91a6d041692][loss functions for regression]]):

\[
\mathbb{E}[t \mid \mathbf{x}] = \int t p(t \mid \mathbf{x}) \mathrm{d} t = y(\mathbf{x}, \mathbf{w}) \tag{3}
\]

#+BEGIN_COMMENT
Note that the Gaussian noise assumption implies that the conditional distribution of \(t\) given \(\mathbf{x}\) is unimodal, which may be inappropriate for some applications.
#+END_COMMENT

* \(N\) input-target pair
** Likelihood function
Given \( N \) input-target pairs \((\mathbf{x}_n, t_n)\), the [[id:82831310-4909-4f8b-8db0-ff0d8bef8b0b][Likelihood function]] is:

\[
p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta) = \prod_{n=1}^{N} \mathcal{N}(t_n \mid \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n), \beta^{-1}) \tag{4}
\]

The log-likelihood function is

\[
\ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = \frac{N}{2} \ln \beta - \frac{N}{2} \ln (2 \pi) - \frac{\beta}{2} \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\}^2. \tag{5}
\]
** Error function (sum of squares) 
We identify an error function called the *sum-of-squared-error*

\[
E_D(\mathbf{w}) = - \frac{1}{2} \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\}^2. \tag{6}
\]
** Closed form solution
Demanding vanishing gradient of (5) with respect to \( \mathbf{w} \)

\[
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\} \boldsymbol{\phi}(\mathbf{x}_n)^\mathrm{T} = 0 \tag{7}
\]

yields

\[
\sum_{n=1}^{N} t_n \boldsymbol{\phi}(\mathbf{x}_n)^\mathrm{T} = \mathbf{w}^\mathrm{T} \left( \sum_{n=1}^{N} \boldsymbol{\phi}(\mathbf{x}_n) \boldsymbol{\phi}(\mathbf{x}_n)^\mathrm{T} \right). \tag{8}
\]

Solving for \(\mathbf{w}\)

\begin{align*}
\boxed{
\mathbf{w}_{\mathrm{ML}} = \boldsymbol{\Phi}^\mathrm{T} \mathbf{t} \, (\boldsymbol{\Phi}^\mathrm{T} \boldsymbol{\Phi})^{-1}  \tag{9}
}
\end{align*}

where \(\boldsymbol{\Phi}\) is a \( N \times M \) matrix called the *design matrix*

\[
\boldsymbol{\Phi} = \begin{pmatrix}
\phi_0(\mathbf{x}_1) & \phi_1(\mathbf{x}_1) & \cdots & \phi_{M-1}(\mathbf{x}_1) \\
\phi_0(\mathbf{x}_2) & \phi_1(\mathbf{x}_2) & \cdots & \phi_{M-1}(\mathbf{x}_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(\mathbf{x}_N) & \phi_1(\mathbf{x}_N) & \cdots & \phi_{M-1}(\mathbf{x}_N)
\end{pmatrix} \tag{10}
\]

whose *Moore-Penrose pseudo-inverse* is:

\[
\boldsymbol{\Phi}^\dagger = (\boldsymbol{\Phi}^\mathrm{T} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^\mathrm{T} \tag{11}
\]

Maximizing (5) with respect to \(\beta\):

\begin{align*}
\boxed{
\beta_{\mathrm{ML}}^{-1} = N^{-1} \sum_{n=1} \{t_n - \mathbf{w}_{\mathrm{ML}}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\}^2 \tag{15}
}
\end{align*}

yielding the *inverse noise precision as the residual variance of the target values around the regression function*.
