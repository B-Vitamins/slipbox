:PROPERTIES:
:ID:       109b816b-41cf-4a08-bf11-d3ea17f13478
:END:
#+TITLE: Solutions to Basic Training in Mathematics by Ramamurthy Shankar
#+FILETAGS: :problem:btm:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org
#+BEGIN: clocktable :maxlevel 1 :scope nil :emphasize nil
#+CAPTION: Clock summary at [2024-11-07 Thu 12:16]
| Headline                        |     Time |
|---------------------------------+----------|
| *Total time*                    | *103:15* |
|---------------------------------+----------|
| Infinite Series                 |     2:15 |
| Functions of a Complex Variable |    44:30 |
| Differential equations          |    56:30 |
#+END:
* Calculus of Single Variable
CLOSED: [2022-11-01 Tue 14:40]
** SOLVED Problem 1.2.1
CLOSED: [2022-11-01 Tue 14:22]
$\textbf{I}$

\begin{equation*}
    D(fg) = \lim_{\Delta x \to 0}\frac{f(x+\Delta x)g(x+\Delta x) - f(x)g(x)}{\Delta x}
\end{equation*}

\begin{equation*}
    = \lim_{\Delta x \to 0}\frac{\{f(x)+[f(x+\Delta x)-f(x)]\}\{g(x)+[g(x+\Delta x)-g(x)]\}-f(x)g(x)}{\Delta x}
\end{equation*}

\begin{equation*}
    = \lim_{\Delta x \to 0}\frac{f(x)[g(x+\Delta x)-g(x)] + g(x)[f(x+\Delta x)-f(x)] +[f(x+\Delta x) -f(x)][g(x+\Delta x)-g(x)]}{\Delta x}
\end{equation*}

\begin{equation*}
    = f(x)\lim_{\Delta x \to 0}\frac{g(x+\Delta x)-g(x)}{\Delta x} + g(x)\lim_{\Delta x \to 0}\frac{f(x+\Delta x)-f(x)}{\Delta x} + \lim_{\Delta x \to 0}\frac{[f(x+\Delta x) -f(x)][g(x+\Delta x)-g(x)]}{\Delta x} 
\end{equation*}

\begin{equation*}
    D(fg) = fDg + gDf
\end{equation*}

$\textbf{II}$

\begin{equation*}
    D[f(u(x))] = \lim_{\Delta x \to 0}\frac{f(u(x+\Delta x))-f(u(x))}{\Delta x}
\end{equation*}

\begin{equation*}
    D[f(u(x))] = \lim_{\Delta x \to 0}\frac{f(u(x)+[u(x+\Delta x) - u(x)])-f(u(x))}{\Delta x}
\end{equation*}

\begin{equation*}
    = \lim_{\Delta x \to 0}\frac{f(u + \Delta u) - f(u)}{\Delta x} = \lim_{\Delta x \to 0}\frac{f(u + \Delta u) - f(u)}{\Delta u}\frac{\Delta u}{\Delta x}
\end{equation*}

\begin{equation*}
    = \lim_{\Delta u \to 0}\frac{f(u + \Delta u) - f(u)}{\Delta u}\lim_{\Delta x \to 0}\frac{\Delta u}{\Delta x}
\end{equation*}

\begin{equation*}
  D[f(u(x))] = \frac{df}{du}\frac{du}{dx}
\end{equation*}

** SOLVED Problem 1.2.2
CLOSED: [2022-11-01 Tue 14:22]

\begin{equation*}
    D(\frac{1}{x}) = \lim_{\Delta x \to 0}\frac{1}{\Delta x}\left(\frac{1}{x + \Delta x} - \frac{1}{x}\right) = \lim_{\Delta x \to 0}-\frac{1}{x^{2} + x\Delta x} = -\frac{1}{x^{2}}
\end{equation*}

\question*{Problem 1.2.3}

\begin{equation*}
    D(\frac{f}{g}) = fD(1/g) + (1/g)D(f) = -\frac{fDg}{g^{2}} + \frac{Df}{g} 
\end{equation*}

\begin{equation*}
    D(\frac{f}{g}) = \frac{gDf-fDg}{g^{2}}
\end{equation*}

** SOLVED Problem 1.3.1
CLOSED: [2022-11-01 Tue 14:22]
$\textbf{I}$

\begin{equation*}
D[\sinh x] = D\left[\frac{e^{x}-e^{-x}}{2}\right] = \frac{D(e^{x})-D(e^{-x})}{2} = \frac{e^{x}+e^{-x}}{2} = \cosh x
\end{equation*}

\begin{equation*}
D[\cosh x] = D\left[\frac{e^{x}+e^{-x}}{2}\right] = \frac{D(e^{x})+D(e^{-x})}{2} = \frac{e^{x}-e^{-x}}{2} = \sinh x
\end{equation*}

$\textbf{II}$

\begin{equation*}
    \cosh^{2} x - \sinh^{2} x = \frac{e^{2x}+e^{-2x}+2e^{x}e^{-x}}{4} - \frac{e^{2x}+e^{-2x}-2e^{x}e^{-x}}{4} = 1
\end{equation*}

\textbf{III}

\begin{align*}
    \sinh (x+y) &= \frac{e^{x+y}-e^{-x-y}}{2} = \frac{e^{x}e^{y}-e^{-x}e^{-y}}{2} = \frac{2e^{x}e^{y}-2e^{-x}e^{-y}}{4} \\
    &= \frac{2e^{x}e^{y} + e^{x}e^{-y} -e^{x}e^{-y} + e^{-x}e^{y} -e^{-x}e^{y} -2e^{-x}e^{-y}}{4} \\
    &= \frac{(e^{x}e^{y} + e^{x}e^{-y} -e^{-x}e^{y} - e^{-x}e^{-y}) + (e^{x}e^{y} -e^{x}e^{-y} + e^{-x}e^{y} - e^{-x}e^{-y})}{4} \\
    &= \left(\frac{e^{x}-e^{-x}}{2}\right) \left(\frac{e^{y} + e^{-y}}{2}\right) + \left(\frac{e^{x}+e^{-x}}{2}\right)\left(\frac{e^{y}-e^{-y}}{2}\right) = \sinh{x}\cosh{y} + \cosh{x}\sinh{y}
\end{align*}

** SOLVED Problem 1.3.2
CLOSED: [2022-11-01 Tue 14:22]

A Taylor series for $f(y) = y^{p}$ about the point $y = 1$ is

\begin{equation}
     y^{p} = f_{\infty}(y) = f(0) + f^{(1)}(y-1) + f^{(2)}\frac{(y-1)^{2}}{2} + f^{(3)}\frac{(y-1)^{3}}{3!} + ...
\end{equation}

where

\begin{equation*}
    f(0) = 1,  f(1) = p, f(2) = p(p-1), f(3) = p(p-1)(p-2)
\end{equation*}

Making the substitution $x = y-1$ in Eq. (1), the series becomes

\begin{equation*}
    (1+x)^{p} = 1 + px + \frac{p(p-1)}{2}x^{2} + \frac{p(p-1)(p-2)}{3!}x^{3} + ...
\end{equation*}
** SOLVED Problem 1.3.3
CLOSED: [2022-11-01 Tue 14:22]
$\textbf{I}$

\begin{equation*}
    \sinh x = \frac{e^{x}-e^{-x}}{2}
\end{equation*}

Substituting the Taylor series for $e^{x}$ and $e^{-x}$ about $x=0$

\begin{equation*}
    \sinh(x) = \frac{1}{2}\left( \sum_{n=0}^{\infty}\frac{x^{n}}{n!} - \sum_{n=0}^{\infty}\frac{(-1)^{n}x^{n}}{n!} \right)
\end{equation*}

\begin{equation*}
    \sinh(x) = \frac{1}{2}\left( \sum_{n=0}^{\infty}\frac{x^{n}}{n!} - \left[\sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!} - \sum_{n=0}^{\infty}\frac{x^{2n+1}}{(2n+1)!} \right) \right] = \frac{1}{2} \left( 2 \sum_{n=0}^{\infty}\frac{x^{2n+1}}{(2n+1)!} \right)
\end{equation*}

\begin{equation*}
    \sinh(x) = \sum_{n=0}^{\infty}\frac{x^{2n+1}}{(2n+1)!} = x + \frac{x^{3}}{3!} + \frac{x^{5}}{5!} + ...
\end{equation*}

$\textbf{II}$

\begin{equation*}
    \cosh x = \frac{e^{x}+e^{-x}}{2}
\end{equation*}

Substituting the Taylor series for $e^{x}$ and $e^{-x}$ about $x=0$

\begin{equation*}
    \cosh = \frac{1}{2}\left( \sum_{n=0}^{\infty}\frac{x^{n}}{n!} + \sum_{n=0}^{\infty}\frac{(-1)^{n}x^{n}}{n!} \right)
\end{equation*}

\begin{equation*}
    \cosh(x) = \frac{1}{2}\left( \sum_{n=0}^{\infty}\frac{x^{n}}{n!} + \left[\sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!} - \sum_{n=0}^{\infty}\frac{x^{2n+1}}{(2n+1)!} \right) \right] = \frac{1}{2} \left( 2 \sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!} \right)
\end{equation*}

\begin{equation*}
    \sinh(x) = \sum_{n=0}^{\infty}\frac{x^{2n}}{(2n)!} = 1 + \frac{x^{2}}{2!} + \frac{x^{4}}{4!} + ...
\end{equation*}

** SOLVED Problem 1.4.1
CLOSED: [2022-11-01 Tue 14:22]
$\textbf{I}$

Given that $D^{(2n+1)} \sin(x) = (-1)^{n}\cos(x)$, $D^{(2n)} \sin(x) = (-1)^{n}\sin(x)$, $\sin(0) = 0$, $\cos(0) = 1$, a Taylor series for $f(x) = \sin(x)$ about the origin is

\begin{equation*}
    \sin(x) = \sin(0) + D^{(1)} \sin(0) x + D^{(2)} \sin(0) \frac{x^{2}}{2!} + D^{(3)} \sin(0) + ... \frac{x^{3}}{3!}
\end{equation*}

\begin{equation*}
    \sin(x) = \sin(0) + D^{(1)} \sin(0) x + D^{(2)} \sin(0) \frac{x^{2}}{2!} + D^{(3)} \sin(0) + ... \frac{x^{3}}{3!}
\end{equation*}

\begin{equation*}
    \sin(x) = \sum_{n=0}^{\infty} D^{(2n+1)} \sin(0) \frac{x^{(2n+1)}}{(2n+1)!} + \sum_{n=0}^{\infty} D^{(2n)} \sin(0) \frac{x^{2n}}{(2n)!}
\end{equation*}

\begin{equation*}
    \sin(x) = \sum_{n=0}^{\infty} (-1)^{n}\frac{x^{(2n+1)}}{(2n+1)!} = x - \frac{x^{3}}{3!} + \frac{x^{5}}{5!} + ...
\end{equation*}

$\textbf{II}$

Given that $D^{(2n+1)} \cos(x) = (-1)^{n+1}\sin(x)$, $D^{(2n)} \cos(x) = (-1)^{n}\cos(x)$, $\sin(0) = 0$, $\cos(0) = 1$, a Taylor series for $f(x) = \sin(x)$ about the origin is

\begin{equation*}
    \cos(x) = \cos(0) + D^{(1)} \cos(0) x + D^{(2)} \cos(0) \frac{x^{2}}{2!} + D^{(3)} \cos(0) + ... \frac{x^{3}}{3!}
\end{equation*}

\begin{equation*}
    \cos(x) = \cos(0) + D^{(1)} \cos(0) x + D^{(2)} \cos(0) \frac{x^{2}}{2!} + D^{(3)} \cos(0) + ... \frac{x^{3}}{3!}
\end{equation*}

\begin{equation*}
    \cos(x) = \sum_{n=0}^{\infty} D^{(2n+1)} \cos(0) \frac{x^{(2n+1)}}{(2n+1)!} + \sum_{n=0}^{\infty} D^{2n} \cos(0) \frac{x^{2n}}{(2n)!}
\end{equation*}

\begin{equation*}
    \cos(x) = \sum_{n=0}^{\infty} (-1)^{n}\frac{x^{2n}}{(2n)!} = 1 - \frac{x^{2}}{2!} + \frac{x^{4}}{4!} + ...
\end{equation*}
** SOLVED Problem 1.5.1
CLOSED: [2022-11-01 Tue 14:22]

By appeal to L'H\^{o}spital's rule, if the ratio of the $k^{th}$ derivative of $x^{n}$ and $e^{x}$ vanishes as $x \to \infty$ for some $k$, then $x^{n}e^{-x} \to 0$ as $x \to \infty$. Consider the ratio $\frac{D^{(k)} x^{n}}{D^{(k)} e^{x}} = \frac{n(n-1)(n-2)...(n-k)x^{(n-k)}}{e^{x}}$. Choose a $k$ such that $k>n$. Then $x^{(n-k)} \to 0$ and $e^{-x} \to 0$ as $x \to \infty$. Therefore $\frac{D^{(k)} x^{n}}{D^{(k)} e^{x}} \to 0$ as $x \to \infty$. Thus the falling exponential can subdue any power.

The ratio $\frac{D ln(x)}{D x^{p}} = \frac{1}{px^{p}} \to 0$ as $x \to \infty$ for $p>0$. Therefore, the growth of $ln(x)$ is weaker than any positive power $x^{p}.$

\question*{Problem 1.5.2}

Consider the function 

\begin{equation*}
    S(x) = \frac{x^{2}+x-6}{4 + \cosh(x)}
\end{equation*}

The ratio of the second derivatives of the numerator and the denominator vanishes as $x \to \pm \infty$. Therefore, $S(x) \to 0$ as $x \to \pm \infty.$ Factorizing the numerator, we get

\begin{equation*}
    S(x) = \frac{(x+3)(x-2)}{4 + \cosh(x)}
\end{equation*}

The denominator $4 + \cosh(x) > 0$ for all $x$. The zeros of $S(x)$ are at $x = -3$ and $x = 2$. To the left of $x = -3$, $(x+3)$ and $(x-2)$ are both negative. Therefore $S(x) > 0$ when $x < -3$. There must be a local maxima somewhere between $x = -\infty$ and $x = -3$. Similarly, to the right of $x = 2$, $S(x) > 0$. There must be a local maxima somewhere between $x = 2$ and $x = \infty$. Between $x = -3$ and $x = 2$, $(x+3)(x-2) < 0$. Therefore, $S(x) < 0$ when $-3 < x < 2$. There must be a local minima somewhere between $x = -3$ and $x = 2$.

** SOLVED Problem 1.6.1
CLOSED: [2022-11-01 Tue 14:22]

A Taylor series for the function $f(x) = \frac{\sin(x)}{\cosh(x)+2}$ about $x = 0$ can be written as

\begin{equation}
    f(x) = f(0) + D f(0) x + D^{(2)} f(0) \frac{x^{2}}{2!} + D^{(3)} f(0) \frac{x^{3}}{3!} + ...
\end{equation}

\begin{equation*}
    f(0) = 0
\end{equation*}

\begin{equation*}
    Df(x) = \frac{\cos(x)(\cosh(x)+2)-\sin(x)\sinh(x)}{(\cosh(x)+2)^{2}}
\end{equation*}

\begin{equation*}
\begin{split}
    D^{(2)}f(x) &= \frac{1}{(\cosh(x)+2)^{2}} D \left[\cos(x)(\cosh(x)+2)-\sin(x)\sinh(x) \right] \\
    &+ (\cos(x)(\cosh(x)+2)-\sin(x)\sinh(x)) D \left[ \frac{1}{(\cosh(x)+2)^{2}} \right] \\
    &= \frac{1}{(\cosh(x)+2)^{2}} \left[\cos(x)\sinh(x) - \sin(x)(\cosh(x)+2) - \sin(x)\cosh(x) - \cos(x)\sinh(x)\right] \\
    &- 2(\cos(x)(\cosh(x)+2)-\sin(x)\sinh(x)) \frac{\sinh(x)}{(\cosh(x)+2)^{3}} \\
    & = -2 \left[ \frac{\sin(x)(1+\cosh(x))}{(\cosh(x)+2)^{2}} + \frac{\sinh(x)[\cos(x)(\cosh(x)+2) - \sin(x)\sinh(x)]}{(\cosh(x)+2)^{3}} \right] \\
\end{split}
\end{equation*}

\begin{equation*}
\begin{split}
    D^{(3)} f(x) &= -2 \Bigg(\frac{1}{(\cosh(x)+2)^{2}} D \left[\sin(x)(1+\cosh(x))\right]\\
    &+ [\sin(x)(1+\cosh(x))] D \left[ \frac{1}{(\cosh(x)+2)^{2}}\right] \\
    &+ \frac{1}{(\cosh(x)+2)^{3}} D \left[\sinh(x)(\cos(x)(\cosh(x)+2) - \sin(x)\sinh(x)) \right] \\ 
    &+ [\sinh(x)[\cos(x)(\cosh(x)+2) - \sin(x)\sinh(x)]] D \left[ \frac{1}{(\cosh(x)+2)^{-3}} \right]\Bigg) \\
    & = -2 \Bigg(\frac{1}{(\cosh(x)+2)^{2}} [\cos(x) + \cos(x)\cosh(x) + \sin(x)\sinh(x)]\\
    &+ [\sin(x)(1+\cosh(x))] \left[\frac{-2\sinh(x)}{(\cosh(x)+2)^{3}}\right] \\
    &+ \frac{1}{(\cosh(x)+2)^{3}} [\cos(x)\cosh^{2}(x) + 2\cos(x)\cosh(x) - 2\sin(x)\sinh(x) - 3\sin(x)\sinh(x)\cosh(x)] \\ 
    &+ [\sinh(x)[\cos(x)(\cosh(x)+2) - \sin(x)\sinh(x)]] \left[\frac{-3\sinh(x)}{(\cosh(x)+2)^{4}} \right]\Bigg) \\
    & = \frac{-2}{(\cosh(x)+2)^{2}} [\cos(x)(1+\cosh(x)) + \sin(x)\sinh(x)]\\
    &- \frac{2}{(\cosh(x)+2)^{3}}[\cos(x)\cosh^{2}(x) + 2\cos(x)\cosh(x) - 4\sin(x)\sinh(x) - 5\sin(x)\sinh(x)\cosh(x)] \\
    &- \frac{2}{(\cosh(x)+2)^{4}} [-3\sinh^{2}(x)\{\cos(x)(\cosh(x)+2) + \sin(x)\sinh(x)\}] \\
\end{split}
\end{equation*}

Substituting $x=0$ in the expressions for $Df(x), D^{2}f(x),$ and $D^{3}f(x)$, we get $D f(0) = \frac{1}{3} , D^{2}f(0) = 0,$ and $D^{3} f(0) = -\frac{2}{3}$. Substituting these in Eq. $(2)$, the Taylor series for $f(x) = \frac{sin(x)}{cosh(x)+2}$ about the point $x=0$ becomes

\begin{equation*}
    f(x) = \frac{1}{3} x - \frac{1}{9} x^{3} + ...
\end{equation*}

Using the series above upto $x^{3}$, $f(0.1) \approx \frac{1}{3} (0.1) - \frac{1}{9} (0.1)^{3} = 0.03322222222$. The exact value is $f(0.1) = 0.03322238874$. The series up to third order has a $0.0005\%$ error for $x = 0.1$.

** SOLVED Problem 1.6.2
CLOSED: [2022-11-01 Tue 14:22]

\textbf{I}
\begin{equation*}
   D [\sin(x^{3}+2)] = 3x^{2}\cos(2+x^{3}) 
\end{equation*}

\textbf{II}

\begin{equation*}
   D [\sin(\cos(2x))] = -2\sin(2x)\cos[\cos(2x)] 
\end{equation*}

\textbf{III}

\begin{equation*}
    D [\tan^{3}(x)] = 3\sec^{2}(x)\tan^{2}(x)
\end{equation*}

\textbf{IV}

\begin{equation*}
    D [ln(\cosh(x))] = \tanh(x)
\end{equation*}

\textbf{V}

\begin{equation*}
    D [\tan^{-1}(x)] = \frac{1}{1+x^{2}}
\end{equation*}

\textbf{VI}

\begin{equation*}
    D [\tanh^{-1}(x)] = \frac{1}{1-x^{2}}
\end{equation*}

\textbf{VII}

\begin{equation*}
    D [\cosh^{2}(x) - \sinh^{2}(x)] = 0
\end{equation*}

\textbf{VIII}

\begin{equation*}
    D [\sin(x)/(1+\cos(x))] = \frac{1}{(1+\cos(x))}
\end{equation*}
** SOLVED Problem 1.6.3
CLOSED: [2022-11-01 Tue 14:22]

At the end of 2 years, a hundred dollars would be worth $e^{2\times 0.06} \times 100 \ \$ = e^{0.12} \times 100 \ \$$. Approximating $e^{x}$ as $1+x+\frac{x^{2}}{2}$, a hundred dollars would be worth $(1+0.12+\frac{(0.12)^{2}}{2})\times100 = 112.72 \ \$$. 
** SOLVED Problem 1.6.4
CLOSED: [2022-11-01 Tue 14:23]
\question*{Problem 1.6.4}

\begin{equation*}
    s'^{2} = t'^{2} - x'^{2} = \Bigg(\frac{t-xv}{\sqrt{1-v^{2}}}\Bigg)^{2} - \Bigg(\frac{x - tv}{\sqrt{1-v^{2}}}\Bigg)^{2} = \frac{t^{2}+x^{2}v^{2}-x^{2}-t^{2}v^{2}}{1-v^{2}} = \frac{(t^{2}-x^{2})(1-v^{2})}{1-v^{2}} = t^{2}-x^{2} = s^{2}
\end{equation*}

\begin{align*}
    s'^{2} = t'^{2}-x'^{2} &= [t\cosh(\theta)-x\sinh(\theta)]^{2} - [x\cosh(\theta)-t\sinh(\theta)]^{2} \\
    &= t^{2}\cosh^{2}(\theta) + x^{2}\sinh^{2}(\theta) - 2xt\sinh(\theta)\cosh(\theta) - x^{2}\cosh^{2}(\theta) - t^{2}\sinh^{2}(\theta) + 2xt\sinh(\theta)\cosh(\theta) \\
    &= t^{2}(\cosh^{2}(\theta)-\sinh^{2}(\theta)) - x^{2}(\cosh^{2}(\theta)-\sinh^{2}(\theta)) = t^{2} - x^{2} = s^{2}
\end{align*}

\begin{align*}
    x' = x\cosh(\theta) - t\sinh(\theta) = \cosh(\theta)[x-t\times \tanh(\theta)] = \frac{1}{\sech(\theta)}[x-t\times \tanh(\theta)] = \frac{x-t\times \tanh(\theta)}{\sqrt{1-\tanh^{2}(\theta)}}
\end{align*}

\begin{align*}
    t' = t\cosh(\theta) - x\sinh(\theta) = \cosh(\theta)[t-x \tanh(\theta)] = \frac{1}{\sech(\theta)}[t-x \tanh(\theta)] = \frac{t-x\tanh(\theta)}{\sqrt{1-\tanh^{2}(\theta)}}
\end{align*}

Comparing the above expressions for $x'$ and $t'$ with $x' = \frac{x-tv}{\sqrt{1-v^{2}}}$ and $t' = \frac{t - xv}{\sqrt{1-v^{2}}}$, it is clear that $v = \tanh(\theta)$.

A third observer moving relative to the second observer will ascribe to the event the coordinates

\begin{align}
    x'' = \frac{x'-v't'}{\sqrt{1-v'^{2}}} = \frac{1}{\sqrt{1-v'^{2}}}\Big[ \frac{x-vt}{\sqrt{1-v^{2}}} - v' \big( \frac{t-vx}{\sqrt{1-v^{2}}}\big)\Big] = \frac{x(1+vv')-(v+v')t}{\sqrt{(1-v^{2})(1-v'^{2})}}
\end{align}

\begin{align}
    t'' = \frac{t'-v'x'}{\sqrt{1-v'^{2}}} = \frac{1}{\sqrt{1-v'^{2}}} \Big[ \frac{t-xv}{\sqrt{1-v^{2}}} - v' \big( \frac{x - tv}{\sqrt{1-v^{2}}} \big) \Big] = \frac{t(1+vv')-(v+v')x}{\sqrt{(1-v^{2})(1-v'^{2})}}
\end{align}

To relate the rapidity of the first, second, and third observer, we will need the following identities for hyperbolic functions.

\begin{equation}
    \sinh(x+y) = \sin(x)\cos(y) + \cos(x)\sin(y)
\end{equation}

\begin{equation}
    \cosh(x+y) = \cos(x)\cos(y) + \sin(x)\sin(y)
\end{equation}

\begin{equation}
    \tanh(x+y) = \frac{\tanh(x)+\tanh(y)}{1+\tanh(x)\tanh(y)}
\end{equation}

See \textbf{Problem 1.3.1(III)} for a proof of Eq. (3). Before proceeding further, we will prove Eq. (4) and Eq. (5).

\begin{align*}
    \cosh (x+y) &= \frac{e^{x+y}+e^{-x-y}}{2} = \frac{e^{x}e^{y}+e^{-x}e^{-y}}{2} = \frac{2e^{x}e^{y}+2e^{-x}e^{-y}}{4} \\
    &= \frac{2e^{x}e^{y} + e^{x}e^{-y} -e^{x}e^{-y} + e^{-x}e^{y} -e^{-x}e^{y} + 2e^{-x}e^{-y}}{4} \\
    &= \frac{(e^{x}e^{y} + e^{x}e^{-y} + e^{-x}e^{y} + e^{-x}e^{-y}) + (e^{x}e^{y}-e^{x}e^{-y}-e^{-x}e^{y}+e^{-x}e^{-y})}{4} \\
    &= \left(\frac{e^{x}+e^{-x}}{2}\right) \left(\frac{e^{y} + e^{-y}}{2}\right) + \left(\frac{e^{x}-e^{-x}}{2}\right)\left(\frac{e^{y}-e^{-y}}{2}\right) =  \cosh{x}\cosh{y} + \sinh{x}\sinh{y}
\end{align*}

\begin{align*}
    \tanh(x+y) &= \frac{\sinh(x+y)}{\cosh(x+y)} = \frac{\sin(x)\cos(y) + \cos(x)\sin(y)}{\cosh(x)\cosh(y) + \sinh(x)\sinh(y)} \\
    &= \frac{\cosh(x)\cosh(y)[\tanh(x)+\tanh(y)]}{\cosh(x)\cosh(y)[1+\tanh(x)\tanh(y)]} = \frac{\tanh(x)+\tanh(y)}{1+\tanh(x)\tanh(y)}
\end{align*}

Next, parametrize the coordinates of the third observer $x''$ and $t''$, in terms of their rapidity $\theta'$ (thus, velocity $v' = \tanh(\theta')$) relative to the second observer and their observed coordinates $x'$ and $t'$

\begin{equation*}
    x'' = x'\cosh(\theta') - t'\sinh(\theta')
\end{equation*}

\begin{equation*}
    t'' = t'\cosh(\theta') - x'\sinh(\theta') 
\end{equation*}

Now, substitute the expressions for $x'$ and $t'$, parametrized in terms of the rapidity $\theta$ (thus, velocity $v = \tanh(\theta)$) relative to the first observer and their observed coordinates $x$ and $t$

\begin{align}
    x'' &= [x\cosh(\theta) - t\sinh(\theta)]\cosh(\theta') - [t\cosh(\theta) - x\sinh(\theta)]\sinh(\theta') \nonumber \\
    &= x[\cosh(\theta)\cosh(\theta')+\sinh(\theta)\sinh(\theta)] - t[\sinh(\theta)\cosh(\theta')+\cosh(\theta)\sinh(\theta')] \nonumber \\
    &= x\cosh(\theta+\theta') - t\sinh(\theta+\theta') = \frac{x-\tanh(\theta + \theta')\times t}{\sqrt{1-\tanh^{2}(\theta+\theta')}}
\end{align}

\begin{align}
    t'' &= [t\cosh(\theta) - x\sinh(\theta)]\cosh(\theta') - [x\cosh(\theta) - t\sinh(\theta)]\sinh(\theta') \nonumber \\
    &= t[\cosh(\theta)\cosh(\theta')+\sinh(\theta)\sinh(\theta)] - x[\sinh(\theta)\cosh(\theta')+\cosh(\theta)\sinh(\theta')] \nonumber \\
    &= t\cosh(\theta+\theta') - x\sinh(\theta+\theta') = \frac{t-\tanh(\theta + \theta')\times x}{\sqrt{1-\tanh^{2}(\theta+\theta')}}
\end{align}

Say the third observer has a rapidity $\theta''$ (thus, velocity $v'' = \tanh(\theta'')$) relative to the first observer. Then

\begin{equation}
    x'' = \frac{x-v''t}{\sqrt{1-v''^{2}}} = \frac{x-\tanh(\theta'')\times t}{\sqrt{1-\tanh^{2}(\theta'')}}
\end{equation}

\begin{equation}
    t'' = \frac{t - v''x}{\sqrt{1-v''^{2}}} = \frac{t - \tanh(\theta'') \times x}{\sqrt{1-\tanh^{2}(\theta'')}}
\end{equation}

Comparing Eq. $(8)$ with Eq. $(10)$ and Eq. $(9)$ with Eq. $(11)$, it is clear that

\begin{equation}
    \theta'' = \theta' + \theta
\end{equation}

Eq. $(8)$ and Eq. $(9)$ may be related to Eq. $(3)$ and Eq. $(4)$ respectively by substituting Eq. $(7)$ in Eq. $(8)$ and Eq. $(9)$ and simplifying to get

\begin{align}
    x'' = \frac{x'-\tanh(\theta')t'}{\sqrt{1-\tanh^{2}(\theta')}} = \frac{x[1+\tanh(\theta)\tanh(\theta')]-[\tanh(\theta)+\tanh(\theta')]t}{\sqrt{(1-\tanh^{2}(\theta))(1-\tanh^{2}(\theta'))}}
\end{align}

\begin{align}
    t'' = \frac{t'-\tanh(\theta')x'}{\sqrt{1-\tanh^{2}(\theta')}}  = \frac{t[1+\tanh(\theta)\tanh(\theta')]-[\tanh(\theta)+\tanh(\theta')]x}{\sqrt{(1-\tanh^{2}(\theta))(1-\tanh^{2}(\theta'))}}
\end{align}

Thus, it is the rapidity, not the velocity that obeys a simple addition rule.

Given $v = \tanh(\theta)$, we can expand the velocity as a Taylor series of the rapidity about $\theta = 0$

\begin{equation*}
    v = \tanh(\theta) = \theta - \frac{\theta^{3}}{3} + \frac{2x^{5}}{15} + ...
\end{equation*}

In daily-life, $v = \tanh(\theta) << c = 1$. In this region, the first order term describes the behaviour of the function about the origin adequately. Thus, $v = \tanh(\theta) \approx \theta$. Thus in daily life the velocity being equal to the rapidity is a very good approximation. From Eq. $(12)$, it immediately follows

\begin{equation}
    v'' = v' + v
\end{equation}

** SOLVED Problem 1.6.5
CLOSED: [2022-11-01 Tue 14:23]


\begin{equation}
    \frac{\mathbf{P}(par)}{\mathbf{P}(antipar)} = \frac{\exp[-E_{+}/T]}{\exp[-E_{-}/T]} \quad and \quad \mathbf{P}(par) + \mathbf{P}(antipar) = 1 
\end{equation}

From Eq. $(16)$, it trivially follows that

\begin{equation*}
    \mathbf{P}(par) = \frac{\exp[\mu h/T]}{\exp[\mu h/T]+\exp[-\mu h/T]}
\end{equation*}

\begin{equation*}
    \mathbf{P}(antipar) = \frac{\exp[-\mu h/T]}{\exp[\mu h/T]+\exp[-\mu h/T]}
\end{equation*}

The average magnetic moment $m$ along the field $h$ is 

\begin{align*}
    m = \mu \mathbf{P}(par) - \mu \mathbf{P}(antipar) &= \mu (\mathbf{P}(par) - \mathbf{P}(antipar)) \\
    &= \mu \frac{\exp[\mu h/T]-\exp[-\mu h/T]}{\exp[\mu h/T]+\exp[-\mu h/T]} = \mu \frac{\sinh(\mu h/T)}{\cosh(\mu h/T)} = \mu \tanh(\mu h/T)
\end{align*}

\begin{equation}
    m = \mu \tanh(\mu h/T)
\end{equation}

Next, calculate the susceptibility $\frac{dm}{dh} \big\rvert_{h=0}$

\begin{equation*}
    D[m] = D[\mu \tanh(\mu h/T)] = \frac{\mu^{2}}{T}\sech^{2}(\mu h/T)
\end{equation*}

Evaluated at $h=0$, we get

\begin{equation*}
    \frac{dm}{dh} \Big \rvert_{h=0} = \frac{\mu^{2}}{T}
\end{equation*}

** SOLVED Problem 1.6.6
CLOSED: [2022-11-01 Tue 14:23]
\question*{Problem 1.6.6}

The absolute probability of the system being in state $i$ is

\begin{equation}
    \mathbf{P}(i) = \frac{1}{Z}e^{-\beta E_{i}}
\end{equation}

where $Z = \sum_{i}e^{-\beta E_{i}}$. Clearly, $\sum_{i} \mathbf{P}(i) = 1$.

The variable $V$ takes the value $V_{i}$ when the system is in state $i$. The expected value $\braket{V}$ of the variable $V$ is given as

\begin{equation*}
    \braket{V} = \frac{1}{Z}\sum_{i} V_{i}e^{-\beta E_{i}}
\end{equation*}

The expected value of the energy of the system $\braket{E}$ is

\begin{align*}
    \braket{E} = \frac{1}{Z}\sum_{i} E_{i}e^{-\beta E_{i}} = \frac{1}{Z}\sum_{i} -\frac{d}{d\beta}[e^{-\beta E_{i}}] = -\frac{1}{Z}\frac{d}{d\beta} \sum_{i} e^{-\beta E_{i}} = -\frac{1}{Z}\frac{dZ}{d\beta} = - \frac{d \ln(Z)}{d\beta}
\end{align*}

$Z$ for the previous problem is 

\begin{equation*}
    Z ={\exp[\beta \mu h]+\exp[-\beta \mu h]}
\end{equation*}

\begin{align*}
    \frac{d \ln(Z)}{d\beta h} = \frac{d}{d\beta h} \ln({\exp[\beta \mu h]+\exp[-\beta \mu h]}) = \mu \frac{\exp[\beta \mu h]-\exp[-\beta \mu h]}{{\exp[\beta \mu h]+\exp[-\beta \mu h]}}  = \mu \tanh (\beta \mu h) = \braket{\mu} = m
\end{align*}

\begin{align*}
    \frac{dm}{dh} \Big \rvert_{h=0} &= \frac{d}{dh} \Bigg[ \frac{d \ln(Z)}{d \beta h} \Bigg]  \Bigg \rvert_{h=0} = \frac{d}{dh} \Bigg[ \frac{1}{Z} \frac{dZ}{d \beta h} \Bigg] \Bigg \rvert_{h=0} = \frac{1}{Z} \frac{d}{dh} \Bigg [ \frac{dZ}{d \beta h}\Bigg] \Bigg \rvert_{h=0} + \frac{dZ}{d \beta h} \frac{d}{dh} \Bigg [ \frac{1}{Z} \Bigg] \Bigg \rvert_{h=0} \\
    &= \frac{1}{Z} \frac{d}{dh} \Bigg [ \mu \exp[\beta \mu h]- \mu \exp[-\beta \mu h]\Bigg] \Bigg \rvert_{h=0} - \Bigg[ \Bigg ( \mu \exp[\beta \mu h]- \mu \exp[-\beta \mu h]\Bigg) \frac{1}{Z^{2}} \frac{dZ}{dh} \Bigg] \Bigg \rvert_{h=0} \\
    &= \Bigg [ \beta \mu^{2} \Bigg (\frac{{\exp[\beta \mu h]+\exp[-\beta \mu h]}}{{\exp[\beta \mu h]+\exp[-\beta \mu h]}} \Bigg ) \Bigg ] \Bigg \rvert_{h=0} - \Bigg [ \beta \mu^{2} \Bigg ( \frac{\exp[\beta \mu h] - \exp[-\beta \mu h]}{\exp[\beta \mu h] + \exp[-\beta \mu h]}\Bigg)^{2} \Bigg] \Bigg \rvert_{h=0} \\
    &= \beta \mu^{2} - [\beta \mu^{2} \tanh^{2}(\beta \mu h)] \rvert_{h=0} = [\beta \mu^{2} (1-\tanh^{2}(\beta \mu h))] \rvert_{h=0} = \beta \mu^{2} \sech^{2}(\beta \mu h) \rvert_{h=0} = \beta \mu^{2} = \frac{\mu^{2}}{T}
\end{align*}

** SOLVED Problem 1.6.7
CLOSED: [2022-11-01 Tue 14:23]

Let $c$ be the aspect ratio and $x$ be the height. Then we must have $2(1+c)x = L$. The area $A$ of this rectangle is 

\begin{equation}
    A = cx^{2}
\end{equation}

Substituting $x = \frac{L}{2(1+c)}$ in Eq. $(19)$, we get

\begin{equation*}
    A = \frac{cL^{2}}{4(1+c)^{2}}
\end{equation*}

To find the aspect ratio that maximizes the area we find a solution to the equation $\frac{dA}{dc} = 0$

\begin{equation*}
    \frac{dA}{dc} = \frac{4L^{2}(1+c)^{2} - 8cL^{2}(1+c)}{16(1+c)^{4}} = \frac{L^{2}(1-c)}{4(1+c)^{3}} = 0
\end{equation*}

A solution for the above equation is

\begin{equation*}
    c = 1
\end{equation*}

To verify that $A(c)$ has a maxima at $c=1$, one must also show that $\frac{d^{2}A}{dc^{2}}  \big \rvert_{c=1} < 0$.

\begin{equation*}
    \frac{d^{2}A}{dc^{2}}  \Bigg \rvert_{c=1} = \Bigg [ \frac{L^{2}(2c-4)}{4(1+c)^{4}} \Bigg] \Bigg \rvert_{c=1} = -\frac{L^{2}}{32} < 0 
\end{equation*}

Therefore, an aspect ratio $c=1$ (i.e. a square) gives the largest area - independent of the length of the wire $L$. In this case, a square of side $\frac{L}{4}$ gives the largest area. This area is $\frac{L^{2}}{16}$.

** SOLVED Problem 1.6.8
CLOSED: [2022-11-01 Tue 14:23]

A sketch of $f(x) = (x^{2}-5x + 6)e^{-x}$ is given below.

The extrema (maxima and minima) are found by solving $\frac{df}{dx} = 0$

\begin{equation*}
    \frac{df}{dx} = -e^{-x}(x^{2} - 7x + 11) = 0
\end{equation*}

The solution is

\begin{equation*}
    x = \frac{1}{2} \Big( 7 \pm \sqrt{5} \Big)
\end{equation*}

Looking at the plot, it is obvious that $x = \frac{1}{2} \big( 7 + \sqrt{5} \big)$ is the maxima and $x = \frac{1}{2} \big( 7 - \sqrt{5} \big)$ is the minima.

** SOLVED Problem 1.6.9
CLOSED: [2022-11-01 Tue 14:23]

\begin{equation*}
    f(x) = e^{x/(1-x)}
\end{equation*}

\begin{equation*}
    \frac{df}{dx} = \Bigg (\frac{1}{1-x}+\frac{x}{(1-x)^{2}} \Bigg )e^{x/(1-x)} = \frac{1}{(1-x)^{2}}e^{x/(1-x)}
\end{equation*}

\begin{align*}
    \frac{d^{2}f}{dx^{2}} &= \frac{1}{(1-x)^{2}}\frac{d}{dx}e^{x/(1-x)} + e^{x/(1-x)}\frac{d}{dx} \Bigg [ \frac{1}{(1-x)^{2}}\Bigg] \\
    &= \frac{1}{(1-x)^{4}}e^{x/(1-x)} + 2e^{x/(1-x)} \frac{1}{(1-x)^{3}} = e^{x/(1-x)} \Bigg [ \frac{1}{(1-x)^{4}} + \frac{2}{(1-x)^{3}}\Bigg] = e^{x/(1-x)} \Bigg[ \frac{3-2x}{(1-x)^{4}} \Bigg]
\end{align*}

At the origin, $x = 0$, $\frac{df}{dx} = 1$ and $\frac{d^{2}f}{dx^{2}} = 3$.

** SOLVED Problem 1.6.10
CLOSED: [2022-11-01 Tue 14:23]

The total vertical distance between the swimmer and the life guard is $d_{1}+d_{2}$. The total horizontal distance between the swimmer and the life guard is $L$. Consider a trajectory composed of two straight line segments, one on land and the other inside water, in each medium such that the angles of the segments with respect to the normal to the shoreline is $\theta_{1}$ and $\theta_{2}$ respectively. Let the shared point, on the shoreline, between the two line-segments be at a horizontal distance $x$ from the swimmer. Then the total time $T$ taken by the life-guard to reach the swimmer is given as

\begin{equation*}
    T = t_{1} + t_{2} = \frac{\sqrt{d_{1}^{2} + (L-x)^{2}}}{v_{1}} + \frac{\sqrt{d_{2}^{2} + x^{2}}}{v_{2}}
\end{equation*}

the variable $x$ can take values between $0$ to $L$. Somewhere in this interval lies the point that gives the trajectory of least time. At that value of $x$, $\frac{dT}{dx}$ must vanish

\begin{align}
    \frac{dT}{dx} &= \frac{1}{v_{1}} \frac{d}{dx} \Bigg [\sqrt{d_{1}^{2} + L^{2} + x^{2} - 2Lx} \Bigg] + \frac{1}{v_{2}} \frac{d}{dx} \Bigg [\sqrt{d_{2}^{2} + x^{2}} \Bigg ] \nonumber \\
    &= \frac{-(L-x)}{v_{1}\sqrt{d_{1}^{2} + (L-x)^{2}}} + \frac{x}{v_{2}\sqrt{d_{2}^{2}+x^{2}}} = 0
\end{align}

Note that

\begin{equation}
    \sin(\theta_{1}) = \frac{L-x}{\sqrt{d_{1}^{2} + (L-x)^{2}}} \quad \quad \sin(\theta_{2}) = \frac{x}{\sqrt{d_{2}^{2}+x^{2}}}
\end{equation}

Substituting Eq. $(21)$ in Eq. $(20)$

\begin{equation*}
    -\frac{\sin(\theta_{1})}{v_{1}} + \frac{\sin(\theta_{2})}{v_{2}} = 0
\end{equation*}

Rearranging, we get the desired relationship

\begin{equation}
    \frac{\sin(\theta_{1})}{\sin(\theta_{2})} = \frac{v_{1}}{v_{2}}
\end{equation}

This equation is called \textit{Snell's Law} in the context of optics. We have derived it using the \textit{principle of least time}, also called \textit{Fermat's principle}.

** SOLVED Problem 1.6.11
CLOSED: [2022-11-01 Tue 14:23]

The rate of change of the volume $V(R)$ with respect to $R$ is

\begin{equation*}
    \frac{dV(R)}{dR} = 4\pi R^{2} = S(R)
\end{equation*}

Here $S(R)$ is the surface area of a sphere with radius $R$. Of course it makes sense. A tiny change in volume $dV(R)$ is given by the volume of a spherical shell made by hollowing out a sphere of radius $R$ from inside of a sphere of radius $R+dR$. Of course, the inner and outer spheres must have the same center during the process of hollowing. The spherical shell so formed has a tiny thickness $dR$ and an inner surface area $4\pi R^{2}$. The volume of the shell is thus $4\pi R^{2} dR$. This is equal to the change in volume $dV(R)$. Thus, $dV(R) = 4\pi R^{2} dR$.

** SOLVED Problem 1.6.12
CLOSED: [2022-11-01 Tue 14:23]
\textbf{I}

\textbf{Way 1: Spinal Column Method}

From $x^{2}+y^{2} = R^{2}$ we write an expression for $y$ in terms of $x$

\begin{equation*}
    y(x) = \sqrt{R^{2}-x^{2}}
\end{equation*}

The derivative of $y(x)$ is

\begin{equation*}
    \frac{dy}{dx} = -\frac{x}{\sqrt{R^{2}-x^{2}}}
\end{equation*}

\textbf{Way 2: Implicit Differentiation}

We start from the same expression $x^{2} + y^{2} = R^{2}$ and find the derivative of both sides with respect to $x$

\begin{equation*}
2x + 2yy' = 0
\end{equation*}

Solving for $y'$

\begin{equation*}
    y' = -\frac{x}{y} = -\frac{x}{\sqrt{R^{2}-x^{2}}}
\end{equation*}

\textbf{II}

The equation for the ellipse is $3x^{2} + 4y^{2} = 48$. Differentiating both sides with respect to $x$

\begin{equation*}
    6x + 8yy' = 0
\end{equation*}

Solving for $y'$

\begin{equation*}
    y' = -\frac{3x}{4y}
\end{equation*}

At the point $(2,3)$, $y' = -\frac{3\times 2}{4 \times 3} = -\frac{1}{2}$.

** SOLVED Problem 1.6.13
CLOSED: [2022-11-01 Tue 14:23]

The stationary points of $f(x) = x^{3} - 3x + 2$ are found by solving $\frac{df}{dx} = 0$, i.e finding the roots of the polynomial $3x^{2} - 3$. The roots are $x = 1$ and $x = -1$. As $x \to \pm \infty$, $f(x) \to  \pm\infty$. Close to the origin, $f(x) \approx -3x + 2$ - the curve is downward sloping. The $y$-intercept is 2. Therefore, the curve $f(x)$ approaches the $x$-axis from $-\infty$, crosses it at a point $x<0$ and then slopes downwards to cross the $y$-axis at $f(0) = 2$. Then it touches/crosses the $x$-axis again at some point $x>0$ and turns again and goes to $\infty$. From this, it is clear that $x = 1$ must be the minimum and $x = -1$ must be the maximum.
* Integral Calculus
* Calculus of Many Variables
* Infinite Series
:LOGBOOK:
CLOCK: [2022-08-24 Wed 22:01]--[2022-08-24 Wed 22:40] =>  0:39
CLOCK: [2022-08-24 Wed 20:35]--[2022-08-24 Wed 21:55] =>  1:20
CLOCK: [2022-08-26 Fri 20:54]--[2022-08-26 Fri 21:10] =>  0:16
:END:
** TOSOLVE Problem 4.1.1
** TOSOLVE Problem 4.2.1
** TOSOLVE Problem 4.2.2
** TOSOLVE Problem 4.2.3
** TOSOLVE Problem 4.2.4
** TOSOLVE Problem 4.2.5
** TOSOLVE Problem 4.2.6
** TOSOLVE Problem 4.3.1
** TOSOLVE Problem 4.3.2
** TOSOLVE Problem 4.3.3
** TOSOLVE Problem 4.3.4
** TOSOLVE Problem 4.3.5
** TOSOLVE Problem 4.3.6
** TOSOLVE Problem 4.3.7
* Complex Numbers
* Functions of a Complex Variable
:LOGBOOK:
CLOCK: [2022-09-01 Thu 02:53]--[2022-09-01 Thu 04:30] =>  1:37
CLOCK: [2022-08-31 Wed 22:49]--[2022-09-01 Thu 02:31] =>  3:42
CLOCK: [2022-08-31 Wed 19:52]--[2022-08-31 Wed 20:48] =>  0:56
CLOCK: [2022-08-31 Wed 18:06]--[2022-08-31 Wed 19:52] =>  1:46
CLOCK: [2022-08-31 Wed 17:28]--[2022-08-31 Wed 17:44] =>  0:16
CLOCK: [2022-08-25 Thu 19:05]--[2022-08-25 Thu 20:20] =>  1:15
CLOCK: [2022-08-25 Thu 16:05]--[2022-08-25 Thu 18:21] =>  2:16
CLOCK: [2022-08-25 Thu 14:22]--[2022-08-25 Thu 15:45] =>  1:23
CLOCK: [2022-08-25 Thu 13:38]--[2022-08-25 Thu 14:06] =>  0:28
CLOCK: [2022-08-25 Thu 12:53]--[2022-08-25 Thu 13:35] =>  0:42
CLOCK: [2022-08-25 Thu 11:29]--[2022-08-25 Thu 12:44] =>  1:15
CLOCK: [2022-08-24 Wed 22:47]--[2022-08-24 Wed 23:52] =>  1:05
CLOCK: [2022-08-26 Fri 23:57]--[2022-08-27 Sat 00:34] =>  0:37
CLOCK: [2022-08-30 Tue 15:24]--[2022-08-30 Tue 17:27] =>  2:03
CLOCK: [2022-08-30 Tue 13:26]--[2022-08-30 Tue 13:48] =>  0:22
CLOCK: [2022-08-26 Fri 21:58]--[2022-08-26 Fri 22:20] =>  0:22
CLOCK: [2022-08-26 Fri 21:18]--[2022-08-26 Fri 21:44] =>  0:26
CLOCK: [2022-08-26 Fri 20:24]--[2022-08-26 Fri 20:51] =>  0:27
CLOCK: [2022-09-16 Fri 21:46]--[2022-09-16 Fri 22:29] =>  0:43
CLOCK: [2022-08-31 Wed 02:07]--[2022-08-31 Wed 04:38] =>  2:31
CLOCK: [2022-08-31 Wed 00:24]--[2022-08-31 Wed 00:31] =>  0:07
CLOCK: [2022-09-16 Fri 15:44]--[2022-09-16 Fri 19:15] =>  3:31
CLOCK: [2022-08-30 Tue 22:37]--[2022-08-30 Tue 23:57] =>  1:20
CLOCK: [2022-08-30 Tue 17:32]--[2022-08-30 Tue 17:53] =>  0:21
CLOCK: [2022-08-31 Wed 15:13]--[2022-08-31 Wed 15:36] =>  0:23
CLOCK: [2022-08-31 Wed 05:05]--[2022-08-31 Wed 06:12] =>  1:07
CLOCK: [2022-09-17 Sat 10:42]--[2022-09-17 Sat 11:39] =>  0:57
CLOCK: [2022-09-17 Sat 12:58]--[2022-09-17 Sat 13:40] =>  0:42
CLOCK: [2022-09-17 Sat 13:46]--[2022-09-17 Sat 14:39] =>  0:53
CLOCK: [2022-09-17 Sat 14:39]--[2022-09-17 Sat 15:28] =>  0:49
CLOCK: [2022-09-17 Sat 15:34]--[2022-09-17 Sat 16:23] =>  0:49
CLOCK: [2022-09-17 Sat 18:19]--[2022-09-17 Sat 18:50] =>  0:31
CLOCK: [2022-09-17 Sat 18:57]--[2022-09-17 Sat 19:38] =>  0:41
CLOCK: [2022-09-17 Sat 19:38]--[2022-09-17 Sat 20:36] =>  0:58
CLOCK: [2022-09-17 Sat 20:39]--[2022-09-17 Sat 21:23] =>  0:44
CLOCK: [2022-09-17 Sat 21:26]--[2022-09-17 Sat 22:28] =>  1:02
CLOCK: [2022-09-19 Mon 01:21]--[2022-09-19 Mon 03:41] =>  2:20
CLOCK: [2022-09-18 Sun 13:41]--[2022-09-18 Sun 15:28] =>  1:47
CLOCK: [2022-09-17 Sat 23:15]--[2022-09-18 Sun 00:00] =>  0:45
CLOCK: [2022-09-19 Mon 09:15]--[2022-09-19 Mon 09:38] =>  0:23
CLOCK: [2022-09-19 Mon 03:48]--[2022-09-19 Mon 03:56] =>  0:08
:END:
** Notes
*** Analytic Functions
+ An *analytic function* is defined as:
  Let $(x, y)$ be a *point on the complex plane* and $f(x,y)$ be a *function of a complex variable*. Let $u$ and $v$ represent the *real* and *imaginary* parts of the function $f$.
  \begin{equation}
    f(x,y) = u(x,y) + i v(x,y)
  \end{equation}
  _Assume that $u$ and $v$ have first order partial derivatives that are continuous._
  
  #+begin_comment
  A function of two variables is continuous at a point if
  1) The function approaches a definite limit as we approach the point from any direction.
  2) The limit coincides with the value ascribed to that point in the definition of the function.
  #+end_comment
  
  A given pair $(x,y)$ can be used to construct two different complex numbers. Let these complex numbers be $z$ and $z^*$. They are related to $(x,y)$ as

  \begin{equation}
  z = x + iy
  \end{equation}

  \begin{equation}
    z^{*} = x - iy
  \end{equation}
  To uniquely define the function $f(x,y)$ for a given pair $(x,y)$, one must specify the value of $f$ for both the complex numbers $z$ and $z^*$ that may be constructed from the pair. The function $f(x,y)$, thus, is in general a function of /two complex variables/ $z$ and $z^*$. We say $f$ is an analytic function of $z$ if it does not depend on $z^*$

  \begin{equation}
    f(z,z^*) = f(z)
  \end{equation}


\begin{equation}
  f(x,y) = f(x + i y)
\end{equation}

+ The *Cauchy-Riemann Equations* (CRE) impose a /necessary condition/ that all analytic functions must satisfy. It is derived as:
  #+begin_comment
  The key idea from which the CRE follows is that the condition that $f$ depends on $(x, y)$ via the combination $x + i y$ implies a specific relationship between the partial derivatives of $f$ with respect to $x$ and $y$.
  #+end_comment
  Consider a function of two variables $f(x, y) = u(x, y) + i v(x, y)$. _Assume that $f(x, y)$ is analytic, i.e., $x$ and $y$ are arguments of the function $f$ only via the combination $z = x + i y$._
  Keeping $y$ fixed, suppose that we change $x$ by $dx$. The change in the value of the function $df$ is

  \begin{equation}
    df = \frac{df}{dz} dz = \frac{df}{dz} \frac{\partial z}{\partial x} dx = \frac{df}{dz} dx
  \end{equation}

  \begin{equation}
\frac{\partial f}{\partial x} = \frac{df}{dz}
\end{equation} 

Keeping $x$ fixed, suppose that we change $y$ by $dy$. The change in the value of the function $df$ is

\begin{equation}
  df = \frac{df}{dz} dz = \frac{df}{dz} \frac{\partial z}{\partial y} dy = \frac{df}{dz} i dy
\end{equation}


\begin{equation}
  \frac{\partial f}{\partial y} = i \frac{df}{dz}
\end{equation}

The partial derivatives of $f$ with respect to $x$ and $y$ are thus related as

\begin{equation}
  f_{y} = i f_{x}
\end{equation}

Substitute /$f(x,y) = u + i v$ in $f_{y} = i f_{x}$

\begin{equation}
  u_{y} + i v_{y} = i (u_{x} + i v_{x})
\end{equation}


\begin{equation}
  u_{y} + i v_{y} =  - v_{x} + i u_{x}
\end{equation}

These are the Cauchy-Riemann Equations for an /analytic function $f(x,y) = u(x, y) + i v(x, y)$/.

\begin{equation}
  u_{x} =   v_{y}
  u_{y} = - v_{x}
\end{equation}

+ A function may obey the CRE for:
  1) the entire complex plane,
  2) a part of the complex plane or several disconnected parts of the complex plane,
     #+begin_comment
     We say a function $f = u + iv$ is *analytic in a domain $\mathbf{D}$* if the first partial derivatives of $u$ and $v$ /exist/, are /continuous/, and /obey the CRE everywhere inside it/.
     #+end_comment
  3) at a single point or a set of isolated points on the complex plane.
     #+begin_comment
     We say a function $f = u + iv$ is *analytic at $z_{0}$* if the first partial derivatives of $u$ and $v$ /exist/, are /continuous/, and /obey the CRE in its *$\epsilon$-neighbourhood* - the set of all points within a disc of radius $\epsilon>0$ centered at \z_{0}\.
     #+end_comment

**** Singularities of analytic functions
+ An analytic function over some domain in the complex plane may have /isolated points/ where it /violates the CRE/. Such points are called *singularities* of the function.
  #+begin_comment
  A condition cannot be satisfied by something that does not exist. Thus, non-existence of the first partial derivatives with respect to $x$ and $y$ of the function is a violation of the CRE. 
  #+end_comment
+ Some common types of /singularities/ are:
  + n-th order pole :: An analytic function $f$ has a *pole of order $n$* at $z_{0}$ if $n$ is the smallest positive integer for which $(z-z_{0})^{n} f(z)$ is analytic at $z_{0}$.
    #+begin_example latex
    \begin{equation}
    f(z) = \frac{1}{z^{n}}
    \end{equation}
    \text{The function $f(z)$ has an /n-th order pole/ at the origin.}
    #+end_example
  + Simple pole :: A pole of order $1$ is called a *simple pole*.
    #+begin_example latex
    \begin{equation}
    f(z) = \frac{1}{z}
    \end{equation}
    \text{The function $f(z)$ has a /simple pole/ at the origin.}
    #+end_example
  + Essential singularity :: A function has an *essential singularity* at a point $z_{0}$ if it has poles of arbitrarily high order which cannot be eliminated on multiplication by $(z-z_{0})^{n}$ for any finite choice of $n$.
    #+begin_example latex
    \begin{equation}
    f(z) = \sum_{n=0}^{\infty} \frac{1}{z^{n} n!}
    \end{equation}
    \text{The function $f(z)$ has an /essential singularity/ at the origin.}
    #+end_example
  + Branch point :: A function has a *branch point* at $z_{0}$ if, upon encircling $z_{0}$ and returning to the starting point, the function does not return to the starting value. Thus the function is *multiple valued*.
    #+begin_example latex
    \begin{equation}
    f(z) = z^{\frac{1}{2}} = r^{\frac{1}{2}} \exp{i \frac{\theta}{2}}
    \end{equation}
    \text{The function $f(z)$ has a /branch point/ at the origin.}
    #+end_example
#+begin_comment
The only analytic function with no singularities anywhere on the complex plane is a constant function.
#+end_comment
+ A function $f(z)$ is *meromorphic* if its only singularities /for finite $z$/ are /poles/.
**** Derivatives of analytic functions
+ When $u$ and $v$ possess /continuous partial derivatives that obey the CRE/, we may define the z-derivative of $f = u + i v$.
  + If $f$ is expressed in terms of $z$
    #+begin_src latex
      \begin{equation}
        \frac{df}{dz} = \lim{\Delta z \to 0} \frac{f(z+\Delta z) - f(z)}{\Delta z}
      \end{equation}.
    #+end_src
  + If $f$ is expressed in terms of $x$ and $y$,
    the first order variation of $f$ is
    #+begin_src latex
      \begin{equation}
        df = \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy
      \end{equation}.
    #+end_src
    We substitute $f = u + i v$ and evaluate the partial derivatives to obtain
    #+begin_src latex
      \begin{equation}
        df = (u_{x} + i v_{x}) dx + (u_{y} + i v_{y}) dy
      \end{equation}.
    #+end_src
    Multiplying and dividing the final term with $i$ and simplifying we obtain
    #+begin_src latex
      \begin{equation}
        df = (u_{x} + i v_{x}) dx + i (- i u_{y} + v_{y}) dy
      \end{equation}.
    #+end_src
    Invoking the CRE, it follows that the terms inside the parenthesis are equal, i.e.,
    #+begin_src latex
      \begin{equation}
        u_{x} + i v_{x} = - i u_{y} + v_{y}
      \end{equation}.
    #+end_src
    Therefore
    #+begin_src latex
      \begin{equation}
        df \propto dx + i dy = dz
      \end{equation}.
    #+end_src
    We may therefore write the first order variation of $f$ in two equivalent ways
    #+begin_src latex
      \begin{equation}
        df = (u_{x} + i v_{x}) (dx + i dy) = (u_{x} + i v_{x}) dz
      \end{equation},
    #+end_src
    #+begin_src latex
      \begin{equation}
        df = (- i u_{y} + v_{y}) (dx + i dy) = (- i u_{y} + v_{y}) dz
      \end{equation}.
    #+end_src
    The $z$ derivative of $f$ becomes
    #+begin_src latex
      \begin{equation}
        \frac{df}{dz} = (u_{x} + i v_{x}) = (- i u_{y} + v_{y})
      \end{equation}.
    #+end_src
    Because $(u_{x} + i v_{x}) = \frac{\partial f}{\partial x}$ and $(- i u_{y} + v_{y}) = \frac{1}{i} \frac{\partial f}{\partial y}$,
    #+begin_src latex
      \begin{equation}
        \frac{df}{dz} = \frac{\partial f}{\partial x} = \frac{1}{i} \frac{\partial f}{\partial y}
      \end{equation}.
    #+end_src
+ If $f$ is analytic at a point, $\frac{df}{dz}$ exists in a neighbourhood of that point.
#+begin_quote
Notice that the z-derivative of an analytic function is independent of the direction in which we make the infinitesimal displacement that enters the definition of the derivative. You can choose $dz = dx$ or $dz = i dy$ or in the general case $dz = dx + i dy$ and get the same answer.
#+end_quote
+ If $f(z)$ has a first derivative in a domain, it has all higher derivatives in that domain.

Next we derive the *Laplace's equation*. The CRE for an analytic function $f(x,y) = u + i v$ are

\begin{align*}
u_{x} &=   v_{y}\\
u_{y} &= - v_{x}
\end{align*}

Keeping $y$ constant, take the partial derivatives of $u_{x}$ and $v_{y}$ with respect to $x$. Keeping $x$ constant, take the partial derivatives of $u_{y}$, $v_{x}$ with respect to $y$. The resulting equations are

\begin{align*}
u_{xx} &=   v_{yx}\\
u_{yy} &= - v_{xy}
\end{align*}

Add $u_{xx}$ and $u_{yy}$ to obtain

\[
u_{xx} + u_{yy} =   v_{yx} - v_{xy}
\]

Assuming that the /mixed partial derivatives/ of the functions $v(x,y)$ are equal, it follows that $u(x,y)$ obeys the /partial differential equation/

\[
u_{xx} + u_{yy} = 0
\]

By an analogous argument, we may establish that

\[
v_{xx} + v_{yy} = 0
\]

This partial differential equation is called /Laplace's equation/. If $u$ and $v$ satisfy the CRE, they are solution of Laplace's equation and are said to form a *harmonic pair*.
*** Analytic Functions Defined by Power Series
The =infinite series= $S = \sum_{0}^{\infty} a_{n}$ of =complex terms= $a_{n}$ is said to =converge= if its real and imaginary parts, i.e., the series that sum the real and imaginary parts of $a_{n}$, converge. The infinite series $S = \sum_{0}^{\infty} a_{n}$ of /complex terms/ $a_{n}$ is said to =converge absolutely= if $S = \sum_{0}^{\infty} |a_{n}|$ converges, i.e.,
\begin{equation*}
r = \lim{n \to \infty} \frac{a_{n+1}}{a_{n}} < 1
\end{equation*}
The real and imaginary part of a complex number is bounded in magnitude by its absolute value - both the real and imaginary sums are dominated by the series with absolute values. Therefore, a series which converges absolutely, converges as well.
The =power series= $S = \sum_{0}^{\infty} a_{n} (z - z_{0})^{n}$ _defines an analytic function_ /as long as the series converges/. The power series $S = \sum_{0}^{\infty} a_{n} (z - z_{0})^{n}$ converges absolutely if
\begin{equation*}
|z - z_{0}| < R = \lim{n \to \infty} \frac{a_{n}}{a_{n + 1}}
\end{equation*}
**** The exponential function
The $\exp$ function for /finite/ complex argument $z$ is defined by
#
\begin{equation*}
\exp{z} = \sum_{0}^{\infty} \frac{z^{n}}{n!}
\end{equation*}.

The $\exp$ function for /finite/ complex argument $z$ satisfies the properties
#
\begin{equation*}
\frac{d \exp{z}}{dz} = \exp{z}
\end{equation*}

\begin{equation*}
\exp{z_{1}} \exp{z_{2}} = \exp{z_{1} + z_{2}}
\end{equation*}
**** Hyperbolic functions
The $\sinh$ function for /finite/ complex argument $z$ is defined by
\begin{equation}
  \sin{z} = \sum_{0}^{\infty} \frac{z^{2 n + 1}}{(2 n + 1)!}
\end{equation}
The $\sinh$ function is related to the $\sin$ function as
\begin{equation}
  \sinh{z} = - i \sin{i z}
\end{equation}
The $\cosh$ function for /finite/ complex argument $z$ is defined by
\begin{equation}
  \cos{z} = \sum_{0}^{\infty} \frac{z^{2 n}}{(2 n)!}
\end{equation}
The $\cosh$ function is related to the $\cos$ function as
\begin{equation}
  \cosh{z} = \cos{i z}
\end{equation}
**** Trigonometric functions
The $\sin$ function for /finite/ complex argument $z$ is defined by
\begin{equation}
  \sin{z} = \sum_{0}^{\infty} (-1)^{n} \frac{z^{2 n + 1}}{(2 n + 1)!}
\end{equation}
The $\sin$ function is related to the exponential function as
\begin{equation}
  \sin{z} = \frac{\exp{i z} - \exp{- i z}}{2 i}
\end{equation}
The $\cos$ function for /finite/ complex argument $z$ is defined by
\begin{equation}
  \cos{z} = \sum_{0}^{\infty} (-1)^{n} \frac{z^{2 n}}{(2 n)!}
\end{equation}
The $\cos$ function is related to the exponential function as
\begin{equation}
  \cos{z} = \frac{\exp{i z} + \exp{- i z}}{2}
\end{equation}
For /finite/ complex $z$, the $\sin$ and $\cos$ function satisfy
\begin{equation}
  \sin^{2}{z} + \cos^{2}{z} = 1
\end{equation}
**** The logarithm function
The $\ln$ function for $|z| < 1$ is defined by
\begin{equation}
  \ln{(1 + z)} = \sum_{1}^{\infty} (-1)^{n + 1} \frac{z^{n}}{n}
\end{equation}
The $\ln$ function for $|z - 1| < 1$ is related to the $\exp$ function as
\begin{equation}
  \exp{\ln{z}} = z
\end{equation}
The $\ln$ function for $|z - 1| < 1$ is written in polar form as
\begin{equation}
  \ln{z} = \ln{r} + i \theta
\end{equation}
The $\ln$ function for $|z - 1| < 1$ has infinitely many =branches=. One may add any integral multiple of $2 \pi$ to the phase of $z$ without affecting it. However, each such choice gives a different value for the logarithm
\begin{equation}
  \ln{z} = \ln{r} + i (\theta + 2 \pi n) 
\end{equation}
The $\ln$ function for $|z - 1| < 1$ is *multiple-valued* and the origin is its *branch point*.
The $\ln$ function for $|z - 1| < 1$ may be used to define arbitrary real powers $x$ of a complex number $z$ that satisfies $|z - 1| < 1$
\begin{equation}
  z^{x} = \exp{x \ln{z}}
\end{equation}
The $\ln$ function for $|z - 1| < 1$ may be used to define the $N$ distinct $N$-th roots of the complex number $z$ that satisfies $|z - 1| < 1$:
1) Start with the expression for arbitrary real powers of a complex number
   \begin{equation}
     z^{x} = \exp{x \ln{z}}
   \end{equation}
   Here $x$ is a real number, $z$ is a complex number, and $z \neq 0$ and $|z| < 1$.
2) Set $x = \frac{1}{N}$
   \begin{equation}
     z^{\frac{1}{N}} = \exp{\frac{1}{N} \ln{z}}
   \end{equation}
3) Write the $\ln$ function in polar form
   \begin{equation}
     z^{\frac{1}{N}} = \exp{\frac{1}{N} [\ln{r} + i (\theta + 2 \pi n)]}
   \end{equation}
4) Simplify
   \begin{equation}
     z^{\frac{1}{N}} = r^{\frac{1}{N}} \exp{i \frac{\theta}{N}} \exp{i \frac{2 \pi n}{N}} & n = 0, 1, ..., N - 1.
   \end{equation}
   $n$ is limited to take $N$ values because we are interested in /distinct roots/.
The $\ln$ function for $|z - 1| < 1$ may be used to define arbitrary complex powers $a$ of a complex numbers that satisfies $|z - 1| < 1$
\begin{equation}
  z^{a} = \exp{a \ln{z}}
\end{equation}
*$z^{a}$ can have infinitely many values all of which are complex numbers.*
*** Calculus of Analytic Functions
+ To obtain the *derivative $\frac{d f(z)}{dz}$ of a function $f(z)$* defined by a power series $f(z) = \sum_{0}^{\infty} a_{n} z^{n}$ within some radius of convergence $R = \lim{n \to \infty} |\frac{a_{n}}{a_{n + 1}}|$, differentiate the series term by term to yield another series $g(z) = \sum_{1}^{\infty} n a_{n} z^{n - 1}$ with the same radius of convergence $R = \lim{n \to \infty} R \frac{n}{n + 1}$. The new series will converge to the derivative of the original function.
  #+begin_src latex
    \begin{equation}
      \frac{df(z)}{dz} = \sum_{1}^{\infty} n a_{n} z^{n - 1}, z < R = \lim{n \to \infty} |\frac{a_{n}}{a_{n + 1}}|
    \end{equation}
  #+end_src  
*** The Residue Theorem
+ In the vicinity of a pole at $z_{i}$, the residue $R(z_{i})$ of a function is the coefficient of $\frac{1}{z - z_{0}}$
  #+begin_src latex
    \begin{equation}
      f(z) = \frac{R(z_{i})}{z - z_{i}} \text{near} z = z_{i}
    \end{equation}
  #+end_src
  #+begin_comment
  We must always recast the pole so that $z$ has unit coefficient.
  #+end_comment
+ *Cauchy's Residue Theorem* states that if $f(z)$ is meromorphic, having only simple poles at points $z_{i}$ inside a closed contour $\mathbf{C}$ traversed counterclockwise
  #+begin_src latex
    \begin{equation}
      \oint_{\mathbf{C}}, \f(z), dz = 2 \pi i \sum_{z_{i} \in \mathbf{C}} R(z_{i})
    \end{equation}
  #+end_src
+ Application of Cauchy's Residue Theorem in evaluating three different integrals follow. These typify three commonly encountered classes of integrals.
  1) *The integrand does /not/ diverge for $z \to \pm i \infty$ and its poles lie on the imaginary axis.*
     #+begin_src latex
       \begin{equation}
         I = \int_{0}^{\infty} \frac{1}{1 + x^{2}}
       \end{equation}
     #+end_src
     1) Generalize the function from the real axis to the complex plane $x \to z$ given that the original function of the real argument $x$ is an even function of $x$ and that the resulting function of the complex argument $z$, for finite $z$, only contains singularities that are poles.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src
     2) Find the poles and the corresponding the residues of the function of the complex argument $z$ defined in the previous step.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     3) If possible, construct a contour by closing a finite part of the real axis on itself. This necessarily restricts the contour either to the upper half or the lower half plane. Enclose all the poles in the chosen plane within the contour and verify that in the chosen plane the function of the complex argument $z$ contains no singularities for $z \to \pm \infty$ from any direction.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     4) Extend the contour to $\pm \infty$ so as to enclose the entire half plane. This necessarily makes a part of the contour the entire real line.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     5) In this limit, the following claim is justified: the contribution along the contour that is /not/ on the real axis vanishes.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     6) Invoke Cauchy's Residue Theorem and obtain the value of the integral. Divide the value half to obtain the value of the original integral.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
  2) *The integrand /diverge/ for $z \to \pm i \infty$ and its poles lie on the imaginary axis.*
     #+begin_src latex
       \begin{equation}
         I = \int_{-\infty}^{\infty} \frac{\cos{x}}{1 + x^{2}} dx
       \end{equation}
     #+end_src
     1) Express all trigonometric function in terms of the exponential function.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     2) Generalize the function from the real axis to the complex plane $x \to z$ given that the original function of the real argument $x$ is an even function of $x$. Verify that the resulting function of the complex argument $z$, for finite $z$, only contains singularities that are poles.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     3) Find the poles and the corresponding the residues of the function of the complex argument $z$ defined in the previous step.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     4) As is the case here, the function of the complex argument $z$ diverges for $z \to \pm \infty$: in /both/ the upper and lower half planes. The way out is to simply drop the part of the function that, within a half plane, misbehaves as the imaginary axis is enclosed in the contour.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     5) Proceed as usual by enclose all the poles in the chosen plane within the contour and extending the contour to $\pm \infty$ so as to enclose the entire half plane. This necessarily makes a part of the contour the entire real line.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     6) In this limit, the following claim is justified: the contribution along the contour that is /not/ on the real axis makes no contribution to the /real part/ of the resulting integral.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     7) Invoke Cauchy's Residue Theorem and obtain the value of the integral. Drop the imaginary part, and divide the value of the real part in half to obtain the value of the original integral.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
  3) *The integrand is well behaved on the circumference of the unit cicle in the complex plane and has poles that lie on the real axis.*
     #+begin_src latex
       \begin{equation}
         I = \int_{0}^{2 \pi} \frac{d \theta}{a + \cos{\theta}} (a > 1)
       \end{equation}
     #+end_src
     1) The limits of the integral running from $0$ to $2 \pi$ is suggestive of interpreting $\theta$ as the phase of a unimodular complex number $z = \exp{i \theta}$. The contour in this case is the circumference of the unit circle in the complex plane.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     2) Transform the integrand using a change of variables from $\theta \to z$.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     3) Verify that the resulting integrand contains only poles within the unit circle. Find their residues.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
     4) Invoke the Cauchy Residue Theorem and evaluate the integral.
        #+begin_src latex
          \begin{equation}
          \end{equation}
        #+end_src        
+ Given the value of an analytic function on a closed contour lying inside its domain of analyticity, its values inside the contour are fully determined. Given that we know the function inside the contour, we also know its $n$-th order derivative.
  #+begin_src latex
    \begin{equation}
      \frac{n!}{2 \pi i} \oint_{C \in D} \frac{f(z) dz}{(z - z_{0})^{n + 1}} = \frac{d^{n} f(z_{0})}{d z_{0}^{n}} = f^{n}(z_{0})
    \end{equation}
  #+end_src
  #+begin_comment
  The $n$-th derivative defined above is well-behaved: the integrand is nonsingular since $f$ is bounded on $C \in D$ as is the factor $\frac{1}{(z - z_{0})^{n + 1}}$ since $z$ lies on the contour while $z_{0}$ is strictly in the interior.
  #+end_comment
$R$ is called the *radius of convergence*.
*** Taylor Series for Analytic Functions
#+begin_verse
Let $a$ be a point of analyticity of an analytic function $f(z)$ and $R$ the distance to the nearest singularity. Then $f$ can be expanded in a Taylor series centered at $z = a$ for $|z - a| < R$.
#+end_verse
+ Presented above is *Taylor's theorem for analytic function*. It is derived as
** SOLVED Problem 6.1.1
CLOSED: [2022-08-25 Thu 14:23]
+ _Ask_
  *Show that the function defined below is *not* continuous at the origin.*

  \begin{equation}
    f(x,y) = \frac{x^{2}}{x^{2}+y^{2}}
  \end{equation}

  + Suppose that we approach the origin along the x-axis $y=0$. We then have

    \begin{equation}
      f(x,y) \eval{y=0} = 1
    \end{equation}

  + Suppose that we approach the origin along the y-axis $x=0$. We then have

    \begin{equation}
      f(x,y) \eval{x=0} = 0
    \end{equation}

  + Let us express the function in polar coordinates

    \begin{equation}
      x = r \cos \theta, y = r \sin \theta
    \end{equation}


\begin{equation}
  f(x,y) = f(x(r,\theta),y(r,\theta)) = \frac{(r \cos \theta)^{2}}{(r \cos \theta)^{2} + (r \sin \theta)^{2}}
\end{equation}


\begin{equation}
  f(r,\theta) = f(\theta) = \cos^{2} \theta
\end{equation}

+ _Argument_
  1) /$f(x,y) = \frac{x^{2}}{x^{2} + y^{2}} \Leftrightarrow f(\theta) = \cos^{2} \theta$/
  2) /$f(\theta)$ is independent of r/
  3) /Points on a line passing through the origin at an angle $\theta_{0}$ with the x-axis may be described in polar coordinates as y(r,\theta = \theta_{0}) = r/
  4) _The function $f$ is constant along lines passing through the origin at an angle $\theta_{0}$ with the x-axis_
+ _Argument_
  1) /By definition, the origin falls on all lines passing through the origin./
  2) /The function $f$ is constant along lines passing through the origin at an angle $\theta_{0}$ with the x-axis./
  3) _The function $f$ is multi-valued at the origin_
+ _Argument_
  1) /A function of two variables is continuous at a point if/
     1) /The function approaches a definite limit as we approach the point from any direction./
     2) /The limit coincides with the value ascribed to that point in the definition of the function./
  2) /The function $f$ is multi-valued at the origin./
  3) _The function $f$ is *not* continuous at the origin._
+ _Remark_
  The x-axis and y-axis in polar coordinates is defined by $\theta = 0$ and $\theta = \frac{\pi}{2}$ respectively. $f(\theta = 0) = 1$ and $f(\theta = \frac{\pi}{2}) = 0$ - this is same as our earlier calculations using cartesian coordinates.
+ _Ask_
  *For the same function, what happens at some other point, say $(x=1, y=1)$?*
  + _Consistency_
    First note that a single line passes through both the origin and $(x=1, y=1)$. This line is defined by $y(r,\theta = \frac{\pi}{4}) = r$. The value of $f$ at $(x=1, y=1)$, $f(x=1,y=1) = \frac{1}{2}$, coincides with the value along the line y(r,\theta = \frac{\pi}{4}) - $f(\theta = \frac{\pi}{4}) = \frac{1}{2}$.
  + _Argument_
    + _Definition_
      Consider an arbitrary line passing through $(x=1, y=1)$, making an intercept $c$ with the y-axis. Such a line maybe written as $y(x) = \frac{1-c}{1-0} x + c$. For points along this line, the function $f$ is
      #+begin_src latex
        \begin{equation}
          f(x,y) \eval{y= (1-c) x + c} = \frac{x^{2}}{x^{2} + ((1-c)x + c)^{2}} = \frac{x^{2}}{x^{2} + (1-c)^{2} x^{2} + c^{2} + 2 c (1-c) x}
        \end{equation}
      #+end_src
    + _Simplify_
    #+begin_src latex
      \begin{equation}
        f(x,y) \eval{y= (1-c) x + c} = \frac{x^{2}}{2 x^{2} + (x^{2} + 1 - 2 x) c^{2} - (2 x^{2} + 2 x) c}
      \end{equation}
    #+end_src
    + _Limit_
      We evaluate the limit $\lim{x \to 1} f(x,y) \eval{y= (1-c) x + c}$ by simply substituting $x=1$ in the right hand side of the previous equation.
      #+begin_src latex
        \begin{equation}
          \lim{x \to 1} f(x,y) \eval{y= (1-c) x + c} = \frac{x^{2}}{2 x^{2} + (x^{2} + 1 - 2 x) c^{2} - (2 x^{2} + 2 x) c} \eval{x=1} = \frac{1}{2}
        \end{equation}
      #+end_src
    + _Result_
      For an arbitrary line passing through $(x=1, y=1)$, $\lim{x \to 1} f(x,y) \eval{y= (1-c) x + c} = \frac{1}{2}$.
  + _Argument_
    1) /For an arbitrary line passing through $(x=1, y=1)$, making an intercept $c$ with the y-axis, $\lim{x \to 1} f(x,y) \eval{y= (1-c) x + c} = \frac{1}{2}$./
    2) /$f(x=1,y=1) = \frac{1}{2}$./
    3) /A function of two variables is continuous at a point if/
       1) /The function approaches a definite limit as we approach the point from any direction./
       2) /The limit coincides with the value ascribed to that point in the definition of the function./
    4) _The function $f(x,y)$ is continuous at $(x = 1, y = 1$._
** SOLVED Problem 6.1.2
CLOSED: [2022-08-25 Thu 14:43]
+ _Ask_
  *Find the equations obeyed by an anti-analytic function $f(x,y)$ (depends only on $x-iy$).*
  + _Definition_
    Consider a function of two variables $f(x,y) = u + i v$. 
  + _Assumption_
    $f(x,y)$ is *anti-analytic*, i.e., $x$ and $y$ enter $f$ only via $z = x - iy$.
  + _Manipulation_
    + Keeping $y$ fixed, suppose that we change $x$ by $dx$. The change in the value of the function $df$ is
    #+begin_src latex
      \begin{equation}
        df = \frac{df}{dz} dz = \frac{df}{dz} \frac{\partial z}{\partial x} dx = \frac{df}{dz} dx
      \end{equation}
    #+end_src
    #+begin_src latex
      \frac{df}{dx} = \frac{df}{dz}
    #+end_src
    + Keeping $x$ fixed, suppose that we change $y$ by $dy$. The change in the value of the function $df$ is
    #+begin_src latex
      \begin{equation}
        df = \frac{df}{dz} dz = \frac{df}{dz} \frac{\partial z}{\partial y} dy = \frac{df}{dz} (-i) dy
      \end{equation}
    #+end_src
    #+begin_src latex
      \frac{df}{dy} = - i \frac{df}{dz}
    #+end_src
  + _Argument_
    1) /\frac{df}{dx} = \frac{df}{dz}/
    2) /\frac{df}{dy} = - i \frac{df}{dz}/
    3) _The partial derivatives of $f$ with respect to $x$ and $y$ are related as_
       #+begin_src latex
         \begin{equation}
           f_{y} = - i f_{x}
         \end{equation}
       #+end_src
  + _Substitution_
    Substitute /$f(x,y) = u + i v$ in $f_{y} = - i f_{x}$.
    #+begin_src latex
      \begin{equation}
        u_{y} + i v_{y} = - i (u_{x} + i v_{x})
      \end{equation}
    #+end_src
  + _Simplification_
    #+begin_src latex
      \begin{equation}
        u_{y} + i v_{y} = v_{x} - i u_{x}
      \end{equation}
    #+end_src
  + _Result_
    These are the Cauchy-Riemann Equations for an /anti-analytic function $f(x,y) = u + i v$/.
    #+begin_src latex
      \begin{equation}
        u_{x} = - v_{y}
        u_{y} =   v_{x}
      \end{equation}
    #+end_src
** SOLVED Problem 6.1.3
CLOSED: [2022-08-25 Thu 15:08]
+ _Ask_
  *For the real function $f(x,y)$ find the condition which ensures that it only depends on $x_{+} = x + y$*
  + _Definition_
    Consider a /real/ function of two variables $f(x,y)$.
  + _Assumption_
    $f$ depends only on $x_{+} = x + y$.
  + _Manipulation_
    + Keeping $y$ fixed, suppose that we change $x$ by $dx$. The change in the value of the function $df$ is
    #+begin_src latex
      \begin{equation}
        df = \frac{df}{dx_{+}} dx_{+} = \frac{df}{dx_{+}} \frac{\partial x_{+}}{\partial x} dx = \frac{df}{dx_{+}} dx
      \end{equation}
    #+end_src
    #+begin_src latex
      \frac{df}{dx} = \frac{df}{dx_{+}}
    #+end_src
    + Keeping $x$ fixed, suppose that we change $y$ by $dy$. The change in the value of the function $df$ is
    #+begin_src latex
      \begin{equation}
        df = \frac{df}{dx_{+}} dx_{+} = \frac{df}{dx_{+}} \frac{\partial x_{+}}{\partial y} dy = \frac{df}{dx_{+}} dy
      \end{equation}
    #+end_src
    #+begin_src latex
      \frac{df}{dy} = \frac{df}{dx_{+}}
    #+end_src
  + _Argument_
    1) /\frac{df}{dx} = \frac{df}{dx_{+}}/
    2) /\frac{df}{dy} = \frac{df}{dx_{+}}/
    3) _The partial derivatives of $f$ with respect to $x$ and $y$ are related as_
       #+begin_src latex
         \begin{equation}
           f_{y} = f_{x}
         \end{equation}
       #+end_src
  + _Result_
    The condition that ensures that the real function $f(x,y)$ only depends on $x_{+} = x + y$ is
    #+begin_src latex
      \begin{equation}
        f_{y} = f_{x}
      \end{equation}
    #+end_src
** SOLVED Problem 6.1.4
CLOSED: [2022-08-25 Thu 15:23]
+ _Ask_
  *Verify that $f(x,y) = \exp{x} \cos y + i \exp{x} \sin y$ obeys the CRE.*
  + _Definitions_
    Let $u(x,y) = \exp{x} \cos y$ and $v(x,y) = \exp{x} \sin y$ such that $f(x,y) = u + i v$.
  + _Calculation_
    #+begin_src latex
      \begin{equation}
        u_{x} = \exp{x} \cos y
      \end{equation}
    #+end_src
    #+begin_src latex
      \begin{equation}
        u_{y} = - \exp{x} \sin y
      \end{equation}
    #+end_src
    #+begin_src latex
      \begin{equation}
        v_{y} = \exp{x} \cos{y}
      \end{equation}
    #+end_src
    #+begin_src latex
      \begin{equation}
        - v_{x} = - \exp{x} \sin y
      \end{equation}
    #+end_src
  + _Result_
    $f(x,y) = \exp{x} \cos y + i \exp{x} \sin y$ obeys the CRE
    #+begin_src latex
      \begin{equation}
        u_{x} =   v_{y}
        u_{y} = - v_{x}
      \end{equation}
    #+end_src
** SOLVED Problem 6.1.5
CLOSED: [2022-08-25 Thu 15:34]
+ _Ask_
  *Show that $f = x^{3} - 3 x^{2} y + i (3 x y^{2} - y^{3})$ is analytic by any means.*
  + _Identification_
    #+begin_src latex
      \begin{equation}
        f(x,y) = x^{3} - 3 x^{2} y + i (3 x y^{2} - y^{3}) = (x + i y)^{3} = f(x + i y)
      \end{equation}
    #+end_src
  + _Argument_
    1) /$g(x,y)$ is an analytic function if $g(x,y) = g(x + i y)$./
    2) /$f(x,y) = f(x + i y)$./
    3) _f(x,y) is an analytic function._
  + _Result_
    $f(x,y) = x^{3} - 3 x^{2} y + i (3 x y^{2} - y^{3}) = (x + i y)^{3} = f(x + i y)$ is an analytic function.
** SOLVED Problem 6.1.6
CLOSED: [2022-08-25 Thu 15:45]
+ _Ask_
  *Show by any means that $f = x^{2} + y^{2}$ is not analytic.*
  + _Calculation_
    #+begin_src latex
      \begin{equation}
        f_{y} = 2 y
      \end{equation}
    #+end_src
    #+begin_src latex
      \begin{equation}
        i f_{x} = 2 i x
      \end{equation}
    #+end_src
  + _Argument_
    1) /If a function $g(x,y)$ is analytic, it obeys the CRE $g_{y} = i g_{x}$./
    2) /$f_{y} \neq i f_{x}$./
    3) _f = x^{2} + y^{2} is not analytic._
** SOLVED Problem 6.1.7
CLOSED: [2022-08-25 Thu 20:19]
+ _Ask_
  *Locate and name the singularities of $f = \frac{1}{1 + z^{4}}$.*
  /(f)/ is singular when
  #+begin_src latex
    \begin{equation}
      z^{4} = - 1
    \end{equation}
  #+end_src
  + _Manipulation_
    Start with
    #+begin_src latex
      \begin{equation}
        z^{4} = - 1
      \end{equation}
    #+end_src
    Taking square root of both sides
    #+begin_src latex
      \begin{equation}
        z^{2} = \sqrt{- 1} = i
      \end{equation}
    #+end_src
    #+begin_src latex
      \begin{equation}
        z = \pm \sqrt{i}
      \end{equation}
    #+end_src
  We will now derive an expression for $\sqrt{i}$.
  + _Manipulation_
    Let $\sqrt{i}$ be a complex number of the form $a + i b$.
    #+begin_src latex
      \begin{equation}
        \sqrt{i} = a + i b
      \end{equation}
    #+end_src
    Squaring both sides and simplifying
    #+begin_src latex
      \begin{equation}
        i = (a + i b)^{2} = a^{2} - b^{2} + 2 i a b
      \end{equation}
    #+end_src
    Matching real and imaginary parts on both sides we get the following equations
    #+begin_src latex
      \begin{equation}
        2 a b = 1
      \end{equation}
      \begin{equation}
        a^{2} - b^{2} = (a + b)(a - b) = 0
      \end{equation}
    #+end_src
    The /two/ solutions for the above system are:
    #+begin_src latex
      \begin{equation}
        \sqrt{i} = \frac{1}{\sqrt{2}} \pm i \frac{1}{\sqrt{2}}
      \end{equation}
    #+end_src
  + _Substitution_
    Using $\sqrt{i} = \frac{1}{\sqrt{2}} \pm i \frac{1}{\sqrt{2}}$ in $z = \pm \sqrt{i}$
    #+begin_src latex
      \begin{equation}
        z = \pm (\frac{1}{\sqrt{2}} \pm i \frac{1}{\sqrt{2}})
      \end{equation}
    #+end_src
  + _Result_
    $f$ is singular at the following isolated points
    #+begin_src latex
      \begin{equation}
        z_{1} =   \frac{1}{\sqrt{2}} + i \frac{1}{\sqrt{2}} \\
        z_{2} =   \frac{1}{\sqrt{2}} - i \frac{1}{\sqrt{2}} \\
        z_{3} = - \frac{1}{\sqrt{2}} + i \frac{1}{\sqrt{2}} \\
        z_{4} = - \frac{1}{\sqrt{2}} - i \frac{1}{\sqrt{2}}
      \end{equation}
    #+end_src
    They are all /poles of order 4/.
+ _Ask_
  *Locate and name the singularities of $f = \frac{1}{(z^{4} + 2 z^{2} + 1)}$.*
  /(f)/ is singular when
  #+begin_src latex
    \begin{equation}
      z^{4} + 2 z^{2} + 1 = 0
    \end{equation}
  #+end_src
  + _Manipulation_
    Start with
    #+begin_src latex
      \begin{equation}
        z^{4} + 2 z^{2} + 1 = 0
      \end{equation}
    #+end_src
    Introduce $w = z^{2}$ and rewrite the equation above
    #+begin_src latex
      \begin{equation}
        w^{2} + 2 w + 1 = (w+1)^{2} = 0
      \end{equation}
    #+end_src
    Solving for w
    #+begin_src latex
      \begin{equation}
        w = - 1
      \end{equation}
    #+end_src
  + _Argument_
    1) /$z^{2} = w$./
    2) /$w = - 1$./
    3) _$z^{2} = - 1$_
  + _Result_
    $f$ is singular at
    #+begin_src latex
      \begin{equation}
        z_{1} =   i \\
        z_{2} = - i
      \end{equation}
    #+end_src
    They are all /poles of order 2/.
** SOLVED Problem 6.1.8
CLOSED: [2022-08-31 Wed 20:20]
*Given $z = r \exp{i \theta}$ show that $dz = (dr + i r d\theta) \exp{i\theta}$. Interpret the two factors in $dz$, in particular the role of the exponential factor. Equating the derivatives in the radial and angular directions, find the CRE in terms of $u_{r}$, $u_{\theta}$, $v_{r}$, and $v_{\theta}$. Begin now with $f(r,\theta) = f(r \exp{i \theta})$ as the definition of analyticity, relate $r$ and $\theta$ derivatives of $f$, and regain the CRE in polar form.*
Given $z = r \exp{i \theta}$
#+begin_src latex
  \begin{equation}
    dz = \frac{\partial z}{\partial r} dr + \frac{\partial z}{\partial \theta} d\theta
  \end{equation},
#+end_src
#+begin_src latex
  \begin{equation}
    dz = \exp{i \theta} dr + i r \exp{i \theta} d\theta = (dr + i r d\theta) \exp{i \theta}
  \end{equation}.
#+end_src
The factor $\exp{i \theta}$ is the polar representation of a *unimodular complex number /subtending the same angle $\theta$ with the real axis as $z$/*. The factor $(dr + i d\theta)$ is the cartesian representation of a complex number. The real part of the complex number $(dr + i d\theta)$, on multiplication with the polar representation of a unimodular complex number subtending the same angle $\theta$ with the real axis as $z$, gives the *component of the increment $dz$ in the radial direction*. The imaginary part of the complex number $(dr + i d\theta)$, on multiplication with the polar representation of a unimodular complex number subtending the same angle $\theta$ with the real axis as $z$, gives the *component of the increment $dz$ in the angular direction*.
Consider an analytic function $f(r, \theta) = f(z) = f(r \exp{i \theta})$ of a complex variable whose real and imaginary parts are written in polar form $f(r, \theta) = u(r, \theta) + i v(r, \theta)$.
Keeping $\theta$ fixed, suppose that we effect an increment $dr$ in the /radial direction/. The change in the function $df$ is
#+begin_src latex
  \begin{equation}
    df = \frac{df}{dz} dz = \frac{df}{dz} \frac{\partial z}{\partial r} dr = \frac{df}{dz} \exp{i \theta} dr
  \end{equation},
#+end_src
#+begin_src latex
  \begin{equation}
    \frac{\partial f}{\partial r} = \frac{df}{dz} \exp{i \theta}
  \end{equation}.
#+end_src
Keeping $r$ fixed, suppose that we effect an increment $d\theta$ in the /angular direction/. The change in the function $df$ is
#+begin_src latex
  \begin{equation}
    df = \frac{df}{dz} dz = \frac{df}{dz} \frac{\partial z}{\partial \theta} d\theta = i \frac{df}{dz} r \exp{i \theta} d\theta
  \end{equation},
#+end_src
#+begin_src latex
  \begin{equation}
    \frac{\partial f}{\partial \theta} = i \frac{df}{dz} r \exp{i \theta}
  \end{equation}.
#+end_src
The partial derivatives of $f$ with respect to $r$ and $\theta$ are thus related as
#+begin_src latex
  \begin{equation}
    \frac{\partial f}{\partial \theta} = i r \frac{\partial f}{\partial r}
  \end{equation}.
#+end_src
Substitute /$f = u + i v$ in $f_{\theta} = i r f_{r}$
#+begin_src latex
  \begin{equation}
    u_{\theta} + i v_{\theta} = i r (u_{r} + i v_{r})
  \end{equation}.
#+end_src
These are the /polar form/ of Cauchy-Riemann Equations for an /analytic function $f(r, \theta) = u(r, \theta) + i v(r, \theta)$/
#+begin_src latex
  \begin{equation}
    u_{r} = \frac{1}{r} v_{\theta}
    u_{\theta} = - r v_{r}
  \end{equation}.
#+end_src
** SOLVED Problem 6.1.9
CLOSED: [2022-08-31 Wed 20:36]
*Confirm the analyticity of:*
#+begin_src latex
  \begin{equation}
    f(x, y) = \frac{x - i y}{x^2 + y^2}
  \end{equation}
#+end_src
Reexpress $f(x, y)$ as
#+begin_src latex
  \begin{equation}
    f(x, y) = \frac{x - i y}{x^2 + y^2} = \frac{x - i y}{(x + i y)(x - i y)} = \frac{1}{x + i y} = f(x + i y)
  \end{equation}
#+end_src
$f(x, y)$ is analytic.
*Confirm the analyticity of:*
#+begin_src latex
  \begin{equation}
    f(x, y) = \sin x \cosh y + i \cos x \sinh y
  \end{equation}
#+end_src
Substitute $\cosh y = \cos{i y}$  and identify $i \sinh y$ as $\sin{i y}$ to reexpress $f(x, y)$ as
#+begin_src latex
  \begin{equation}
    f(x, y) = \sin x \cos{i y} + \cos x \sin{i y}
  \end{equation}
#+end_src
Identify $\sin x \cos{i y} + \cos x \sin{i y}$ as $\sin{x + i y}$ to reexpress $f(x, y)$ as
#+begin_src latex
  \begin{equation}
    f(x, y) = \sin{x + i y} = f(x + i y)
  \end{equation}
#+end_src
$f(x, y)$ is analytic.
*

** SOLVED Problem 6.1.10
CLOSED: [2022-08-31 Wed 23:11]
*Consider $df$, the first order change in response to a change $dx$ and $dy$ in the coordinates, of a function $f = u + i v$ where $u$ and $v$ have continuous derivatives which do not however obey the CRE. The shift in $x$ and $y$ corresponds to changing $z$ by $dz = dx + i dy$ and $z^* by dz^* = dx - i dy$. In other words, as we move in the complex plane labeled by $x$, $y$, we change both $z$ and $z^*$. Show that $df$ generally has parts proportional to both $dz$ and $dz^*$ by reexpressing $dx$ and $dy$ in terms of the former. Show that as a result the symbol $\frac{df}{dz}$ makes no sense in general: it is like trying to define $df(x, y)/dx$ for a function of two variables, when all one can define is the aprtial derivative. If however, the function of $x$ and $y$ happened to have no dependence on $y$, we could define the total derivative with respect to $x$. That is what is happening with analytic functions $f$ which depend only on $z$.*
The first order change $df$ is
#+begin_src latex
  \begin{equation}
    df = \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy
  \end{equation}.
#+end_src
Because $z = x + i y$ and $z^* = x - i y$
#+begin_src latex
  \begin{equation}
    x = \frac{z + z^*}{2}
  \end{equation},
#+end_src
#+begin_src latex
  \begin{equation}
    y = \frac{z - z^*}{2 i}
  \end{equation}.
#+end_src
It follows that
#+begin_src latex
  \begin{equation}
    dx = \frac{\partial x}{\partial z} dz + \frac{\partial x}{\partial z^*} dz^* = \frac{dz + dz^*}{2}
  \end{equation},
#+end_src
#+begin_src latex
  \begin{equation}
    dy = \frac{\partial y}{\partial z} dz + \frac{\partial y}{\partial z^*} dz^* = \frac{dz - dz^*}{2 i}
  \end{equation}.
#+end_src
Substituting the expressions above for $dx$ and $dy$ in the expression for $df$
#+begin_src latex
  \begin{equation}
    df = \frac{1}{2} \long [ \long (\frac{\partial f}{\partial x} + \frac{1}{i} \frac{\partial f}{\partial y} \long ) dz + \long (\frac{\partial f}{\partial x} - \frac{1}{i} \frac{\partial f}{\partial y} \long) dz^* \long ]
  \end{equation}.
#+end_src
Clearly, $df$ generally has parts proportional to both $dz$ and $dz^*$. Therefore, the symbol $\frac{df}{dz}$ makes no sense in general.
** SOLVED Problem 6.1.11
CLOSED: [2022-08-31 Wed 23:26]
*Prove that $u$ and $v$ are harmonic given the CRE.*
Begin with the CRE (given)
#+begin_src latex
  \begin{equation}
    u_{x} =   v_{y}
    u_{y} = - v_{x}
  \end{equation}.
#+end_src
/Functions satisfying the CRE have continuous partial derivatives of arbitrary order./ We take the partial derivative of $u_{x}$ and $v_{y}$ with respect to $x$. Similarly, we take the partial derivative of $u_{y}$ and $- v_{x}$ with respect to $y$ to obtain
#+begin_src latex
  \begin{equation}
    u_{xx} = v_{xy}
    u_{yy} = - v_{yx}
  \end{equation}.
#+end_src
Adding the two equations above
#+begin_src latex
  \begin{equation}
    u_{xx} + u_{yy} = v_{xy} - v_{yx}
  \end{equation}.
#+end_src
_Assuming the equality of mixed partial derivatives $v_{xy} = v_{yx}$_
#+begin_src latex
  \begin{equation}
    u_{xx} + u_{yy} = 0
  \end{equation}.
#+end_src
The equation above is called the *Laplace equation*. The solution of the Laplace equation is a *harmonic function*. Therefore $u$ is a harmonic function. By taking the partial derivative of $v_{y}$ and $u_{x}$ with respect to $y$ and the partial derivative of $u_{y}$ and $v_{x}$ with respect to $x$ and retracing the steps above we may easily show that
#+begin_src latex
  \begin{equation}
    v_{xx} + v_{yy} = 0
  \end{equation}.
#+end_src
Therefore, $v$ as well is a harmonic function.
** SOLVED Problem 6.1.12
CLOSED: [2022-09-01 Thu 00:04]
*You are given that $u = x^{3} - 3 x y^{2}$ is the real part of an analytic function. (i) Find $v$. Reconstruct $f(z)$. Verify that $u$ and $v$ are harmonic. (ii) Repeat for the case $v = \exp{- y} \sin{x}$. (iii) You are given that $u = x^{3} - 3 x^{2} y$ and asked to find $v$. You run into real problems. Why?*
Because $u(x, y)$ is the real part of an analytic function, the partial derivatives of $v(x, y)$ with respect to $x$ and $y$, by appeal to CRE, are respectively
#+begin_src latex
  \begin{equation}
    v_{x} = - u_{y} = 6 x y
  \end{equation},
#+end_src
#+begin_src latex
  \begin{equation}
    v_{y} = u_{x} = 3 x^2 - 3 y^2
  \end{equation}.
#+end_src
By inspection we guess $v$ to be
#+begin_src latex
  \begin{equation}
    v(x, y) = 3 x^2 y - y^3
  \end{equation}.
#+end_src
Using $u(x, y)$ and $v(x, y)$, $f(x, y) = u + i v$ is reconstructed as
#+begin_src latex
  \begin{equation}
    f(x, y) = x^3 - 3 x y^2 + i (3 x^2 y - y^3)
  \end{equation}.
#+end_src
Turning to the case where the imaginary part of an analytic function $v = \exp{- y} \sin{x}$ is given, the partial derivatives of $u(x, y)$ with respect to $x$ and $y$, by appeal to CRE, are respectively
#+begin_src latex
  \begin{equation}
    u_{x} = - \exp{- y} \sin{x}
  \end{equation},
#+end_src
#+begin_src latex
  \begin{equation}
    u_{y} = - \exp{- y} \cos{x}
  \end{equation}.
#+end_src
By inspection we guess $u$ to be
#+begin_src latex
  \begin{equation}
    u(x, y) = \exp{- y} \cos{x}
  \end{equation}.
#+end_src
Using $u(x, y)$ and $v(x, y)$, $f(x, y) = u + i v$ is reconstructed as
#+begin_src latex
  \begin{equation}
    f(x, y) = \exp{- y} \cos{x} + i (\exp{- y} \sin{x})
  \end{equation}.
#+end_src
We now turn to the case where the real part of a function $u(x, y) - x^3 - 3 x^2 y$. Suppose that the function, say $f$, is analytic. Then, by appeal to CRE, we must have
#+begin_src latex
  \begin{equation}
    v_{x} = 3 x^2
  \end{equation},
#+end_src
#+begin_src latex
  \begin{equation}
    v_{y} = 3 x^2 - 6 x y
  \end{equation}.
#+end_src
We run into /real/ problems because the real part $u(x, y)$ implies, via CRE, partial derivatives of $v$ that are unobtainable without violating consistency: inspection of the expression for $v_{y}$ suggests, unambiguously, a form for $v$ that is inconsistent with the expression for $v_{x}$. The form is
#+begin_src latex
  \begin{equation}
    v(x, y) = 3 x^2 y - 3 x y^2
  \end{equation}.
#+end_src
Our assumption that $f$ is analytic must therefore be unwarranted.
** SOLVED Problem 6.1.13
CLOSED: [2022-09-01 Thu 01:54]
Prove that
#+begin_src latex
  \begin{equation}
    \long ( \frac{\partial^{2}}{\partial x^{2}} + \frac{\partial^{2}}{\partial y^{2}} \long ) |f(z)|^{2} = 4 |f'(z)|^{2}
  \end{equation}.
#+end_src
Reexpress the left hand side of the equation above as
#+begin_src latex
  \begin{equation}
    \long ( \frac{\partial^{2}}{\partial x^{2}} + \frac{\partial^{2}}{\partial y^{2}} \long ) |f(z)|^{2} = \frac{\partial^2 f f^*}{\partial x^2} + \frac{\partial^2 f f^*}{\partial y^2}
  \end{equation}.
#+end_src
Effect the first partial derivative with respect to $x$ and $y$ to obtain
#+begin_src latex
  \begin{equation}
    \long ( \frac{\partial^{2}}{\partial x^{2}} + \frac{\partial^{2}}{\partial y^{2}} \long ) |f(z)|^{2} = \frac{\partial (f f^*_{x} + f^* f_{x})}{\partial x} + \frac{\partial (f f^*_{y} + f^* f_{y})}{\partial y}
  \end{equation}.
#+end_src
Identify $f_{x} = f'$, $f^*_{x} = f'^*$, $f_{y} = i f'$, and $f^*_{y} = - i f'^*$. Further, use the linearity of the partial derivative to obtain
#+begin_src latex
  \begin{equation}
    \long ( \frac{\partial^{2}}{\partial x^{2}} + \frac{\partial^{2}}{\partial y^{2}} \long ) |f(z)|^{2}
    &= f'^* f_{x} + f' f^*_{x} + i \long(- f'^* f_{y} + f' f^*_{y} \long)
  \end{equation}.
#+end_src
Identify $f_{x} = f'$, $f^*_{x} = f'^*$, $f_{y} = i f'$, and $f^*_{y} = - i f'^*$ to obtain
#+begin_src latex
  \begin{equation}
    \long ( \frac{\partial^{2}}{\partial x^{2}} + \frac{\partial^{2}}{\partial y^{2}} \long ) |f(z)|^{2}
    &= f'^* f' + f' f'^* + i \long(- f'^* (i f') + f' (- i f'^*) \long) \\
    &= f'^* f' + f' f'^* + f'^* f' + f' f'^* \\
    &= 4 |f'(z)|^2
  \end{equation}.
#+end_src
** SOLVED Problem 6.1.14
CLOSED: [2022-09-01 Thu 04:30]
*Given that $(u,v)$ form a harmonic pair, show without brute force that $2 u v$ and $u^{2} - v^{2}$ are also such a pair.*
Because $(u, v)$ form a harmonic pair, $u$ and $v$ obey the CRE
#+begin_src latex
  \begin{equation}
    u_{x} = v_{y}
  \end{equation}
  \begin{equation}
    v_{x} = - u_{y}
  \end{equation}
#+end_src
Take the difference of the first and second equation after multiplication with $2 u$ and $2 v$ respectively to obtain
#+begin_src latex
  \begin{equation}
    2 u u_{x} - 2 v v_{x} = 2 u v_{y} + 2 v u_{y}
  \end{equation}
#+end_src
Identify $2 u u_{x} - 2 v v_{x}$ as $(u^2 - v^2)_{x}$ and $2 u v_{y} + 2 v u_{y}$ as $(2 u v)_{y}$ to obtain
#+begin_src latex
  \begin{equation}
    (u^2 - v^2)_{x} = (2 u v)_{y}
  \end{equation}
#+end_src
Now, sum both sides of the first and second equation after multiplication with $2 v$ and $2 u$ respectively to obtain
#+begin_src latex
  \begin{equation}
    2 v v_{y} - 2 u u_{y} = 2 u v_{x} + 2 v u_{x}
  \end{equation}
#+end_src
Identify $2 v v_{y} - 2 u u_{y}$ as $(v^2 - u^2)_{y}$ and $2 v u_{x} + 2 u v_{x}$ as $(2 u v)_{x}$ to obtain
#+begin_src latex
  \begin{equation}
    (u^2 - v^2)_{y} = - (2 u v)_{x}
  \end{equation}
#+end_src
We thus have
#+begin_src latex
  \begin{equation}
    (u^2 - v^2)_{x} = (2 u v)_{y}
  \end{equation}
  \begin{equation}
    (u^2 - v^2)_{y} = - (2 u v)_{x}
  \end{equation}
#+end_src
Because $u^2 - v^2$ and $2 u v$ satisfy the CRE, they form a harmonic pair.
***

** TOSOLVE Problem 6.2.1
*Show why the absolute value of a sum of complex numbers is bounded by the sum of their absolute values. You may do this algebraically (in which case it is better to square both sides of the inequality) or by graphical arguments, i.e., by viewing the sum as addition of arrows in the complex plane. In the first approach you may want to start with a sum with just two numbers if you are having trouble.*
** TOSOLVE Problem 6.2.2
*Show that the hyperbolic functions obey $\cosh^{z} - \sinh^{z} = 1$ by writing the functions in terms of exponentials.* 
** TOSOLVE Problem 6.2.3
*Argue that the old formula for $\sin (z_{1} + z_{2})$ must be valid for complex arguments by writing $\sin z$ in terms of exponentials. Find the real part, imaginary part, and modulus of $\sin (x + i y)$. Where does the function vanish in the complex plane?*
** TOSOLVE Problem 6.2.4
*Locate the zeros in the complex plane of $\sin z, \cos z, \sinh z, and \cosh z$. A suggestion: work with the exponential function as far as possible.*
** TOSOLVE Problem 6.2.5
*Show that $|\sin z|^{2} = \sin^{2} x + \sinh^{2} y$ and $|\cos z|^{2} = \cos^{2} x + \sinh^{2} y$. What does this tell you about the zero's of $\sin z$?*
** TOSOLVE Problem 6.2.6
*Show that the solutions to $\sin z = 5$ are $z = \frac{\pi}{2} + 2 n \pi + i \cosh^{-1} 5$.*
** TOSOLVE Problem 6.2.7
*Verify $\exp{\ln{1 + z}} = 1 + z$ to order $z^{3}$ by using the respective series.*
** TOSOLVE Problem 6.2.8
*Verify that the cubes of the roots of 8 (in cartesian form) in the preceding equation indeed give 8.*
** TOSOLVE Problem 6.2.9
*What do the N roots of unity look like? Write down explicit cartesian forms for the sixth roots. What is the sum of all the six roots?*
** TOSOLVE Problem 6.2.10
*Find the roots of $z^{2} - (15 + 9 i) z + (16 + 63 i)$. (/Hint: the radical contains a perfect square./)*
** TOSOLVE Problem 6.2.11
*Write down the two square roots of (i) $i$ and (ii) 3 + 4 i in cartesian form.*
** TOSOLVE Problem 6.2.12
*Find all the roots of $\exp{z} + 2 = 0$.*
** TOSOLVE Problem 6.2.13
*Express the following in cartesian form: (i) 3^{i}, (ii) \ln{\frac{1 + i}{1 - i}}. First express in polar form and take the square roots of: (iii) (3 + 4 i)(4 + 3 i), (iv) \frac{1 + i \sqrt{3}}{1 - i \sqrt{3}}*
** TOSOLVE Problem 6.2.14
*(i) Find the roots of $z^{4} + 2 z^{2} + 1 = 0$. How many roots do you expect? (ii) Repeat with $z^{6} + 2 z^{3} + 1$.*
** TOSOLVE Problem 6.2.15
*Write $1 + i$ in polar form and find its cube roots in cartesian form. Check that their cubes do give the original number.*
** TOSOLVE Problem 6.2.16
*Give all the values of $i^{i}$.*
** SOLVED Problem 6.3.1
*Verify that the derivative of the series for $ln(1 + z)$ gives the series for $1/(1 + z)$. Verify that the derivative of the $\sin$ is the $\cos$ by looking at the series. Make sure $R$ is the same for the function and the derivative.*

\begin{equation*}
\ln \left( 1 + z \right) = \sum_{1}^{\infty} (-1)^{n+1} \dfrac{z^{n}}{n}, \quad R = \left \Big \lvert \dfrac{a_{n}}{a_{n+1}} \right \Big \rvert = 1.
\end{equation*}

\begin{equation*}
\dfrac{d}{dz} \left( \sum_{1}^{\infty} (-1)^{n+1} \dfrac{z^{n}}{n}  \right) = \sum_{1}^{\infty} (-1)^{n + 1} z^{n-1} = \sum_{0}^{\infty} \left( \dfrac{d^{n}}{dz^{n}} (1 + z)^{-1}  \Bigg \vert_{z = 0} \right) \dfrac{z^{n}}{n!} = \dfrac{1}{1+z}, \quad R = 1.
\end{equation*}

\begin{equation*}
\sin z = \sum_{0}^{\infty} (-1)^{n} \dfrac{z^{2n+1}}{(2n + 1)!}, \quad R = \infty
\end{equation*}

\begin{equation*}
\dfrac{d}{dz} \left( \sum_{0}^{\infty} (-1)^{n} \dfrac{z^{2n+1}}{(2n + 1)!} \right) = \sum_{0}^{\infty} (-1)^{n} \dfrac{z^{2n}}{2n!} = \cos z, \quad R = \infty.
\end{equation*}
** SOLVED Problem 6.3.2
*Calculate the integral of $f = (x + i y)^{2}$ around a unit square with its lower left corner at the origin and obtain zero as you must.*

#+begin_src gnuplot :file 6.3.2.png :export none :eval never-export
  set terminal pngcairo  background "#ffffff" enhanced crop font "Arial, 25" fontscale 1.0 size 800, 800 
  set output '6.3.2.png'

  set title 'f(x,y) = (x + i y)^2'
  unset key # removes legend
  set view map # projects image onto 2D plane
  set size square # sets aspect ratio of plot to be square
  set xrange [ - 0.5 : 1.5 ]; set yrange [ - 0.5 : 1.5 ]
  set cbrange [ -3.14 : 3.14 ]

  set xtics ("0.00" 0.00, "1" 1.00)
  set ytics ("0.00" 0.00, "1" 1.00)
  set cbtics ("0" -3.14, '2' 3.14)
  set xlabel "Re(z)"
  set ylabel "Im(z)"
  set cblabel "Phase Angle" 
  set cblabel  offset character -2, 0, 0 font "" textcolor lt -1 rotate

  set palette model HSV
  set palette defined ( 0 0 1 1, 1 1 1 1 )

  set isosamples 3000, 3000
  set samples 3000, 3000
  thresh = 0.1

  r(x,y) = sqrt(x*x + y*y)
  theta(x,y) = atan2(y,x)
  z(x,y) = r(x,y)*exp(theta(x,y)*sqrt(-1))
  f(z) = z*z
  imaginary_f(z) = imag(f(z))
  real_f(z) = real(f(z))

  hue(x,y) = (pi + atan2(-y,-x)) / (2*pi)

  saturation(x,y) = 0.5 + 0.5*(abs(f(z(x,y)))-floor(abs(f(z(x,y)))))
  gridlines(x,y) = (abs(sin(real_f(z(x,y))*pi)**thresh) * abs(sin(imaginary_f(z(x,y))*pi))**thresh)

  color(x,y) = hsv2rgb(hue(real_f(z(x,y)), imaginary_f(z(x,y))), saturation(x,y), gridlines(x,y))
  save_encoding = "utf8"

  splot '++' using 1:2:(color($1,$2)) with pm3d lc rgb variable nocontour  
#+end_src

\begin{align*}
\oint_{C} dz f(z) &= \int_{x,y=0}^{x=1,y=0} \left[ dx + i dy \right] \left( x + iy \right)^{2}
+ \int_{x=1, y=0}^{x , y = 1} \left[ dx + i dy \right] \left( x + iy \right)^{2}
+ \int_{x, y = 1}^{x = 0, y = 1} \left[ dx + i dy \right] \left( x + iy \right)^{2}
+ \int_{x = 0, y = 1}^{x, y = 0} \left[ dx + i dy \right] \left( x + iy \right)^{2} \\
&= \int_{0}^{1} dx \thinspace x^{2}
+ i\int_{0}^{1}  dy \thinspace \left( 1 + iy \right)^{2}
+ \int_{1}^{0} dx \thinspace \left( x + i \right)^{2}
- i \int_{1}^{0} dy \thinspace y^{2} \\
&= \int_{0}^{1} dx \thinspace x^{2}
+ i\int_{0}^{1}  dy \thinspace \left( 1 - y^{2} + 2 i y \right)
+ \int_{0}^{1} dx \thinspace \left( 1 - x^{2} - 2 i x \right)
+ i \int_{0}^{1} dy \thinspace y^{2} \\
&= \left[ \int_{0}^{1} dx \thinspace x^{2} \right]
+ \left[ i\int_{0}^{1}  dy \right]
- \left[ i\int_{0}^{1}  dy \thinspace y^{2} \right]
- \left[ 2\int_{0}^{1}  dy \thinspace y \right]
+ \left[ \int_{0}^{1} dx  \right]
- \left[ \int_{0}^{1} dx \thinspace x^{2} \right]
+ \left[ 2 i\int_{0}^{1} dx \thinspace x \right]
+ \left[ i \int_{0}^{1} dy \thinspace y^{2} \right] \\
&= \left( \left[ \int_{0}^{1} dx \thinspace x^{2} \right] - \left[ \int_{0}^{1} dx \thinspace x^{2} \right] \right)
+ \left( \left[ i \int_{0}^{1} dy \thinspace y^{2} \right] - \left[ i\int_{0}^{1}  dy \thinspace y^{2} \right] \right)
+ \left( \left[ i\int_{0}^{1}  dy \right] - \left[ 2 i\int_{0}^{1} dx \thinspace x \right] \right)
+ \left(\left[ \int_{0}^{1} dx  \right] - \left[ 2\int_{0}^{1}  dy \thinspace y \right] \right) \\
&= 0
\end{align*}

** SOLVED Problem 6.3.3
*Verify that the last two methods reproduce the answer derived above.*

/Up then left./

\begin{align*}
\int_{0}^{1 + i} dz \thinspace z^{2} &= \int_{x,y=0}^{x=0,y=1} \left[ dx + i dy \right] \left( x + i y \right)^{2} + \int_{x=0,y=1}^{x,y=1} \left[ dx + i dy \right] \left( x + i y \right)^{2} \\
&= - i\int_{0}^{1} dy  \thinspace y^{2} + \int_{0}^{1} dx \left( x + i \right)^{2} \\
&= - i\int_{0}^{1} dy  \thinspace y^{2} + \int_{0}^{1} dx \left( x^{2} - 1 + 2 i x \right) \\
&= -\dfrac{i}{3} + \dfrac{1}{3} - 1 + i = \dfrac{2}{3} \left( i - 1 \right).
\end{align*}

/Along the $\pi/4$ line./

\begin{equation*}
\int_{0}^{1+i} dz \thinspace z^{2} = (1 + i) \int_{0}^{1} dx \left( x^{2} - x^{2} + 2 i x x \right) = 2 i (1 + i) \int_{0}^{1} dx x^{2} = \dfrac{2}{3} \left( i - 1 \right).
\end{equation*}

** SOLVED Problem 6.3.4
*Evaluate the integral of $z^{2}$ on a rectangular path going from $-1$ to $1$ to $+i$ to $-1 + i$ and back*

\begin{align*}
\oint_{C} dz \thinspace z^{2} &= \int_{-1}^{1} dx \thinspace \thinspace x^{2}
+ i \int_{0}^{1} dy \thinspace \left(1 + i y \right)^{2}
+ \int_{1}^{-1} dx \thinspace \left( x + i \right)^{2}
+ i \int_{1}^{0} dy \thinspace \left(-1 + i y \right)^{2} \\
&= \int_{-1}^{1} dx \thinspace \thinspace x^{2}
+ i \int_{0}^{1} dy \thinspace \left(1 - y^{2} + 2 i y \right)
+ \int_{1}^{-1} dx \thinspace \left( x^{2} - 1 + 2 i x \right)
+ i \int_{1}^{0} dy \thinspace \left(1 - y^{2} - 2 i y \right) \\
&= \left[ \int_{-1}^{1} dx \thinspace \thinspace x^{2} \right]
+ \left[ i \int_{0}^{1} dy \thinspace  \right]
- \left[ i \int_{0}^{1} dy \thinspace y^{2} \right]
- \left[ 2 \int_{0}^{1} dy \thinspace y \right]
+ \left[ \int_{1}^{-1} dx \thinspace x^{2} \right]
- \left[ \int_{1}^{-1} dx \thinspace \right]
+ \left[ 2 i \int_{1}^{-1} dx \thinspace x \right]
+ \left[ i \int_{1}^{0} dy \thinspace \right]
- \left[ i \int_{1}^{0} dy \thinspace y^{2} \right]
+ \left[ 2 \int_{1}^{0} dy \thinspace y \right] \\
&= \left( \left[ \int_{-1}^{1} dx \thinspace \thinspace x^{2} \right] - + \left[ \int_{-1}^{1} dx \thinspace x^{2} \right] \right)
+ \left( \left[ \int_{-1}^{1} dx \thinspace \right] - \left[ 2 \int_{0}^{1} dy \thinspace y \right] - \left[ 2 \int_{0}^{1} dy \thinspace y \right] \right)
+ \left(\left[ i \int_{0}^{1} dy \thinspace y^{2} \right] - \left[ i \int_{0}^{1} dy \thinspace y^{2} \right] \right)
+ \left( + \left[ i \int_{0}^{1} dy \thinspace  \right] - \left[ i \int_{0}^{1} dy \thinspace \right] \right)
+ \left[ 2 i \int_{1}^{-1} dx \thinspace x \right] = 0
\end{align*}

** SOLVED Problem 6.3.5
*Integrate $z^{2} - z^{3}$ from the origin to $1 + i$ by first going to $z = 1$ along the real axis and then moving vertically up. Compare to what you get going at $\pi/4$. Now compare this to the short cut in which you treat $z$ just like $x$.*

_To $x=1$ then $y=1$._

\begin{align*}
\int_{0}^{1 + i} dz \thinspace \left( z^{2} - z^{3} \right) &= \int_{0}^{1 + i} dz \thinspace  z^{2} - \int_{0}^{1 + i} dz \thinspace z^{3} \\
&= \dfrac{2}{3} \left( i - 1 \right) - \int_{0}^{1 + i} dz \thinspace z^{3} \\
& = \dfrac{2}{3} \left( i - 1 \right) - \int_{0}^{1} dx \thinspace x^{3} - i \int_{0}^{1} dy \thinspace \left( 1 + iy \right)^{3} \\
&= \dfrac{2}{3} \left( i - 1 \right) - \int_{0}^{1} dx \thinspace x^{3} - i \int_{0}^{1} dy \thinspace \left( 1  - i y^{3} + 3 i y - 3 y^{2} \right) \\
&= \dfrac{2}{3} \left( i - 1 \right) - \int_{0}^{1} dx \thinspace x^{3} - \left[ i \int_{0}^{1} dy \thinspace \right] - \left[ \int_{0}^{1} dy \thinspace y^{3} \right] + \left[ 3 \int_{0}^{1} dy \thinspace y \right] + \left[ 3 i \int_{0}^{1} dy \thinspace y^{2} \right] \\
&= \dfrac{2}{3} \left( i - 1 \right) - \dfrac{1}{2} + i + \dfrac{3}{2} -  i \\
&= \dfrac{2}{3} \left( i - 1 \right) + 1 = \dfrac{1}{3} \left( 2i + 1 \right).
\end{align*}

_Along $\pi/4$ to $r=1$._

\begin{align*}
\int_{0}^{1 + i} dz \thinspace \left( z^{2} - z^{3} \right) &= \int_{0}^{1 + i} dz \thinspace  z^{2} - \int_{0}^{1 + i} dz \thinspace z^{3} \\
&= \dfrac{2}{3} \left( i - 1 \right) - (1 + i)^{4} \int_{0}^{1} dx \thinspace x^{3} \\
&= \dfrac{2}{3} \left( i - 1 \right) + 1 = \dfrac{1}{3} \left( 2i + 1 \right).
\end{align*}

/"Treating $z$ just like $x$"./

\begin{align*}
\int_{0}^{1 + i} dz \thinspace \left( z^{2} - z^{3} \right) &=  \left[ \dfrac{z^{3}}{3} - \dfrac{z^{4}}{4} \right]_{0}^{1 + i} = \left[ \dfrac{4z^{3}-3z^{4}}{12} \right]_{0}^{1 + i} = \dfrac{4(1 + i)^{3}-3(1 + i)^{4}}{12} = \dfrac{4 \left( 2i - 2 \right)-3 \left( - 4 \right) }{12} = \dfrac{1}{3} \left( 2i + 1 \right).
\end{align*}

** SOLVED Problem 6.3.6
*Verify the above claim by carrying out the ratio test for the integral.*

Let $f(z)$ be an analytic function with a power series that converges for $\left \lvert z - z_{0}  \right \rvert < R$:

\begin{equation*}
f(z) = \sum_{0}^{\infty} a_{n} \left( z-z_{0} \right)^{n}.
\end{equation*}

Let $z^{(1)}$ and $z^{(2)}$ be two points, and $z_{C}$ be one among a set of points on a contour $C$, such that $\left \lvert z^{(1)} - z_{0}  \right \rvert < R$, $\left \lvert z^{(2)} - z_{0}  \right \rvert < R$, and $\left \lvert z_{C} - z_{0} \right \rvert < R$. The $m-$th integral of $f(z)$ along the contour $C$ may also be represented as a power series

\begin{equation*}
\idotsint\limits_{C} dz \thinspace f(z) = \sum_{0}^{\infty} a_{n} \idotsint\limits_{C} dz \thinspace \left( z - z_{0} \right)^{n} = \sum_{0}^{\infty} \dfrac{a_{n} \thinspace n!}{(n+m)!} \left( z - z_{0} \right)^{n+m}
\end{equation*}

The radius of convergence of the power series for the integral is

\begin{equation*}
\lim_{n \to \infty} \left \lvert \dfrac{a_{n} \thinspace n! (n + m + 1)!}{a_{n+1} \thinspace (n+1)! (n + m)!}  \right \rvert = \lim_{n \to \infty} \left \lvert \dfrac{a_{n} \thinspace (n + m + 1)}{a_{n+1} (n + 1)}  \right \rvert = \lim_{n \to \infty} \left \lvert \dfrac{a_{n}}{a_{n+1}} + \left( \dfrac{m}{n} \right) \dfrac{a_{n}}{a_{n+1}}  \right \rvert = R
\end{equation*}

The power series for the $m-$th integral of $f(z)$ has the same radius of convergence as $f(z)$.

** SOLVED Problem 6.4.1
*What is $\oint_{C} \dfrac{\exp \left \lbrace i z  \right \rbrace dz}{z^{2} + 1}$ where $C$ is a unit circle centered at $z = i$? Repeat when the center is at $z = -i$.*

#+begin_src gnuplot :file btm-6.4.1.png :export none :eval never-export
  set terminal pngcairo  background "#ffffff" enhanced crop font "Arial, 12" fontscale 1.0 size 400, 400 
  set output 'btm-6.4.1.png'

  set title 'f(z) = exp(iz)/(z^2 + 1)'
  unset key # removes legend
  set view map # projects image onto 2D plane
  set size square # sets aspect ratio of plot to be square
  set xrange [ -2 : 2 ]; set yrange [ -2 : 2 ]
  set cbrange [ -3.14 : 3.14 ]

  set xtics ("-1" -1.0, "1" 1.0)
  set ytics ("-i" -1.0, "i" 1.0)
  set cbtics ("0" -3.14, '2' 3.14)
  set xlabel "Re(z)"
  set ylabel "Im(z)"
  set cblabel "Phase Angle" 
  set cblabel  offset character -2, 0, 0 font "" textcolor lt -1 rotate

  set palette model HSV
  set palette defined ( 0 0 1 1, 1 1 1 1 )

  set isosamples 3000, 3000
  set samples 3000, 3000
  thresh = 0.1

  r(x,y) = sqrt(x*x + y*y)
  theta(x,y) = atan2(y,x)
  z(x,y) = r(x,y)*exp(theta(x,y)*sqrt(-1))
  f(z) = exp(sqrt(-1)*z)/(z*z + 1)
  imaginary_f(z) = imag(f(z))
  real_f(z) = real(f(z))

  hue(x,y) = (pi + atan2(-y,-x)) / (2*pi)

  saturation(x,y) = 0.5 + 0.5*(abs(f(z(x,y)))-floor(abs(f(z(x,y)))))
  gridlines(x,y) = (abs(sin(real_f(z(x,y))*pi)**thresh) * abs(sin(imaginary_f(z(x,y))*pi))**thresh)

  color(x,y) = hsv2rgb(hue(real_f(z(x,y)), imaginary_f(z(x,y))), saturation(x,y), gridlines(x,y))
  save_encoding = "utf8"

  splot '++' using 1:2:(color($1,$2)) with pm3d lc rgb variable nocontour
#+end_src

#+RESULTS:
[[file:btm-6.4.1.png]]

At $z = i$, $\dfrac{\exp \left \lbrace i z  \right \rbrace}{z^{2} + 1}$ has a residue $R(i) = \dfrac{\exp \left \lbrace - 1  \right \rbrace}{2i}$. Invoking Cauchy's Residue Theorem immediately yields the answer:

\begin{equation*}
\int_{C} \dfrac{\exp \left \lbrace i z  \right \rbrace}{z^{2} + 1} = \dfrac{\pi}{e}.
\end{equation*}

At $z = - i$, $\dfrac{\exp \left \lbrace i z  \right \rbrace}{z^{2} + 1}$ has a residue $R(i) = \dfrac{\exp \left \lbrace 1 \right \rbrace}{-2i}$. Invoking Cauchy's Residue Theorem immediately yields the answer:

\begin{equation*}
\int_{C} \dfrac{\exp \left \lbrace i z  \right \rbrace}{z^{2} + 1} = - \pi e.
\end{equation*}

** SOLVED Problem 6.4.2
#+begin_comment
I know, how come this triviality took 58 minutes? It's cause I created the =yasnippets= (bless you!) for plotting complex functions and functions of one variable using =gnuplot= (bless you too!).
#+end_comment

*There was no need to take the real part of this answer since it was already real. Can you argue using symmetries of the original integral that changing $\cos x$ to $\exp \left \lbrace i x  \right \rbrace$ does not change the integral? How would you deal with a function that had $\sin x$ in the numerator instead?*

The integral under consideration is

\begin{equation*}
\int_{-\infty}^{\infty} dx \thinspace \dfrac{\cos x}{1 + x^{2}} = \dfrac{\pi}{e}.
\end{equation*}

#+begin_src gnuplot :file btm-6.4.2.png :export none :eval never-export
  set terminal pngcairo  background "#ffffff" enhanced crop font "Arial, 12" fontscale 1.0 size 400, 400
  set output 'btm-6.4.2.png'
  set samples 500, 500
  set size square

  set title "cos(x)/(1 + x*x)"
  set title  font ",12" textcolor lt -1 norotate

  set xrange [ -20 : 20 ]; set yrange [ -0.25 : 1.25 ]
  set xtics ("-20" -20, "20" 20); set ytics ("-0.25" -0.25, "1.25" 1.25)
  set xlabel "x"; set ylabel "f(x)"
  plot cos(x)/(1 + x*x)
#+end_src 

#+RESULTS:
[[file:btm-6.4.2.png]]

When viewed as the integral of a complex function on the real axis, the integrand is transformed as $x \to z$.

#+begin_src gnuplot :file btm-6.4.2b.png :export none :eval never-export
  set terminal pngcairo  background "#ffffff" enhanced crop font "Arial, 12" fontscale 1.0 size 400, 400
  set output 'btm-6.4.2b.png'

  set title 'f(z) = cos(z)/(1 + z*z)'
  unset key # removes legend
  set view map # projects image onto 2D plane
  set size square # sets aspect ratio of plot to be square
  set xrange [ -5 : 5 ]; set yrange [ -5 : 5 ]
  set cbrange [ -3.14 : 3.14 ]

  set xtics ("-5" -5, "5" 5)
  set ytics ("-5" -5, "-5" 5)
  set cbtics ("0" -3.14, '2' 3.14)
  set xlabel "Re(z)"
  set ylabel "Im(z)"
  set cblabel "Phase Angle" 
  set cblabel  offset character -2, 0, 0 font "" textcolor lt -1 rotate

  set palette model HSV
  set palette defined ( 0 0 1 1, 1 1 1 1 )

  set isosamples 1000, 1000
  set samples 1000, 1000
  thresh = 0.1

  r(x,y) = sqrt(x*x + y*y)
  theta(x,y) = atan2(y,x)
  z(x,y) = r(x,y)*exp(theta(x,y)*sqrt(-1))
  f(z) = cos(z)/(1 + z*z)
  imaginary_f(z) = imag(f(z))
  real_f(z) = real(f(z))

  hue(x,y) = (pi + atan2(-y,-x)) / (2*pi)

  saturation(x,y) = 0.5 + 0.5*(abs(f(z(x,y)))-floor(abs(f(z(x,y)))))
  gridlines(x,y) = (abs(sin(real_f(z(x,y))*pi)**thresh) * abs(sin(imaginary_f(z(x,y))*pi))**thresh)

  color(x,y) = hsv2rgb(hue(real_f(z(x,y)), imaginary_f(z(x,y))), saturation(x,y), gridlines(x,y))
  save_encoding = "utf8"

  splot '++' using 1:2:(color($1,$2)) with pm3d lc rgb variable nocontour  
#+end_src 

#+RESULTS:
[[file:btm-6.4.2b.png]]

The semicircle at infinity is closed in the upper half plane after substituting $\cos x$ with $\exp \left \lbrace i x  \right \rbrace$. The resulting integral is what we encountered in =6.4.1=. $\cos(x) \to \exp \left \lbrace ix  \right \rbrace$ does not change the integral because $exp \left \lbrace i x  \right \rbrace = \cos x + i \sin x$, and since $\sin x$ is an odd function of $x$ the integral of $\sin x$ from $x = -\infty$ to $x = + \infty$ vanishes.

In case we were faced with the ask of evaluating the integral

\begin{equation*}
\int_{-\infty}^{\infty} dx \thinspace \dfrac{\sin x}{1 + x^{2}},
\end{equation*}

which looks like

#+begin_src gnuplot :file btm-6.4.2c.png :export none :eval never-export
  set terminal pngcairo  background "#ffffff" enhanced crop font "Arial, 12" fontscale 1.0 size 400, 400
  set output 'btm-6.4.2c.png'
  set samples 500, 500

  set title "sin(x)/(1 + x*x)"
  set title  font ",12" textcolor lt -1 norotate

  set xrange [ -5 : 5 ]; set yrange [ -0.5 : 0.5 ]
  set xtics ("-5" -5, "5" 5); set ytics ("-0.5" -0.5, "0.5" 0.5)
  set xlabel "x"; set ylabel "f(x)"
  plot sin(x)/(1 + x*x)
#+end_src 

#+RESULTS:
[[file:btm-6.4.2c.png]]

we need close no contour. We can directly assert that it vanishes, $\frac{\sin x}{1 + x^{2}}$ being an odd function of $x$. If we were particularly dumb today, we could transform $\sin x \to \exp \left \lbrace i x  \right \rbrace$, close the same contour as before and claim that the imaginary part of the answer is equal to the desired integral, the imaginary part being $0$. What we will close though is this =TODO= item. These sinful pleasures, bless you =Org=!
** SOLVED Problem 6.4.3
*Show that*

\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{\cos a x + x \sin a x}{1 + x^{2}} = \pi \exp \left \lbrace -a  \right \rbrace \quad \text{if} \quad a > 0, \quad \dfrac{\pi}{2} \quad \text{if} \quad a = 0, \quad  0 \quad \text{if} \quad a < 0.
\end{equation*}

For $a = 1 > 0$ The integrand looks like

If $a > 0$,

\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{\cos (ax)}{1 + x^{2}} = \dfrac{1}{2} \int_{-\infty}^{\infty} dx \thinspace \dfrac{\cos (ax)}{1 + x^{2}} = \Re \left \lbrace \dfrac{1}{2} \int_{-\infty}^{\infty} dx \thinspace \dfrac{\exp \left \lbrace i a x  \right \rbrace}{1 + x^{2}}  \right \rbrace = \dfrac{1}{2} \pi \exp \left \lbrace - a  \right \rbrace;
\end{equation*}

having closed the contour in the upper-half plane, and

\begin{equation*}
\int_{0}^{\infty} dx \dfrac{x \sin (ax)}{1 + x^{2}} = \Im \left \lbrace \dfrac{1}{2} \int_{-\infty}^{\infty} dx \dfrac{x \exp \left \lbrace i a x  \right \rbrace}{1 + x^{2}} \right \rbrace  = \dfrac{1}{2} \pi \exp \left \lbrace - a  \right \rbrace;
\end{equation*}

having closed the contour in the upper-half plane once again. Thus

\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{\cos a x + x \sin a x}{1 + x^{2}} = \pi \exp \left \lbrace - a  \right \rbrace \quad a > 1.
\end{equation*}

For $a = -1 < 0$ The integrand looks like

If $a < 0$, the integral becomes

\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{\cos a x - x \sin a x}{1 + x^{2}},
\end{equation*}

which of course (go to sleep if the previous "of course" seems unsubstantiated) evaluates to

\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{\cos a x - x \sin a x}{1 + x^{2}} = 0.
\end{equation*}

For $a = 0$  The integrand looks like

#+begin_src gnuplot :file btm-6.4.3c.png :export none :eval never-export
  set terminal pngcairo  background "#ffffff" enhanced crop font "Arial, 12" fontscale 1.0 size 400, 400
  set output 'btm-6.4.3c.png'
  set samples 500, 500

  set title "(cos(0) + x*sin(0))/(1 + x*x) = 1/(1 + x^2)"
  set title  font ",12" textcolor lt -1 norotate

  set xrange [ 0 : 10 ]; set yrange [ 0 : 1.05 ]
  set xtics ("0" 0, "10" 10); set ytics ("0" 0, "1.00" 1.00)
  set xlabel "x"; set ylabel "f(x)"
  plot (cos(0) + x*sin(0))/(1 + x*x)
#+end_src 

If $a = 0$, the integral becomes

\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{1}{1 + x^{2}},
\end{equation*}

which on, first, halving in tandem with spreading it over the entire $x$-axis and then, second, closing contours in the upper half plane immediately yields

\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{1}{1 + x^{2}} = \dfrac{\pi}{2}.
\end{equation*}

** SOLVED Problem 6.4.4
*Evaluate the integral*

\begin{equation*}
I = \int_{0}^{2 \pi} d \theta \thinspace \dfrac{1}{a + \cos \theta} \quad (a > 1).
\end{equation*}

Making the substitutions $z = \exp \left \lbrace i \theta  \right \rbrace$ and $\cos \theta = \left( 1/2 \right)^{-1} \left( \exp \left \lbrace i \theta  \right \rbrace + \exp \left \lbrace - i \theta  \right \rbrace\right)$ on the unit-circle, $I$ is transformed as

\begin{equation*}
I = \oint_{\left \lvert z  \right \rvert = 1} dz \thinspace \dfrac{- 2 i }{2 a z + z^{2} + 1}
\end{equation*}

The quadratic equation in the denominator has roots

\begin{equation*}
z = \dfrac{ \left( - 2 a \pm \sqrt{4 a^{2} - 4} \right) }{2} = - a \pm \sqrt{a^{2} - 1}.
\end{equation*}

The product of the roots is unity

\begin{equation*}
\left( - \sqrt{a^{2} - 1} - a \right) \left( \sqrt{a^{2} - 1} - a \right) = 1.
\end{equation*}

The root $z = \sqrt{a^{2} - 1} - a$ lies inside the unit circle and the root $z = -\sqrt{a^{2} - 1} - a$ lies outside it. For example, if $a = 2$, the roots are $z = \sqrt{3} - 2$ and $z = - \sqrt{3} - 2$, and the integrand looks like

For $a = 2$, the integrand looks like

What is the residue at $z = \sqrt{a^{2} - 1} - a$?

\begin{equation*}
R(z = \sqrt{a^{2} - 1} - a) = \lim_{z \to \sqrt{a^{2} - 1} - a} \left \lbrace \left( z - \sqrt{a^{2} - 1} + a \right) \left( \dfrac{-2i}{2 a z + z^{2} + 1} \right)  \right \rbrace = \dfrac{-i}{\sqrt{a^{2} - 1}}.
\end{equation*}

On invoking Cauchy's Residue Theorem

\begin{equation*}
I = \dfrac{2 \pi}{\sqrt{a^{2} - 1}}.
\end{equation*}
** SOLVED Problem 6.4.5
*Evaluate by the residue theorem:*

1) $\int_{0}^{\infty} dx \thinspace \dfrac{1}{1 + x^{4}}$,

#+begin_src gnuplot :exports code :file btm-6.4.5a.png
  reset
  set terminal pngcairo enhanced font "arial,10" fontscale 1.0 size 500, 300
  unset key
  set output 'btm-6.4.5a.png'
  set title "f(x)=1/(1+x^4)"

  set xlabel "x"
  set xrange [0:10]

  set ylabel "f(x)"

  f(x) = 1/(1 + x**2)

  plot f(x)
#+end_src    

#+RESULTS:
[[file:btm-6.4.5a.png]]
   
\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{1}{1 + x^{4}} = \int_{0}^{\infty} dx \thinspace \dfrac{1}{\left(x^{2} + i\right)\left(x^{2} - i\right)} \xrightarrow[dx \thinspace = \thinspace dy \thinspace y^{-1/2}]{y = x^{2}} \thinspace \int_{0}^{\infty} dy \thinspace \left(\dfrac{1}{\sqrt{y}}\right) \thinspace \dfrac{1}{\left(y + i\right)\left(y - i\right)} \xrightarrow[dy \thinspace = \thinspace z]{y \thinspace = \thinspace z} \int_{0}^{\infty} dz \thinspace \left(\dfrac{1}{\sqrt{z}}\right) \thinspace \dfrac{1}{\left(z + i\right)\left(z - i\right)}
\end{equation*}   

Using the residues

\begin{equation*}
R(z = i) = \dfrac{1}{2i} \exp \left \lbrace - i \pi / 4  \right \rbrace,
\end{equation*}

\begin{equation*}
R (z = 0) = 0,
\end{equation*}

close contours in the the upper half plane to obtain

\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{1}{1 + x^{4}} = \dfrac{1}{2} \left( \Re \left \lbrace 2 \pi i \left[ \dfrac{1}{2i} \exp \left \lbrace - i \pi / 4 \right]  \right \rbrace  \right \rbrace \right) = \dfrac{\pi}{2 \sqrt{2}}.
\end{equation*}

2) $\oint dz \thinspace \dfrac{\sin z}{z - \pi} \quad \text{on} \left \lvert z  \right \rvert = 1, \thinspace 2, \thinspace$

#+begin_src gnuplot :file btm-6.4.5b.png :export none :eval never-export
  reset
  set terminal pngcairo  background "#ffffff" enhanced crop font "arial,10" fontscale 1.0 size 800, 500
  set output 'btm-6.4.5b.png'

  set title 'f(z)=sin(z)/(z-)'
  unset key # removes legend
  set view map # projects image onto 2D plane
  set size square # sets aspect ratio of plot to be square

  set xrange [ -2.5 : 2.5 ]; set yrange [ -2.5 : 2.5 ]
  set cbrange [ -3.14 : 3.14 ]

  # set xtics ()
  # set ytics ()
  set cbtics ("0" -3.14, '2' 3.14)
  set xlabel "Re(z)"
  set ylabel "Im(z)"
  set cblabel  offset character -2, 0, 0 font "" textcolor lt -1 rotate

  set palette model HSV
  set palette defined ( 0 0 1 1, 1 1 1 1 )

  set isosamples 2000, 2000 # isosamples for output grid
  set samples 2000, 2000 # samples for input grid
  thresh = 0.1 # threshold for gridlines; small values make for small gridlines

  r(x,y) = sqrt(x*x + y*y)
  theta(x,y) = atan2(y,x)
  z(x,y) = r(x,y)*exp(theta(x,y)*sqrt(-1))

  f(z) = sin(z)/(z-3.14)

  imaginary_f(z) = imag(f(z))
  real_f(z) = real(f(z))

  hue(x,y) = (pi + atan2(-y,-x)) / (2*pi)
  saturation(x,y) = 0.5 + 0.5*(abs(f(z(x,y)))-floor(abs(f(z(x,y)))))
  gridlines(x,y) = (abs(sin(real_f(z(x,y))*pi)**thresh) * abs(sin(imaginary_f(z(x,y))*pi))**thresh)

  color(x,y) = hsv2rgb(hue(real_f(z(x,y)), imaginary_f(z(x,y))), saturation(x,y), gridlines(x,y))
  save_encoding = "utf8"

  splot '++' using 1:2:(color($1,$2)) with pm3d lc rgb variable nocontour  
#+end_src    

#+RESULTS:
[[file:btm-6.4.5b.png]]
   
The only singularity of $\dfrac{\sin z}{z - \pi}$ for finite $z$ is a simple pole at $z = \pi$. Since $z = \pi$ is not contained within the domain enclosed by either $\left\lvert z \right\rvert < 1$ or $\left\lvert z \right\rvert < 2$, these contours perimeter a simply connected domain of analyticity of $\dfrac{\sin z}{z - \pi}$. Invoking Cauchy's Integral Theorem implies that the integral on both of these contours vanishes.
   
3) $\int_{0}^{\infty} dx \thinspace \dfrac{x^{2}}{(x^{2} + 25)(x^{2} + 16)}$,

#+begin_src gnuplot :exports code :file btm-6.4.5c.png
  reset
  set terminal pngcairo enhanced font "arial,10" fontscale 1.0 size 500, 300
  unset key
  set output 'btm-6.4.5c.png'
  set title "f(x)=x^2(1/(x^2 + 25)(x^2 + 16))"

  set xlabel "x"
  set xrange [0:25]

  set ylabel "f(x)"

  f(x) = (x*2)/((x*x + 25)*(x*x + 16))

  plot f(x)
#+end_src    

#+RESULTS:
[[file:btm-6.4.5c.png]]
   
\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{x^{2}}{(x^{2} + 5^{2})(x^{2} + 4^{2})} = \int_{0}^{\infty} dx \thinspace \dfrac{x^{2}}{(x + 5i)(x - 5 i)(x + 4i)(x - 4i)} \xrightarrow[dx \thinspace = \thinspace dz]{x \thinspace = \thinspace z} = \int_{0}^{\infty} dx \thinspace \dfrac{z^{2}}{(z + 5i)(z - 5 i)(z + 4i)(z - 4i)}
\end{equation*}

Using the residues

\begin{equation*}
R(z = 5 i ) = \dfrac{5}{18 i},
\end{equation*}

\begin{equation*}
R(z = 4i) = \dfrac{-2}{9 i },
\end{equation*}

\begin{equation*}
\int_{0}^{\infty} dx \thinspace \dfrac{x^{2}}{\left( x^{2} + 5^{2} \right) \left( x^{2} + 4^{2} \right)} = \dfrac{1}{2} \left( \Re \left \lbrace 2 \pi i \left[ \dfrac{5}{18i} - \dfrac{1}{4i} \right]  \right \rbrace \right) = \dfrac{1}{2} \left( \Re \left \lbrace 2 \pi i \left[ \dfrac{5}{18i} - \dfrac{2}{9i} \right]  \right \rbrace \right) = \dfrac{\pi}{18}.
\end{equation*}

4) $\int_{0}^{2 \pi} d \theta \thinspace \dfrac{\cos \theta}{5 + 4 \cos \theta}$,

#+begin_src gnuplot :exports code :file btm-6.4.5d.png
  reset
  set terminal pngcairo enhanced font "arial,10" fontscale 1.0 size 500, 300
  unset key
  set output 'btm-6.4.5d.png'
  set title "f(x)=cosx/(5 + 4cosx)"

  set xlabel "x"
  set xrange [0:6.28]
  set xtics ("0" 0.0, "" 3.14, "2" 6.28)

  set ylabel "f(x)"

  f(x) = cos(x)/(5 + 4*cos(x))
  plot f(x)
#+end_src    

#+RESULTS:
[[file:btm-6.4.5d.png]]
   
\begin{equation*}
\int_{0}^{2 \pi} d \theta \thinspace \dfrac{\cos \theta d \theta}{5 + 4 \cos \theta} \xrightarrow[\cos \theta = \left( 1/2 \right)^{-1/2} \left( \exp \left \lbrace i \theta  \right \rbrace + \exp \left \lbrace - i \theta  \right \rbrace \right) = \left( z + 1/z \right)/2]{z = \exp \left \lbrace i \theta  \right \rbrace, dz = i z d \theta} = -\oint_{\left \lvert z  \right \rvert = 1} \dfrac{dz}{2 i z} \thinspace \dfrac{z + 1/z}{5 + 2 \left( z + 1/z \right)} = -\oint_{\left \lvert z  \right \rvert = 1} \dfrac{dz}{2 i z} \thinspace \dfrac{z^{2} + 1}{2 z^{2} + 5z  + 2} = -\oint_{\left \lvert z  \right \rvert = 1} \dfrac{dz}{2 i z} \thinspace \dfrac{z^{2} + 1}{2 z^{2} + 5z  + 2} = -\oint_{\left \lvert z  \right \rvert = 1} \dfrac{dz}{2 i z} \thinspace \dfrac{z^{2} + 1}{(z + 2)(2 z + 1)}
\end{equation*}

The integrand has 3 simple poles: at $z = - 2$, $z = -1/2$, and $z = 0$. The last two are of concern to us given that we are closing a contour around the unit circle. Invoke Cauchy's Residue Theorem on the residues

\begin{equation*}
R(z = 0) = - \dfrac{1}{2 i},
\end{equation*}

\begin{equation*}
R (z = - 1/2) = + \dfrac{1}{3i},
\end{equation*}

to obtain

\begin{equation*}
\int_{0}^{2 \pi} d \theta \thinspace \dfrac{\cos \theta d \theta}{5 + 4 \cos \theta} = 2 \pi i \left[ \dfrac{1}{3 i} - \dfrac{1}{2i} \right] =  - \dfrac{\pi}{3}.
\end{equation*}

5) $\int_{0}^{2 \pi} \dfrac{\sin \theta d \theta}{5 + 4 \cos \theta}$, and

#+begin_src gnuplot :exports code :file btm-6.4.5e.png
  reset
  set terminal pngcairo enhanced font "arial,10" fontscale 1.0 size 500, 300
  unset key
  set output 'btm-6.4.5e.png'
  set title "f(x) = sin x/(5 + 4 cos x)"

  set xlabel "x"
  set xrange [0:6.28]
  set xtics ("0" 0.0, "" 3.14, "2" 6.28)

  set ylabel "f(x)"
  set yrange [-1:1]

  f(x) =sin(x)/(5 + 4*cos(x)) 

  plot f(x)
#+end_src 

#+RESULTS:
[[file:btm-6.4.5e.png]]
   
$\sin$ is an odd function about $\theta$ while $\cos$ is an even function function about $\theta$. They are both periodic functions with a period of $2\pi$: a translation by integer multiples of $\pi/2$ changes their symmetry, while a translation by $\pi$ keeps their symmetry invariant. It should now be clear that the integrand is an odd function of $\theta$ about $\theta=\pi$. The integral from $0$ to $2 \pi$, therefore, vanishes.

#+begin_comment
That didn't really respect "Evaluate by residue theorem", but whatever. Maybe we'll do it that way later sometime.
#+end_comment

6) $\int_{0}^{2 \pi} \dfrac{\sin^{2} \theta d \theta}{5 + 4 \cos \theta}$.

#+begin_src gnuplot :exports code :file btm-6.4.5f.png
  reset
  set terminal pngcairo enhanced font "arial,10" fontscale 1.0 size 500, 300
  unset key
  set output 'btm-6.4.5f.png'
  set title "f(x)=sin^2 x/(5+4cosx)"

  set xlabel "x"
  set xrange [0:6.28]
  set xtics("0" 0, "" 3.14, "2" 6.28)

  set ylabel "f(x)"
  set yrange [0:0.3]

  f(x) = (sin(x))**2/(5 + cos(x))

  plot f(x)
#+end_src 

#+RESULTS:
[[file:btm-6.4.5f.png]]

\begin{equation*}
\int_{0}^{2 \pi} d \theta \thinspace \dfrac{\sin^{2} \theta}{5 + 4 \cos \theta}
\xrightarrow[\thinspace \cos \theta \thinspace = \thinspace (1/2)^{-1} \left( \exp \left \lbrace i \theta  \right \rbrace + \exp \left \lbrace - i \theta  \right \rbrace\right) \thinspace = \thinspace (1/2) \left( z + 1/z \right)]{z \thinspace = \thinspace \exp \left \lbrace i \theta  \right \rbrace, \thinspace dz = i \exp \left \lbrace i \theta  \right \rbrace \thinspace d\theta, \thinspace \sin \theta \thinspace = \thinspace (1/2)^{-1} \left[ \exp \left \lbrace i \theta  \right \rbrace - \exp \left \lbrace - i \theta  \right \rbrace \right] \thinspace = \thinspace (1/2) \left[ z - 1/z \right]}
-i \thinspace \oint_{\left \lvert z  \right \rvert < 1} dz \thinspace \dfrac{1}{4z} \thinspace  \dfrac{ \left( z - 1/z \right)^{2} }{5 + 2 \left( z + 1/z \right)} =
-i \thinspace \oint_{\left \lvert z  \right \rvert < 1} dz \thinspace \dfrac{1}{4z^{2}} \thinspace  \dfrac{\left(z^{2} - 1 \right)^{2}}{5z + 2 z^{2} + 2}.
\end{equation*}

The roots of $5z + 2z^{2} + 2$ are

\begin{equation*}
z = -2, \quad z = -\frac{1}{2}.
\end{equation*}

Using the residues

\begin{equation*}
R \left(z = 0 \right) = - \dfrac{5}{16},
\end{equation*}

\begin{equation*}
R \left(z = - \dfrac{1}{2} \right) = \dfrac{3}{16},
\end{equation*}

the contour integral on the unit circle, by appeal to Cauchy's Residue Theorem evaluates to

\begin{equation*}
-i \oint_{\left \lvert z  \right \rvert < 1} dz \thinspace \dfrac{1}{z^{2}} \thinspace  \dfrac{(z + 1)(z - 1)(z + 1)(z - 1)}{5z + 4 z^{2} + 4} =
-i \left( 2 \pi i \left[ 3/16 - 5/16 \right] \right) = \dfrac{\pi}{4}.
\end{equation*}

** TOSOLVE Problem 6.4.6
*Show that*

a) $\int_{-\infty}^{\infty} \dfrac{x \sin x}{1 + x^{4}} dx = \pi \exp \left \lbrace \dfrac{-1}{\sqrt{2}} \right \rbrace \sin \left( \dfrac{1}{\sqrt{2}} \right)$,

#+begin_src gnuplot :exports code :file btm-6.4.6a.png
  reset
  set terminal pngcairo transparent enhanced font "arial,10" fontscale 1.0 size 500, 300
  unset key
  set output 'btm-6.4.6a.png'
  set title "f(x)=xsinx/(1+x^4)"
  set samples 1000

  set xlabel "x"
  set xrange [-5:5]

  set ylabel "f(x)"

  f(x) = x*sin(x)/(1+x**4)

  plot f(x)
#+end_src 

#+RESULTS:
[[file:btm-6.4.6a.png]]

\begin{align*}
\int_{-\infty}^{\infty} dx \thinspace \dfrac{x \thinspace \sin x}{1 + x^{4}} =& \int_{-\infty}^{\infty} dx \dfrac{x \thinspace \sin x}{\left( x^{2} + i \right) \left( x^{2} - i \right)}
= -\Im \left \lbrace \int_{-\infty}^{\infty} dx \dfrac{x \left( \cos x - i \sin x \right)}{\left( x^{2} + i \right) \left( x^{2} - i \right)}  \right \rbrace \\
&= - \Im \left \lbrace \int_{-\infty}^{\infty} dx \dfrac{x \exp \left \lbrace - i x  \right \rbrace}{\left( x^{2} + i \right) \left( x^{2} - i \right)}  \right \rbrace \xrightarrow[]{x \to z} - \Im \left \lbrace \int_{-\infty}^{\infty} dz \dfrac{z \exp \left \lbrace - i z  \right \rbrace}{\left( z^{2} + i \right) \left( z^{2} - i \right)}  \right \rbrace \\
& = - \Im \left \lbrace \int_{-\infty}^{\infty} dz \dfrac{z \exp \left \lbrace - i z  \right \rbrace}{\left( z + i \sqrt{i} \right) \left( z - i \sqrt{i} \right) \left( z + \sqrt{i} \right) \left( z - \sqrt{i} \right)} \right \rbrace \\
&= - \Im \left \lbrace \int_{-\infty}^{\infty} dz \dfrac{z \exp \left \lbrace - i z  \right \rbrace}{\left( z + i \exp \left \lbrace i \dfrac{\pi}{4}  \right \rbrace \right) \left( z - i \exp \left \lbrace i \dfrac{\pi}{4}  \right \rbrace \right) \left( z + \exp \left \lbrace i \dfrac{\pi}{4}  \right \rbrace \right) \left( z - \exp \left \lbrace i \dfrac{\pi}{4}  \right \rbrace \right)} \right \rbrace \\
\end{align*}
Using the residues

\begin{equation*}
R \left( z = - i \exp \left \lbrace i \dfrac{\pi}{4}  \right \rbrace \right) = 
\end{equation*}

\begin{equation*}
R \left( z = - \exp \left \lbrace i \dfrac{\pi}{4}  \right \rbrace \right) = 
\end{equation*}

b) $\int_{0}^{\infty} \dfrac{x \sin 3x}{x^{2} + 9} dx = \dfrac{\pi}{2} \exp \left \lbrace - 9  \right \rbrace$,

#+begin_src gnuplot :exports code :file btm-6.4.6b.png
  reset
  set terminal pngcairo transparent enhanced font "arial,10" fontscale 1.0 size 500, 300
  unset key
  set output 'btm-6.4.6b.png'
  set title "f(x)=xsin3x/(x^2+9)"
  set samples 1000

  set xlabel "x"
  set xrange [0:40]

  set ylabel "f(x)"
  set yrange [:]

  set samples 1000

  f(x) = x*sin(3*x)/(x**2+9)

  plot f(x)
#+end_src 

#+RESULTS:
[[file:btm-6.4.6b.png]]

c) $\int_{0}^{\infty} \dfrac{x^{2} + 1}{1 + x^{4}} dx = \dfrac{\pi}{\sqrt{2}}$,

#+begin_src gnuplot :exports code :file btm-6.4.6c.png
  reset
  set terminal pngcairo transparent enhanced font "arial,10" fontscale 1.0 size 500, 300
  unset key
  set output 'btm-6.4.6c.png'
  set title "f(x)=(x^2+1)/(1 + x^4)"
  set samples 1000

  set xlabel "x"
  set xrange [0:10]

  set ylabel "f(x)"

  set samples 1000

  f(x) = (x**2 + 1)/(1 + x**4)

  plot f(x)
#+end_src 

#+RESULTS:
[[file:btm-6.4.6c.png]]

d) $\int_{-\infty}^{\infty} dx \thinspace \dfrac{x \sin x}{x^{2} + 2x + 17} = \dfrac{(4 \cos 1 + \sin 1) \pi}{4 \exp \left \lbrace 4  \right \rbrace}$.

#+begin_src gnuplot :exports code :file btm-6.4.6d.png
  reset
  set terminal pngcairo transparent enhanced font "arial,10" fontscale 1.0 size 500, 300
  unset key
  set output 'btm-6.4.6d.png'
  set title "f(x)=xsinx/(x^2+2x+17)"
  set samples 1000

  set xlabel "x"
  set xrange [-20:20]

  set ylabel "f(x)"

  set samples 1000

  f(x) = x*sin(x)/(x**2+2*x+17)

  plot f(x)
#+end_src

#+RESULTS:
[[file:btm-6.4.6d.png]]

** TOSOLVE Problem 6.4.7
*Evaluate*

\begin{equation*}
\oint \dfrac{dz}{(z-2)^{2} z^{3}} \quad \text{on} \quad \left \lvert z - 3  \right \rvert = 2 \quad \text{and} \quad \left \lvert z - 1  \right \rvert = 3.
\end{equation*}

** TOSOLVE Problem 6.4.8
*Evaluate*

\begin{equation*}
\int_{0}^{\infty} \dfrac{\cos x \thinspace dx}{(x^{2} + 9)^{2}}.
\end{equation*}

*** Problem 6.5.1
*First recall that the sum of a set of complex numbers is bounded by the sum of the absolute values of the individual terms. Since the integral is a sum, this implies the integral is bounded by one in which we replace the integrand and $dz$ by their absolute values. You can now bound every factor in the integrand, calculate the length of the integration contour, and argue that $1/(\left \lvert z - z^{\prime}  \right \rvert) \leq \left \lvert \left \lvert z^{\prime} - a  \right \rvert - \left \lvert z - a  \right \rvert \right \rvert^{-1}$ by looking at Fig. 6.5*

*** Problem 6.5.2
*Compare the two sides of the above relation on the imaginary axis $z = i y$ and show that they do not match.*

*** Problem 6.5.3
*On the real axis we had $\exp \left \lbrace i x  \right \rbrace = \cos x + i \sin x$. Is this an analytic relationship which will survive the continuation to complex $z$?*

*** Problem 6.5.4
*On the real axis we have $\exp \left \lbrace ix  \right \rbrace \exp \left \lbrace i x  \right \rbrace^{\ast} = 1$. Is it true that $\exp \left \lbrace iz  \right \rbrace \exp \left \lbrace i z  \right \rbrace^{\ast} = 1$? Give reasons for your answer and the nthe supporint calclulation.*

* Vector Calculus
** TOSOLVE Problem 7.1.1
** TOSOLVE Problem 7.1.2
** TOSOLVE Problem 7.1.3
** TOSOLVE Problem 7.2.1
** TOSOLVE Problem 7.2.2
** TOSOLVE Problem 7.2.3
** TOSOLVE Problem 7.2.4
** TOSOLVE Problem 7.2.4
** TOSOLVE Problem 7.4.1
** TOSOLVE Problem 7.4.2
** TOSOLVE Problem 7.4.3
** TOSOLVE Problem 7.4.4
** TOSOLVE Problem 7.5.1
** TOSOLVE Problem 7.5.2
** TOSOLVE Problem 7.5.3
** TOSOLVE Problem 7.5.4
** TOSOLVE Problem 7.5.5
** TOSOLVE Problem 7.5.6
** TOSOLVE Problem 7.5.7
** TOSOLVE Problem 7.5.8
** TOSOLVE Problem 7.6.1
** TOSOLVE Problem 7.6.2
** TOSOLVE Problem 7.6.3
** TOSOLVE Problem 7.6.4
** TOSOLVE Problem 7.6.5
** TOSOLVE Problem 7.6.6
** TOSOLVE Problem 7.6.7
** TOSOLVE Problem 7.6.8
** TOSOLVE Problem 7.6.9
** TOSOLVE Problem 7.6.10
** TOSOLVE Problem 7.6.11
** TOSOLVE Problem 7.6.12
** TOSOLVE Problem 7.6.13
** TOSOLVE Problem 7.6.14
** TOSOLVE Problem 7.6.15
** TOSOLVE Problem 7.7.1
** TOSOLVE Problem 7.7.2
** TOSOLVE Problem 7.7.3
** TOSOLVE Problem 7.7.4
** TOSOLVE Problem 7.7.5
** TOSOLVE Problem 7.7.6
** TOSOLVE Problem 7.7.7
** TOSOLVE Problem 7.8.1
** TOSOLVE Problem 7.10.1
** TOSOLVE Problem 7.10.2
** TOSOLVE Problem 7.11.1
** TOSOLVE Problem 7.11.2
** TOSOLVE Problem 7.11.3
** TOSOLVE Problem 7.11.4
** TOSOLVE Problem 7.11.5
** TOSOLVE Problem 7.11.6
** TOSOLVE Problem 7.11.7
** TOSOLVE Problem 7.11.8
** TOSOLVE Problem 7.11.9
* Matrices and Determinants
** SOLVED Problem 8.1.1
CLOSED: [2022-11-01 Tue 14:38]
$R_{\theta}$ and $R_{\theta'}$ are matrices that counter-clockwise rotate the axis by $\theta$ and $\theta'$ respectively. 

\begin{equation*}
    \begin{split}
      R_{\theta'}R_{\theta} = \begin{bmatrix} cos\theta' & sin\theta' \\ -sin\theta' & cos\theta' \end{bmatrix} \begin{bmatrix} cos\theta & sin\theta \\ -sin\theta & cos\theta \end{bmatrix} & = \begin{bmatrix} cos\theta'cos\theta - sin\theta'sin\theta & sin\theta'cos\theta + cos\theta' sin \theta \\ - (sin\theta'cos\theta + cos\theta' sin \theta) & cos\theta'cos\theta - sin\theta'sin\theta \end{bmatrix} \\
      & = \begin{bmatrix} cos(\theta + \theta') & sin(\theta + \theta') \\ -sin(\theta + \theta') & cos(\theta + \theta')\end{bmatrix} = R_{\theta + \theta'}  
    \end{split}
\end{equation*}

** SOLVED Problem 8.1.2
CLOSED: [2022-11-01 Tue 14:38]

The relativistic transformation of coordinates when we go from one frame of reference to another is 

\begin{align*}
    x' & = x cosh\theta - tsinh\theta \\
    t' & = -x sinh\theta + tcosh\theta
\end{align*}

where $\theta$ is the rapidity difference between the two frames. In matrix form 

\begin{equation*}
    \Lambda(\theta) = \begin{bmatrix} cosh\theta & -sinh\theta \\ - sinh\theta & cosh\theta \end{bmatrix}
\end{equation*}

where $\Lambda(\theta)$ is the matrix mediating a relativistic transformation of coordinate frames with a rapidity difference of $\theta$. Say we go to a third frame with coordinates $x'',t''$, moving with rapidity $\theta'$ with respect to the one with primed coordinates. The following sequence of transformations mediate the change from $x,t$ to $x'',t''$.

\begin{equation*}
    \begin{split}
        \begin{bmatrix} x'' \\ t'' \end{bmatrix} = \Lambda(\theta')\Lambda(\theta) \begin{bmatrix} x \\ t \end{bmatrix} & = \begin{bmatrix} cosh\theta' & -sinh\theta' \\ - sinh\theta' & cosh\theta' \end{bmatrix} \begin{bmatrix} cosh\theta & -sinh\theta \\ - sinh\theta & cosh\theta \end{bmatrix} \begin{bmatrix} x \\ t \end{bmatrix} \\
        & = \begin{bmatrix} cosh\theta'cosh\theta + sinh\theta'sinh\theta & -(sinh\theta'cosh\theta + cosh\theta' sinh \theta) \\ -(sinh\theta'cosh\theta + cosh\theta' sinh \theta) & cosh\theta'cosh\theta + sinh\theta'sinh\theta \end{bmatrix} \begin{bmatrix} x \\ t \end{bmatrix} \\
        & = \begin{bmatrix} cosh(\theta'+\theta) & -sinh(\theta'+\theta) \\ - sinh(\theta'+\theta) & cosh(\theta'+\theta) \end{bmatrix} \begin{bmatrix} x \\ t \end{bmatrix} = \Lambda(\theta' + \theta) \begin{bmatrix} x \\ t \end{bmatrix}  
    \end{split}
\end{equation*}

Therefore the matrix relating the doubly primed coordinates to the unprimed ones corresponds to rapidity $\theta + \theta'$, i.e. $\Lambda(\theta')\Lambda(\theta) = \Lambda(\theta' + \theta).$ \\

** SOLVED Problem 8.1.3
CLOSED: [2022-11-01 Tue 14:38]

If $M$ is an $m$ by $n$ matrix and $N$ is an $n$ by $r$ matrix, their product $MN$ is an $m$ by $r$ matrix with entries

\begin{equation*}
    (MN)_{ij} = \sum_{k=1}^{n} M_{ik}N_{kj}
\end{equation*}

Subtitute $M=I$ (the unit matrix). 

\begin{equation*}
    (IN)_{ij} = \sum_{k=1}^{n} I_{ik}N_{kj}
\end{equation*}

The diagonal elements of a unit matrix $(I_{ii})$ are unity and the rest are zero. So all the terms in the summation vanish for $k \neq i$. 

\begin{equation*}
    (IN)_{ij} = N_{ij}
\end{equation*}

Therefore, $IN = N$  for all $N$.

** SOLVED Problem 8.1.4
CLOSED: [2022-11-01 Tue 14:38]
Matrix multiplication in general is \textit{non-commutative}. This means in general

\begin{equation*}
    MN \neq NM
\end{equation*}

We will illustrate this by multiplying a column vector with elements $x_{1}=0$ and $x_{2}=1$ with the following matrices in the two possible orders

\begin{align*}
    P_{1} & = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} & R_{\theta} &= \begin{bmatrix} cos\theta & sin\theta \\ -sin\theta & cos\theta \end{bmatrix} 
\end{align*}

$P_{1}$ is called the \textit{projection operator} along direction 1. $R_{\theta}$ is the rotation matrix. Projecting and then rotating:

\begin{equation*}
    \begin{split}
        \begin{bmatrix} x' \\ t' \end{bmatrix} = \begin{bmatrix} cos\theta & sin\theta \\ -sin\theta & cos\theta \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} 
    \end{split}
\end{equation*}

Rotating and then projecting: 

\begin{equation*}
    \begin{split}
        \begin{bmatrix} x'' \\ t'' \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} cos\theta & sin\theta \\ -sin\theta & cos\theta \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} sin\theta \\ 0\end{bmatrix} 
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{bmatrix} x'' \\ t'' \end{bmatrix} \neq \begin{bmatrix} x' \\ t' \end{bmatrix}
\end{equation*}

Of course, we could also show

\begin{equation*}
    P_{1}R_{\theta} = \begin{bmatrix} cos\theta & sin\theta \\ 0 & 0 \end{bmatrix} \neq \begin{bmatrix} cos\theta & 0 \\ -sin\theta & 0 \end{bmatrix} = R_{\theta}P_{1}
\end{equation*}

** SOLVED Problem 8.1.5
CLOSED: [2022-11-01 Tue 14:38]
Noncommutativity makes it sensible to define the \textit{commutator} 

\begin{equation*}
    [M,N] \coloneqq MN - NM
\end{equation*}

Two matrices are said to commute when their commutator vanishes. 

Set M = I (the unit matrix). We have 

\begin{equation*}
    [I,N] \coloneqq IN - NI
\end{equation*}

We know $IN = N$ for all $N$. We will evaluate $NI$

\begin{equation*}
    (NI)_{ij} = \sum_{k=1}^{n} N_{ik}I_{kj}
\end{equation*}

The diagonal elements of a unit matrix $(I_{jj})$ are unity and the rest are zero. So all the terms in the summation vanish for $k \neq j$. Therefore 

\begin{equation*}
    (NI)_{ij} = N_{ij}
\end{equation*}

Thus $NI = N$ for all $N$. Hence,

\begin{equation*}
    [I,N] = IN - NI = N - N = 0 
\end{equation*}

So the unit matrix commutes with all matrices. 

** SOLVED Problem 8.1.6
CLOSED: [2022-11-01 Tue 14:38]

$MN = 0$ does not imply $M=0$ or $N=0$. As an example, consider the projection operators $P_{1}$ and $P_{2}$ along direction 1 and 2 respectively:

\begin{align*}
    P_{1} & = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} & P_{2} & = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} 
\end{align*}

It is clear that 

\begin{align*}
    P_{1}P_{2} & = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}  & P_{2}P_{1} & =  \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix} 
\end{align*}

Thus the product of these two matrices in any order will kill any vector. Needless to say, projection operators along direction 1 and 2 commute. 

It is also not true that $M^{2} =0$ implies $M=0$. As an example consider 

\begin{equation*}
    \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}
\end{equation*}

\question*{Problem 8.1.7}

\begin{align*}
    M & = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} & N & = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}
\end{align*}

\begin{equation*}
    M + N = \begin{bmatrix} 1+5 & 2+6 \\ 3+7 & 4+8 \end{bmatrix} = \begin{bmatrix} 6 & 8 \\ 10 & 12 \end{bmatrix}
\end{equation*}

\begin{equation*}
    M^{2} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} = \begin{bmatrix} 1.1 + 2.3 & 1.2 + 2.4 \\ 3.1 + 4.3 & 3.2 +  4.4 \end{bmatrix} = \begin{bmatrix} 7 & 10 \\ 15 & 22 \end{bmatrix}
\end{equation*}

\begin{equation*}
    MN = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}\begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} = \begin{bmatrix} 1.5 + 2.7 & 1.6 + 2.8 \\ 3.5 + 4.7 & 3.6 +  4.8 \end{bmatrix} = \begin{bmatrix} 19 & 22 \\ 43 & 50 \end{bmatrix}
\end{equation*}

\begin{equation*}
    \begin{split}
        [M,N] = MN - NM = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}\begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} - \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} & = \begin{bmatrix} 1.5 + 2.7 & 1.6 + 2.8 \\ 3.5 + 4.7 & 3.6 +  4.8 \end{bmatrix} - \begin{bmatrix} 5.1 + 6.3 & 5.2 + 6.4 \\ 7.1 + 8.3 & 7.2 +  8.4 \end{bmatrix} \\ 
        & = \begin{bmatrix} 19 & 22 \\ 43 & 50 \end{bmatrix} - \begin{bmatrix} 23 & 34 \\ 31 & 46 \end{bmatrix} = \begin{bmatrix} -4 & -12 \\ 12 & 4 \end{bmatrix}
    \end{split}
\end{equation*}
** SOLVED Problem 8.2.1
CLOSED: [2022-11-01 Tue 14:38]

For any $2 \times 2$ matrix $M$

\begin{equation*}
    M^{-1} = \frac{1}{|M|} \begin{bmatrix} M_{22} & -M_{12} \\ -M_{21} & M_{11} \end{bmatrix}
\end{equation*}

where the \textit{determinant of the matrix} $|M|$ is given by

\begin{equation*}
    |M| = M_{11}M_{22}-M_{12}M_{21}
\end{equation*}

We now show that the $M^{-1}$ given above satisfies $M^{-1}M=I.$

\begin{equation*}
    \begin{split}
        M^{-1}M = \frac{1}{|M|} \begin{bmatrix} M_{22} & -M_{12} \\ -M_{21} & M_{11} \end{bmatrix} \begin{bmatrix} M_{11} & M_{12} \\ M_{21} & M_{22} \end{bmatrix} & = \frac{1}{|M|} \begin{bmatrix} M_{11}M_{22}-M_{12}M_{21} & M_{22}M_{12}-M_{22}M_{12} \\ M_{11}M_{21}-M_{11}M_{21} & M_{11}M_{22}-M_{12}M_{21} \end{bmatrix} \\
        & = \frac{1}{|M|} \begin{bmatrix} |M| & 0 \\ 0 & |M| \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I
    \end{split}
\end{equation*}

** SOLVED Problem 8.2.2
CLOSED: [2022-11-01 Tue 14:38]

The rotation matrix $R__{\theta}$ is 

\begin{equation*}
    R_{\theta} = \begin{bmatrix} cos\theta & sin\theta \\ -sin\theta & cos\theta \end{bmatrix}
\end{equation*}

\begin{equation*}
    R_{\theta}^{-1} = \frac{1}{sin^{2}\theta+cos^{2}\theta}\begin{bmatrix} cos\theta & -sin\theta \\ sin\theta & cos\theta\end{bmatrix} = \begin{bmatrix} cos\theta & -sin\theta \\ sin\theta & cos\theta\end{bmatrix}
\end{equation*}

This answer is readily found by substituting $-\theta$ in the expression of $R_{\theta}$ and using the fact that sine is an odd function and cosine is an even function of $\theta$. $R_{\theta}^{-1}$ is therefore equal to $R_{-\theta}$. The inverse of a rotation matrix that rotates the axes in the counterclockwise direction by $\theta$ is a rotation matrix that rotates the axes in the clockwise direction by the same angle $\theta$. It is also clear that $R_{\theta}R^{-1}_{\theta} = R_{\theta}R_{-\theta} = R_{0} = I.$

The determinant of a rotation matrix $R_{\theta}$ is will never vanish for any $\theta$. This is easily shown by using a basic trigonometric identity.

\begin{equation*}
    |R_{\theta}| = sin^{2}\theta + cos^{2}\theta = 1
\end{equation*}

This had to be the case because of the non-existence of irreversible rotations!

** SOLVED Problem 8.2.3
CLOSED: [2022-11-01 Tue 14:38]

The system of equation is written in matrix form as 

\begin{equation*}
    \begin{bmatrix} 9 \\  23 \end{bmatrix} = \begin{bmatrix} 1 & 2 \\ 3 & 4\end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix}
\end{equation*}

$\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}^{-1}$ exists because the determinant is non-zero. The solution to the system is written as

\begin{equation*}
    \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}^{-1} \begin{bmatrix} 9 \\  23 \end{bmatrix} = \frac{1}{4.1-3.2}\begin{bmatrix} 4 & -2 \\ -3 & 1 \end{bmatrix}\begin{bmatrix} 9 \\  23 \end{bmatrix} = \frac{1}{-2}\begin{bmatrix} -10 \\ -4 \end{bmatrix} = \begin{bmatrix} 5 \\ 2 \end{bmatrix}
\end{equation*}

** SOLVED Problem 8.2.4
CLOSED: [2022-11-01 Tue 14:38]

The Lorentz Tranformation matrix is 

\begin{equation*}
    \Lambda(\theta) = \begin{bmatrix} cosh\theta & -sinh\theta \\ - sinh\theta & cosh\theta \end{bmatrix}
\end{equation*}

where $\theta$ is the rapidity difference between the two frames of reference. Its inverse is

\begin{equation*}
    \Lambda^{-1}(\theta) = \frac{1}{cosh^{2}\theta-sinh^{2}\theta}\begin{bmatrix} cosh\theta & sinh\theta \\ sinh\theta & cosh\theta \end{bmatrix} = \begin{bmatrix} cosh\theta & sinh\theta \\ sinh\theta & cosh\theta \end{bmatrix} 
\end{equation*}

We have used basic identities for hyperbolic functions. Again, this answer is readily found by using the fact that $sinh$ is an odd function and $cosh$ is an even function of $\theta$. $\Lambda^{-1}(\theta)$ is therefore equal to $\Lambda(-\theta).$

** SOLVED Problem 8.2.5
CLOSED: [2022-11-01 Tue 14:38]

A solution cannot exist for the system

\begin{align*} 
2x + 3y &=  5 \\ 
4x + 6y &=  10
\end{align*}

This is because both these equations describe the same straight line and therefore there are infinite $(x,y)$ pairs that satisfy these equations. Sure enough, the determinant for this system vanishes $(2 \times 6 - 4 \times 3 = 0).$

** SOLVED Problem 8.2.6
CLOSED: [2022-11-01 Tue 14:38]

For the given situation, we have the following system

\begin{align*} 
C_{1} &= 0.05P_{1} + 0.05P_{2} = 0.05(P_{1}+P_{2}) \\ 
C_{2} &= 0.06P_{1} + 0.06P_{2} = 0.06(P_{1}+P_{2})
\end{align*}

It is clear that the income of the children depends only on the sum of the incomes of the parents. In other words, multiple combinations of parental incomes producing the same total parental income give identical incomes for the children. Therefore, it must not be possible to find the individual parental incomes from the children's incomes. Sure enough, the matrix for this system is not invertible because the determinant vanishes (0.05 \times 0.06 - 0.05 \times 0.06 = 0).

\question*{Problem 8.2.7}

The inverse of the product is the product of the inverses in reverse. 

\begin{equation*}
    (PQR)^{-1} = R^{-1}Q^{-1}P^{-1}
\end{equation*}

To prove this, it is sufficient to show that $R^{-1}Q^{-1}P^{-1}PQR=I$.

\begin{equation*}
    R^{-1}Q^{-1}P^{-1}PQR = R^{-1}Q^{-1}IQR = R^{-1}Q^{-1}QR = R^{-1}IR = R^{-1}R = I
\end{equation*}

** SOLVED Problem 8.3.1
CLOSED: [2022-11-01 Tue 14:38]

Consider $2 \times 2$ matrices $M$ and $N$

\begin{align*}
    M & = \begin{bmatrix} M_{11} & M_{12} \\ M_{21} & M_{22} \end{bmatrix} & N & = \begin{bmatrix} N_{11} & N_{12} \\ N_{21} & N_{22} \end{bmatrix} 
\end{align*}

We will prove six results for $2 \times 2$ matrices:

The determinant of a product of is the product of the determinants: 

\begin{equation*}
   |MN| = |M||N|
\end{equation*}

\begin{proof}
\begin{equation*}
\begin{split}
    |MN| & = \begin{vmatrix} M_{11}N_{11}+M_{12}N_{21} & M_{11}N_{12}+M_{12}N_{22} \\ M_{21}N_{11}+M_{22}N_{21} & M_{21}N_{12}+M_{22}N_{22} \end{vmatrix} \\
    & = (M_{11}N_{11}+M_{12}N_{21})(M_{21}N_{12}+M_{22}N_{22}) - (M_{11}N_{12}+M_{12}N_{22})(M_{21}N_{11}+M_{22}N_{21}) \\
    & = M_{11}N_{11}M_{21}N_{12} + M_{11}N_{11}M_{22}N_{22} + M_{12}N_{21}M_{21}N_{12} + M_{12}N_{21}M_{22}N_{22} \\
    & - M_{11}N_{12}M_{21}N_{11} - M_{11}N_{12}M_{22}N_{21} - M_{12}N_{22}M_{21}N_{11} - M_{12}N_{22}M_{22}N_{21} \\
    & = M_{11}M_{22}(N_{11}N_{22} - N_{12}N_{21}) - M_{12}M_{21}(N_{22}N_{11}-N_{21}N_{12})\\
    & = (M_{11}M_{22}-M_{12}M_{21})(N_{11}N_{22} - N_{12}N_{21}) = |M||N|
\end{split}
\end{equation*}
\end{proof}

The determinant of a matrix and its transpose are identical:

\begin{equation*}
    |M^{T}| = |M|
\end{equation*}

\begin{proof}
\begin{align*}
    M & = \begin{bmatrix} M_{11} & M_{12} \\ M_{21} & M_{22} \end{bmatrix} & M^{T} & = \begin{bmatrix} M_{11} & M_{21} \\ M_{12} & M_{22} \end{bmatrix}
\end{align*}

\begin{equation*}
    |M| = |M^{T}| = M_{11}M_{22} - M_{21}M_{12}
\end{equation*}
\end{proof} 

A matrix $M_{ex}$ obtained from $M$ by exchanging any two rows or any two columns satisfies

\begin{equation*}
    |M_{ex}| = -|M|
\end{equation*}

\begin{proof}
For a $2 \times 2$ matrix there is only one possible exchange of rows and one possible exchange of columns. The matrices $M_{ex,R}$ and $M_{ex,C}$ obtained by exchanging rows and columns respectively are:

\begin{align*}
    M_{ex,R} & = \begin{bmatrix} M_{21} & M_{22} \\ M_{11} & M_{12} \end{bmatrix} & M_{ex,C} & = \begin{bmatrix} M_{21} & M_{11} \\ M_{22} & M_{12} \end{bmatrix}
\end{align*}

\begin{align*}
    |M_{ex,R}| & = M_{21}M_{12} - M_{22}M_{11}  & |M_{ex,C}| & = M_{21}M_{12} - M_{11}M_{22} \\  
    & = -(M_{11}M_{22}-M_{12}M_{21}) = -|M|  & & = -(M_{11}M_{22}-M_{12}M_{21}) = -|M|
\end{align*}
\end{proof}

A matrix $M_{a}$ obtained by re-scaling one of the rows or columns of a matrix $M$ by a factor $a$ satisfies

\begin{equation*}
    |M_{a}| = a|M|
\end{equation*}

\begin{proof}
Since the determinant involves taking products of cross-terms of a matrix while $a$ scales a single column or row, it follows that the determinant of the re-scaled matrix will always be of the form

\begin{equation*}
    |M_{a}| = a(M_{11}M_{22}-M_{12}M_{21}) = a|M|
\end{equation*}
\end{proof} 


If $M^{'}$ is a matrix with identical rows or columns

\begin{equation*}
    M = M^{'}_{a} \implies |M| = 0 
\end{equation*}

\begin{proof}
The following equality follows from combining the third and fourth result: 

\begin{equation*}
    |M| = a|M^{'}| = -a|M^{'}_{ex}| 
\end{equation*}


$M^{'}_{ex}$ is obtained by exchanging the identical rows or columns. Clearly the matrix remains unchanged, i.e. $M^{'}_{ex} = M^{'}$. The above equality can only be satisfied if $|M^{'}|$ vanishes. Therefore $|M| = 0$
\end{proof}

The determinant of a matrix and its inverse are related as follows:

\begin{equation*}
    |M^{-1}| = \frac{1}{|M|}
\end{equation*}

\begin{proof}
Substitute $N = M^{-1}$ in the first result and obtain the above result. 
\end{proof}

** SOLVED Problem 8.3.2
CLOSED: [2022-11-01 Tue 14:38]

A unique solution does not exist for the system.

** SOLVED Problem 8.3.3
CLOSED: [2022-11-01 Tue 14:38]

Consider a $2 \times 2$ matrix $M$

\begin{equation*}
    M = \begin{bmatrix} M_{11} & M_{12} \\ M_{21} & M_{22} \end{bmatrix}
\end{equation*}

The co-factor matrix $M_{C}$ has the elements

\begin{equation*}
    (M_{C})_{ij} = (-1)^{i+j} \textit{determinant of matrix with row i and column j deleted}
\end{equation*}

For the matrix $M$, the co-factor matrix $M_{C}$ is 

\begin{equation*}
    M_{C} = \begin{bmatrix} M_{22} & -M_{21} \\ -M_{12} & M_{11}\end{bmatrix}
\end{equation*}

The determinant of $M$ is 

\begin{equation*}
    |M| = M_{11}(M_{C})_{11} + M_{12}(M_{C})_{12} = M_{11}M_{22} - M_{12}M_{21}
\end{equation*}

Thus, we have used the notion of the co-factor matrix to get the familiar definition of the determinant of a $2 \times 2$ matrix. Here the determinant of a $1 \times 1$ matrix is interpreted as just the number. 

** SOLVED Problem 8.3.4
CLOSED: [2022-11-01 Tue 14:38]

The inverse of a matrix $M$ is given by: 

\begin{equation*}
    M^{-1} = \frac{M_{C}^{T}}{|M|}
\end{equation*}

To solve the system 

\begin{align*} 
3x - y - z &=  2 \\ 
x - 2y -3z &=  0 \\
4x + y + 2z &= 4
\end{align*}

we need to invert the following matrix:

\begin{equation*}
    M = \begin{bmatrix} 3 & -1 & -1 \\ 1 & -2 & -3 \\ 4 & 1 & 2 \end{bmatrix}
\end{equation*}

The co-factor matrix and determinant of $M$ are

\begin{align*}
    M_{C} & = \begin{bmatrix} -1 & -14 & 9 \\ 1 & 10 & -7 \\ 1 & 8 & -5 \end{bmatrix} & |M| & = M_{11}(M_{C})_{11} + M_{12}(M_{C})_{12} + M_{13}(M_{C})_{13} = 2 
\end{align*}

Taking the transpose of $M_{C}$ and substituting in the formula for the inverse of a matrix

\begin{equation*}
    M^{-1} = \frac{1}{2}\begin{bmatrix} -1 & 1 & 1 \\ -14 & 10 & 8 \\ 9 & -7 & -5 \end{bmatrix}
\end{equation*}

To solve the system 

\begin{align*} 
3x + y + 2z &=  3 \\ 
2x - 3y -z &=  -2 \\
x + y + z &= 1
\end{align*}

we need to invert the following matrix:

\begin{equation*}
    M = \begin{bmatrix} 3 & 1 & 2 \\ 2 & -3 & -1 \\ 1 & 1 & 1 \end{bmatrix}
\end{equation*}

The co-factor matrix and determinant of $M$ are

\begin{align*}
    M_{C} & = \begin{bmatrix} -2 & -3 & 5 \\ 1 & 1 & -2 \\ 5 & 7 & -11 \end{bmatrix} & |M| & = M_{11}(M_{C})_{11} + M_{12}(M_{C})_{12} + M_{13}(M_{C})_{13} = 1 
\end{align*}

Taking the transpose of $M_{C}$ and substituting in the formula for the inverse of a matrix

\begin{equation*}
    M^{-1} = \begin{bmatrix} -2 & 1 & 5 \\ -3 & 1 & 7 \\ 5 & -2 & -11 \end{bmatrix}
\end{equation*}

** SOLVED Problem 8.3.5
CLOSED: [2022-11-01 Tue 14:38]

The given matrix is

\begin{equation*}
    M = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 10 \end{bmatrix}
\end{equation*}

The inverse of a matrix $M$ is given by: 

\begin{equation*}
    M^{-1} = \frac{M_{C}^{T}}{|M|}
\end{equation*}

The co-factor matrix and determinant of $M$ are

\begin{align*}
    M_{C} & = \begin{bmatrix} 2 & 2 & -3 \\ 4 & -11 & 6 \\ -3 & 6 & -3 \end{bmatrix} & |M| & = M_{11}(M_{C})_{11} + M_{12}(M_{C})_{12} + M_{13}(M_{C})_{13} = -3
\end{align*}

Taking the transpose of $M_{C}$ and substituting in the formula for the inverse of a matrix

\begin{equation*}
    M^{-1} = \frac{1}{3}\begin{bmatrix} -2 & -4 & 3 \\ -2 & 11 & -6 \\ 3 & -6 & 3 \end{bmatrix}
\end{equation*}

To verify that the inverse does the job it is sufficient to show 

\begin{equation*}
    M^{-1}M = I 
\end{equation*}

\begin{equation*}
\begin{split}
     M^{-1}M = \frac{1}{3}\begin{bmatrix} -2 & -4 & 3 \\ -2 & 11 & -6 \\ 3 & -6 & 3 \end{bmatrix}\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 10 \end{bmatrix} & = \frac{1}{3}\begin{bmatrix} -2.1 -4.4 + 3.7 & -2.2 -4.5 + 3.8 & -2.3 -4.6 + 3.10 \\ -2.1 + 11.4 -6.7 & -2.2 + 11.5 -6.8 & -2.3 + 11.6 -6.10 \\ 3.1 -6.4 +3.7 & 3.2 -6.5 +3.8 & 3.3 -6.6 + 3.10 \end{bmatrix} \\
     & = \frac{1}{3}\begin{bmatrix} 3 & 0 & 0 \\  0 & 3 & 0 \\ 0 & 0 & 3\end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\  0 & 1 & 0 \\ 0 & 0 & 1\end{bmatrix} = I
\end{split}
\end{equation*}

** TOSOLVE Problem 8.3.6
** SOLVED Problem 8.3.7
CLOSED: [2022-11-01 Tue 14:39]

The cross product of two vectors linearly independent vectors $\Vec{A}$ and $\Vec{B}$ denoted by: $\Vec{A} \times \Vec{B}$ is defined as a vector of magnitude

\begin{equation*}
    |\Vec{A}\times \Vec{B}| = |A||B||sin\theta_{AB}|
\end{equation*}

and direction perpendicular to the plane containing  $\Vec{A}$ and $\Vec{B}$. There are two perpendicular directions so we adopt a convention given by the \textit{right-hand rule}: Curl your right hand from $\Vec{A}$ to $\Vec{B}$ along the shortest rotation angle; the thumb points in the direction of the cross product. Clearly, this convention implies that the cross product is \textit{anti-commutative}, i.e. $\Vec{A} \times \Vec{B}$ = - $\Vec{B} \times \Vec{A}$. It is also a fact that the cross product is \textit{distributive over addition}, i.e. $\Vec{A} \times (\Vec{B}+\Vec{C}) = \Vec{A}\times \Vec{B} + \Vec{A}\times \Vec{C}$. Note that the cross product is defined only in three-dimensional spaces. 

Based on the definition of the cross product and it anti-commutativity the standard basis vectors satisfy the following equalities in a right-hand coordinate system:

\begin{align*}
    \vec{i}\times \vec{j} & = \vec{k} = -\vec{j}\times \vec{i} \\
    \vec{j}\times \vec{k} & = \vec{i} = -\vec{k}\times \vec{j} \\
    \vec{k}\times \vec{i} & = \vec{j} = -\vec{i}\times \vec{k} \\
\end{align*}

The lack of linear independence implies 

\begin{equation*}
    \vec{i}\times \vec{i} = \vec{j}\times \vec{j} = \vec{k}\times \vec{k} = \vec{0}
\end{equation*}

These equalities along with the distributivity and linearity of the cross product allows us to write the cross product of two vectors $\vec{A}$ and $\vec{B}$ written in terms of the standard basis vectors:

\begin{align*}
    \Vec{A} & = A_{x}\vec{i} + A_{y}\vec{j} + A_{z}\vec{k} \\
    \Vec{B} & = B_{x}\vec{i} + B_{y}\vec{j} + B_{z}\vec{k}
\end{align*}

\begin{equation*}
    \begin{split}
    \Vec{A} \times \Vec{B} & =  (A_{x}\vec{i} + A_{y}\vec{j} + A_{z}\vec{k})\times (B_{x}\vec{i} + B_{y}\vec{j} + B_{z}\vec{k}) \\
    & = A_{x}B_{x}\vec{i}\times\vec{i} + A_{x}B_{y}\vec{i}\times\vec{j} + A_{x}B_{z}\vec{i}\times\vec{k} \\
    & + A_{y}B_{x}\vec{j}\times\vec{i} + A_{y}B_{y}\vec{j}\times\vec{j} + A_{y}B_{z}\vec{j}\times\vec{k} \\ 
    & + A_{z}B_{x}\vec{k}\times\vec{i} + A_{z}B_{y}\vec{k}\times\vec{j} + A_{z}B_{z}\vec{k}\times\vec{k} \\
    & = A_{x}B_{y}\vec{k} - A_{x}B_{Z}\vec{j} - A_{y}B_{x}\vec{k} + A_{y}B_{z}\vec{i} + A_{z}B_{x}\vec{j} - A_{z}B_{y}\vec{i} \\
    & = (A_{y}B_{z}-A_{z}B_{y})\vec{i} - (A_{x}B_{z}-A_{z}B_{x})\vec{j} + (A_{x}B_{y}-A_{y}B_{x})\vec{k}
    \end{split}
\end{equation*}

This is also what we get on evaluating the formal determinant $\begin{vmatrix} \vec{i} & \vec{j} & \vec{k} \\ A_{x} & A_{y} & A_{z} \\ B_{x} & B_{y} & B_{z} \end{vmatrix}$. Therefore

\begin{equation*}
    \Vec{A}\times \vec{B} = \begin{vmatrix} \vec{i} & \vec{j} & \vec{k} \\ A_{x} & A_{y} & A_{z} \\ B_{x} & B_{y} & B_{z} \end{vmatrix} = (A_{y}B_{z}-A_{z}B_{y})\vec{i} - (A_{x}B_{z}-A_{z}B_{x})\vec{j} + (A_{x}B_{y}-A_{y}B_{x})\vec{k}
\end{equation*}

The curl of a sufficiently smooth vector field $\vec{V}$ in three dimensions denoted by $\Vec{\nabla} \times \Vec{V}$ is defined as 

\begin{equation*}
    \Vec{\nabla} \times \Vec{V} = \left(\frac{\partial V_{z}}{\partial y}-\frac{\partial V_{y}}{\partial z}\right)\vec{i} - \left(\frac{\partial V_{z}}{\partial x}-\frac{\partial V_{x}}{\partial z}\right)\vec{j} + \left(\frac{\partial V_{y}}{\partial x}-\frac{\partial V_{x}}{\partial y}\right)\vec{k}  
\end{equation*}

This is also what we get on evaluating the formal determinant $\begin{vmatrix} \vec{i} & \vec{j} & \vec{k} \\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\ V_{x} & V_{y} & V_{z} \end{vmatrix}$. Therefore

\begin{equation*}
    \Vec{\nabla} \times \Vec{V} = \begin{vmatrix} \vec{i} & \vec{j} & \vec{k} \\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z} \\ V_{x} & V_{y} & V_{z} \end{vmatrix} = \left(\frac{\partial V_{z}}{\partial y}-\frac{\partial V_{y}}{\partial z}\right)\vec{i} - \left(\frac{\partial V_{z}}{\partial x}-\frac{\partial V_{x}}{\partial z}\right)\vec{j} + \left(\frac{\partial V_{y}}{\partial x}-\frac{\partial V_{x}}{\partial y}\right)\vec{k} 
\end{equation*}

We have two vectors $\Vec{A}$ and $\Vec{B}$ with lengths $|A|$ and $|B|$ respectively and lies in the x-y plane making angle $0 < \theta_{A} < \theta_{B} < 90^{\circ}$, with respect to the $x-$axis. The cross product $\Vec{A}\times \Vec{B}$ is a vector with length and $|\vec{A}\times \vec{B}| = |A||B|sin(\theta_{B}-\theta_{A}) = |A||B|(sin\theta_{B}cos\theta_{A}-cos\theta_{B}sin\theta_{A})$. Because vectors $\Vec{A}$ and $\Vec{B}$ live in the $x-y$-plane the direction of $\Vec{A}\times \Vec{B}$ is easily determined using the right-hand rule: it points in the direction of the unit vector $\vec{k}.$ 
$\Vec{A}$ and $\Vec{B}$ can be written in cartesian form as

\begin{align*}
    \Vec{A} & = |A|cos\theta_{A}\vec{i} + |A|sin\theta_{A}\vec{j} \\
    \Vec{B} & = |B|cos\theta_{B}\vec{i} + |B|sin\theta_{B}\vec{j}
\end{align*}

The formal determinant for the cross product $\Vec{A}\times \Vec{B}$ is $\begin{vmatrix} \vec{i} & \vec{j} & \vec{k} \\ |A|cos\theta_{A} & |A|sin\theta_{A} & 0 \\ |B|cos\theta_{B} & |B|sin\theta_{B} & 0 \end{vmatrix}$. Evaluating it gives us the old answer

\begin{equation*}
    \Vec{A} \times \Vec{B} = |A||B|(sin\theta_{B}cos\theta_{A}-cos\theta_{B}sin\theta_{A})\vec{k}
\end{equation*}

** SOLVED Problem 8.3.8
CLOSED: [2022-11-01 Tue 14:39]

The "box" or scalar triple product of three linearly independent vectors $\vec{A}$, $\vec{B}$ and $\vec{C}$ denoted by $\Vec{A}.(\Vec{B}\times \Vec{C})$ is defined as the dot product of one of the vectors with the cross product of the other two. 

\begin{align*}
    \Vec{A}.(\Vec{B}\times \Vec{C}) & = ( A_{x}\vec{i} + A_{y}\vec{j} + A_{z}\vec{k})([B_{y}C_{z}-B_{z}C_{y}]\vec{i} - [B_{x}C_{z}-B_{z}C_{x}]\vec{j} + [B_{x}C_{y}-B_{y}C_{x}]\vec{k}) \\
    & = A_{x}(B_{y}C_{z}-B_{z}C_{y}) - A_{y}(B_{x}C_{z}-B_{z}C_{x}) + A_{z}(B_{x}C_{y}-B_{y}C_{x})
\end{align*}

This is also what we get on evaluating the determinant $\begin{vmatrix} A_{x} & A_{y} & A_{z} \\ B_{x} & B_{y} & B_{z} \\ C_{x} & C_{y} & C_{z}\end{vmatrix}$. Therefore

\begin{equation*}
    \Vec{A}.(\Vec{B}\times \Vec{C}) = \begin{vmatrix} A_{x} & A_{y} & A_{z} \\ B_{x} & B_{y} & B_{z} \\ C_{x} & C_{y} & C_{z}\end{vmatrix} = A_{x}(B_{y}C_{z}-B_{z}C_{y}) - A_{y}(B_{x}C_{z}-B_{z}C_{x}) + A_{z}(B_{x}C_{y}-B_{y}C_{x})
\end{equation*}

The determinant remains unchanged under cyclic change of rows

\begin{equation*}
    \begin{vmatrix} A_{x} & A_{y} & A_{z} \\ B_{x} & B_{y} & B_{z} \\ C_{x} & C_{y} & C_{z}\end{vmatrix} = \begin{vmatrix} C_{x} & C_{y} & C_{z} \\ A_{x} & A_{y} & A_{z} \\ B_{x} & B_{y} & B_{z} \end{vmatrix} = \begin{vmatrix} B_{x} & B_{y} & B_{z} \\ C_{x} & C_{y} & C_{z} \\ A_{x} & A_{y} & A_{z}\end{vmatrix} 
\end{equation*}

This implies 

\begin{equation*}
    \Vec{A}.(\Vec{B}\times \Vec{C}) = \Vec{C}.(\Vec{A}\times \Vec{B}) = \Vec{B}.(\Vec{C}\times \Vec{A})
\end{equation*}

The anti-symmetry of the determinants (i.e., its change of sign) under exchange of rows corresponds to the anti-symmetry of the cross product of two vectors under the exchange.

\begin{equation*}
    \Vec{A}.(\Vec{B}\times \Vec{C}) =  -\Vec{A}.(\Vec{C}\times \Vec{B}) = \Vec{C}.(\Vec{A}\times \Vec{B}) = -\Vec{C}.(\Vec{B}\times \Vec{A}) = \Vec{B}.(\Vec{C}\times \Vec{A}) = -\Vec{B}.(\Vec{A}\times \Vec{C})
\end{equation*}

The vanishing of the determinant when two rows are proportional corresponds to the vanishing of the "box" when two of the adjacent edges become parallel.

** SOLVED Problem 8.3.9
CLOSED: [2022-11-01 Tue 14:39]

Consider the passage from cartesian coordinates $x,y,z$ to some general non-orthogonal coordinates $u,v,w$. Since the coordinates are not orthogonal, the solid bounded by the surfaces $u,v,w$ and $u+du, v+dv, w+dw$ does not have perpendicular edges and we cannot write the volume element as $h_{u}h_{v}h_{w}dudvdw$. We can however express the edges of this solid by using the cartesian coordinates as basis vectors. A small change $du$ in $u$ will cause changes in $x,y$ and $z$ given by $\frac{\partial x}{\partial u}du, \frac{\partial y}{\partial u}du$ and $\frac{\partial z}{\partial u}du$ respectively. Therefore we may write the edge of the solid along the $u$ axis as an infinitesimal vector written with cartesian basis vectors: $\vec{dr_{u}} = \left[ \vec{i}\frac{\partial x}{\partial u} +  \vec{j}\frac{\partial y}{\partial u} +  \vec{k}\frac{\partial z}{\partial u}\right]du$. We can write the edges of the solid along the $v$ and $w$ direction as an infinitesimal vectors written with cartesian basis vectors in a similar fashion: 

\begin{align*}
    \vec{dr_{u}} & = \left[ \vec{i}\frac{\partial x}{\partial u} +  \vec{j}\frac{\partial y}{\partial u} +  \vec{k}\frac{\partial z}{\partial u}\right]du \\
    \vec{dr_{v}} & = \left[ \vec{i}\frac{\partial x}{\partial v} +  \vec{j}\frac{\partial y}{\partial v} +  \vec{k}\frac{\partial z}{\partial v}\right]dv \\
    \vec{dr_{w}} & = \left[ \vec{i}\frac{\partial x}{\partial w} +  \vec{j}\frac{\partial y}{\partial w} +  \vec{k}\frac{\partial z}{\partial w}\right]dw
\end{align*}

Now the volume of the solid is found easily by taking the box product of these three vectors:

\begin{equation*}
    \vec{dr_{u}}.(\vec{dr_{v}}\times \vec{dr_{w}}) = dudvdw \begin{vmatrix} \frac{\partial x}{\partial u} & \frac{\partial y}{\partial u} & \frac{\partial z}{\partial u}\\ \frac{\partial x}{\partial v} & \frac{\partial y}{\partial v} & \frac{\partial z}{\partial v} \\ \frac{\partial x}{\partial w} & \frac{\partial y}{\partial w} & \frac{\partial z}{\partial w}  \end{vmatrix}
\end{equation*}

The volume of the solid is given by $J\left(\frac{x,y,z}{u,v,w}\right)dudvdw.$ Therefore

\begin{equation*}
    J\left(\frac{x,y,z}{u,v,w}\right) = \begin{vmatrix} \frac{\partial x}{\partial u} & \frac{\partial y}{\partial u} & \frac{\partial z}{\partial u}\\ \frac{\partial x}{\partial v} & \frac{\partial y}{\partial v} & \frac{\partial z}{\partial v} \\ \frac{\partial x}{\partial w} & \frac{\partial y}{\partial w} & \frac{\partial z}{\partial w}  \end{vmatrix}
\end{equation*}

This means that 

\begin{equation*}
    \int dxdydz \longrightarrow \int J\left(\frac{x,y,z}{u,v,w}\right)dudvdw 
\end{equation*}

** SOLVED Problem 8.4.1
CLOSED: [2022-11-01 Tue 14:39]

If a matrix $M$ satisfies 

\begin{equation*}
    M_{ij} = \pm M_{ji}
\end{equation*}

i.e., it equals $\pm$ its own transpose it is said to be \textit{symmetric/anti-symmetric}. For the diagonal elements of an anti-symmetric matrix we must have

\begin{equation*}
    M_{ii} = -M_{ii}
\end{equation*}

This is satisfied only if the diagonal elements vanish identically. A $4 \times 4$ anti-symmetric matrix is given below

\begin{equation*}
    M = \begin{bmatrix} 0 & 1 & 2 & 3 \\ -1 & 0 & 4 & 5 \\ -2 & -4 & 0 & 6 \\ -3 & -5 & -6 & 0 \end{bmatrix} = -M^{T}
\end{equation*}
\\

** SOLVED Problem 8.4.2
CLOSED: [2022-11-01 Tue 14:39]

The adjoint of a matrix $M^{\dagger}$ is defined as follows:

\begin{equation*}
    M^{\dagger}_{ij} = M^{*}_{ji}
\end{equation*}

The adjoint of a matrix is obtained by transposing the given matrix and taking the complex conjugate of all the elements. A matrix that obeys 

\begin{equation*}
    M^{\dagger} = \pm M
\end{equation*}

is said to be \textit{hermitian/anti-hermitian}. Any real symmetric matrix is automatically hermitian. The diagonal elements of a hermitian matrix $M$ must satisfy

\begin{equation*}
    M^{\dagger}_{ii} = M^{*}_{ii} = M_{ii}
\end{equation*}

This is only satisfied if $M_{ii}$ is real. Therefore the diagonal elements of a hermitian matrix are real. 

In general, a hermitian matrix $M$ must satisfy 

\begin{equation*}
    M^{\dagger}_{ij} = M^{*}_{ji} = M_{ij}
\end{equation*}

It follows that a new matrix $iM$ obtained by multiplying $M$ with $i$ must satisfy

\begin{equation*}
     (iM)^{\dagger}_{ij} = (iM)^{*}_{ji} = -(iM)_{ji} = -(iM)_{ij}
\end{equation*}

Therefore the matrix $iM$ is anti-hermitian.

** SOLVED Problem 8.4.3
CLOSED: [2022-11-01 Tue 14:39]

The adjoint of a product of two matrices $M$ and $N$ that are hermitian is defined as:

\begin{equation*}
    \begin{split}
        (MN)^{\dagger}_{ij} & = (MN)^{*}_{ji} \\
        &= \sum_{k}(M_{jk}N_{ki})^{*} \\
        &= \sum_{k}M^{*}_{jk}N^{*}_{ki} \\
        & = \sum_{k}M^{\dagger}_{kj}N^{\dagger}_{ik} \\
        & = \sum_{k}N^{\dagger}_{ik}M^{\dagger}_{kj} \\
        & = (N^{\dagger}M^{\dagger})_{ij}
    \end{split}
\end{equation*}

Therefore

\begin{equation*}
    (MN)^{\dagger} = N^{\dagger}M^{\dagger} = NM
\end{equation*}

just like in the case of the inverse of a product and the transpose of a product. Consequently even if $M = M^{\dagger}$ and $N = N^{\dagger}$, $(MN)^{\dagger} \neq MN$ unless the matrices commute. The product of two hermitian matrices is not generally hermitian unless they commute.

** SOLVED Problem 8.4.4
CLOSED: [2022-11-01 Tue 14:39]

A \textit{unitary matrix} is one whose adjoint equals its inverse

\begin{align*}
    UU^{\dagger} & = I = U^{\dagger}U \\
    U^{-1} & = U^{\dagger}
\end{align*}

Unlike symmetric and hermitian matrices, the product of two unitary matrices is unitary.

The matrix $U$ below is a unitary matrix

\begin{equation*}
    U = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & i \\ i & 1 \end{bmatrix}
\end{equation*}

The adjoint $U^{\dagger}$ of $U$ is 

\begin{equation*}
    U^{\dagger} = \frac{1}{\sqrt{2}}\begin{bmatrix} 1 & -i \\ -i & 1 \end{bmatrix}
\end{equation*}

\begin{equation*}
    UU^{\dagger} = \frac{1}{2}\begin{bmatrix} 1 & i \\ i & 1 \end{bmatrix}\begin{bmatrix} 1 & -i \\ -i & 1 \end{bmatrix} = \frac{1}{2}\begin{bmatrix} 2 & 0 \\ 0 & 2\end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} = I 
\end{equation*}

** SOLVED Problem 8.4.5
CLOSED: [2022-11-01 Tue 14:39]

\begin{align*}
    U &= \begin{bmatrix} \frac{1+i\sqrt{3}}{4} & \frac{\sqrt{3}(1+i)}{2\sqrt{2}} \\ -\frac{\sqrt{3}(1+i)}{2\sqrt{2}} & \frac{i+\sqrt{3}}{4} \end{bmatrix} & U^{\dagger} &= \begin{bmatrix} \frac{1-i\sqrt{3}}{4} & -\frac{\sqrt{3}(1-i)}{2\sqrt{2}} \\ \frac{\sqrt{3}(1-i)}{2\sqrt{2}} & \frac{\sqrt{3}-i}{4} \end{bmatrix} 
\end{align*}

\begin{equation*}
    UU^{\dagger} = \begin{bmatrix} \frac{1+i\sqrt{3}}{4} & \frac{\sqrt{3}(1+i)}{2\sqrt{2}} \\ -\frac{\sqrt{3}(1+i)}{2\sqrt{2}} & \frac{i+\sqrt{3}}{4} \end{bmatrix}\begin{bmatrix} \frac{1-i\sqrt{3}}{4} & -\frac{\sqrt{3}(1-i)}{2\sqrt{2}} \\ \frac{\sqrt{3}(1-i)}{2\sqrt{2}} & \frac{\sqrt{3}-i}{4} \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I
\end{equation*}

Since $U^{\dagger}=U^{-1}$, $U$ is a unitary matrix. 

To prove that the determinant of a unitary matrix must be a uni-modular complex number we will use the result 

\begin{equation*}
    |U^{*}| = |U|^{*}
\end{equation*}

Let us prove this first. 

\begin{equation*}
    \begin{split}
        |U^{*}| = \sum_{k} U^{*}_{1k}(U^{*}_{C})_{1k} = \sum_{k} (U_{1k}(U_{C})_{1k})^{*} = \left(\sum_{k} U_{1k}(U_{C})_{1k} \right)^{*} = |U|^{*}
    \end{split}
\end{equation*}

Here $U^{*}_{C}$ and $U_{C}$ are the co-factor matrices of $U^{*}$ and $U$ respectively. We have used the fact that the complex conjugate of a product is the product of the complex conjugates and that the complex conjugate of a sum is the sum of the complex conjugates.

Now for a unitary matrix we have $UU^{\dagger} = I$ Thus using the property of determinants we write 

\begin{equation*}
    |UU^{\dagger}| = |I| = 1 = |U||U^{\dagger}| = |U||(U^{T})^{*}| = |U||(U^{T})|^{*} = |U||U|^{*}  
\end{equation*}

Because $|U||U|^{*} = ||U||^{2} = 1$, $|U|$ must be a uni-modular complex number. For the given matrix 

\begin{equation*}
    |U| = \left(\frac{1+i\sqrt{3}}{4}\right) \left(\frac{i+\sqrt{3}}{4}\right) - \left(\frac{\sqrt{3}(1+i)}{2\sqrt{2}}\right)\left(-\frac{\sqrt{3}(1+i)}{2\sqrt{2}}\right) = i
\end{equation*}

Sure enough, $|U|=i$ is a uni-modular complex number.

** SOLVED Problem 8.4.6
CLOSED: [2022-11-01 Tue 14:39]

An orthogonal matrix is one that obeys 

\begin{align*}
    OO^{T} & = I = O^{T}O \\
    O^{-1} & = O^{T}
\end{align*}

Thus to show that the rotation matrix $R_{\theta}$ is orthogonal it is sufficient to show that $R_{\theta}R^{T}_{\theta} = I$. We know 

\begin{align*}
    R_{\theta} & = \begin{bmatrix} cos\theta & sin\theta \\ -sin\theta & cos\theta \end{bmatrix} & R^{T}_{\theta} & = \begin{bmatrix} cos\theta & - sin\theta \\ sin\theta & cos\theta \end{bmatrix}    
\end{align*}

\begin{equation*}
    \begin{split}
        R_{\theta}R^{T}_{\theta} = \begin{bmatrix} cos\theta & sin\theta \\ -sin\theta & cos\theta \end{bmatrix}\begin{bmatrix} cos\theta & - sin\theta \\ sin\theta & cos\theta \end{bmatrix}  = \begin{bmatrix} sin^{2}\theta + cos^{2}\theta & -cos\theta sin\theta + sin\theta cos\theta \\ -sin\theta cos\theta + cos\theta sin\theta & sin^{2}\theta + cos^{2}\theta\end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1\end{bmatrix} = I
    \end{split}
\end{equation*}

** SOLVED Problem 8.4.7
CLOSED: [2022-11-01 Tue 14:39]

Write $O$ and $O^{T}$ as 

\begin{align*}
    O & = \begin{bmatrix} A_{x} & A_{y} & A_{z} \\ B_{x} & B_{y} & B_{z} \\ C_{x} & C_{y} & C_{z} \end{bmatrix} & O^{T} & = \begin{bmatrix} A_{x} & B_{x} & C_{x} \\ A_{y} & B_{y} & C_{y} \\ A_{z} & B_{z} & C_{z} \end{bmatrix}
\end{align*}

Assuming $O$ is orthogonal and interpreting the rows as components of vectors $\vec{A}$, $\vec{B}$ and $\vec{C}$, the following holds 

\begin{equation*}
    OO^{T} = \begin{bmatrix} A_{x} & A_{y} & A_{z} \\ B_{x} & B_{y} & B_{z} \\ C_{x} & C_{y} & C_{z} \end{bmatrix}\begin{bmatrix} A_{x} & B_{x} & C_{x} \\ A_{y} & B_{y} & C_{y} \\ A_{z} & B_{z} & C_{z} \end{bmatrix} = \begin{bmatrix} \vec{A}.\vec{A} & \vec{A}.\vec{B} & \vec{A}.\vec{C} \\ \vec{B}.\vec{A} & \vec{B}.\vec{B} & \vec{B}.\vec{C} \\ \vec{C}.\vec{A} & \vec{C}.\vec{B} & \vec{C}.\vec{C} \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = I
\end{equation*}

Therefore, all of the following must be satisfied 

\begin{equation*}
    \vec{A}.\vec{A} = \vec{B}.\vec{B} = \vec{C}.\vec{C} = 1 
\end{equation*}

\begin{equation*}
    \vec{A}.\vec{B} = \vec{A}.\vec{C} = \vec{B}.\vec{A} = \vec{B}.\vec{C} = \vec{C}.\vec{A} = \vec{C}.\vec{B} = 0 
\end{equation*}

Clearly, these hold only for ortho-normal vectors. Therefore, the rows of an $N \times N$ orthogonal matrix constitute the components of $N$ ortho-normal vectors. The same goes for the columns.

** SOLVED Problem 8.4.8
CLOSED: [2022-11-01 Tue 14:39]

If $L = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$ then 

\begin{equation*}
    L^{2} = LL = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} = -I
\end{equation*}

Now consider $F(L) = e^{\theta L}$. Writing it out as an infinite series 

\begin{equation*}
\openup 3\jot
\begin{split}
    F(L) = e^{\theta L} & = \frac{(\theta L)^{0}}{0!} + \frac{(\theta L)^{1}}{1!} + \frac{(\theta L)^{2}}{2!} + \frac{(\theta L)^{3}}{3!} + \frac{(\theta L)^{4}}{4!} + ...\\
    & = \frac{\theta^{0} L^{0}}{0!} + \frac{\theta^{1} L^{1}}{1!} + \frac{\theta^{2} L^{2}}{2!} + \frac{\theta^{3} L^{3}}{3!} + \frac{\theta^{4} L^{4}}{4!} + ... \\
    & = I + \theta L - \frac{\theta^{2} I}{2!} - \frac{\theta^{3} L}{3!} + \frac{\theta^{4}I}{4!} + ... \\
    & = I(1-\frac{\theta^{2}}{2!} + \frac{\theta^{4}}{4!}+ ...) + L(\theta - \frac{\theta^{3}}{3!} + ...) \\ 
    & = Icos\theta + Lsin\theta \\
    & = \begin{bmatrix} cos\theta & - sin\theta \\ sin\theta & cos\theta \end{bmatrix} = R^{T}_{\theta} = R^{-1}_{\theta}
\end{split}
\end{equation*}

** TOSOLVE Problem 8.4.9
** SOLVED Problem 8.4.10
CLOSED: [2022-11-01 Tue 14:39]

We intend to show that if matrix $H$ is hermitian then

\begin{equation*}
    U = e^{iH}
\end{equation*}

is unitary. Writing $e^{iH}$ as a power series in $iH$ 

\begin{equation*}
    \begin{split}
        U & = e^{iH} = I + iH + \frac{(iH)^{2}}{2} + \frac{(iH)^{3}}{3} + ... \\
    \end{split}
\end{equation*}

Taking the adjoint of both sides 

\begin{equation*}
    \begin{split}
        U^{\dagger} & = I + (-iH^{\dagger}) + \frac{(-iH^{\dagger})^{2}}{2} + \frac{(-iH^{\dagger})^{3}}{3} + ... \\
        & = e^{-iH^{\dagger}}
    \end{split}
\end{equation*}

To  show that $U$ is unitary, we must show that $UU^{\dagger} = I$

\begin{equation*}
    \begin{split}
        UU^{\dagger} & = e^{iH}e^{-iH^{\dagger}} \\
        & = e^{iH - iH^{\dagger}} \\
        & = e^{iH - iH} = I
    \end{split}
\end{equation*}

In combining the exponentials we have used the fact that a hermitian matrix commutes with its adjoint. In the next step we have used the definition of hermitian matrix to write $H^{\dagger}$ as $H$. Finally we have used the fact that the exponential of a zero matrix is the identity matrix.

** SOLVED Problem 8.4.11
CLOSED: [2022-11-01 Tue 14:39]

The income of the parents was transformed into the income of the children by taking the product of a square matrix with a column vector: 

\begin{equation*}
    \begin{bmatrix} C_{1} \\ C_{2} \end{bmatrix} = \begin{bmatrix} M_{11} & M_{12} \\ M_{21} & M_{22} \end{bmatrix} \begin{bmatrix} P_{1} \\ P_{2} \end{bmatrix} 
\end{equation*}

Taking the transpose of both sides

\begin{equation*}
    \begin{bmatrix} C_{1} &  C_{2} \end{bmatrix} = \begin{bmatrix} P_{1} & P_{2} \end{bmatrix} \begin{bmatrix} M_{11} & M_{21} \\ M_{12} & M_{22} \end{bmatrix}  
\end{equation*}

In taking the transpose we have used the rule that the transpose of the product is the product of the transpose in reverse and that the transpose of column vector is a row vector and vice-versa. 

Both these formulations give the same system 

\begin{align*}
C_{1} & = M_{11}P_{1} + M_{12}P_{2} \\
C_{2} & = M_{21}P_{1} + M_{22}P_{2}
\end{align*}

Thus if $C = MP$, then $C^{T} = P^{T}M^{T}$.

** SOLVED Problem 8.4.12
CLOSED: [2022-11-01 Tue 14:39]

Writing $C^{T} = P^{T}M^{T}$ explicitly in matrix form

\begin{equation*}
    \begin{bmatrix} C_{1} &  C_{2} \end{bmatrix} = \begin{bmatrix} P_{1} & P_{2} \end{bmatrix} \begin{bmatrix} M_{11} & M_{21} \\ M_{12} & M_{22} \end{bmatrix}  
\end{equation*}

Carrying out the multiplication: 

\begin{equation*}
    \begin{bmatrix} C_{1} &  C_{2} \end{bmatrix} = \begin{bmatrix} M_{11}P_{1} + M_{12}P_{2} &  M_{21}P_{1} + M_{22}P_{2} \end{bmatrix}
\end{equation*}

Thus $C^{T} = P^{T}M^{T}$ correctly represents the income flows.

** SOLVED Problem 8.4.13
CLOSED: [2022-11-01 Tue 14:39]

We intend to show that if $H$ is hermitian and $U$ is unitary, then $U^{\dagger}HU$ is also hermitian. For this it is sufficient to show that $(U^{\dagger}HU)^{\dagger} = U^{\dagger}HU $  

\begin{equation*}
    (U^{\dagger}HU)^{\dagger} = U^{\dagger}H^{\dagger}(U^{\dagger})^{\dagger} = U^{\dagger}HU
\end{equation*}

The the first step we have used the rule that the adjoint of a product is the product of the adjoints. In the last step we have used the fact that $H$ is hermitian and that the adjoint of the adjoint of a matrix is the matrix itself.

** SOLVED Problem 8.4.14
CLOSED: [2022-11-01 Tue 14:39]

See Problem 8.4.5 for the proof of the statement that the determinant of a unitary matrix is uni-modular.

An orthogonal matrix $O$ is a unitary matrix with real entries. Because it is an unitary matrix we must have $|O||O|^{*} = 1$. Since the orthogonal matrix has real elements, $|O|^{*} = |O|$. Thus we must have $|O|^{2}=1$ or $|O| = \pm 1$

** SOLVED Problem 8.4.15
CLOSED: [2022-11-01 Tue 14:39]

\begin{equation*}
    MN = 0
\end{equation*}

Taking the determinant of both sides and using the property of determinants 

\begin{equation*}
    |M||N| = 0 
\end{equation*}


The product of two numbers vanishes if and only if one of them vanishes.  So one of the matrices $M$ and $N$ must have zero determinant. \\

** SOLVED Problem 8.4.16
CLOSED: [2022-11-01 Tue 14:39]

The Pauli matrices are 

\begin{align*}
    \sigma_{x} & = \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix} & \sigma_{y} & = \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} & \sigma_{z} & = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}
\end{align*}

\begin{equation*}
    \sigma_{x}^{\dagger} & = \left(\begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix}^{T}\right)^{*} = \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix}^{*} = \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix} = \sigma_{x}
\end{equation*}

\begin{equation*}
    \sigma_{y}^{\dagger} & = \left(\begin{bmatrix} 0 & -i\\ i & 0 \end{bmatrix}^{T}\right)^{*} = \begin{bmatrix} 0 & i\\ -i & 0 \end{bmatrix}^{*} = \begin{bmatrix} 0 & -i\\ i & 0 \end{bmatrix} = \sigma_{y}
\end{equation*}

\begin{equation*}
    \sigma_{z}^{\dagger} & = \left(\begin{bmatrix} 1 & 0\\ 0 & -1 \end{bmatrix}^{T}\right)^{*} = \begin{bmatrix} 1 & 0\\ 0 & -1 \end{bmatrix}^{*} = \begin{bmatrix} 1 & 0\\ 0 & -1 \end{bmatrix} = \sigma_{z}
\end{equation*}

Therefore, the Pauli matrices are hermitian. 

\begin{equation*}
    \sigma_{x}\sigma_{x} = \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix}\begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix} = I
\end{equation*}

\begin{equation*}
    \sigma_{y}\sigma_{y} = \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}\begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} = \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix} = I
\end{equation*}

\begin{equation*}
    \sigma_{z}\sigma_{z} = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix} = I
\end{equation*}

Thus the squares of Pauli matrices equals the unit matrix.

From these two features it follows that

\begin{align*}
    \sigma_{x}\sigma_{x}^{\dagger} & = I & \sigma_{y}\sigma_{y}^{\dagger} & = I & \sigma_{z}\sigma_{z}^{\dagger} & = I \\
    \sigma_{x}^{\dagger} & = \sigma_{x}^{-1} & \sigma_{y}^{\dagger} & = \sigma_{y}^{-1} & \sigma_{z}^{\dagger} & = \sigma_{z}^{-1}
\end{align*}

Therefore, the Pauli matrices are unitary too. 

\begin{equation*}
    [\sigma_{x},\sigma_{y}] = \sigma_{x}\sigma_{y} - \sigma_{y}\sigma_{x} =  \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix}\begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} - \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}\begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix} = \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} - \begin{bmatrix} -i & 0 \\ 0 & i \end{bmatrix} = 2i\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} = 2i\sigma_{z}
\end{equation*}

\begin{equation*}
    [\sigma_{y},\sigma_{z}] = \sigma_{y}\sigma_{z} - \sigma_{z}\sigma_{y} =  \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix}\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} - \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}\begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} = \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} - \begin{bmatrix} 0 & -i \\ -i & 0 \end{bmatrix} = 2i\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} = 2i\sigma_{x}
\end{equation*}

\begin{equation*}
    [\sigma_{z},\sigma_{x}] = \sigma_{z}\sigma_{x} - \sigma_{x}\sigma_{z} =  \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}\begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix} - \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix}\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} - \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} = 2i\begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} = 2i\sigma_{y}
\end{equation*}

The Pauli matrices obey the commutator relation $[\sigma_{x},\sigma_{y}] = 2i\sigma_{z} \textit{ et cycl.}$

\begin{equation*}
    [\sigma_{x},\sigma_{y}]_{+} = [\sigma_{y},\sigma_{x}]_{+} = \sigma_{x}\sigma_{y} + \sigma_{y}\sigma_{x} = \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix} + \begin{bmatrix} -i & 0 \\ 0 & i \end{bmatrix} = 0 
\end{equation*}

\begin{equation*}
    [\sigma_{y},\sigma_{z}]_{+} = [\sigma_{z},\sigma_{y}]_{+} = \sigma_{y}\sigma_{z} + \sigma_{z}\sigma_{y} = \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix} + \begin{bmatrix} 0 & -i \\ -i & 0 \end{bmatrix} = 0 
\end{equation*}

\begin{equation*}
    [\sigma_{z},\sigma_{x}]_{+} = [\sigma_{x},\sigma_{z}]_{+} = \sigma_{z}\sigma_{x} + \sigma_{x}\sigma_{z} = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} + \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} = 0 
\end{equation*}

Any two the Pauli matrices anti-commute, i.e., the anti-commutator vanishes.

\begin{equation*}
    [\sigma_{x},\sigma_{y}] + [\sigma_{x},\sigma_{y}]_{+} = 2\sigma_{x}\sigma_{y} = 2i\sigma_{z}
\end{equation*}

\begin{equation*}
    \sigma_{x}\sigma_{y} = i\sigma_{z} 
\end{equation*}

Similarly we have $\sigma_{y}\sigma_{x} = -i\sigma_{z}$, $\sigma_{y}\sigma_{z} = i\sigma_{x}$, $\sigma_{z}\sigma_{y} = -i\sigma_{x}$, $\sigma_{z}\sigma_{x} = i\sigma_{y}$ and $\sigma_{x}\sigma_{z} = -i\sigma_{y}$

The determinants of the Pauli matrices are

\begin{align*}
    |\sigma_{x}| & = -1 & |\sigma_{y}| & = 1 & |\sigma_{z}| & = -1
\end{align*}

Because the Pauli matrices are hermitian and unitary, their inverses are the matrix themselves. 

\begin{align*}
    \sigma_{x}^{-1} & = \sigma_{x}  & \sigma_{y}^{-1} & = \sigma_{y} & \sigma_{z}^{-1} & = \sigma_{z}
\end{align*}

** SOLVED Problem 8.4.17
CLOSED: [2022-11-01 Tue 14:40]

Given that $\vec{a}$ and $\vec{b}$ are ordinary three dimensional vectors and $\vec{\sigma} = \vec{i}\sigma_{x} + \vec{j}\sigma_{y} + \vec{k}\sigma_{z}$

\begin{equation*}
\begin{split}
    (\vec{a}.\vec{\sigma})(\vec{b}.\vec{\sigma}) & = (a_{x}\sigma_{x} + a_{y}\sigma_{y} + a_{z}\sigma_{z})(b_{x}\sigma_{x} + b_{y}\sigma_{y} + b_{z}\sigma_{z}) \\
    & = a_{x}b_{x}\sigma_{x}\sigma_{x} + a_{x}b_{y}\sigma_{x}\sigma_{y} + a_{x}b_{z}\sigma_{x}\sigma_{z} \\
    & + a_{y}b_{x}\sigma_{y}\sigma_{x} + a_{y}b_{y}\sigma_{y}\sigma_{y} + a_{y}b_{z}\sigma_{y}\sigma_{z} \\
    & + a_{z}b_{x}\sigma_{z}\sigma_{x} + a_{z}b_{y}\sigma_{z}\sigma_{y} + a_{z}b_{z}\sigma_{z}\sigma_{z}
\end{split}
\end{equation*}

We simplify the above expression by using the fact that square of Pauli matrices are unit matrices and that the product of two non-identical Pauli matrices is $\pm i$ times the third Pauli matrix where the sign is determined by the order of multiplication. 

\begin{equation*}
\begin{split}
    (\vec{a}.\vec{\sigma})(\vec{b}.\vec{\sigma}) & = (a_{x}\sigma_{x} + a_{y}\sigma_{y} + a_{z}\sigma_{z})(b_{x}\sigma_{x} + b_{y}\sigma_{y} + b_{z}\sigma_{z}) \\
    & = a_{x}b_{x}I + i\sigma_{z}a_{x}b_{y} -  i\sigma_{z}a_{x}b_{z} \\
    & - i\sigma_{z}a_{y}b_{x} + a_{y}b_{y}I + i\sigma_{x}a_{y}b_{z} \\
    & + i\sigma_{y}a_{z}b_{x} - i\sigma_{x}a_{z}b_{y} + a_{z}b_{z}I \\
    & = (a_{x}b_{x} + a_{y}b_{y} + a_{z}b_{z})I + i\sigma_{x}\left(a_{y}b_{z}-a_{z}b_{y}\right) - i\sigma_{y}\left(a_{x}b_{z}-a_{z}b_{x}\right) + i\sigma_{z}\left(a_{x}b_{y}-a_{y}b_{x}\right) \\
    & = (a_{x}b_{x} + a_{y}b_{y} + a_{z}b_{z})I + i(\sigma_{x}\vec{i} + \sigma_{y}\vec{j} + \sigma_{z}\vec{k})\left(\left(a_{y}b_{z}-a_{z}b_{y}\right)\vec{i} - \left(a_{x}b_{z}-a_{z}b_{x}\right)\vec{j} + \left(a_{x}b_{y}-a_{y}b_{x}\right)\vec{k}\right) \\
    & = \vec{a}.\vec{b}I + i\vec{\sigma}.(\vec{a}\times\vec{b})
\end{split}
\end{equation*}

** SOLVED Problem 8.4.18
CLOSED: [2022-11-01 Tue 14:40]

Using Euler's formula 
                
\begin{equation*}
    e^{i\vec{a}.\vec{\sigma}} = cos(\vec{a}.\vec{\sigma}) + isin(\vec{a}.\vec{\sigma})
\end{equation*}

Setting $\vec{b}=\vec{a}$ in the relation $(\vec{a}.\vec{\sigma})(\vec{b}.\vec{\sigma}) = \vec{a}.\vec{b}I + i\vec{\sigma}.(\vec{a}\times\vec{b})$ 

\begin{equation*}
    (\vec{a}.\vec{\sigma})^{2} = a^{2}I
\end{equation*}

where we have denoted the length of $\vec{a}$ with $a$. Thus we may also write $\vec{a} = a\vec{\hat{a}}$ where $\vec{\hat{a}} = \frac{\vec{a}}{a}$. Thus, the expression for $e^{i\vec{a}.\vec{\sigma}}$ becomes

\begin{equation*}
    e^{i\vec{a}.\vec{\sigma}} = cos(aI) + isin(a\vec{\hat{a}}.\vec{\sigma})
\end{equation*}

** TOSOLVE Problem 8.4.19
** TOSOLVE Problem 8.4.20
* Linear Vector Spaces
** SOLVED Problem 9.1.1
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.1.1}

Yes, all hermitian $2 \times 2$ matrices form a vector space over a real scalar field. The rules of addition and multiplication by scalars is the same as that for matrices. 

The sum of two hermitian matrices $M$ and $N$ is hermitian. 

\begin{equation*}
    (M + N)^{\dagger} = M^{\dagger} + N^{\dagger} = M + N 
\end{equation*}

The multiplication of a hermitian matrix with a real scalar gives a hermitian matrix. It is clear that this would not have been true if the scalar were complex. 

\begin{equation*}
    (aM)^{\dagger} = a^{*}M^{\dagger} = aM
\end{equation*}

Therefore the closure property is satisfied. Scalar  multiplication is distributive in the matrices as well as in the scalars. It is also associative. Matrix addition is commutative and associative. A $2 \times 2$ matrix of zeros (which is also hermitian) when added to any hermitian matrix gives the same hermitian matrix. For every hermitian matrix $M$ there is another hermitian matrix $-M$ which is the inverse under addition. Clearly the inverse is obtained by the multipication of the scalar $-1$ with the matrix. 

A unitary matrix $U$ is one for which the adjoint of the matrix is its inverse, i.e. $U^{\dagger} = U^{-1}$. The sum of unitary matrices is not unitary in general, so the closure axiom is unmet. Hence all $2 \times 2$ unitary matrices do not form a vector space. 

$2 \times 2$ interger matrices form a vector space over an integer scalar field. The rules of addition and multiplication by scalars is the same as that for matrices. 

The sum of two interger matrices is an interger matrix. Similarly, the multiplication of an integer matrix with an integer scalar gives another integer matrix. It is clear that this would not be true if the scalar field was real or complex. Therefore, the closure property is satisfied. Scalar multiplication is distributive over both scalars and vectors. It is also associative. Matrix addition is commutative and associative. A $2 \times 2$ matrix of zeros (which is also an integer matrix) when added to any integer matrix, gives the same integer matrix. For every integer matrix $I$ there is another integer matrix $-I$ which is the inverse under addition. Clearly, the inverse is obtained by the multiplication of the scalar $-1$ with the matrix.

** SOLVED Problem 9.1.2
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.1.2}

Consider all functions $f(x)$ defined in an interval $0 \leq x \leq L$. We define scalar multiplication by $a$ simply as $af(x)$ and addition as point-wise addition: the sum of two functions $f$ and $g$ has the value $f(x) + g(x)$ at the point $x.$ The null function is zero everywhere and the additive inverse of $f$ is $-f.$

Functions that vanish at the end points $x=0$ and $x=L$ form a vector space. Periodic functions obeying $f(0) = f(L)$ also form a vector space. Functions that obey $f(0) = 4$ do not form a vector space because they do not satisfy the closure property. 
** SOLVED Problem 9.1.3
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.1.3}

If $\ket{0'}$ has all the properties of $\ket{0}$, then $\ket{0}$ and $\ket{0'}$ must obey the following equations:

\begin{equation*}
    \begin{split}
        \ket{0} + \ket{0'} & = \ket{0} \\
        \ket{0} + \ket{0'} & = \ket{0'} 
    \end{split}
\end{equation*}

This is possible only when $\ket{0} = \ket{0'}$. In other words, the zero vector is unique. 

Next we show that $0\ket{V}=\ket{0}$. 

\begin{equation*}
    \begin{split}
        \ket{0} & = \ket{V} +\ket{-V} \textit{(axiom for the existence of inverse under addition)}\\
        \ket{0} & = 1\ket{V} + \ket{-V} \textit{(axiom for the existence of identity element of scalar multiplication)} \\
        \ket{0} & = (0 + 1)\ket{V} + \ket{-V} \\
        \ket{0} & = 0\ket{V} + 1\ket{V} + \ket{-V} \textit{(axiom for the distributive property of scalar multiplication over the scalars)} \\
        \ket{0} & = 0\ket{V} + \ket{V} + \ket{-V} \textit{(axiom for the existence of identity element of scalar multiplication)}\\
        \ket{0} & = 0\ket{V} + \ket{0} \textit{(axiom for  the existence of inverse under addition)} \\
        \ket{0} &= 0\ket{V} \textit{(uniqueness of the zero vector)}
    \end{split}
\end{equation*}

Next we show that $\ket{-V} = -\ket{V}.$ We start with the previous result.  

\begin{equation*}
    \begin{split}
        \ket{0} &= 0\ket{V} \\
        \ket{0} &= (1-1)\ket{V} \\
        \ket{0} &= 1\ket{V} -1\ket{V} \textit{(axiom for the distributive property of scalar multiplication over the scalars)}\\
        \ket{0} &= \ket{V} -\ket{V} \textit{(axiom for the existence of identity element of scalar multiplication)} \\
    \end{split}
\end{equation*}

But we know $\ket{V} + \ket{-V} = \ket{0}$ and that $\ket{0}$ is unique. Therefore we must have

\begin{equation*}
    \ket{-V} = -\ket{V}
\end{equation*}

Next we will prove that $\ket{-V}$ is the unique additive inverse of $\ket{V}$. To that end, say we have another additive inverse $\ket{W}$ such that $\ket{V}+\ket{W} = \ket{0}.$ We must also have $\ket{V} + \ket{-V} = \ket{0}.$ Because $\ket{0}$ is unique, we must have 

\begin{equation*}
    \ket{V} + \ket{W} = \ket{V} + \ket{-V}
\end{equation*}

It follows that

\begin{equation*}
    \ket{W} = \ket{-V}
\end{equation*}

\\

** SOLVED Problem 9.1.4
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.1.4}

Consider the set of all entities of the form $(a,b,c)$ where the entries are real numbers. Addition and scalar multiplication are defined as follows:

\begin{equation*}
    \begin{split}
        (a,b,c) + (d,e,f) &= (a+d,b+e,c+f) \\
        \alpha (a,b,c) &= (\alpha a, \alpha b, \alpha c)
    \end{split}
\end{equation*}

Using the result from the previous problem we can use $\alpha = 0$ to find the null vector. Therefore, the null vector is $(0,0,0).$ We can find the inverse of $(a,b,c)$ by multiplying it with $-1$ to get $(-a,-b,-c).$ It is clear that $(a,b,c) + (-a,-b,-c) = (0,0,0)$ as it should be. Vectors of the form $(a,b,1)$ do not form a vector space. To see this, add two vectors of this kind. $(a,b,1) + (c,d,1) = (a+c,b+d,1+1) = (a+c,b+d,2).$ Clearly, the closure axiom is violated.  

** SOLVED Problem 9.1.5
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.1.5}

Consider three elements from the vector space of real $2 \times 2$ matrices:

\begin{align*}
    \ket{1} & = \begin{bmatrix} 0 & 1 \\ 0 & 0  \end{bmatrix} & \ket{2} & =   \begin{bmatrix} 1 & 1 \\ 0 & 1  \end{bmatrix} & \ket{3} &= \begin{bmatrix} -2 & -1 \\ 0 & -2  \end{bmatrix}
\end{align*}

To show that $\ket{1}, \ket{2}$ and $\ket{3}$ are linearly dependent it is sufficient to show that for scalars $a_{1}, a_{2}$ and $a_{3}$, at least 2 of which are non-zero, the quantity $a_{1}\ket{1} + a_{2}\ket{2} + a_{3}\ket{3} = 0.$ For $a_{1}= 1, a_{2}=-2$ and $a_{3}=1$ this is satisfied. Therefore, $\ket{1}, \ket{2}$ and $\ket{3}$ are not linearly independent. 
** SOLVED Problem 9.1.6
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.1.6}

The row vectors $(1,1,0), (1,0,1)$ and $(3,2,1)$ are linearly dependent because $2(1,1,0) + (1,0,1) -1(3,2,1) = (0,0,0).$ Now for the case where the row vectors are $(1,1,0), (1,0,1)$ and $(0,1,1).$ Assume that these vectors are linearly dependent. Then $a(1,1,0) + b(1,0,1) + c(0,1,1) = 0$ for some $a,b,c$. This is equivalent to saying that a solution exists for the following system of equations:

\begin{align*}
    a + b & = 0 \\
    a + c & = 0 \\
    b + c & = 0 \\
\end{align*}

The only solution is $a = b = c = 0.$ Therefore,  $(1,1,0), (1,0,1)$ and $(0,1,1)$ are linearly independent.
** SOLVED Problem 9.2.1
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.2.1}

We need to expand the vector $\ket{V} = \begin{bmatrix} 1 + i \\ \sqrt{3} + i \end{bmatrix}$  in a basis with basis vectors 

\begin{align*}
    \ket{I} &= \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ i \end{bmatrix} & \ket{II} &= \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -i \end{bmatrix}
\end{align*}

First we must show that $\ket{I}$ and $\ket{II}$ form an ortho-normal basis. To that end, it is sufficient to show that:

\begin{align*}
    \braket{I}{I} = \braket{II}{II} & = 1 & \braket{I}{II} = \braket{II}{I} = 0
\end{align*}

\begin{equation*}
\begin{split}
     \braket{I}{I} &= \frac{1}{2} \begin{bmatrix} 1 & i \end{bmatrix}^{*} \begin{bmatrix} 1 \\ i \end{bmatrix} = \frac{1-i^{2}}{2} = 1 \\
      \braket{II}{II} &= \frac{1}{2} \begin{bmatrix} 1 & -i \end{bmatrix}^{*} \begin{bmatrix} 1 \\ -i \end{bmatrix} = \frac{1-i^{2}}{2} = 1
\end{split}
\end{equation*} 

\begin{equation*}
    \begin{split}
        \braket{I}{II} = \frac{1}{2} \begin{bmatrix} 1 & i \end{bmatrix}^{*} \begin{bmatrix} 1 \\ -i \end{bmatrix} = \frac{1+i^{2}}{2} = 0 \\
        \braket{II}{I} = \frac{1}{2} \begin{bmatrix} 1 & -i \end{bmatrix}^{*} \begin{bmatrix} 1 \\ i \end{bmatrix} = \frac{1+i^{2}}{2} = 0 \\
    \end{split}
\end{equation*}

Thus $\ket{I}$ and $\ket{II}$ form an ortho-normal basis. Now let us write  

\begin{equation*}
    \ket{V} = v_{1}\ket{I} + v_{2}\ket{II}
\end{equation*}

$v_{I}, v_{II}$ are found as follows: 

\begin{equation*}
    \begin{split}
        v_{I} &= \braket{I}{V} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & i\end{bmatrix}^{*} \begin{bmatrix} 1 + i \\ \sqrt{3} + i \end{bmatrix} = \frac{1}{\sqrt{2}}(1+i - i\sqrt{3} - i^{2}) = \frac{1}{\sqrt{2}}(2 +i(1-\sqrt{3})) \\
        v_{II} &= \braket{II}{V} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & -i\end{bmatrix}^{*} \begin{bmatrix} 1 + i \\ \sqrt{3} + i \end{bmatrix} = \frac{1}{\sqrt{2}}(1+i + i\sqrt{3} + i^{2}) = \frac{i}{\sqrt{2}}(1+\sqrt{3}) 
    \end{split}
\end{equation*}

The norm squared of the vector is given below  

\begin{equation*}
    |v_{I}|^{2} + |v_{II}|^{2} = \frac{1}{2}[4-1 + 1 + 3 - 2\sqrt{3} + 1 + 3 + 2\sqrt{3}] = 6
\end{equation*}

It is unchanged, as it should be. Next we wish to expand the same vector $\ket{V} = \begin{bmatrix} 1 + i \\ \sqrt{3} + i \end{bmatrix}$  in a basis with basis vectors 

\begin{align*}
    \ket{I} &= \begin{bmatrix} \frac{1+i\sqrt{3}}{4} \\ -\frac{\sqrt{3}(1+i)}{\sqrt{8}} \end{bmatrix} & \ket{II} &= \begin{bmatrix} \frac{\sqrt{3}(1+i)}{\sqrt{8}} \\ \frac{\sqrt{3}+i}{4} \end{bmatrix}
\end{align*}

First we must show that $\ket{I}$ and $\ket{II}$ form an ortho-normal basis:

\begin{align*}
    \braket{I}{I} = \braket{II}{II} & = 1 & \braket{I}{II} = \braket{II}{I} = 0
\end{align*}

\begin{equation*}
\begin{split}
     \braket{I}{I} &=  \begin{bmatrix} \frac{1+i\sqrt{3}}{4} & -\frac{\sqrt{3}(1+i)}{\sqrt{8}} \end{bmatrix}^{*} \begin{bmatrix} \frac{1+i\sqrt{3}}{4} \\ -\frac{\sqrt{3}(1+i)}{\sqrt{8}} \end{bmatrix} = \frac{1}{16}(1+3) + \frac{3}{8}(1+1) = 1\\
      \braket{II}{II} &=  \begin{bmatrix} \frac{\sqrt{3}(1+i)}{\sqrt{8}} & \frac{\sqrt{3}+i}{4} \end{bmatrix}^{*} \begin{bmatrix} \frac{\sqrt{3}(1+i)}{\sqrt{8}} \\ \frac{\sqrt{3}+i}{4} \end{bmatrix} = \frac{3}{8}(1+1) + \frac{1}{16}(1+3) = 1 
\end{split}
\end{equation*} 

\begin{equation*}
    \begin{split}
        \braket{I}{II} &=  \begin{bmatrix} \frac{1+i\sqrt{3}}{4} & -\frac{\sqrt{3}(1+i)}{\sqrt{8}} \end{bmatrix}^{*} \begin{bmatrix}  \frac{\sqrt{3}(1+i)}{\sqrt{8}} \\ \frac{\sqrt{3}+i}{4} \end{bmatrix} = \frac{\sqrt{3}}{4\sqrt{8}}[(1-i\sqrt{3})(1+i)-(1-i)(\sqrt{3}+i)] = 0 \\
        \braket{II}{I} & = \begin{bmatrix} \frac{\sqrt{3}(1+i)}{\sqrt{8}} & \frac{\sqrt{3}+i}{4} \end{bmatrix}^{*} \begin{bmatrix} \frac{1+i\sqrt{3}}{4} \\ -\frac{\sqrt{3}(1+i)}{\sqrt{8}} \end{bmatrix} = \frac{\sqrt{3}}{4\sqrt{8}}[(1-i)(1+i\sqrt{3})-(\sqrt{3}-i)(1+i)] = 0
    \end{split}
\end{equation*}

Now let us write  

\begin{equation*}
    \ket{V} = v_{1}\ket{I} + v_{2}\ket{II}
\end{equation*}

$v_{I}, v_{II}$ are found as follows: 

\begin{equation*}
    \begin{split}
        v_{I} &= \braket{I}{V} = \begin{bmatrix} \frac{1+i\sqrt{3}}{4} & -\frac{\sqrt{3}(1+i)}{\sqrt{8}}\end{bmatrix}^{*} \begin{bmatrix} 1 + i \\ \sqrt{3} + i \end{bmatrix} = \frac{1}{4}(1-i\sqrt{3})(1+i) - \sqrt{\frac{3}{8}}(1-i)(\sqrt{3}+i) =  \frac{\sqrt{6}-1}{4}(i-1)(i+\sqrt{3})\\
        v_{II} &= \braket{II}{V} = \begin{bmatrix} \frac{\sqrt{3}(1+i)}{\sqrt{8}} & \frac{\sqrt{3}+i}{4} \end{bmatrix}^{*} \begin{bmatrix} 1 + i \\ \sqrt{3} + i \end{bmatrix} = \sqrt{\frac{3}{8}}(1-i)(1+i) + \frac{1}{4}(\sqrt{3}-i)(\sqrt{3}+i) = 1 + \sqrt{3/2}
    \end{split}
\end{equation*}

The norm squared of the vector is given below  

\begin{equation*}
    |v_{I}|^{2} + |v_{II}|^{2} =(i - 1) (i + \sqrt{3}) (-1 - i) (\sqrt{3} - i) (\frac{1}{4} (\sqrt{6} - 1))^{2} + (1 + \sqrt{3/2}))^{2} = 6
\end{equation*}

Once again it is unchanged, as it should be.
** SOLVED Problem 9.2.2
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.2.2}

We need to apply the Gram-Schmidt procedure to the following vectors: 

\begin{align*}
    \ket{A} & = 3 \Vec{i} + 4\Vec{j} & \ket{B} & = 2 \Vec{i} - 6 \Vec{j}
\end{align*}

Let $\ket{1}$ and $\ket{2}$ be our desired ortho-normal basis. From $\ket{A}$ we form $\ket{1}$ as:

\begin{equation*}
    \ket{1} = \frac{\ket{A}}{\sqrt{\braket{A}{A}}} = \frac{3 \Vec{i} + 4\Vec{j}}{\sqrt{3^{2}+4^{2}}} = \frac{3}{5}\Vec{i} + \frac{4}{5}\Vec{j}
\end{equation*}

We form $\ket{2'}$ as:

\begin{equation*}
    \ket{2'} = \Vec{B} - \ket{1}\braket{1}{B} = 2 \Vec{i} - 6 \Vec{j} - \frac{18}{25}\Vec{A} = 104\Vec{i} - 78 \Vec{j}
\end{equation*}

\begin{equation*}
    \ket{2} = \frac{\ket{2'}}{\sqrt{\braket{2'}{2'}}} = \frac{1}{\sqrt{104^{2}+78^{2}}}(104\Vec{i} - 78 \Vec{j})
\end{equation*}

Yes. We may produce infinitely many ortho-normal basis simply by rotating $\ket{1}$ and $\ket{2}$ by the same angle $\theta$. Let us rotate $\ket{1}$ and $\ket{2}$ by $\pi/2$ to obtain $\ket{1''}$ and $\ket{2''}$ respectively. In this case, the rotation matrix is $R_{\pi/2} = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}$; $\ket{1''}$ and $\ket{2''}$ are given as:

\begin{align*}
    \ket{1''} & = \ket{2} \\
    \ket{2''} & = -\ket{1}
\end{align*}

Sure enough, we have $\braket{1''}{1''} = \braket{2''}{2''} = 1$ and $\braket{1''}{2''} = \braket{2''}{1''} = 0$; $\ket{1''}$ and $\ket{2''}$ are another ortho-normal basis. To obtain $\ket{1''}$ and $\ket{2''}$ from $\ket{A}$ and $\ket{B}$ we substitute the expressions of $\ket{1'}$ and $\ket{2'}$ in terms of $\ket{A}$ and $\ket{B}$ in the expressions for $\ket{1''}$ and $\ket{2''}$ respectively.  
** SOLVED Problem 9.2.3
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.2.3}

We need to show how to go from the basis

\begin{align*}
    \ket{I} & = \begin{bmatrix} 3 \\ 0 \\ 0 \end{bmatrix} & \ket{II} & = \begin{bmatrix} 0 \\ 1 \\ 2 \end{bmatrix} & \ket{III} & = \begin{bmatrix} 0 \\ 2 \\ 5 \end{bmatrix}
\end{align*}

to the ortho-normal basis 

\begin{align*}
    \ket{1} & = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} & \ket{2} & = \begin{bmatrix} 0 \\ 1/\sqrt{5} \\ 2/\sqrt{5} \end{bmatrix} & \ket{3} & = \begin{bmatrix} 0 \\ -2/\sqrt{5} \\ 1/\sqrt{5} \end{bmatrix}
\end{align*}

$\ket{1}$ and $\ket{2}$ are obtained simply by diving $\ket{I}$ and $\ket{II}$ by $3$ and $\sqrt{5}$ respectively. 

\begin{align*}
    \ket{1} & = \frac{1}{3}\ket{I} \\
    \ket{2} & = \frac{1}{\sqrt{5}}\ket{II}
\end{align*}

$\ket{3}$ is obtained by realizing that $\ket{1}$ lies along the $x$ axis and  $\ket{2}$ is in the $y-z$ plane. We may swap the components of $\ket{2}$ along $\Vec{j}$ and $\Vec{k}$ and negate the component along the $\Vec{j}$ direction to obtain a vector perpendicular to $\ket{2}$ (this is equivalent to rotating $\ket{2}$ by an angle of $\pi/2$). Alternatively, we may obtain $\ket{3}$ from $\ket{II}$ and $\ket{III}$ as $\ket{3} = \frac{1}{\sqrt{5}}(5\ket{III} - 12\ket{II}).$

\\
** SOLVED Problem 9.2.4
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.2.4}

The Schwarz inequality for vectors $\ket{V}$ and $\ket{W}$ given below

\begin{equation*}
    |\braket{V}{W}| \leq |V||W|
\end{equation*}

When $\ket{W} = c\ket{V}$ the left hand side reduces to $|\braket{V}{W}| =  |\braket{V}{cV}| = |c\braket{V}{V}| = |c||V||V|$ and the right hand side reduces to $|V||cV| = |c||V||V|.$ Therefore for the Schwartz inequality to be satisfied when one of the two vectors $V$ and $W$ is a scaled version of another. This agrees with our experience with arrows, because this condition means that the vectors point in the same direction; therefore the dot product becomes the product of the lengths of the arrows.  

** SOLVED Problem 9.2.5
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.2.5}

We intend to prove the triangle inequality which states that for two vectors $\ket{V}$ and $\ket{W}$

\begin{equation*}
    |V+W| \leq |V| + |W|
\end{equation*}

Start by writing $|V+W|^{2}$ as 

\begin{equation*}
    \begin{split}
        |V+W|^{2} & = \braket{V+W}{V+W} \\
        & = \braket{V}{V} + \braket{V}{W} + \braket{W}{V} + \braket{W}{W} \\
        & = |V|^{2} + \braket{W}{V}^{*} + \braket{W}{V} + |W|^{2} \\
        & = |V|^{2} + 2Re(\braket{W}{V}) + |W|^{2} \\
        & \leq |V|^{2} + 2|\braket{V}{W}| + |W|^{2} \\
        & \leq |V|^{2} + 2|V||W| + |W|^{2} \\
        & \leq (|V|+|W|)^{2}
    \end{split}
\end{equation*}

We have used the Schwarz inequality and the fact that $Re(\braket{V}{W}) \leq |\braket{V}{M}|$ Therefore, we have  

\begin{equation*}
    |V+W| \leq |V| + |W|
\end{equation*}

** SOLVED Problem 9.2.6
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.2.6}

We have to verify if the following vectors are ortho-normal:

\begin{align*}
    \ket{1} &= (1/\sqrt{2})[1,-i,0]^{T} & \ket{2} &= (1/\sqrt{2})[1,i,0]^{T} & \ket{3} &= [0,0,1]^{T}
\end{align*}

To that end, it is sufficient to show that 

\begin{equation*}
\begin{split}
    \braket{1}{1} = \braket{2}{2} = \braket{3}{3} = 1 \textit{ and } \braket{1}{2} = \braket{2}{1} = \braket{1}{3} = \braket{3}{1} = \braket{2}{3} = \braket{3}{2} = 0 
\end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        \braket{1}{1} &= \frac{1}{2}(1 + i(-i) + 0) = 1 \\
        \braket{2}{2} &= \frac{1}{2}(1+(-i)i + 0) = 1 \\
        \braket{3}{3} &= (0 + 0 + 1) = 1 
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        \braket{1}{2} &= \frac{1}{2}(1+i^{2} + 0) = 0 \\
        \braket{2}{1} &= \braket{1}{2}^{*} = 0 \\
        \braket{1}{3} &= \frac{1}{\sqrt{2}}(0 + 0 + 0) = 0 \\
        \braket{3}{1} &= \braket{1}{3}^{*} = 0 \\
        \braket{2}{3} &= \frac{1}{\sqrt{2}}(0+ 0 + 0) = 0 \\
        \braket{3}{2} &= \braket{2}{3}^{*} = 0
    \end{split}
\end{equation*}

Next, we need to find the coefficients $\alpha, \beta$ and $\gamma$ in the expansion of $\ket{V} = [3-4i,5-6i,8] = \alpha \ket{1} + \beta \ket{2} + \gamma \ket{3}$. This is done easily by using the fact that $\ket{1}, \ket{2}$ and $\ket{3}$ are ortho-normal.  

\begin{equation*}
\begin{split}
    \alpha &= \braket{1}{V} = \frac{1}{\sqrt{2}}[3-4i + 6 + 5i] = \frac{1}{\sqrt{2}}(9+i) \\
    \beta &= \braket{2}{V} = \frac{1}{\sqrt{2}}[3-4i - 6 - 5i] = -\frac{1}{\sqrt{2}}(3 + 9i) \\
    \gamma &= \braket{3}{V} = 8 
\end{split}    
\end{equation*}

** SOLVED Problem 9.3.1
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.3.1}

Consider a linear operator $\Omega$ acting on vectors in an n-dimensional space. Let 

\begin{equation*}
    \Omega \ket{j} = \ket{j'}
\end{equation*}

be the action on the basis vectors. We assume $\ket{j'}$ is known through its components. Then 

\begin{equation*}
    \begin{split}
        \ket{V'} & = \Omega \ket{V} \\
        & = \Omega \sum_{j} v_{j}\ket{j} \\
        & = \sum_{j} v_{j} \Omega \ket{j} \\
        & = \sum_{j} v_{j} \ket{j'}
    \end{split}
\end{equation*}

Say we wish to write the components of the transformed or output vector $\ket{V'}$. To find $v_{i}'$ we take the dot product with $\bra{i}:$

\begin{equation*}
    v_{i}^{'} = \sum_{j}v_{j}\braket{i}{j'}
\end{equation*}

To proceed further we must know $\ket{j'}$. This information we assemble as a matrix with elements $\Omega_{ij}$ given by 

\begin{equation*}
    \Omega_{ij} = \braket{i}{j'} = \bra{i}\Omega \ket{j}
\end{equation*}

So finally we have 

\begin{equation*}
    v_{j}^{'} = \sum_{j} \Omega_{ij}v_{j}
\end{equation*}

which is just the matrix equation 

\begin{equation*}
    \begin{bmatrix} v_{1}^{'} \\ v_{2}^{'} \\ \vdots \\ v_{n}^{'} \end{bmatrix} = \begin{bmatrix} \Omega_{11} & \Omega_{12} & \dots & \Omega_{1n} \\ \Omega_{21} & \Omega_{22} & \dots & \Omega_{2n} \\ \vdots & \vdots & \vdots & \vdots \\ \Omega_{n1} & \Omega_{n2} & \dots & \Omega_{nn} \end{bmatrix} \begin{bmatrix} v_{1} \\ v_{2} \\ \vdots \\ v_{n} \end{bmatrix}
\end{equation*}

The rotation operator $R_{z}(\pi/2)$ is an example of a linear operator. Its action on the basis vectors $\Vec{i} \equiv \ket{1}$, $\Vec{j} \equiv \ket{2}$ and $\Vec{k} \equiv \ket{3}$ is 

\begin{align*}
    R_{z}(\pi/2)\ket{1} & = \ket{2} & R_{z}(\pi/2)\ket{2} & = -\ket{1} & R_{z}(\pi/2) &= \ket{3}
\end{align*}

If we wanted to write the rotation operator $R_{z}(\pi/2)$ as a matrix, we can determine its elements as follows:

\begin{equation*}
    \begin{split}
        R_{z}(\pi/2)_{11} &= \bra{1} R_{z}(\pi/2) \ket{1} = \braket{1}{2} = 0 \\ 
        R_{z}(\pi/2)_{12} &= \bra{1} R_{z}(\pi/2) \ket{2} = -\braket{1}{1} = -1 \\ 
        R_{z}(\pi/2)_{13} &= \bra{1} R_{z}(\pi/2) \ket{3} = \braket{1}{3} = 0 \\ 
        R_{z}(\pi/2)_{21} &= \bra{2} R_{z}(\pi/2) \ket{1} = \braket{2}{2} = 1 \\ 
        R_{z}(\pi/2)_{22} &= \bra{2} R_{z}(\pi/2) \ket{2} = -\braket{2}{1} = 0 \\ 
        R_{z}(\pi/2)_{23} &= \bra{2} R_{z}(\pi/2) \ket{3} = \braket{2}{3} = 0 \\ 
        R_{z}(\pi/2)_{31} &= \bra{3} R_{z}(\pi/2) \ket{1} = \braket{3}{2} = 0 \\ 
        R_{z}(\pi/2)_{32} &= \bra{3} R_{z}(\pi/2) \ket{2} = -\braket{3}{1} = 0 \\ 
        R_{z}(\pi/2)_{33} &= \bra{3} R_{z}(\pi/2) \ket{3} = \braket{3}{3} = 1 \\ 
    \end{split}
\end{equation*}

Thus, $R_{z}(\pi/2)$ in matrix form is 

\begin{equation*}
    R_{z}(\pi/2) = \begin{bmatrix} 0 & -1 & 0 \\ 1& 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}
\end{equation*}

** SOLVED Problem 9.3.2
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.3.2}

The matrices corresponding to the projection operators are: 

\begin{align*}
    P_{x} &= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} & P_{y} &= \begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} & P_{z} &= \begin{bmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}
\end{align*}

When one forms the sum of the projection operators, one gets the identity matrix or the identity operator. This is expected because the sum will preserved the components along $x$, $y$ and $z$. In other words it will preserve the vector itself. 

** SOLVED Problem 9.3.3
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.3.3}

We have the square operator $SQ$ that squares the components of s vector. We will now argue that it is not a linear. Let $\ket{V}$ be a vector in n-dimensional space. Say we get $\ket{V'}$ when the operator $SQ$ acts on $\ket{V}.$ Then, if $\ket{V} = \sum_{j} v_{j}\ket{j}$, we must have:

\begin{equation*}
\begin{split}
     \ket{V'} &= \sum_{j} v_{j}^{2} \ket{j} \\
     SQ \ket{V} &= \sum_{j}SQ (v_{j}\ket{j}) 
\end{split}
\end{equation*}

Clearly, this does not satisfy the definition of a linear operator namely: $\Omega(\sum_{j} v_{j} \ket{j}) = \sum_{j} v_{j} \Omega \ket{j}.$ Therefore, $SQ$ is not a linear operator. 

** SOLVED Problem 9.3.4
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.3.4}

Say the rotation matrix $R$ transforms the usual unit vectors $\Vec{i},\Vec{j},$ and $\Vec{k}$ to $\Vec{i'}, \Vec{j'},$ and $\Vec{k'}.$ In matrix form we may write $R$ as 

\begin{equation*}
    \begin{bmatrix} \Vec{i}.\Vec{i'} & \Vec{i}.\Vec{j'} & \Vec{i}.\Vec{k'} \\ \Vec{j}.\Vec{i'} & \Vec{j}.\Vec{j'} & \Vec{j}.\Vec{k'} \\ \Vec{k}.\Vec{i'} & \Vec{k}.\Vec{j'} & \Vec{k}.\Vec{k'} \end{bmatrix}
\end{equation*}

To show that the rotation matrix is orthogonal it is sufficient to show that $R^{T}R = I.$

\begin{equation*}
\begin{split}
    R^{T}R  &=  \begin{bmatrix} \Vec{i}.\Vec{i'} & \Vec{j}.\Vec{i'} & \Vec{k}.\Vec{i'} \\ \Vec{i}.\Vec{j'} & \Vec{j}.\Vec{j'} & \Vec{k}.\Vec{j'} \\ \Vec{i}.\Vec{k'} & \Vec{j}.\Vec{k'} & \Vec{k}.\Vec{k'} \end{bmatrix} \begin{bmatrix} \Vec{i}.\Vec{i'} & \Vec{i}.\Vec{j'} & \Vec{i}.\Vec{k'} \\ \Vec{j}.\Vec{i'} & \Vec{j}.\Vec{j'} & \Vec{j}.\Vec{k'} \\ \Vec{k}.\Vec{i'} & \Vec{k}.\Vec{j'} & \Vec{k}.\Vec{k'} \end{bmatrix} \\
      & = \begin{bmatrix} R_{11} & R_{12} & R_{13} \\ R_{21} & R_{22} & R_{23} \\ R_{31} & R_{32} & R_{33} \end{bmatrix}
\end{split}
\end{equation*}

where 

\begin{align*}
    R_{11} &= (\Vec{i'}.\Vec{i})(\Vec{i'}.\Vec{i})+(\Vec{i'}.\Vec{j})(\Vec{i'}.\Vec{j})+(\Vec{i'}.\Vec{k})(\Vec{i'}.\Vec{k}) = \Vec{i'}.\Vec{i'} = 1\\
    R_{12} & = (\Vec{i'}.\Vec{i})(\Vec{j'}.\Vec{i})+(\Vec{i'}.\Vec{j})(\Vec{j'}.\Vec{j})+(\Vec{i'}.\Vec{k})(\Vec{j'}.\Vec{k}) = \Vec{i'}.\Vec{j'} = 0\\
    R_{13} & = (\Vec{i'}.\Vec{i})(\Vec{k'}.\Vec{i})+(\Vec{i'}.\Vec{j})(\Vec{k'}.\Vec{j})+(\Vec{i'}.\Vec{k})(\Vec{k'}.\Vec{k}) = \Vec{i'}.\Vec{k'} = 0\\
    R_{21} & = (\Vec{j'}.\Vec{i})(\Vec{i'}.\Vec{i})+(\Vec{j'}.\Vec{j})(\Vec{i'}.\Vec{j})+(\Vec{j'}.\Vec{k})(\Vec{i'}.\Vec{k}) = \Vec{j'}.\Vec{i'} = 0\\
    R_{22} & = (\Vec{j'}.\Vec{i})(\Vec{j'}.\Vec{i})+(\Vec{j'}.\Vec{j})(\Vec{j'}.\Vec{j})+(\Vec{j'}.\Vec{k})(\Vec{j'}.\Vec{k}) = \Vec{j'}.\Vec{j'} = 1\\
    R_{23} & = (\Vec{j'}.\Vec{i})(\Vec{k'}.\Vec{i})+(\Vec{j'}.\Vec{j})(\Vec{k'}.\Vec{j})+(\Vec{j'}.\Vec{k})(\Vec{k'}.\Vec{k}) = \Vec{j'}.\Vec{k'} = 0\\
    R_{31} & = (\Vec{k'}.\Vec{i})(\Vec{i'}.\Vec{i})+(\Vec{k'}.\Vec{j})(\Vec{i'}.\Vec{j})+(\Vec{k'}.\Vec{k})(\Vec{i'}.\Vec{k}) = \Vec{k'}.\Vec{i'} = 0\\
    R_{32} &= (\Vec{k'}.\Vec{i})(\Vec{j'}.\Vec{i})+(\Vec{k'}.\Vec{j})(\Vec{j'}.\Vec{j})+(\Vec{k'}.\Vec{k})(\Vec{j'}.\Vec{k}) = \Vec{k'}.\Vec{j'} = 0\\
    R_{33} & = (\Vec{k'}.\Vec{i})(\Vec{k'}.\Vec{i})+(\Vec{k'}.\Vec{j})(\Vec{k'}.\Vec{j})+(\Vec{k'}.\Vec{k})(\Vec{k'}.\Vec{k}) = \Vec{k'}.\Vec{k'} = 1\\
\end{align*}

The penultimate equality in the above equations follows from $\Vec{A}.\Vec{B} = A_{x}B_{x} + A_{y}B_{y} + A_{z}B_{z} = (\Vec{A}.\Vec{i})(\Vec{B}.\Vec{i}) + (\Vec{A}.\Vec{j})(\Vec{B}.\Vec{j}) + (\Vec{A}.\Vec{k})(\Vec{B}.\Vec{k})$. The last equality in the above equations follows from the fact that under a rigid rotation the resulting set of vectors $\Vec{i'}, \Vec{j'}$ and $\Vec{k'}$ form an orthonormal set  of vectors: the rotation does not change the lengths or angles between angles.In compact form we may write $R_{ij} = x\Vec{i'}\Vec{j'} = \delta_{i'j'}$. Therefore 

\begin{equation*}
    R^{T}R = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = I
\end{equation*}

** SOLVED Problem 9.3.5
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.3.5}

Given below is a matrix representation for a linear operator $R_{z}(\theta)$ that rotates a vector by $\theta$ about the $z$ axis in the counterclockwise direction looking from above and another linear operator $R_{x}(\phi)$ that rotates a vector by $\phi$ about the $x$ axis in the counterclockwise direction looking from above. 

\begin{align*}
    R_{z}(\theta) &= \begin{bmatrix} cos\theta & -sin\theta & 0 \\ sin\theta &  cos\theta & 0 \\0  & 0 & 1 \end{bmatrix} & R_{x}(\phi) &= \begin{bmatrix} 1 & 0 & 0 \\ 0 &  cos\phi & -sin\phi \\ 0  & sin\phi & cos\phi \end{bmatrix}
\end{align*}

Next we show that these matrices are orthogonal 

\begin{equation*}
    \begin{split}
        R_{z}(\theta)^{T}R_{z}(\theta) &= \begin{bmatrix} cos\theta & sin\theta & 0 \\ -sin\theta &  cos\theta & 0 \\0  & 0 & 1 \end{bmatrix}\begin{bmatrix} cos\theta & -sin\theta & 0 \\ sin\theta &  cos\theta & 0 \\0  & 0 & 1 \end{bmatrix} \\
        & = \begin{bmatrix} sin^{2}\theta + cos^{2}\theta & 0 & 0 \\ 0 & sin^{2}\theta + cos^{2}\theta & 0 \\ 0 & 0 & 1\end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = I 
    \end{split}
\end{equation*}

\begin{equation*}
    \begin{split}
        R_{x}(\phi)^{T}R_{x}(\phi) & = \begin{bmatrix} 1 & 0 & 0 \\ 0 &  cos\phi & sin\phi \\ 0  & -sin\phi & cos\phi \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 &  cos\phi & -sin\phi \\ 0  & sin\phi & cos\phi \end{bmatrix} \\
        & = \begin{bmatrix} 1 & 0 & 0 \\ 0 & sin^{2}\phi + cos^{2}\phi & 0 \\ 0 & 0 & sin^{2}\phi + cos^{2}\phi \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = I 
    \end{split}
\end{equation*}

Next we find the product of the two matrices

\begin{equation*}
\begin{split}
    R_{z}(\theta)R_{x}(\phi) &= \begin{bmatrix} cos\theta & -sin\theta & 0 \\ sin\theta &  cos\theta & 0 \\0  & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\ 0 &  cos\phi & -sin\phi \\ 0  & sin\phi & cos\phi \end{bmatrix} \\
    & = \begin{bmatrix} cos\theta & -sin\theta cos\phi & sin\theta sin\phi \\ sin\theta & cos\theta cos\phi & - cos\theta sin\phi \\ 0 & sin\phi & cos\phi\end{bmatrix}
\end{split}
\end{equation*}

We show that $R_{z}(\theta)R_{x}(\phi)$ is orthogonal. 

\begin{equation*}
\begin{split}
    (R_{z}(\theta)R_{x}(\phi))^{T}(R_{z}(\theta)R_{x}(\phi)) &= \begin{bmatrix} cos\theta & sin\theta & 0 \\ -sin\theta cos \phi & cos \theta cos \phi & sin\phi \\ sin \theta sin\phi & -cos \theta sin\phi & cos\phi \end{bmatrix} \begin{bmatrix} cos\theta & -sin\theta cos\phi & sin\theta sin\phi \\ sin\theta & cos\theta cos\phi & - cos\theta sin\phi \\ 0 & sin\phi & cos\phi\end{bmatrix} \\
   & = \begin{bmatrix} sin^{2}\theta + cos^{2}\theta & 0 & 0 \\ 0 & cos^{2}\phi (sin^{2}\theta + cos^{2}) + sin^{2}\phi & 0 \\ 0 & 0 & sin^{2}\phi (sin^{2}\theta + cos^{2}\theta) + cos^{2}\phi\end{bmatrix} \\
   & = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = I 
\end{split}
\end{equation*}

Given two orthogonal matrices $M$ and $N$ we show that the product is orthogonal too. To this end, it is sufficient to show that $(MN)^{T}(MN) = I$

\begin{equation*}
    \begin{split}
        (MN)^{T}(MN) = N^{T}M^{T}MN = N^{T}N = I 
    \end{split}
\end{equation*}

In the first equality we have used the fact that the transpose of a product is a product of the transposes in reverse order. In the second and third we have used the fact that $M^{T}M = I$ and $N^{T}N = I$ due to them being orthogonal matrices. 

** SOLVED Problem 9.3.6
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.3.6}

Given below is a matrix representation for a linear operator $R_{z}(\pi/2)$ that rotates a vector by $\pi/2$ about the $z$ axis in the counterclockwise direction looking from above and another linear operator $R_{x}(\pi/2)$ that rotates a vector by $\pi/2$ about the $x$ axis in the counterclockwise direction looking from above. 

\begin{align*}
    R_{z}(\pi/2) &= \begin{bmatrix} 0 & -1 & 0 \\ 1 &  0 & 0 \\0  & 0 & 1 \end{bmatrix} & R_{x}(\pi/2) &= \begin{bmatrix} 1 & 0 & 0 \\ 0 &  0 & -1 \\ 0  & 1 & 0 \end{bmatrix}
\end{align*}

Next we calculate the commutator of these two matrices: 

\begin{equation*}
\begin{split}
     [R_{z}(\pi/2),R_{x}(\pi/2)] &= R_{z}(\pi/2)R_{x}(\pi/2) - R_{x}(\pi/2)R_{z}(\pi/2) \\
     & = \begin{bmatrix} 0 & -1 & 0 \\ 1 &  0 & 0 \\0  & 0 & 1 \end{bmatrix}\begin{bmatrix} 1 & 0 & 0 \\ 0 &  0 & -1 \\ 0  & 1 & 0 \end{bmatrix} - \begin{bmatrix} 1 & 0 & 0 \\ 0 &  0 & -1 \\ 0  & 1 & 0 \end{bmatrix}\begin{bmatrix} 0 & -1 & 0 \\ 1 &  0 & 0 \\0  & 0 & 1 \end{bmatrix} \\
     & = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & 0\end{bmatrix} - \begin{bmatrix} 0 & -1 & 0 \\ 0 & 0 & -1 \\ 1 & 0 & 0 \end{bmatrix} =\begin{bmatrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ -1 & 1 & 0  \end{bmatrix}  
\end{split}
\end{equation*}

** SOLVED Problem 9.3.7
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.3.7}

Given below is a matrix representation for a linear operator $R_{z}(\pi/4)$ that rotates a vector by $\pi/4$ about the $z$ axis in the counterclockwise direction looking from above and another linear operator $R_{x}(\pi/4)$ that rotates a vector by $\pi/4$ about the $x$ axis in the counterclockwise direction looking from above. 

\begin{align*}
    R_{z}(\pi/4) &= \begin{bmatrix} 1/\sqrt{2} & -1/\sqrt{2} & 0 \\ 1/\sqrt{2} &  1/\sqrt{2} & 0 \\0  & 0 & 1 \end{bmatrix} & R_{x}(\pi/4) &= \begin{bmatrix} 1 & 0 & 0 \\ 0 &  1/\sqrt{2} & -1/\sqrt{2} \\ 0  & 1/\sqrt{2} & 1/\sqrt{2} \end{bmatrix}
\end{align*}

Next we calculate the commutator of these two matrices: 

\begin{equation*}
\begin{split}
     [R_{z}(\pi/4),R_{x}(\pi/4)] &= R_{z}(\pi/4)R_{x}(\pi/4) - R_{x}(\pi/4)R_{z}(\pi/4) \\
     & = \begin{bmatrix} 1/\sqrt{2} & -1/\sqrt{2} & 0 \\ 1/\sqrt{2} &  1/\sqrt{2} & 0 \\0  & 0 & 1 \end{bmatrix}\begin{bmatrix} 1 & 0 & 0 \\ 0 &  1/\sqrt{2} & -1/\sqrt{2} \\ 0  & 1/\sqrt{2} & 1/\sqrt{2} \end{bmatrix} - \begin{bmatrix} 1 & 0 & 0 \\ 0 &  1/\sqrt{2} & -1/\sqrt{2} \\ 0  & 1/\sqrt{2} & 1/\sqrt{2} \end{bmatrix}\begin{bmatrix} 1/\sqrt{2} & -1/\sqrt{2} & 0 \\ 1/\sqrt{2} &  1/\sqrt{2} & 0 \\0  & 0 & 1 \end{bmatrix} \\
     & = \begin{bmatrix} 1/\sqrt{2} & -1/2 & 1/2 \\ 1/\sqrt{2} & 1/2 & -1/2 \\ 0 & 1/\sqrt{2} & 1/\sqrt{2} \end{bmatrix} - \begin{bmatrix} 1/\sqrt{2} & -1/\sqrt{2} & 0 \\ 1/2 & 1/2 & -1/\sqrt{2} \\ 1/2 & 1/2 & 1/\sqrt{2} \end{bmatrix} \\
     & =\begin{bmatrix} 0 & 1/\sqrt{2}(1-1/\sqrt{2}) & 1/2 \\ 1/\sqrt{2}(1-1/\sqrt{2}) & 0 & 1/\sqrt{2}(1-1/\sqrt{2}) \\ -1/2 & 1/\sqrt{2}(1-1/\sqrt{2}) & 0  \end{bmatrix}  
\end{split}
\end{equation*}

** SOLVED Problem 9.5.1
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.5.1}

We intend to find the eigenvalues and eigenvectors of the operator $M$ written in matrix form as:

\begin{equation*}
    \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}
\end{equation*}

The eigenvalue equation can be written as 

\begin{equation*}
    M\ket{m} = m\ket{m}
\end{equation*}

The characteristic equation is 

\begin{equation*}
    \begin{vmatrix} 0-m & 1 \\ 1 & 0-m \end{vmatrix} = m^{2} - 1 = 0 
\end{equation*}

This polynomial has two roots $m = \pm 1$. We find the eigenvector corresponding to $m=1$ next. 

\begin{equation*}
    \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} v_{1} \\ v_{2}\end{bmatrix} = + \begin{bmatrix} v_{1} \\ v_{2} \end{bmatrix}
\end{equation*}

Comparing coefficients we get two equations: 

\begin{align*}
    v_{1} & = v_{2} \\
    v_{2} & = v_{1}
\end{align*}

One of these equations is redundant. In other words, vectors satisfying them can only be specified in direction but not in magnitude. Any vector in this direction is an eigenvector. We pick a simple normalized one. 

\begin{equation*}
    \ket{m = +1} \longrightarrow \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1\end{bmatrix}
\end{equation*}

We may multiply all components of this vector by a uni-modular phase factor without changing the norm. Eigenvectors related by re-scaling are never counted as different solutions. The second truly distinct eigenvector corresponding to eigenvalue $-1$ can be obtained in a similar manner. 

\begin{equation*}
    \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} v_{1} \\ v_{2}\end{bmatrix} = - \begin{bmatrix} v_{1} \\ v_{2} \end{bmatrix}
\end{equation*}

Once again we get two equations one of which is redundant: 

\begin{align*}
    v_{1} & = -v_{2} \\
    v_{2} & = -v_{1}
\end{align*}

We pick the following normalized eigenvector corresponding to the eigenvalue $-1.$

\begin{equation*}
    \ket{m = -1} \longrightarrow \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1\end{bmatrix}
\end{equation*}

Assembling together the full solution we have:

\begin{align*}
    \ket{m=+1} &= \frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1\end{bmatrix} & \ket{m = -1} &= \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix}
\end{align*}

** SOLVED Problem 9.5.2
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.5.2}

We need to find the eigenvalues and normalized eigenvectors of $R_{z}(\theta)$ 

\begin{equation*}
    R_{z}(\theta) = \begin{bmatrix} cos\theta & -sin\theta & 0 \\ sin\theta & cos\theta & 0 \\ 0 &  0 & 1 \end{bmatrix}
\end{equation*}

The eigenvalue equation is 

\begin{equation*}
    R_{z}(\theta)\ket{r} = r\ket{r}
\end{equation*}

The characteristic equation for the eigenvalue $r$ is 

\begin{equation*}
    \begin{vmatrix} cos\theta - r & -sin\theta & 0 \\ sin\theta & cos\theta - r & 0 \\ 0 & 0  & 1-r\end{vmatrix} = (1-r)(r^{2}-2rcos\theta  + 1) = 0 
\end{equation*}

with the solutions

\begin{equation*}
    r = 1, e^{i\theta}, e^{-i\theta}
\end{equation*}

The eigenvector corresponding to $r=1$ is trivial

\begin{equation*}
    \ket{r=1} = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}
\end{equation*}

We find the eigenvector corresponding to $r = e^{i\theta}$ next

\begin{equation*}
    \begin{split}
        \begin{bmatrix} cos\theta & -sin\theta & 0 \\ sin\theta & cos\theta & 0 \\ 0 &  0 & 1 \end{bmatrix} \begin{bmatrix} v_{1} \\ v_{2} \\ v_{3}\end{bmatrix} = e^{i\theta} \begin{bmatrix} v_{1} \\ v_{2} \\ v_{3} \end{bmatrix}    
    \end{split}
\end{equation*}

Comparing coefficients we get three equations 

\begin{align*}
    v_{1}cos\theta - v_{2}sin\theta &= e^{i\theta}v_{1} \\
    v_{1}sin\theta + v_{2}cos\theta & = e^{i\theta}v_{2} \\
    v_{3} = e^{i\theta}v_{3}
\end{align*}

The solution for this system is 

\begin{align*}
    v_{2} &= -iv_{1} \\
    v_{1} & = -iv_{2} \\
    v_{3} &= e^{i\theta}v_{3}
\end{align*}

The last equation tells us that $v_{3} = 0.$ One among the first two is redundant. We choose $v_{1} = \frac{1}{\sqrt{2}}$ and $v_{2} = \frac{-i}{\sqrt{2}}.$ Thus, the eigenvector corresponding to $r = e^{i\theta}$ is

\begin{equation*}
    \ket{r=e^{i\theta}} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -i \\ 0 \end{bmatrix}
\end{equation*}

We get three similar equations in solving for the components of the eigenvector corresponding to the eigenvalue $r = e^{-i\theta}$ 

\begin{align*}
    v_{1}cos\theta - v_{2}sin\theta &= e^{-i\theta}v_{1} \\
    v_{1}sin\theta + v_{2}cos\theta & = e^{-i\theta}v_{2} \\
    v_{3} = e^{-i\theta}v_{3}
\end{align*}

The solution for this system is 

\begin{align*}
    v_{2} &= iv_{1} \\
    v_{1} & = iv_{2} \\
    v_{3} &= e^{-i\theta}v_{3}
\end{align*}

We must have $v_{3} = 0$. Once again we choose $v_{1} = \frac{1}{\sqrt{2}}$ and $v_{2} = \frac{i}{\sqrt{2}}.$ The eigenvector corresponding to $r= e^{-i\theta}$ is 

\begin{equation*}
    \ket{r=e^{-i\theta}} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ i \\ 0 \end{bmatrix}
\end{equation*}

Assembling together the full solution we have:

\begin{align*}
    \ket{r=1} &= \begin{bmatrix} 0 & 0 & 1 \end{bmatrix} & \ket{r=e^{i\theta}} &= \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -i \\ 0 \end{bmatrix} & \ket{r=e^{-i\theta}} &= \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ i \\ 0 \end{bmatrix}
\end{align*}

** SOLVED Problem 9.5.3
CLOSED: [2022-11-01 Tue 14:50]
\question*{Problem 9.5.3}

DO THIS LATER 

** SOLVED Problem 9.5.4
CLOSED: [2022-11-01 Tue 14:51]
\question*{Problem 9.5.4}

If $H$ is hermitian, $U = e^{iH}$ is unitary. We need to show that if $\ket{h}$ is a eigenket of $H$, i.e, if $H\ket{h} = h\ket{h}$ then $U\ket{h} = e^{ih}\ket{h}.$ We write $U \ket{h}$ as follows:

\begin{equation*}
\begin{split}
     U\ket{h} &= \left(I + iH + \frac{(iH)^{2}}{2} + \frac{(iH)^{3}}{3} + ...\right)\ket{h} \\
      & = \ket{h} + ih\ket{h} + \frac{(ih)^{2}}{2}\ket{h} + \frac{(ih)^{3}}{3}\ket{h} + ... \\
      & = \left(1 + ih + \frac{(ih)^{2}}{2} + \frac{(ih)^{3}}{3} + ...\right)\ket{h} \\
      & = e^{ih}\ket{h}
\end{split}
\end{equation*}

Thus the theorem that assures us that hermitian operators have an ortho-normal set of eigenvectors implies the same for unitary operators which can be written as $e^{iH}$. It turns out that every unitary operator can be so written.   

** SOLVED Problem 9.5.5
CLOSED: [2022-11-01 Tue 14:51]
\question*{Problem 9.5.5}

Say we have specialized to a complex two-dimensional vector space. It takes four complex numbers or eight real numbers to fully specify a complex matrix acting on its vectors. A hermitian matrix $H$ may be written as:

\begin{equation*}
    H = \begin{bmatrix} a + ib & c + id \\ e + if & g + ih \end{bmatrix} 
\end{equation*}

Hermiticity implies that $H^{\dagger} = H$ 

\begin{equation*}
    \begin{split}
        \begin{bmatrix} a + ib & c + id \\ e + if & g + ih \end{bmatrix}^{\dagger} &= \begin{bmatrix} a + ib & c + id \\ e + if & g + ih \end{bmatrix} \\
        \begin{bmatrix} a - ib & e - if \\ c - id & g - ih \end{bmatrix} &= \begin{bmatrix} a + ib & c + id \\ e + if & g + ih \end{bmatrix} \\
    \end{split}
\end{equation*}

Comparing co-efficient, the followings real constraints

\begin{align*}
    -b &= b & e & = c & -f &= d & -h & = h 
\end{align*}

The first and last equation can be satisfied only if $b$ and $h$ vanish. Using the second and third equation we may rewrite $H$ as 

\begin{equation*}
    \Omega = \begin{bmatrix} a & c + id \\ c  - id & g \end{bmatrix}
\end{equation*}

Therefore it takes $4$ real numbers to specify $H.$ Another way to state this is that the diagonal elements of a hermitian matrix must be real and the off-diagonal elements must form complex conjugate pairs. Now, we may write a unitary matrix $U$ in general form as: 

\begin{equation*}
    U = \begin{bmatrix} a + ib & c + id \\ e + if & g + ih \end{bmatrix} 
\end{equation*}

The condition $U^{\dagger}U = I$ implies 

\begin{equation*}
    \begin{split}
        \begin{bmatrix} a + ib & c + id \\ e + if & g + ih \end{bmatrix}^{\dagger}\begin{bmatrix} a + ib & c + id \\ e + if & g + ih \end{bmatrix} &= I \\
        \begin{bmatrix} a - ib & e - if \\ c - id & g - ih \end{bmatrix}\begin{bmatrix} a + ib & c + id \\ e + if & g + ih \end{bmatrix} &= I \\
    \end{split}
\end{equation*}

\begin{equation*}
        \begin{bmatrix} (a^{2} + b^{2} + e^{2} + f^{2}) & ac + bd + eg + fh + i(ad + eh - bc - fg) \\ ac + bd + eg + fh + i(bc + fg - ad -eh) & (c^{2} + d^{2} + g^{2} + h^{2})  \end{bmatrix} & = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
\end{equation*}

Comparing co-efficient, we have the following real constraints

\begin{align*}
    a^{2} + b^{2} + e^{2} + f^{2} & = 1 \\
    ac + bd + eg + fh & = 0 \\
    ad + eh - bc - fg & = 0 \\
    c^{2} + d^{2} + g^{2} + h^{2} & = 1
\end{align*}

Hence, in this case too, the eight potential real degrees of freedom are reduced to four free ones by imposing four real constraints. 

** SOLVED Problem 9.5.6
CLOSED: [2022-11-01 Tue 14:51]
\question*{Problem 9.5.6}

The Cayley-Hamilton Theorem states that every matrix obeys its characteristic equation. In other words, if $P(\omega)$ is the characteristic polynomial for the matrix $\Omega$, then $P(\Omega)$ vanishes as a matrix. This means that it will annihilate any vector. We will first prove the theorem for a hermitian matrix $\Omega$ with non-degenerate eigenvectors. We start with the action of $P(\Omega)$ on the eigenvectors:

\begin{equation*}
    \begin{split}
        P(\Omega)\ket{j} &= \left( a_{n}\Omega^{n} + a_{n-1}\Omega^{n-1} + ... + a_{0} \right) \ket{j} \\
        & = a_{n}\Omega^{n}\ket{j} + a_{n-1}\Omega^{n-1}\ket{j} + \dots + a_{0}\ket{j} \\
        & = a_{n}j^{n}\ket{j} + a_{n-1}j^{(n-1)} \ket{j} + \dots + a_{0}\ket{j} \\
        & = \left( a_{n}j^{n} + a_{n-1}j^{n-1} + \dots + a_{0} \right)\ket{j} 
    \end{split}
\end{equation*}

The factor multiplying $\ket{j}$ is the characteristic polynomial evaluated at $j$ the eigenvalue of $\ket{j}.$ Since $j$ is a root of the characteristic polynomial, the factor must vanish. This is true for all $\ket{j}$ that are non-degenerate eigenvectors of the hermitian matrix $\Omega.$  Thus we have shown that $P(\Omega)$ vanishes all vectors (Any vector $\ket{V}$ may be expanded in the basis composed of the non-degenerate eigenvectors of the hermitian matrix $\Omega$).  Next we consider a non-hermitian but non-degenerate matrix $\Phi.$
** TOSOLVE Problem 9.5.7
** TOSOLVE Problem 9.5.8
** TOSOLVE Problem 9.5.9
** TOSOLVE Problem 9.5.10
** TOSOLVE Problem 9.5.11
** TOSOLVE Problem 9.6.1
** TOSOLVE Problem 9.6.2
** TOSOLVE Problem 9.6.3
** TOSOLVE Problem 9.6.4
** TOSOLVE Problem 9.6.5
** TOSOLVE Problem 9.6.6
** TOSOLVE Problem 9.6.7
** TOSOLVE Problem 9.7.1
** TOSOLVE Problem 9.7.2
** TOSOLVE Problem 9.7.3
** TOSOLVE Problem 9.7.4
** TOSOLVE Problem 9.7.5
** TOSOLVE Problem 9.7.6
** TOSOLVE Problem 9.7.7
** TOSOLVE Problem 9.7.8
** TOSOLVE Problem 9.7.9
** TOSOLVE Problem 9.7.10
** TOSOLVE Problem 9.7.11
** TOSOLVE Problem 9.7.12
** TOSOLVE Problem 9.7.13
** TOSOLVE Problem 9.7.14
** TOSOLVE Problem 9.7.15
** TOSOLVE Problem 9.8.1
** TOSOLVE Problem 9.8.2
** TOSOLVE Problem 9.9.1
** TOSOLVE Problem 9.9.2
* Differential equations
:LOGBOOK:
CLOCK: [2022-10-24 Mon 22:03]--[2022-10-24 Mon 22:49] =>  0:46
CLOCK: [2022-10-24 Mon 12:07]--[2022-10-24 Mon 12:41] =>  0:34
CLOCK: [2022-10-23 Sun 22:39]--[2022-10-23 Sun 23:09] =>  0:30
CLOCK: [2022-10-23 Sun 22:31]--[2022-10-23 Sun 22:38] =>  0:07
CLOCK: [2022-10-23 Sun 16:31]--[2022-10-23 Sun 16:40] =>  0:09
:END:
** SOLVED Problem 10.2.1
CLOSED: [2022-10-01 Sat 01:39]
:LOGBOOK:
CLOCK: [2022-09-30 Fri 22:42]--[2022-10-01 Sat 01:39] =>  2:57
:END:
*Show that for the oscillator*

\begin{equation*}
S \left[ x_{1}, t_{1}; x_{2}, t_{2} \right] = \dfrac{m \omega}{2 \sin \omega t} \left[ \left( x_{1}^{2} + x_{2}^{2} \right) \cos \omega t - 2 x_{1} x_{2} \right].
\end{equation*}

By definition

\begin{equation*}
S \left[ x_{1}, t_{1}; x_{2}, t_{2} \right] = \int_{t_{1}}^{t_{2}} \left[ \dfrac{1}{2} m \dot{x}^{2} - \dfrac{1}{2} m \omega^{2} x^{2} \right] dt
\end{equation*}

The solution for the oscillator equation is

\begin{equation*}
x(t) = A \cos \omega t + B \sin \omega t, \quad \omega = \sqrt{k/m},
\end{equation*}

\begin{equation*}
\dot{x(t)} = B \omega \cos \omega t - A \omega \sin \omega t
\end{equation*}

Subtituting in the expression that defines the action

\begin{equation*}
S \left[ x_{1}, t_{1}; x_{2}, t_{2} \right] = \int_{t_{1}}^{t_{2}} \left[ \dfrac{1}{2} m \left( B \omega \cos \omega t - A \omega \sin \omega t \right)^{2} - \dfrac{1}{2} m \omega^{2} \left( A \cos \omega t + B \sin \omega t \right)^{2} \right] dt.
\end{equation*}

Simplifying

\begin{align*}
S \left[ x_{1}, t_{1}; x_{2}, t_{2} \right] &= m \omega^{2} \int_{t_{1}}^{t_{2}} \left[ \left(B^{2} - A^{2} \right) \cos 2 \omega t - 2 AB \sin 2 \omega t \right] dt \\
& = \dfrac{m \omega}{2} \left[ \left( B^{2} - A^{2} \right) \sin 2 \omega t + 2 AB \cos 2 \omega t  \right]_{t_{1}}^{t_{2}} \\
& = \dfrac{m \omega}{2} \left[ \left( B^{2} - A^{2} \right) \sin \omega t \cos \omega t + 2AB - 2AB \sin^{2} \omega t  \right]_{t_{1}}^{t_{2}}.
\end{align*}

With $t_{1} = 0$ and $t_{2} = t$

\begin{equation*}
S = \dfrac{m \omega}{2} \left[ 2 \left( B^{2} - A^{2} \right) \sin \omega t \cos \omega t - 2 AB \sin^{2} \omega t \right].
\end{equation*}

We require

\begin{equation*}
x(t_{1} = 0) = A = x_{1},
\end{equation*}

and

\begin{equation*}
x(t_{2} = t) = A \cos \omega t + B \sin \omega t = x_{2}.
\end{equation*}

Solving for $B$

\begin{equation*}
B = \dfrac{x_{2} - x_{1} \cos \omega t}{\sin \omega t}.
\end{equation*}

Substituting in the expression for $S$ and simplifying

\begin{align*}
S &= \dfrac{m \omega}{2} \left[ \left( \dfrac{x_{2}^{2} + x_{1}^{2} \cos^{2} \omega t - 2 x_{1} x_{2} \cos \omega t - x_{1}^{2} \sin^{2} \omega t}{\sin \omega t} \right) \cos \omega t - 2 x_{1} x_{2} \sin \omega t + 2 x_{1} x_{2} \sin \omega t \cos \omega t \right] \\
&= \dfrac{m \omega}{2} \left[ \left( \dfrac{x_{2}^{2} \cos \omega t + x_{1}^{2} \cos^{3} \omega t  - x_{1}^{2} \sin^{2} \omega t \cos \omega t - 2 x_{1} x_{2} \cos^{2} \omega t - 2 x_{1} x_{2} \sin^{2} \omega t + 2 x_{1}^{2} \sin^{2} \omega t \cos \omega t}{\sin \omega t} \right) \right] \\
&= \dfrac{m \omega}{2 \sin \omega t} \left[ x_{2}^{2} \cos \omega t + x_{1}^{2} \cos \omega t \left( \cos^{2} \omega t + \sin^{2} \omega t \right)  - x_{1} x_{2} \left( \sin^{2} \omega t + \cos^{2} \omega t \right) \right] \\
&= \dfrac{m \omega}{2 \sin \omega t} \left[ \left( x_{1}^{2} + x_{2}^{2} \right) \cos \omega t - 2 x_{1} x_{2}  \right].
\end{align*}

** SOLVED Problem 10.2.2
CLOSED: [2022-10-18 Tue 15:18]
:LOGBOOK:
CLOCK: [2022-10-18 Tue 15:21]--[2022-10-18 Tue 15:32] =>  0:11
CLOCK: [2022-10-18 Tue 15:11]--[2022-10-18 Tue 15:18] =>  0:07
:END:
*Consider the equation $\dfrac{dx}{dt} + Ax + B =0$. Show that if $x(t)$ is a solution, so is $x(t) + C \exp \left \lbrace - A t  \right \rbrace$, where $C$ is an arbitrary constant.*

We want to ascertain

\begin{equation*}
x^{\prime}(t) - C A \exp \left \lbrace - A t  \right \rbrace + A x(t) + CA \exp \left \lbrace - A t  \right \rbrace + B \stackrel{?}{=} 0,
\end{equation*}

given

\begin{equation*}
x^{\prime}(t) + A x(t) + B = 0.
\end{equation*}

Need I say more?

** SOLVED Problem 10.2.3
CLOSED: [2022-10-18 Tue 15:50]
:LOGBOOK:
CLOCK: [2022-10-18 Tue 15:32]--[2022-10-18 Tue 15:50] =>  0:18
:END:

*Given*

\begin{equation*}
C \cos \omega t + D \sin \omega t = E \cos \left( \omega t - \phi \right),
\end{equation*}

*find $E$ and $\phi$ in terms of $C$ and $D$*.

With a rewrite of the RHS as

\begin{align*}
E \cos \left( \omega t - \phi \right) = E \cos \omega t \cos \phi + E \sin \omega t \sin \phi,
\end{align*}

we obtain

\begin{align*}
E \cos \phi &= C, \\
E \sin \phi &= D.
\end{align*}

Squaring both sides of both equations and adding

\begin{equation*}
E = \sqrt{C^{2} + D^{2}}.
\end{equation*}

Dividing the second equation by the first

\begin{equation*}
\phi = \tan^{-1} \dfrac{D}{C} + n \pi.
\end{equation*}

** SOLVED Problem 10.2.4
CLOSED: [2022-10-18 Tue 16:08]
:LOGBOOK:
CLOCK: [2022-10-18 Tue 15:50]--[2022-10-18 Tue 16:08] =>  0:18
:END:
*Find the general solution to $(D^{2} - w^{2})x = 0$. Find the parameters that ensure that $x(0) = 2$, $\dot{x}(0) = 0$.*

With the ansatz $x(t) = A \exp \left \lbrace \alpha t  \right \rbrace$, the ODE reduces to

\begin{equation*}
\left(\alpha^{2}  - \omega^{2}\right) A \exp \left \lbrace \alpha t  \right \rbrace = 0.
\end{equation*}

For a non-trivial $(A \neq 0)$ solution

\begin{equation*}
\alpha^{2} = \omega^{2} \Longrightarrow \alpha = \pm \omega.
\end{equation*}

The ODE being linear and homogenous

\begin{equation*}
x(t) = A_{+} \exp \left \lbrace + \omega t  \right \rbrace + A_{-} \exp \left \lbrace - \omega t  \right \rbrace
\end{equation*}

is the most general solution. Demanding $x(0) = 2$, $\dot{x}(0) = 0$ we must have

\begin{equation*}
A_{+} + A_{-} = 2,
\end{equation*}

\begin{equation*}
\omega \left( A_{+} - A_{-} \right) = 0.
\end{equation*}

Solving for $A_{+}$ and $A_{-}$

\begin{equation*}
A_{+} = A_{-} = 1.
\end{equation*}

The solution is thus

\begin{equation*}
x(t) = \exp \left \lbrace \omega t  \right \rbrace + \exp \left \lbrace - \omega t  \right \rbrace = 2 \cosh \left( \omega t \right).
\end{equation*}

** SOLVED Problem 10.2.5
CLOSED: [2022-10-18 Tue 16:50]
:LOGBOOK:
CLOCK: [2022-10-18 Tue 16:29]--[2022-10-18 Tue 16:50] =>  0:21
:END:
*Find the general solution of $(D^{3} - D^{2} + D - 1)x = 0$.*

With the ansatz $x(t) = A \exp \left \lbrace \alpha t  \right \rbrace$, the ODE reduces to

\begin{equation*}
(\alpha^{3} - \alpha^{2} + \alpha - 1) A \exp \left \lbrace \alpha t  \right \rbrace = 0.
\end{equation*}

For a non-trivial $(A \neq 0)$ solution

\begin{equation*}
\alpha \left(\alpha^{2} - \alpha + 1\right) = 1.
\end{equation*}

$\alpha = \alpha_{1} \equiv 1$ is a root. Let the other two roots be $\alpha_{2}$ and $\alpha_{3}$. We must have

\begin{align*}
\left( \alpha - 1 \right) \left( \alpha - \alpha_{2} \right) \left( \alpha - \alpha_{3} \right) &= \left( \alpha - 1 \right) \left( \alpha^{2} - \left( \alpha_{2} + \alpha_{3}  \right) \alpha + \alpha_{2} \alpha_{3} \right) \\
&= \alpha^{3} - \left( \alpha_{2} + \alpha_{3} + 1 \right) \alpha^{2} + \left( \alpha_{2} \alpha_{3} + \alpha_{2} + \alpha_{3} \right) \alpha - \alpha_{2} \alpha_{3} \\
&= \alpha^{3} - \alpha^{2} + \alpha - 1.
\end{align*}
Matching coefficients

\begin{equation*}
\left( 1 - \alpha_{2} - \alpha_{3} \right) = 1, \quad \alpha_{2} \alpha_{3} + \alpha_{2} + \alpha_{3} = 1, \quad \alpha_{2} \alpha_{3} = 1.
\end{equation*}

Solving for $\alpha_{2}$ and $\alpha_{3}$

\begin{equation*}
\alpha_{2} = i, \quad \alpha_{3} = - i.
\end{equation*}

The general solution is thus of the form

\begin{equation*}
x(t) = A_{1} \exp \left \lbrace t  \right \rbrace + A_{2} \exp \left \lbrace i t  \right \rbrace + A_{3} \exp \left \lbrace - i t  \right \rbrace.
\end{equation*}

** SOLVED Problem 10.2.6
CLOSED: [2022-10-18 Tue 17:22]
:LOGBOOK:
CLOCK: [2022-10-18 Tue 17:16]--[2022-10-18 Tue 17:22] =>  0:06
:END:
*Verify that*

\begin{equation*}
x(t) = \left( A + B t \right) \exp \left \lbrace \alpha t  \right \rbrace
\end{equation*}

*is indeed a solution to $(D - \alpha)^{2} x = 0$*

Substituting $x(t)$ in $(D - \alpha)^{2} x$ to obtain

\begin{align*}
\left( D - \alpha \right)^{2} \left( A + B t \right) \exp \left \lbrace \alpha t  \right \rbrace
&= \left( D - \alpha \right) \left[ A \alpha \exp \left \lbrace \alpha t  \right \rbrace + B \alpha t \exp \left \lbrace \alpha t  \right \rbrace - A \alpha \exp \left \lbrace  \alpha t  \right \rbrace - B \alpha t \exp \left \lbrace \alpha t  \right \rbrace\right] \\
&= \left( D - \alpha \right) 0 = 0,
\end{align*}
which should satisfy the ask.

** SOLVED Problem 10.2.7
CLOSED: [2022-10-18 Tue 17:39]
:LOGBOOK:
CLOCK: [2022-10-18 Tue 17:22]--[2022-10-18 Tue 17:39] =>  0:17
:END:
*First show that $(D - \alpha) \exp \left \lbrace \alpha t  \right \rbrace f(t) = \exp \left \lbrace \alpha t  \right \rbrace D f$. Then show that $(D - \alpha)^{2} \exp \left \lbrace \alpha t  \right \rbrace f(t) = \exp \left \lbrace \alpha t \right \rbrace D^{2} f$. Argue on the basis of the above that $(D - \alpha)^{2} x = 0$ has a solution of the form $\exp \left \lbrace \alpha t  \right \rbrace f(t)$, where $D^{2} f = 0$. Make contact with* $x(t) = \left( A + Bt \right) \exp \left \lbrace \alpha t  \right \rbrace$.

\begin{align*}
\left( D - \alpha \right) \exp \left \lbrace \alpha t  \right \rbrace f(t)
&= \exp \left \lbrace \alpha t \right \rbrace D f + \alpha \exp \left \lbrace \alpha t  \right \rbrace f - \alpha \exp \left \lbrace \alpha t  \right \rbrace f \\
&= \exp \left \lbrace \alpha t  \right \rbrace D f.
\end{align*}

Operate $(D - \alpha)$ on both sides of the equation above to obtain

\begin{equation*}
\left( D - \alpha \right)^{2} \exp \left \lbrace \alpha t  \right \rbrace f(t) = \exp \left \lbrace \alpha t \right \rbrace D^{2} f  + \alpha \exp \left \lbrace \alpha t \right \rbrace D f - \alpha \exp \left \lbrace \alpha t  \right \rbrace D f = \exp \left \lbrace \alpha t  \right \rbrace D^{2} f.
\end{equation*}

If $D^{2} f = 0$, the equation above reduces to

\begin{equation*}
\left( D - \alpha \right)^{2} \exp \left \lbrace \alpha t  \right \rbrace f(t) = 0,
\end{equation*}

and thus $x(t) = \exp \left \lbrace \alpha t  \right \rbrace f(t)$ is a solution of $\left( D - \alpha \right)^{2} x = 0$ given $D^{2} f = 0$. Now $D^{2} f = 0$ has solutions of the form $f(t) = A + B t$ and thus

\begin{equation*}
x(t) = \left( A + B t \right) \exp \left \lbrace \alpha t  \right \rbrace
\end{equation*}

is solution of the ODE $\left( D - \alpha \right)^{2} = 0$.

** SOLVED Problem 10.2.8
CLOSED: [2022-10-18 Tue 20:35]
:LOGBOOK:
CLOCK: [2022-10-18 Tue 17:40]--[2022-10-18 Tue 17:52] =>  0:12
:END:
*Find the solutions to*

*** $\left( D^{2} + 2D + 1 \right) x(t) = 0$ with $x(0) = 1, \quad \dot{x}(0) = 0$.

Recasting the ODE

\begin{equation*}
\left( D^{2} + 2D + 1 \right) x(t) = 0 \Longleftrightarrow \left( D + 1 \right)^{2} x(t) = 0,
\end{equation*}

we get solutions of the form

\begin{equation*}
x(t) = \left( A + Bt \right) \exp \left \lbrace \alpha t  \right \rbrace.
\end{equation*}

Demanding $x(0) = 1$ and $\dot{x}(0) = 0$, we get

\begin{equation*}
A = 1, \quad B = -1.
\end{equation*}

and $x(t)$ thus reduces to

\begin{equation*}
x(t) = \left( 1 - t \right) \exp \left \lbrace \alpha t  \right \rbrace.
\end{equation*}

*** $\left(D^{4} + 1\right) x(t) = 0$.
:LOGBOOK:
CLOCK: [2022-10-18 Tue 19:17]--[2022-10-18 Tue 19:34] =>  0:17
:END:

With the ansatz $x(t) = A \exp \left \lbrace \alpha t  \right \rbrace$

\begin{equation*}
\left( D^{4} + 1 \right) x(t) = \left( \alpha^{4} + 1 \right)x(t) = 0.
\end{equation*}

The four roots of unity solve this equation. The general solution is thus of the form

\begin{equation*}
x(t) = A_{1} \exp \left \lbrace \left( \dfrac{1 + i}{\sqrt{2}} \right) t \right \rbrace
+ A_{2} \exp \left \lbrace \left( \dfrac{i - 1}{\sqrt{2}} \right) t \right \rbrace
+ A_{3} \exp \left \lbrace - \left( \dfrac{1 + i}{\sqrt{2}} \right) t \right \rbrace
+ A_{4} \exp \left \lbrace \left( \dfrac{1 - i}{\sqrt{2}} \right) t \right \rbrace.
\end{equation*}

*** $\left(D^{3} - 3 D^{2} - 9 D - 5 \right) x(t) = 0 \quad \text{(5 is a root)} \quad$.
:LOGBOOK:
CLOCK: [2022-10-18 Tue 19:52]--[2022-10-18 Tue 20:26] =>  0:34
:END:
Given $5$ is a root

\begin{align*}
\left( \alpha - 5 \right) \left( \alpha - \alpha_{1} \right) \left( \alpha - \alpha_{2} \right)
&= \left( \alpha - 5 \right) \left(\alpha^{2} - \left( \alpha_{1} + \alpha_{2} \right) \alpha + \alpha_{1} \alpha_{2}\right)  \\
&= \alpha^{3} - \left( \alpha_{1} + \alpha_{2} + 5 \right) \alpha^{2} + \left( \alpha_{1} \alpha_{2} + 5 \alpha_{1} + 5 \alpha_{2} \right) \alpha - 5 \alpha_{1} \alpha_{2}\\
&= \alpha^{3} - 3 \alpha^{2} - 9 \alpha - 5.
\end{align*}

Matching coefficients

\begin{equation*}
\alpha_{1} + \alpha_{2} = - 2,
\end{equation*}

\begin{equation*}
\alpha_{1} \alpha_{2} + 5 \alpha_{1} + 5 \alpha_{2} = -9,
\end{equation*}

\begin{equation*}
\omega_{1} \omega_{2} = 1.
\end{equation*}

$\alpha_{1} = \alpha_{2} = -1$.

The general solution is thus of the form

\begin{equation*}
x(t) = A_{1} \exp \left \lbrace 5t \right \rbrace + \left( A_{2} + A_{3} t \right) \exp \left \lbrace - t  \right \rbrace.
\end{equation*}

*** $(D + 1)^{2} \left( D^{4} - 256 \right) x(t) = 0$.
:LOGBOOK:
CLOCK: [2022-10-18 Tue 20:26]--[2022-10-18 Tue 20:35] =>  0:09
:END:

A rewrite of the ODE as

\begin{equation*}
\left( D + 1 \right)^{2}\left( D^{4} - 4^{4} \right) x(t) = 0,
\end{equation*}

indicates that we require the roots of

\begin{equation*}
\left( \alpha + 1 \right)^{2} \left( \alpha^{4} - 4^{4} \right) = 0;
\end{equation*}

our requirement satisfied by

\begin{equation*}
\alpha_{1} = \alpha_{2} = -1, \quad \alpha_{3} = 4, \quad \alpha_{4} = -4, \quad \alpha_{5} = 4i, \quad \alpha_{6} = - 4i,
\end{equation*}

which in turn leads us to the conclusion that

\begin{equation*}
x(t) = \left( A_{1} + A_{2} t \right) \exp \left \lbrace - t  \right \rbrace
+ A_{3} \exp \left \lbrace 4 t  \right \rbrace
+ A_{4} \exp \left \lbrace -4 t  \right \rbrace
+ A_{5} \exp \left \lbrace 4i t  \right \rbrace
+ A_{6} \exp \left \lbrace -4i t  \right \rbrace,
\end{equation*}

solves the ODE.

** SOLVED Problem 10.2.9
CLOSED: [2022-10-18 Tue 21:54]
:LOGBOOK:
CLOCK: [2022-10-18 Tue 21:48]--[2022-10-18 Tue 21:54] =>  0:06
:END:
*Consider the ODE*

\begin{equation*}
\left( a_{n} D^{n} + a_{n-1} D^{n-1} + \dotso + a_{0} \right) x = f(t),
\end{equation*}

*with $f(t) = F \exp \left \lbrace i \Omega t \right \rbrace$. Show that the solution is of the form*

\begin{equation*}
x(t) = \dfrac{F \exp \left \lbrace i \Omega t  \right \rbrace}{\left(a_{n} \left( i \Omega \right)^{n} + a_{n-1} \left( i \Omega \right)^{n-1} + \dotso + a_{0} \right)}.
\end{equation*}


We assume that $x(t)$ has the form

\begin{equation*}
x(t) = x_{0} \exp \left \lbrace i \Omega t \right \rbrace.
\end{equation*}

Substituting in the ODE we obtain

\begin{equation*}
\left( a_{n} \left( i \Omega \right)^{n} + a_{n-1} \left( i \Omega \right)^{n-1} + \dotso + a_{0} \right) x_{0} \exp \left \lbrace i \Omega t  \right \rbrace = F \exp \left \lbrace i \Omega t \right \rbrace,
\end{equation*}

which on solving for $x_{0}$ gives

\begin{equation*}
x_{0} = \dfrac{F}{\left( a_{n} \left( i \Omega \right)^{n} + a_{n-1} \left( i \Omega \right)^{n-1} + \dotso + a_{0} \right)}
\end{equation*}

so that

\begin{equation*}
x(t) = \dfrac{F \exp \left \lbrace i \Omega t \right \rbrace}{\left( a_{n} \left( i \Omega \right)^{n} + a_{n-1} \left( i \Omega \right)^{n-1} + \dotso + a_{0} \right)}.
\end{equation*}

** SOLVED Problem 10.2.10
CLOSED: [2022-10-20 Thu 01:58]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 00:05]--[2022-10-20 Thu 01:58] =>  1:53
CLOCK: [2022-10-19 Wed 00:11]--[2022-10-19 Wed 01:33] =>  1:22
CLOCK: [2022-10-18 Tue 21:56]--[2022-10-18 Tue 23:18] =>  1:22
:END:
*Consider a damped oscillator subject to a cosine force:*

\begin{equation*}
\left( D^{2} + 2 \gamma D + \omega_{0}^{2} \right) x = F \cos \Omega t.
\end{equation*}

*Show that the frequency at which the steady state solution has maximum amplitude, i.e., resonance, is given by* $\Omega^{2} = \omega_{0}^{2} - 2 \gamma^{2}$.

Start with a contract that after all is said and done, you'll only accept what is real. Then rewrite

\begin{equation*}
\left( D^{2} + 2 \gamma D + \omega_{0}^{2} \right) x = F \exp \left \lbrace i \Omega t  \right \rbrace.
\end{equation*}

An auxillary steady state solution is simply the /particular solution/ and in this case is equal to

\begin{equation*}
y(t) = \dfrac{F \exp \left \lbrace i \Omega t  \right \rbrace}{- \Omega^{2} + 2 i \gamma \Omega + \omega_{0}^{2}}.
\end{equation*}

Hold up the contract and write the /real/ solution as

\begin{align*}
x(t) &= (y + y^{\ast})/2 \\
&= \dfrac{1}{2} \left[ \dfrac{F}{- \Omega^{2} + 2 i \gamma \Omega + \omega_{0}^{2}} \exp \left \lbrace i \Omega t  \right \rbrace + \dfrac{F}{- \Omega^{2} - 2 i \gamma \Omega + \omega_{0}^{2}} \exp \left \lbrace - i \Omega t  \right \rbrace \right] \\
&= \dfrac{F}{2 \left( \left( \omega_{0}^{2} - \Omega^{2} \right)^{2} + \left(2 \gamma \Omega  \right)^{2}\right)} \left[ \left( \omega_{0}^{2} - \Omega^{2}  \right) \left[ \exp \left \lbrace i \Omega t  \right \rbrace + \exp \left \lbrace - i \Omega t  \right \rbrace \right] - 2 i \gamma \Omega \left( \exp \left \lbrace i \Omega t \right \rbrace - \exp \left \lbrace - i \Omega t  \right \rbrace \right) \right] \\
&= \dfrac{F}{\left( \omega_{0}^{2} - \Omega^{2} \right)^{2} + \left(2 \gamma \Omega  \right)^{2}} \left[ \left( \omega_{0}^{2} - \Omega^{2}  \right) \cos \left( \Omega t \right) + 2 \gamma \Omega \sin \left( \Omega t \right) \right].
\end{align*}

We would like to write the general solution as

\begin{equation*}
x(t) = E \cos \left( \Omega t - \phi \right).
\end{equation*}

Equating with the previous equation and using $\cos \left( \Omega t - \phi \right) = \cos \Omega t \cos \phi + \sin \Omega t \sin \phi$

\begin{equation*}
E \cos \Omega t \cos \phi + E \sin \Omega t \sin \phi = \dfrac{F}{\left( \omega_{0}^{2} - \Omega^{2} \right)^{2} + \left(2 \gamma \Omega  \right)^{2}} \left[ \left( \omega_{0}^{2} - \Omega^{2}  \right) \cos \left( \Omega t \right) + 2 \gamma \Omega \sin \left( \Omega t \right) \right]
\end{equation*}

so that

\begin{equation*}
E \cos \phi = \dfrac{\left( \omega_{0}^{2} - \Omega^{2} \right) F}{\left( \omega_{0}^{2} - \Omega^{2} \right)^{2} + \left(2 \gamma \Omega  \right)^{2}},
\end{equation*}

\begin{equation*}
E \sin \phi = \dfrac{2 \gamma \Omega F}{\left( \omega_{0}^{2} - \Omega^{2} \right)^{2} + \left(2 \gamma \Omega  \right)^{2}}.
\end{equation*}

Adding both sides after squaring them

\begin{equation*}
E^{2} = \dfrac{\left( \omega_{0}^{2} - \Omega^{2} \right)^{2} F^{2} + \left(2 \gamma \Omega  \right)^{2} F^{2}}{\left[ \left( \omega_{0}^{2} - \Omega^{2} \right)^{2} + \left(2 \gamma \Omega  \right)^{2} \right]^{2}},
\end{equation*}

so that

\begin{equation*}
E = \dfrac{F}{\sqrt{\left( \omega_{0}^{2} - \Omega^{2} \right)^{2} + \left( 2 \gamma \Omega \right)^{2}}},
\end{equation*}

\begin{equation*}
\phi = \tan^{-1} \left( \dfrac{2 \gamma \Omega}{\omega_{0}^{2} - \Omega^{2}} \right).
\end{equation*}

Resonance condition is obtained as

\begin{equation*}
\dfrac{d E}{d \Omega} = - \dfrac{1}{2} \left[\left( \omega_{0}^{2} - \Omega^{2} \right)^{2} + \left( 2 \gamma \Omega \right)^{2} \right]^{-3/2} \left[ 4 \Omega \gamma^{2} + 2 \left( \omega_{0}^{2} - \Omega^{2} \right) \left( - 2 \Omega \right) \right] = 0.
\end{equation*}

which furnishes the ask

\begin{equation*}
\Omega^{2} = \omega_{0}^{2} - 2 \gamma^{2}.
\end{equation*}

*Let* $F = 25$, $\omega_{0} = 1$, $2 \gamma = \Omega = 2 \omega_{0}$. *(i) What can you say about the transient (complementary) part?*

Given $F = 25$, $\omega_{0} = 1$, $2 \gamma = \Omega = 2 \omega_{0}$

\begin{equation*}
E = 5, \quad 3 \sin \phi = - 4 \cos \phi.
\end{equation*}

The /complementary solution/ is obtained by solving the ODEs

\begin{equation*}
\left( D^{2} + 2 \gamma D + \omega_{0}^{2} \right) x = 0,
\end{equation*}

which reduces to

\begin{equation*}
\left( D^{2} + 2 D + 1\right) x = \left( D + 1\right)^{2} x = 0.
\end{equation*}

This represents repeated roots and the solution is of the form

\begin{equation*}
x(t) = \left( A + B t \right) \exp \left \lbrace - t  \right \rbrace.
\end{equation*}

Thus the complementary solution is *critically damped.*

*(ii) Write down the solution that has initial displacement and velocity zero.*

The general solution is obtained by summing the particular (/steady-state/) and complementary (/transient solutions/):

\begin{equation*}
x(t) = \left( A + B t \right) \exp \left \lbrace - t  \right \rbrace + E \cos \left( \Omega t - \phi \right).
\end{equation*}

Demanding $x(t) = 0$ and $\dot{x}(t) = 0$

\begin{equation*}
A + E \cos \left(-\phi\right)  = A + E \cos \phi = 0,
\end{equation*}

\begin{equation*}
-A + B - E \Omega \sin \left( - \phi \right) = -A + B + E \Omega \sin \left(\phi \right) = 0.
\end{equation*}

Solving for $A$ and $B$ with $F = 25$, $\omega_{0} = 1$, $2 \gamma = \Omega = 2 \omega_{0}$,

\begin{equation*}
A = 3, \quad B = -5.
\end{equation*}

so that

\begin{align*}
x(t) &= \left( A + B t \right) \exp \left \lbrace - t \right \rbrace + E \cos \left( \Omega t - \phi \right) \\
&= \exp \left \lbrace - t \right \rbrace \left[ 3 - 5 t \right] - 3 \cos 2t + 4 \sin 2t.
\end{align*}

*(iii) Repeat for the case where the initial position is unity and velocity is zero*

Demanding $x(t) = 1$ and $\dot{x}(t) = 0$

\begin{equation*}
A + E \cos \left( - \phi \right) = A + E \cos \phi = 1,
\end{equation*}

\begin{equation*}
-A + B - E \Omega \sin \left( - \phi \right) = - A + B + E \Omega \sin \left( \phi \right) = 0.
\end{equation*}

Solving for $A$ and $B$ with $F = 25$, $\omega_{0} = 1$, $2 \gamma = \Omega = 2 \omega_{0}$,

\begin{equation*}
A = 4, \quad B = 4.
\end{equation*}

so that

\begin{equation*}
x(t) = \exp \left \lbrace - t  \right \rbrace \left[ 4 - 4 t \right] - 3 \cos 2t + 4 \sin 2t.
\end{equation*}

*and (iv) vice versa.*

Demanding $x(t) = 0$ and $\dot{x}(t) = 1$

\begin{equation*}
A + E \cos \left( - \phi \right) = A + E \cos \phi = 0,
\end{equation*}

\begin{equation*}
-A + B - E \Omega \sin \left( - \phi \right) = - A + B + E \Omega \sin \left( \phi \right) = 1.
\end{equation*}

Solving for $A$ and $B$ with $F = 25$, $\omega_{0} = 1$, $2 \gamma = \Omega = 2 \omega_{0}$,

\begin{equation*}
A = 3, \quad B = 4.
\end{equation*}

so that

\begin{equation*}
x(t) = \exp \left \lbrace - t  \right \rbrace \left[ 3 - 4 t \right] - 3 \cos 2t + 4 \sin 2t.
\end{equation*}

** SOLVED Problem 10.2.11
CLOSED: [2022-10-20 Thu 05:07]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 02:32]--[2022-10-20 Thu 05:07] =>  2:35
:END:
*Solve the following subject to $y(0) = 1$, $\dot{y}(0) = 0$*

*(i)* $\ddot{y} - \dot{y} - 2 y = \exp \left \lbrace 2 x \right \rbrace$.

With a rewrite

\begin{equation*}
\left( D^{2} - D - 2 \right)y = \exp \left \lbrace 2x  \right \rbrace.
\end{equation*}

The /complimentary function/ is

\begin{equation*}
y_{c} (t) = A \exp \left \lbrace 2 x  \right \rbrace + B \exp \left \lbrace - x  \right \rbrace.
\end{equation*}

Another rewrite for the ODE is

\begin{equation*}
\left( D - 2 \right) \left( D + 1 \right) y(x) = \exp \left \lbrace 2x  \right \rbrace.
\end{equation*}

For the /particular solution/, use the ansatz $y(x) = C x \exp \left \lbrace 2 x  \right \rbrace$ (the $x$ multiplying the exponential is because one of the exponentials of the complimentary function is equal to the inhomogeneity) such that

\begin{align*}
4 C \exp \left \lbrace 2x  \right \rbrace + 4 C x \exp \left \lbrace 2x  \right \rbrace - 2 C x \exp \left \lbrace 2x  \right \rbrace - C \exp \left \lbrace 2x  \right \rbrace - 2 C x \exp \left \lbrace 2x  \right \rbrace = \exp \left \lbrace 2x  \right \rbrace.
\end{align*}

Solving for $C$

\begin{equation*}
C = 1/3,
\end{equation*}

so that

\begin{equation*}
y_{p}(x) = \dfrac{x}{3} \exp \left \lbrace 2x  \right \rbrace.
\end{equation*}

The general solution thus is

\begin{equation*}
y(x) = \left( A + \dfrac{x}{3}  \right)\exp \left \lbrace 2x  \right \rbrace + B \exp \left \lbrace -x  \right \rbrace.
\end{equation*}

Demanding $y(0) = 1$ and $\dot{y}(0) = 0$,

\begin{equation*}
A + B = 1, \quad 2A + \dfrac{1}{3} - B = 0.
\end{equation*}

Solving for $A$ and $B$

\begin{equation*}
A = 2/9, \quad B = 7/9,
\end{equation*}

so that

\begin{equation*}
y(x) = \dfrac{2 + 3x}{9} \exp \left \lbrace  2x  \right \rbrace + \dfrac{7}{9} \exp \left \lbrace -x  \right \rbrace.
\end{equation*}

*(ii)* $\left( D^{2} - 2D + 1 \right) y = 2 \cos x$.

The complimentary function is

\begin{equation*}
y_{c}(x) = \left( A + Bx \right) \exp \left \lbrace x  \right \rbrace.
\end{equation*}

For the particular function sign the contract and rewrite

\begin{equation*}
\left( D^{2} - 2D + 1 \right) y = 2 \exp \left \lbrace i x  \right \rbrace
\end{equation*}

and use the ansatz $y(x) = y_{0} \exp \left \lbrace i x  \right \rbrace$ to obtain

\begin{equation*}
y_{0} = -1/2i.
\end{equation*}

so that (holding up the contract)

\begin{equation*}
y_{p}(x) = - \sin x.
\end{equation*}

The general solution thus is

\begin{equation*}
y(x) = \left( A + Bx \right) \exp \left \lbrace x  \right \rbrace - \sin x.
\end{equation*}

Demanding $y(0) = 1$ and $\dot{y}(0) = 0$,

\begin{equation*}
A = 1, \quad B = 0,
\end{equation*}

so that

\begin{equation*}
y(x) = \exp \left \lbrace x  \right \rbrace  - \sin x.
\end{equation*}

*(iii)* $y^{\prime \prime} + 16y = 16 \cos 4x$.

The complimentary function is

\begin{equation*}
y_{c}(x) = A \exp \left \lbrace 4x  \right \rbrace.
\end{equation*}

For the particular solution use the ansatz $y(x) = \left( B + x C \right) \exp \left \lbrace i 4 x  \right \rbrace$ in

\begin{equation*}
y^{\prime \prime} + 16y = 16 \exp \left \lbrace 4 i x  \right \rbrace
\end{equation*}

to obtain

\begin{equation*}
-16 B \exp \left \lbrace i 4 x  \right \rbrace + 4 C i \exp \left \lbrace i 4 x  \right \rbrace +  4 C i \exp \left \lbrace i 4 x \right \rbrace - 16 C x \exp \left \lbrace i 4 x  \right \rbrace = 16 \exp \left \lbrace 4 i x  \right \rbrace,
\end{equation*}

so that

\begin{equation*}
C = 2/i,
\end{equation*}

and

\begin{equation*}
y_{p}(x) = B \cos 4x + 2 x \sin 4x.
\end{equation*}

The general solution is

\begin{equation*}
y(x) = A \exp \left \lbrace 4x  \right \rbrace + B \cos 4x + 2 x \sin 4x.
\end{equation*}

Demanding $y(0) = 1$ and $\dot{y}(0) = 0$,

\begin{equation*}
A + B = 1, \quad A  = 0,
\end{equation*}

so that

\begin{equation*}
y(x) = \cos 4x + 2x \sin 4x.
\end{equation*}

*(iv)* $y^{\prime \prime} - y = \cosh x$.

The complimentary function is

\begin{equation*}
y_{c} (x) = A \exp \left \lbrace x  \right \rbrace.
\end{equation*}

For the particular solution use $\cosh x = \left( \exp \left \lbrace x  \right \rbrace + \exp \left \lbrace -x  \right \rbrace \right)/2$ to obtain the rewrite

\begin{equation*}
y^{\prime \prime} - y = \left(\exp \left \lbrace x  \right \rbrace + \exp \left \lbrace -x  \right \rbrace\right)/2.
\end{equation*}

With the ansatz $y(x) = (B + Cx) \exp \left \lbrace x  \right \rbrace$ obtain

\begin{equation*}
B \exp \left \lbrace x  \right \rbrace + 2 C \exp \left \lbrace x  \right \rbrace + C x \exp \left \lbrace x  \right \rbrace - B \exp \left \lbrace x  \right \rbrace - C x \exp \left \lbrace x  \right \rbrace = \exp \left \lbrace x  \right \rbrace/2,
\end{equation*}

so that

\begin{equation*}
C = 1/4,
\end{equation*}

and one of the particular solution is

\begin{equation*}
y_{p_{1}} = \left( B + \dfrac{x}{4} \right) \exp \left \lbrace x  \right \rbrace.
\end{equation*}

Similarly, with the ansatz $y(x) = \left( E - Fx \right) \exp \left \lbrace  - x  \right \rbrace$ obtain

\begin{equation*}
E \exp \left \lbrace - x  \right \rbrace - 2 F \exp \left \lbrace - x  \right \rbrace + F x \exp \left \lbrace - x \right \rbrace - E \exp \left \lbrace  - x  \right \rbrace - F x \exp \left \lbrace - x \right \rbrace = \exp \left \lbrace - x  \right \rbrace/2,
\end{equation*}

so that

\begin{equation*}
F = - 1/4,
\end{equation*}

and the other of the particular solution is

\begin{equation*}
y_{p_{2}} = \left( E - \dfrac{x}{4} \right) \exp \left \lbrace -x  \right \rbrace.
\end{equation*}

The general solution is thus

\begin{equation*}
y(x) = A^{\prime} \exp \left \lbrace x  \right \rbrace + E \exp \left \lbrace -x  \right \rbrace + \dfrac{x}{2} \sinh \left( x \right),
\end{equation*}

where $A^{\prime} = A + B$.

Demanding $y(0) = 1$ and $\dot{y}(0) = 0$,

\begin{equation*}
A^{\prime} + E = 1, \quad A^{\prime} - E = 0,
\end{equation*}

so that $A^{\prime} = E = 1/2$ and the general solution reduces to

\begin{equation*}
y(x) = \cosh \left( x \right) + \dfrac{x}{2} \sinh \left( x \right).
\end{equation*}
** SOLVED Problem 10.3.1
CLOSED: [2022-10-20 Thu 07:29]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 06:36]--[2022-10-20 Thu 07:29] =>  0:53
:END:
*Solve for $c$ and obtain this result. Verify that when $x = x_{0}$, $y$ indeed reduces to $y_{0}$. Verify also that adding a constant to $P$ makes no difference.*

In the solution

\begin{equation*}
y(x) = \exp \left \lbrace - P(x)  \right \rbrace \left[ \int^{x} q(x^{\prime}) \exp \left \lbrace P(x^{\prime})  \right \rbrace dx^{\prime} + c \right].
\end{equation*}

set both the upper and lower limit (it is a free parameter) of the integral to $x_{0}$ so that it vanishes. We then obtain

\begin{equation*}
y(x_{0}) = c \thinspace \exp \left \lbrace - P(x_{0})  \right \rbrace,
\end{equation*}

which on solving for $c$ gives

\begin{equation*}
c = y_{0} \exp \left \lbrace P(x_{0})  \right \rbrace,
\end{equation*}

thus furnishing the solution,

\begin{equation*}
y(x) = \exp \left \lbrace - P(x)  \right \rbrace \left[ \int_{x_{0}}^{x} q(x^{\prime}) \exp \left \lbrace P(x^{\prime})  \right \rbrace dx^{\prime} + y_{0} \exp \left \lbrace P(x_{0})  \right \rbrace  \right].
\end{equation*}

Suppose that we add a constant $R$ to $P$
\begin{align*}
y(x) &= \exp \left \lbrace - P(x) - R  \right \rbrace \left[ \int_{x_{0}}^{x} q(x^{\prime}) \exp \left \lbrace P(x^{\prime}) + R  \right \rbrace dx^{\prime} + y_{0} \exp \left \lbrace P(x_{0}) + R  \right \rbrace  \right] \\
&= \exp \left \lbrace - P(x) \right \rbrace \exp \left \lbrace - R  \right \rbrace \left[ \exp \left \lbrace R  \right \rbrace \int_{x_{0}}^{x} q(x^{\prime}) \exp \left \lbrace P(x^{\prime}) \right \rbrace dx^{\prime} + y_{0} \exp \left \lbrace P(x_{0}) \right \rbrace  \right] \\
&= \exp \left \lbrace - P(x)  \right \rbrace \left[ \int_{x_{0}}^{x} q(x^{\prime}) \exp \left \lbrace P(x^{\prime})  \right \rbrace dx^{\prime} + y_{0} \exp \left \lbrace P(x_{0})  \right \rbrace  \right].
\end{align*}

** SOLVED Problem 10.3.2
CLOSED: [2022-10-20 Thu 14:02]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 13:47]--[2022-10-20 Thu 14:02] =>  0:15
:END:

*Show that the solution to*

\begin{equation*}
\dfrac{dy}{dx} - y = \exp \left \lbrace 2x  \right \rbrace
\end{equation*}

*is*

\begin{equation*}
y = c \exp \left \lbrace x  \right \rbrace + \exp \left \lbrace 2 x  \right \rbrace.
\end{equation*}

Feed $y$ into the equation and you'll find that the expression

\begin{equation*}
c \exp \left \lbrace x  \right \rbrace + 2 \exp \left \lbrace 2 x  \right \rbrace - c \exp \left \lbrace x  \right \rbrace - \exp \left \lbrace 2x  \right \rbrace \stackrel{?}{=} \exp \left \lbrace x  \right \rbrace
\end{equation*}

evaluates to true.

** SOLVED Problem 10.3.3
CLOSED: [2022-10-20 Thu 15:51]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 14:08]--[2022-10-20 Thu 15:09] =>  1:01
:END:

*Solve $x^{2} y^{\prime} + 2 xy -x + 1 = 0$ (where prime denotes differentiation) with $y(1) = 0$.*

This is an inhomogenous first order ODE with variable coefficients whose general form is

\begin{equation*}
\dfrac{dy}{dx} + p(x) y = q(x), \quad \text{with} \quad  p(x) = \dfrac{2}{x}, \quad q(x) = \dfrac{x - 1}{x^{2}},
\end{equation*}

which has the solution

\begin{equation*}
y(x) = \exp \left \lbrace - P(x)  \right \rbrace \left[ \int_{x_0}^{x} q(x^{\prime}) \exp \left \lbrace P(x^{\prime}) \right \rbrace d x^{\prime}  + y_{0} \exp \left \lbrace P(x_{0})  \right \rbrace \right], \quad P(x) = \int_{x_{0}}^{x} p(x) dx.
\end{equation*}

For this case

\begin{equation*}
P(x) = \int_{x_{0}}^{x} \dfrac{2}{x} dx = 2 \ln \left( \dfrac{x}{x_{0}} \right),
\end{equation*}

so that

\begin{equation*}
y(x) = \exp \left \lbrace - 2 \ln \left( \dfrac{x}{x_{0}} \right)  \right \rbrace \left[ \int_{x_0}^{x} \dfrac{x - 1}{x^{2}} \exp \left \lbrace 2 \ln \left( \dfrac{x^{\prime}}{x_{0}} \right) \right \rbrace d x^{\prime}  + c \right].
\end{equation*}

\begin{align*}
I &= \int_{x_0}^{x} \dfrac{x - 1}{x^{2}} \exp \left \lbrace 2 \ln \left( \dfrac{x^{\prime}}{x_{0}} \right) \right \rbrace d x^{\prime} \\
&= - \dfrac{1}{x_{0}^{2}}  \right \rbrace \int_{x_{0}}^{x} \dfrac{x^{\prime}-1}{x^{\prime}^{2}} x^{\prime}^{2}  dx^{\prime} = - \dfrac{1}{x_{0}^{2}} \right \rbrace \int_{x_{0}}^{x} \left(x^{\prime}-1\right) dx^{\prime}.
\end{align*}

#+NAME: solve-maxima
#+begin_src maxima :results raw
  programmode: false;
  b : integrate(x-1,x,x_0,x);
  tex(b);
#+end_src

  
$${{x^2-2\,x}\over{2}}-{{x_{0}^2-2\,x_{0}}\over{2}}$$


\begin{equation*}
I = - \left({{x_{0}^2-2\,x_{0}}\over{2x_{0}^{2}}} - {{x^2-2\,x}\over{2x_{0}^{2}}}\right).
\end{equation*}

Therefore

\begin{equation*}
y(x) = \left[ - \left({{x_{0}^2-2\,x_{0}}\over{2x^{2}}} - {{x^2-2\,x}\over{2x^{2}}}\right) + \dfrac{x_{0}^{2} c}{x^{2}}\right].
\end{equation*}

Demanding $y(1) = 0$

\begin{equation*}
y(x) = - \left({{1-2}\over{2x^{2}}} - {{x^2-2\,x}\over{2x^{2}}}\right) =  \dfrac{1}{2} - \dfrac{1}{x} + \dfrac{1}{2x^{2}}.
\end{equation*}

** SOLVED Problem 10.3.4
CLOSED: [2022-10-20 Thu 16:15]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 15:55]--[2022-10-20 Thu 16:15] =>  0:20
:END:
*Solve $y^{\prime} + y = \left( x + 1 \right)^{2}$ with $y(0) = 0$.*

This is an inhomogenous first order ODE with variable coefficients whose general form is

\begin{equation*}
\dfrac{dy}{dx} + p(x) y = q(x), \quad \text{with} \quad  p(x) = 1, \quad q(x) = \left( x + 1 \right)^{2}.
\end{equation*}

which has the solution
\begin{equation*}
y(x) = \exp \left \lbrace - P(x)  \right \rbrace \left[ \int_{x_0}^{x} q(x^{\prime}) \exp \left \lbrace P(x^{\prime}) \right \rbrace d x^{\prime}  + y_{0} \exp \left \lbrace P(x_{0})  \right \rbrace \right], \quad P(x) = \int_{x_{0}}^{x} p(x) dx.
\end{equation*}

In this case

\begin{align*}
y(x) &= \exp \left \lbrace - x \right \rbrace \left[ \int_{x_{0}}^{x} \left( x + 1 \right)^{2} \exp \left \lbrace x \right \rbrace dx^{\prime} + y_{0} \exp \left \lbrace x_{0}  \right \rbrace \right] \\
\end{align*}

#+NAME: solve-maxima
#+begin_src maxima :results raw
  programmode: false;
  b : integrate(((x+1)^2)*exp(x), x,x_0,x);
  tex(b);
#+end_src

\begin{equation*}
y(x) = \exp \left \lbrace - x  \right \rbrace \left[\left(x^2+1\right) \thinspace \exp \left \lbrace x  \right \rbrace-\left(x_{0}^2+1\right) \thinspace \exp \left \lbrace x_{0}  \right \rbrace + y_{0} \exp \left \lbrace x_{0} \right \rbrace\right] 
\end{equation*}

Demanding $y(0) = 0$

\begin{equation*}
x_{0} = 0, y_{0} = 0,
\end{equation*}

so that

\begin{equation*}
y(x) =  x^{2} + 1  - \exp \left \lbrace - x  \right \rbrace.
\end{equation*}

** SOLVED Problem 10.3.5
CLOSED: [2022-10-20 Thu 17:01]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 16:42]--[2022-10-20 Thu 17:01] =>  0:19
:END:
*Solve $x^{2} y^{\prime} + 2 x y = \sinh x$ with $y(1) = 2$.*

Start with a rewrite

\begin{equation*}
y^{\prime} + \dfrac{2y}{x} = \dfrac{\exp \left \lbrace x  \right \rbrace - \exp \left \lbrace -x  \right \rbrace}{2 x^{2}}.
\end{equation*}

This is an inhomogenous first order ODE with variable coefficients whose general form is

\begin{equation*}
\dfrac{dy}{dx} + p(x) y = q(x), \quad \text{with} \quad  p(x) = \dfrac{2}{x}, \quad q(x) = \dfrac{\exp \left \lbrace x  \right \rbrace - \exp \left \lbrace -x  \right \rbrace}{2 x^{2}}.
\end{equation*}

which has the solution
\begin{equation*}
y(x) = \exp \left \lbrace - P(x)  \right \rbrace \left[ \int_{x_0}^{x} q(x^{\prime}) \exp \left \lbrace P(x^{\prime}) \right \rbrace d x^{\prime}  + y_{0} \exp \left \lbrace P(x_{0})  \right \rbrace \right], \quad P(x) = \int_{x_{0}}^{x} p(x) dx.
\end{equation*}

Now
\begin{equation*}
P(x) = 2 \ln \left( \dfrac{x}{x_{0}} \right) \Longrightarrow \exp \left \lbrace P(x)  \right \rbrace = \left(\dfrac{x}{x_{0}}\right)^{2}, \quad \exp \left \lbrace - P(x)  \right \rbrace = \left( \dfrac{x_{0}}{x} \right)^{2}.
\end{equation*}

The solution reduces to

\begin{equation*}
y(x) = \dfrac{1}{x^{2}} \left[ \int_{x_{0}}^{x} \dfrac{\exp \left \lbrace x^{\prime}  \right \rbrace - \exp \left \lbrace - x^{\prime}  \right \rbrace}{2} dx^{\prime} + y_{0} \right] = \dfrac{\cosh x - \cosh x_{0} + y_{0}}{x^{2}}.
\end{equation*}

#+begin_src gnuplot :exports code :file 10.3.5.png
  reset
  set terminal pngcairo enhanced font "arial,10" fontscale 1.0 size 500, 300
  set output '10.3.5.png'
  set title "The hyperbolic functions"

  set xlabel "x"
  set xrange [-2:2]

  set ylabel "f(x)"

  sinh(x) = sinh(x)
  cosh(x) = cosh(x)

  plot sinh(x), cosh(x)
#+end_src 

#+RESULTS:
[[file:10.3.5.png]]

Demanding $y(1) = 2$

\begin{equation*}
y_{0} = 2, x_{0} = 1,
\end{equation*}

so that

\begin{equation*}
y(x) = \dfrac{\cosh x - \cosh 1 + 2}{x^{2}}.
\end{equation*}

** SOLVED Problem 10.3.6
CLOSED: [2022-10-20 Thu 17:37]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 17:02]--[2022-10-20 Thu 17:37] =>  0:35
:END:
*Solve $y^{\prime} + \dfrac{y}{1 - x} + 2x - x^{2} = 0$.*

With the rewrite

\begin{equation*}
y^{\prime} + \dfrac{y}{1 - x} = x^{2} - 2x
\end{equation*}

we identify this as an inhomogenous first order ODE with variable coefficients whose general form is

\begin{equation*}
\dfrac{dy}{dx} + p(x) y = q(x), \quad \text{with} \quad  p(x) = \dfrac{1}{1 - x}, \quad q(x) = x^{2} - 2x,
\end{equation*}

and solution is

\begin{equation*}
y(x) = \exp \left \lbrace - P(x)  \right \rbrace \left[ \int_{x_0}^{x} q(x^{\prime}) \exp \left \lbrace P(x^{\prime})  \right \rbrace d x^{\prime} + y_{0} \exp \left \lbrace P(x_{0})  \right \rbrace \right], \quad P(x) = \int_{x_{0}}^{x} p(x) dx.
\end{equation*}

In this case

\begin{equation*}
P(x) = \int_{x_{0}}^{x} \dfrac{dx^{\prime}}{1 - x^{\prime}} = \ln \left( 1 - x_{0} \right) - \ln \left( 1 - x \right) \Longrightarrow \exp \left \lbrace - P(x)  \right \rbrace = \dfrac{1 - x}{1 - x_{0}}, \quad \exp \left \lbrace P(x)  \right \rbrace = \dfrac{1 - x_{0}}{1 - x}.
\end{equation*}

so that the solution reduces to

\begin{align*}
y(x) &= \left(1 - x\right) \left[ \int_{x_{0}}^{x} \dfrac{\left( x^{\prime}^{2} - 2x^{\prime} \right)}{\left(1-x^{\prime}\right)} dx^{\prime} + y_{0} \left(\dfrac{1 - x}{1 - x_{0}}\right) \right] \\
&= \left( 1 - x \right) \left[ \int_{x_{0}}^{x} \dfrac{\left( x^{\prime}^{2} - 2x^{\prime} \right)}{\left(1-x^{\prime}\right)} dx^{\prime} + y_{0} \left(\dfrac{1 - x}{1 - x_{0}}\right) \right] \\
&= \left( 1 - x \right) \left[ \left[ \ln \left \lvert x - 1  \right \rvert - \dfrac{x^{2} - 2 x}{2} \right]_{x_{0}}^{x} + y_{0} \left(\dfrac{1 - x}{1 - x_{0}}\right) \right] \\
\end{align*}

Just absorb all those constants into a single mammoth $C$ to rewrite the final solution

\begin{equation*}
y(x) = \dfrac{\left( 1 - x \right)}{2} \left[ 2 \ln \left \lvert x - 1 \right \rvert - x^{2} + 2x + C \right].
\end{equation*}

** SOLVED Problem 10.3.7
CLOSED: [2022-10-20 Thu 17:44]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 17:41]--[2022-10-20 Thu 17:44] =>  0:03
:END:

*Solve $y^{\prime} + \dfrac{y}{1 - x} + x - x^{2} = 0$.*

In this problem we retrace the steps of the previous problem and pick up at the point of divergence that is

\begin{align*}
y(x) &= \left(1 - x\right) \left[ \int_{x_{0}}^{x} \dfrac{\left( x^{\prime}^{2} - x^{\prime} \right)}{\left(1-x^{\prime}\right)} dx^{\prime} + y_{0} \left(\dfrac{1 - x}{1 - x_{0}}\right) \right] \\
&= \left( 1 - x \right) \left[ \int_{x_{0}}^{x} x^{\prime} dx^{\prime} + y_{0} \left(\dfrac{1 - x}{1 - x_{0}}\right) \right] \\
&= \left( 1 - x \right) \left[ \left[ \dfrac{x^{2}}{2} \right]_{x_{0}}^{x} + y_{0} \left(\dfrac{1 - x}{1 - x_{0}}\right) \right].
\end{align*}

Tidy it up

\begin{equation*}
y(x) = \left( 1 - x \right) \left[ \dfrac{x^{2}}{2} + C \right].
\end{equation*}

** SOLVED Problem 10.3.8
CLOSED: [2022-10-20 Thu 18:09]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 17:44]--[2022-10-20 Thu 18:09] =>  0:25
:END:

*Solve $(1 + x^{2}) y^{\prime} = 1 + xy$.*

Creature of habit, obtain that rewrite

\begin{equation*}
y^{\prime} - \dfrac{xy}{1 + x^{2}} = \dfrac{1}{1 + x^{2}}.
\end{equation*}

The solution is
\begin{equation*}
y(x) = \exp \left \lbrace - P(x)  \right \rbrace \left[ \int^{x} \dfrac{1}{1 + x^{\prime}^{2}} \exp \left \lbrace P(x^{\prime}) \right \rbrace dx^{\prime} + c \right], \quad P(x) = - \int^{x} \dfrac{x}{1 + x^{2}} = - \dfrac{\ln \left( x^{2} + 1 \right)}{2}
\end{equation*}

Substitute $P(x)$ in the expression for $y(x)$ and let off

\begin{align*}
y(x) &= \sqrt{1 + x^{2}} \left[ \int^{x} \dfrac{1}{\left( 1 + x^{\prime}^{2} \right)^{3/2}} dx^{\prime} + C \right] = \sqrt{1 + x^{2}} \left[ \dfrac{x}{\sqrt{1 + x^{2}}} + C \right] = x + C \sqrt{1 + x^{2}}.
\end{align*}

** SOLVED Problem 10.3.9
CLOSED: [2022-10-20 Thu 19:07]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 18:10]--[2022-10-20 Thu 19:07] =>  0:57
:END:
*Bernoulli's equation. We will make an exception and handle the following nonlinear equation since it can be made linear by a trick:*

\begin{equation*}
y^{\prime} + p(x) y = q(x) y^{m}.
\end{equation*}

*Show that if we set $v(x) = y^{1 - m}$, $v$ obeys a linear equation*

\begin{equation*}
v^{\prime} + \left( 1 - m \right) p(x) v = \left( 1 - m \right) q(x).
\end{equation*}

Solve $v(x) = y^{1-m}$ for $y$ to obtain

\begin{equation*}
y(x) = v^{1/(1-m)}(x),
\end{equation*}

and feed it as an ansatz (ok, at this point, this my favourite word of the month) into the ODE to obtain

\begin{equation*}
\left(\dfrac{1}{1-m}\right) v^{m/(1-m)}(x) v^{\prime}(x) + p(x) v^{1/(1-m)}(x) = q(x) v^{m/(1-m)} (x),
\end{equation*}

which upon dividing by $v^{m/(1-m)}/(1-m)$ yields the ask:

\begin{equation*}
v^{\prime} (x) + \left( 1 - m \right) p(x) v(x) = \left( 1 - m \right) q(x).
\end{equation*}

*Use this trick to solve*

\begin{equation*}
a: y^{\prime} + xy = xy^{2}
\end{equation*}

Our ansatz is

\begin{equation*}
y(x) = \dfrac{1}{v(x)},
\end{equation*}

which upon feeding into the ODE yields

\begin{equation*}
v^{\prime} - xv = - x.
\end{equation*}

Separating those variables

\begin{equation*}
\dfrac{dv}{v-1} = x \thinspace dx,
\end{equation*}

and integrating to obtain

\begin{equation*}
\ln \left( v - 1 \right) = \dfrac{x^{2}}{2} + C^{\prime}.
\end{equation*}

Solving for $v$ yields

\begin{equation*}
v(x) = 1 + C \exp \left \lbrace \dfrac{x^{2}}{2} \right \rbrace
\end{equation*}

where $C = \exp \left \lbrace C^{\prime}  \right \rbrace$ so that

\begin{equation*}
y(x) = \dfrac{1}{v(x)} = \dfrac{1}{1 + C \exp \left \lbrace x^{2}/2  \right \rbrace}.
\end{equation*}

\begin{equation*}
b: 3 x y^{\prime} + y + x^{2} y^{4} = 0.
\end{equation*}

Recast as

\begin{equation*}
y^{\prime} + y / 3x = - x y^{4}/3.
\end{equation*}

Our ansatz is

\begin{equation*}
y(x) = \dfrac{1}{v^{1/3}(x)},
\end{equation*}

which upon feeding into the ODE yields

\begin{equation*}
- \dfrac{v^{-4/3}}{3} v^{\prime} + \dfrac{1}{3 x v^{1/3}} = - \dfrac{x}{3 v^{4/3}}.
\end{equation*}

Tidy up by multiplying $- 3 v^{4/3}$ throughout

\begin{equation*}
v^{\prime} - \dfrac{v}{x} = x.
\end{equation*}

Not so lucky. No matter, forging forth

\begin{equation*}
v(x) = \exp \left \lbrace - P(x)  \right \rbrace \left[ \int^{x} q(x^{\prime}) \exp \left \lbrace P(x^{\prime})  \right \rbrace dx^{\prime} + C \right], \quad P(x) = - \int^{x} \dfrac{1}{x^{\prime}} dx^{\prime} = -\ln \left \lvert x  \right \rvert, \quad q(x) = x,
\end{equation*}

so that
\begin{equation*}
v(x) = x \left[ \int^{x} x^{\prime} \dfrac{1}{\left \lvert x^{\prime}  \right \rvert} dx^{\prime} + C \right].
\end{equation*}

We can always choose the lower limit as a positive value by absorbing the leftovers into $C$; allowing us to drop the $\left \lvert \cdot  \right \rvert$ and integrate with impunity. Doing so yields

\begin{equation*}
v(x) = x^{2} + C x,
\end{equation*}

so that

\begin{equation*}
y(x) = \dfrac{1}{\sqrt[3]{x^{2} + Cx}}.
\end{equation*}
** SOLVED Problem 10.4.1
CLOSED: [2022-10-20 Thu 20:29]
:LOGBOOK:
CLOCK: [2022-10-20 Thu 20:48]--[2022-10-20 Thu 20:48] =>  0:00
CLOCK: [2022-10-20 Thu 19:58]--[2022-10-20 Thu 20:29] =>  0:31
:END:
*Solve the equation $y^{\prime} + a y = 0$ using this method.*

Behold the /method of Frobenius/: "In this method we *assume* the existence of a power series about the origin and determine all the coefficients of the series using the ODE" (pp. 318).

Our ansatz is

\begin{equation*}
y(x) = \sum_{n=0}^{\infty} c_{n} x^{n}.
\end{equation*}

Feed it into the ODE to obtain

\begin{equation*}
\sum_{m=1}^{\infty} m c_{m} x^{m-1} + a \sum_{n=0}^{\infty} c_{n} x^{n} = 0.
\end{equation*}

Re-index to unroll both sums with a common index:

\begin{equation*}
\sum_{n=0}^{\infty} \left[(n+1) c_{n+1} + a c_{n} \right]  x^{n} = 0.
\end{equation*}

Imposing the condition of vanishing coefficients yields the recursion relation

\begin{equation*}
c_{n+1} = - \left( \dfrac{a}{n + 1} \right) \thinspace c_{n},
\end{equation*}

which we set off by choosing $c_{0} = 1$:

\begin{equation*}
c_{n+1} = \left( -1 \right)^{n+1} \dfrac{a^{n+1}}{(n+1)!}.
\end{equation*}

Re-index again:

\begin{equation*}
c_{n} = \left( -1 \right)^{n} \dfrac{a^{n}}{n!},
\end{equation*}

and substitute in the original ansatz to obtain

\begin{equation*}
y(x) = \sum_{n=0}^{\infty}  \dfrac{ \left( - a x \right)^{n}}{n!} = \exp \left \lbrace - a x  \right \rbrace.
\end{equation*}

Indeed we have a solution

\begin{equation*}
y^{\prime} + a y = - a \exp \left \lbrace - a x  \right \rbrace + a \exp \left \lbrace - a x  \right \rbrace = 0.
\end{equation*}

** SOLVED Problem 10.4.2
CLOSED: [2022-10-20 Thu 20:41]
*Show that*

\begin{equation*}
c_{2} = - \epsilon c_{0}
\end{equation*}

\begin{equation*}
c_{4} = \dfrac{c_{0} - 2 \epsilon c_{2}}{12} = \dfrac{1 + 2 \epsilon^{2}}{12} c_{0}.
\end{equation*}

The three term recursion relation was found as

\begin{equation*}
\left[ \left( n + 2 \right) \left( n + 1 \right) c_{n+2} - c_{n-2} + 2 \epsilon c_{n} \right] = 0
\end{equation*}

With $n = 0$ and $n = 2$ we obtain

\begin{equation*}
2 c_{2} + 2 \epsilon c_{0} = 0, \quad \left[ 12 c_{4} - c_{0} + 2 \epsilon c_{2} \right] = 0.
\end{equation*}

Solving the first for $c_{2}$ we obtain

\begin{equation*}
c_{2} = - \epsilon c_{0},
\end{equation*}

and feeding this in the second we obtain

\begin{equation*}
c_{4} = \dfrac{c_{0} - 2 \epsilon c_{2}}{12} = \dfrac{1 + 2 \epsilon^{2}}{12} c_{0}.
\end{equation*}

** SOLVED Problem 10.4.3
CLOSED: [2022-10-22 Sat 19:23]
:LOGBOOK:
CLOCK: [2022-10-22 Sat 15:52]--[2022-10-22 Sat 19:23] =>  3:31
CLOCK: [2022-10-22 Sat 14:46]--[2022-10-22 Sat 15:40] =>  0:54
CLOCK: [2022-10-20 Thu 21:10]--[2022-10-20 Thu 23:24] =>  2:14
:END:
*Show that the first four Hermite polynomials are*

\begin{equation*}
H_{0} = 1, \quad H_{1} = 2y, \quad H_{2} = -2 \left( 1 - 2y^{2} \right), \quad H_{3} = - 12 \left( y - \dfrac{2}{3} y^{3} \right),
\end{equation*}

*where the overall normalization (choice of $a_{0}$ or $a_{1}$) is as per some convention we need not get into. To compare your answers to the above, choose the starting coefficients to agree with the above. Show that*

\begin{equation*}
\int_{-\infty}^{\infty} \exp \left \lbrace - y^{2}  \right \rbrace H_{n} \left( y \right) H_{m} \left( y \right) dy = \delta_{nm} \left( \sqrt{\pi} 2^{n} n! \right)
\end{equation*}
 
*for the cases $m, n \leq 2$. Notice that the Hermite polynomials are not themselves orthogonal or even normalizable, we need the weight function $\exp \left \lbrace - y^2  \right \rbrace$ in the integration measure. We understand this as follows: the exponential factor converts u's to $\psi$'s, which are the eigenfunctions of a hermitian operator (hermitian with respect to normalizable functions that vanished at infinity) and hence orthogonal for different eigenvalues.*

On feeding the ansatz $\psi = u(y) \exp \left \lbrace - y^{2} /2  \right \rbrace$ into the ODE

\begin{equation*}
\left( D^{2} - y^{2} + 2 \epsilon \right) \psi = 0
\end{equation*}

\begin{equation*}
\exp \left \lbrace - \dfrac{y^{2}}{2}  \right \rbrace u^{\prime \prime}
- 2 u^{\prime} y \exp \left \lbrace - \dfrac{y^{2}}{2}  \right \rbrace
+ y^{2} u \exp \left \lbrace - \dfrac{y^{2}}{2}  \right \rbrace
+2 \epsilon \exp \left \lbrace - \dfrac{y^2}{2}  \right \rbrace u
- u \exp \left \lbrace - \dfrac{y^2}{2} \right \rbrace
- y^{2} u \exp \left \lbrace - \dfrac{y^2}{2}  \right \rbrace = 0
\end{equation*}

and tidying up,

\begin{equation*}
u^{\prime \prime} - 2 y u^{\prime} + \left(2 \epsilon - 1 \right) u = 0.
\end{equation*}

Now feed the ansatz $u(y) = \sum_{n=0}^{\infty} c_{n} y^{n}$ in the ODE above to obtain

\begin{equation*}
\sum_{l=2}^{\infty} l \left( l - 1 \right) c_{l} y^{l-2} - 2 y \sum_{m=1}^{\infty} m c_{m} y^{m-1} + \left(2 \epsilon - 1\right) \sum_{n=0}^{\infty} c_{n} y^{n} = 0
\end{equation*}

Appropriate re-indexing to unroll all the three sums with a common dummy $n$ from $n = 0$

\begin{equation*}
\left( \sum_{n=0}^{\infty} \left( n + 2 \right) \left( n + 1 \right) c_{n + 2} \thinspace - 2 \sum_{n=0}^{\infty} n  c_{n} + \left( 2 \epsilon - 1 \right)  \sum_{n=0}^{\infty} c_{n} \right) y^{n} = 0,
\end{equation*}

furnishes the recurrence relation

\begin{equation*}
c_{n + 2} = \dfrac{2 n + 1 - 2 \epsilon}{ \left( n + 2 \right) \left( n + 1 \right)} \thinspace c_{n}.
\end{equation*}

For physical reasons, we need to be able to truncate the series. To be able to truncate the series we propose that $2 \epsilon = 2 m + 1$ and write

\begin{equation*}
c_{n + 2} = \dfrac{2\left( n - m \right)}{ \left( n + 2 \right) \left( n + 1 \right)} \thinspace c_{n}.
\end{equation*}

To lay birth the specified Hermite polynomials set $c_{1} = 0$ $c_{0} = 1$,for $H_{0}$, $c_{1}= 0, \thinspace c_{0} = -2$, $m=2$ for $H_{2}$, $c_{0}=0$ $c_{1} = 2$  for $H_{1}$, and $c_{0}=0$ $c_{1} = - 12$ and $m = 3$ for $H_{3}$.

$H_{0} = c_{0} = 1$,

$H_{1} = c_{1} y = 2 y$,

$H_{2} = c_{0} + c_{2} y^{2} = - 2 + 4 y^{2} = -2 \left( 1 - 2 y^{2} \right)$,
$H_{3} = c_{1} y + c_{3} y^{3} = -12 y + \dfrac{24}{3} y^{3} = - 12 \left( y - \dfrac{2}{3} y^{3}\right)$.

Next up is to show

\begin{equation*}
\int_{-\infty}^{\infty} \exp \left \lbrace - y^{2}  \right \rbrace H_{n} \left( y \right) H_{m} \left( y \right) dy = \delta_{nm} \left( \sqrt{\pi} 2^{n} n! \right)
\end{equation*}

for $0 \leq m, n \leq 2$. No problem, I'll show it for $m,n \geq 0$; here it is, here you go.

/First/, notice that for $n \geq 1$

\begin{equation*}
H_{n}^{\prime} = 2 n \thinspace H_{n-1}.
\end{equation*}

/Second/, observe that

\begin{equation*}
H_{n} = 2y H_{n-1} - H^{\prime}_{n-1},
\end{equation*}

so that

\begin{equation*}
H_{n} = 2y \thinspace H_{n-1} - 2n \thinspace H_{n-2}.
\end{equation*}

/Third/, recognize that

\begin{equation*}
\dfrac{d}{dy} \left( H_{n} \exp \left \lbrace -y^{2}  \right \rbrace \right) = - \exp \left \lbrace -y^{2}  \right \rbrace \left(2 y H_{n} - 2n H_{n-1}  \right) = - H_{n+1} \exp \left \lbrace - y^{2}  \right \rbrace,
\end{equation*}

that is

\begin{equation*}
H_{n} \exp \left \lbrace - y^{2}  \right \rbrace = - \dfrac{d}{dy}  \left( H_{n-1} \exp \left \lbrace - y^{2}  \right \rbrace \right).
\end{equation*}

/Fourth/, appreciate the fact that the integral is ripe for integration by parts: the integrand, and therefore the surface term, vanishes at the extremities of the real axis - it's fate sealed by the falling exponential. Life is good. Use $H_{n} \exp \left \lbrace - y^{2}  \right \rbrace = - \frac{d}{dy}  \left( H_{n-1} \exp \left \lbrace - y^{2}  \right \rbrace \right)$ to write

\begin{align*}
I(m,n) &= \int_{-\infty}^{\infty} H_{m} H_{n} \exp \left \lbrace - y^{2}
\right \rbrace dy \\
&= 2 \thinspace m \thinspace\int_{-\infty}^{\infty} H_{m-1} H_{n-1} \exp \left \lbrace - y^{2}
\right \rbrace dy,
\end{align*}

which yields a recursion for the integral after a trivial identification

\begin{equation*}
I(m,n) = \left( 2m \right) I (m-1, n-1).
\end{equation*}

Suppose that $n > m$. Then stepping down the above recursion for a total of $m$ steps, we have

\begin{equation*}
I(m,n) = \left( 2^{m} m! \right) I (0, \thinspace n-m).
\end{equation*}

$I(0, n-m)$ is non-vanishing only when $n-m$ is even as is easily seen by writing

\begin{align*}
I(0, n - m) &= \int_{-\infty}^{\infty} H_{0} H_{n-m} \exp \left \lbrace - y^{2}  \right \rbrace dy \\
&= \int_{-\infty}^{\infty} H_{n-m} \exp \left \lbrace - y^{2}  \right \rbrace dy,
\end{align*}

so make that supposition. $n- m$ is even. We can then write

\begin{align*}
I(0, n - m) &= 2 \int_{0}^{\infty} H_{n-m} \exp \left \lbrace - y^{2}  \right \rbrace dy \\
&= - \left[2 H_{n-m-1} \exp \left \lbrace - y^{2}  \right \rbrace \Big \vert_{y=\infty} - 2 H_{n-m-1} \exp \left \lbrace - y^{2}  \right \rbrace \Big \vert_{y=0}\right] \\
&= 0.
\end{align*}

Now suppose that $m > n$. Then stepping down the above recursion for a total of $n$ steps, we have

\begin{equation*}
I(m,n) = \left( 2^{n} n! \right) I (m - n, 0).
\end{equation*}

$I(m-n, 0)$ is non-vanishing only when $m-n$ is even as easily seen by writing

\begin{align*}
I(m-n, 0) &= \int_{-\infty}^{\infty} H_{m-n} H_{0} \exp \left \lbrace - y^{2}  \right \rbrace dy \\
&= \int_{-\infty}^{\infty} H_{m-n} \exp \left \lbrace - y^{2}  \right \rbrace dy.
\end{align*}

so make that supposition. $m - n$ is even. We can then write

\begin{align*}
I(m-n, 0) &= 2 \int_{0}^{\infty} H_{m-n} \exp \left \lbrace - y^{2}  \right \rbrace dy \\
&= - \left[2 H_{m-n-1} \exp \left \lbrace - y^{2}  \right \rbrace \Big \vert_{y=\infty} - 2 H_{m-n-1} \exp \left \lbrace - y^{2}  \right \rbrace \Big \vert_{y=0}\right] \\
&= 0.
\end{align*}

Now suppose that $m = n$. Go through the same rigmarole, use the Gaussian Integral result and you'll find

\begin{equation*}
I(0,0) = \sqrt{\pi}.
\end{equation*}

Using the Kronecker for embellishment and compactification yields the ask

\begin{equation*}
\int_{-\infty}^{\infty} \exp \left \lbrace - y^{2}  \right \rbrace H_{m} \left( y \right) H_{n} \left( y \right) dy = \delta_{mn} \left( \sqrt{\pi} 2^{n} n! \right).
\end{equation*}

** TOSOLVE Intermezzo 1
:LOGBOOK:
CLOCK: [2022-10-23 Sun 01:31]--[2022-10-23 Sun 03:54] =>  2:23
:END:
I can't believe this energy! Let me prove a useful formula relevant to the broader context of current discussion, the Rodrigue's Formula which emerges from the statement:

Let $\{P_{n}(x)\}_{n=0}^{\infty }}$ be a sequence of orthogonal polynomials satisfying the orthogonality condition

\begin{equation*}
\int _{a}^{b}P_{m}(x)P_{n}(x)w(x)\,dx=K_{n}\delta _{m,n}.
\end{equation*}

where $w(x)$ is a suitable weight function, $K_{n}$ is a constant depending on $n$, and $\delta _{m,n}$ is the Kronecker delta. If the weight function $w(x)$ satisfies the following differential equation (called Pearson's differential equation),

\begin{equation*}
\frac {w'(x)}{w(x)}}={\frac {A(x)}{B(x)}},}
\end{equation*}

where $A(x)$ is a polynomial with degree at most 1 and $B(x)$ is a polynomial with degree at most 2 and, further, the limits

\begin{equation*}
\lim _{x\to a}w(x)B(x)=0,\qquad \lim _{x\to b}w(x)B(x)=0
\end{equation*}

then, it can be shown that $P_n(x)$ satisfies a recurrence relation of the form,

\begin{equation*}
P_{n}(x)={\frac {c_{n}}{w(x)}}{\frac {d^{n}}{dx^{n}}}\left[B(x)^{n}w(x)\right]
\end{equation*}

for some constants $c_{n}$. This relation is called Rodrigues' formula.

Start with a rewrite of Pearson's differential equation

\begin{equation*}
B^{n} w = w^{\prime} \left(\dfrac{B^{n+1}}{A}\right).
\end{equation*}

We will prove this and then we will write the Rodrigue's formula for the Hermite polynomials.

The orthogonality condition for $m = n$ reduces to

\begin{equation*}
\int _{a}^{b}P_{n}^{2}(x)w(x)\,dx=K_{n}.
\end{equation*}

Solve Pearson's differential equation to obtain

\begin{equation*}
w(x) = C \exp \left \lbrace \int^{x} \dfrac{A(x)}{B(x)}  \right \rbrace.
\end{equation*}

The orthogonality condition for $m = n$ reduces to

\begin{equation*}
\int _{a}^{b}P_{n}^{2}(x)w(x)\,dx=K_{n},
\end{equation*}

and on solving the Pearson's differential equation for $w$ yields

\begin{equation*}
\int _{a}^{b}P_{n}^{2}(x) \exp \left \lbrace \int^{x} \dfrac{A(x)}{B(x)}  \right \rbrace \,dx= \dfrac{K_{n}}{C} \equiv c_{n}.
\end{equation*}

Clearly

\begin{align*}
\dfrac{K_{n}}{C} &=  \int _{a}^{b}P_{n}^{2} \thinspace w \thinspace dx = \int _{a}^{b} \dfrac{B P_{n}^{2}}{A} \thinspace w^{\prime} \thinspace dx.
\end{align*}

Integrate by parts the right hand side to obtain

\begin{align*}
\dfrac{K_{n}}{C} &=  \dfrac{B w P_{n}^{2}}{A} \Bigg \vert_{a}^{b}  - \int _{a}^{b} \dfrac{d}{dx} \left(\dfrac{B P_{n}^{2}}{A}\right) \thinspace w \thinspace dx \\
&= \dfrac{B w P_{n}^{2}}{A} \Bigg \vert_{a}^{b} - \int _{a}^{b} \left(\dfrac{B}{A}\right) \dfrac{d}{dx} \left(\dfrac{B P_{n}^{2}}{A}\right) \thinspace w^{\prime} \thinspace dx \\
&= \dfrac{B w P_{n}^{2}}{A} \Bigg \vert_{a}^{b} - \int _{a}^{b} \left(\dfrac{B}{A}\right) B \dfrac{d}{dx} \left(\dfrac{P_{n}^{2}}{A}\right) \thinspace w^{\prime} \thinspace dx  - \int _{a}^{b} \left(\dfrac{B}{A}\right) B^{\prime} \left(\dfrac{P_{n}^{2}}{A}\right) \thinspace w^{\prime} \thinspace dx \\
&= - \int _{a}^{b} \left(\dfrac{P_{n}^{2}}{A}\right) \thinspace B^{\prime} \thinspace w \thinspace dx = - \int _{a}^{b} \left(\dfrac{P_{n}^{2}}{A}\right) \thinspace \left( B w \right)^{\prime} \thinspace dx  + \int _{a}^{b} P_{n}^{2} w \thinspace dx \\
&= \int _{a}^{b}P_{n}^{2} \thinspace w \thinspace dx.
\end{align*}

The condition is satisfied only if

\begin{equation*}
\int _{a}^{b} \left(\dfrac{P_{n}^{2}}{A}\right) \thinspace \left( B w \right)^{\prime} \thinspace dx = 0,
\end{equation*}

i.e.,

\begin{equation*}
\int _{a}^{b} P_{n}^{2}\thinspace \left( \dfrac{B}{A} \right) w^{\prime} dx = \int _{a}^{b}P_{n}^{2} \thinspace w \thinspace dx = \dfrac{K_n}{C} = - \int _{a}^{b} \left( \dfrac{B^{\prime}}{A} \right) P_{n}^{2} w \thinspace dx.
\end{equation*}
** TOSOLVE Problem 10.4.4
:LOGBOOK:
CLOCK: [2022-10-22 Sat 23:02]--[2022-10-23 Sun 01:16] =>  2:14
CLOCK: [2022-10-22 Sat 20:07]--[2022-10-22 Sat 20:39] =>  0:32
CLOCK: [2022-10-22 Sat 19:29]--[2022-10-22 Sat 20:03] =>  0:34
:END:
*Consider the Legendre Equation*

$\left(1 - x^{2}\right) y^{\prime \prime} - 2 x y^{\prime} + l (l + 1) y = 0$.

*Argue that the power series method will lead to a two term recursion relation and find the latter. Show that if $l$ is an even (odd) integer, the even (odd) series will reduce to polynomials, called $P_{l}$, the Legendre polynomials of order $l$. Show that*

\begin{equation*}
P_{0} = 1, \quad P_{1} = x, \quad P_{2} = \dfrac{1}{2} \left( 3 x^{2} - 1 \right), \quad P_{3} = \dfrac{1}{2} \left( 5 x^{3} - 3x \right).
\end{equation*}

*(The overall scale of these functions is not defined by the equation, but by convention as above.) Pick any two of the above and show that they are orthogonal over the interval $-1 \leq x \leq 1$.*

Feed the ansatz

\begin{equation*}
y = \sum_{n=0}^{\infty} c_{n} x^{n}
\end{equation*}

in the Legendre Equation to obtain

\begin{align*}
0 &= \left( 1 - x^{2} \right) \sum_{p=2}^{\infty} p \left( p -1 \right) c_{p} x^{p-2} - 2 x \sum_{m=1}^{\infty} m c_{m} x^{m-1} + l \left(l + 1  \right) \sum_{n=0}^{\infty} c_{n} x^{n} \\
&= \sum_{p=2}^{\infty} p \left( p -1 \right) \left[c_{p} x^{p-2} - c_{p} x^{p} \right] - 2 \sum_{m=1}^{\infty} m c_{m} x^{m} + l \left(l + 1  \right) \sum_{n=0}^{\infty} c_{n} x^{n} \\
\end{align*}

Re-index

\begin{equation*}
\sum_{n=0}^{\infty} \left[\left( n + 2 \right) \left( n + 1 \right) \left[c_{n+2} x^{n} - c_{n+2} x^{n+2} \right] - 2 n c_{n} x^{n} + l \left(l + 1  \right) c_{n} x^{n}\right]  = 0
\end{equation*}

then simplify to obtain

\begin{equation*}
\sum_{n=0}^{\infty} \left[ \left( l \left( l + 1 \right) - 2 n \right) c_{n}  + \left( n + 2 \right) \left( n + 1 \right) c_{n+2} \right] x^{n} = \sum_{m=0}^{\infty} \left[ \left( m+2 \right) \left( m + 1 \right) \right] c_{m+2} x^{m+2}.
\end{equation*}

Matching coefficients

\begin{equation*}
\left[ \left( l \left( l + 1 \right) - 2 n \right) c_{n}  + \left( n + 2 \right) \left( n + 1 \right) c_{n+2} \right] = n \left( n - 1 \right) c_{n},
\end{equation*}

and simplification furnishes the sought after recurrence

\begin{equation*}
c_{n+2} = \dfrac{n(n+1) - l(l+1)}{(n+2)(n+1)} \thinspace c_{n},
\end{equation*}

The series truncates itself when $n = l$. To lay birth the specified Legendre polynomials set $c_{1} = 0$, $c_{0} = 1$ for $P_{0}$, $c_{1}= 0, \thinspace c_{0} = -1/2$ for $P_{2}$, $c_{1}=1$ $c_{0} = 0$  for $P_{1}$, and $c_{1}=-3/2$ $c_{0} = 0$ for $P_{3}$:

$P_{0} = 1$,

$P_{1} = x$,

$P_{2} = \dfrac{1}{2} \left( 3 x^{2} - 1 \right)$,

$P_{3} = \dfrac{1}{2} \left( 5 x^{3} - 3x \right)$.

Once again, since I'm feeling good, I'll show the orthogonality of $P_{l}$ and $P_{l^{\prime}}$ in the interval $-1 \leq x \leq 1$ for $l, l^{\prime} \geq 0$.

/Second/, recognize that the Legendre polynomials themselves satisfy a three term recurrence

\begin{equation*}
\left( l + 1 \right) \left(P_{l+1} - x P_{l}\right) = - l \left( P_{l-1} - x P_{l} \right),
\end{equation*}

called Bonnet's Recursion formula. This may be verified by induction.

/Second/, differentiating Bonnet's Recurrence and solving for $P_{l}$ yields

\begin{equation*}
\left( 2l + 1 \right) P_{l} = \dfrac{d}{dx} \left( P_{l+1} - P_{l} \right).
\end{equation*}

/Fourth/, pin down the object of interest as the integral

\begin{equation*}
I(l, l^{\prime}) = \int_{-1}^{+1} d x P_{l} \left( x \right) P_{l^{\prime}} \left( x \right),
\end{equation*}

The /orthogonality relation/ for the Legendre polynomial is

\begin{equation*}
\int_{-1}^{+1} d x P_{l} \left( x \right) P_{l^{\prime}} \left( x \right) = \left(\dfrac{2}{2l + 1}\right) \delta_{l l^{\prime}}.
\end{equation*}

** SOLVED Problem 10.4.5
CLOSED: [2022-10-23 Sun 15:57]
:LOGBOOK:
CLOCK: [2022-10-23 Sun 15:43]--[2022-10-23 Sun 15:57] =>  0:14
:END:
*The functions $1, x, x^{2}, \dotso$ are linearly independent - there is no way, for example, to express $x^{3}$ in terms of sums of other powers. Use the Gram-Schmidt procedure to extract from this set the first four Legendre polynomials (up to normalization) known to be orthonormal in the interval $-1 \leq x \leq 1$.*

We don't gotta do nothing to $P_{0} = 1$. Let it be. Start with $x$. It's component along $1$ is

\begin{equation*}
\int_{-1}^{+1} 1 \cdot x \thinspace dx = \left[\dfrac{x^{2}}{2}\right]_{-1}^{1} = 0,
\end{equation*}

so leave it alone too. $P_{1} = x$. Now for $x^{2}$

\begin{equation*}
\int_{-1}^{+1} 1 \cdot x^{2} \thinspace dx = \left[\dfrac{x^{3}}{3}\right]_{-1}^{1} = \dfrac{2}{3},
\end{equation*}

and

\begin{equation*}
\int_{-1}^{+1} x \cdot x^{2} \thinspace dx = \left[\dfrac{x^{4}}{4}\right]_{-1}^{1} = 0,
\end{equation*}

so that $P_{2}=x^{2} - \dfrac{2}{3}$. Now for $x^{3}$

\begin{equation*}
\int_{-1}^{+1} 1 \cdot x^{3} \thinspace dx = \left[\dfrac{x^{4}}{4}\right]_{-1}^{1} = 0,
\end{equation*}

\begin{equation*}

\end{equation*}
\begin{equation*}
\int_{-1}^{+1} x \cdot x^{3} \thinspace dx = \left[\dfrac{x^{5}}{5}\right]_{-1}^{1} = \dfrac{2}{5},
\end{equation*}

and

\begin{equation*}
\int_{-1}^{+1} x^{2} \cdot x^{3} \thinspace dx = \left[\dfrac{x^{6}}{6}\right]_{-1}^{1} = 0,
\end{equation*}

so that

\begin{equation*}
P_{3} = x^{3} - \dfrac{2}{5}x.
\end{equation*}

This is of course upto normalization.
** SOLVED Problem 10.4.6
CLOSED: [2022-10-23 Sun 21:43]
:LOGBOOK:
CLOCK: [2022-10-23 Sun 16:41]--[2022-10-23 Sun 21:43] =>  5:02
:END:
*Write the general solution out to $c_{5}$.*

Of concern is the differential equation

\begin{equation*}
x^{2} y^{\prime \prime} + x y^{\prime} + \left( 4 x^{2} - 3 \right)y = 0,
\end{equation*}

for which we use a /generalized power series/ of the form

\begin{equation*}
y(x) = x^{s} \sum_{n=0}^{\infty} c_{n} x^{n},
\end{equation*}

as the ansatz. Why? Because if we feed in $y(x) = \sum_{n=0}^{\infty} c_{n} x^{n}$ we have

\begin{equation*}
 \sum_{k=2}^{\infty} k \left( k - 1 \right) c_{k} x^{k} + \sum_{l=1}^{\infty} l c_{l} x^{l} + 4  \sum_{m=0}^{\infty} c_{m} x^{m+2} - 3 \sum_{n=0}^{\infty} c_{n} x^{n} = 0,
\end{equation*}

which on re-indexing, simplification, and introducing $c_{-1} = c_{-2} \equiv 0$ leads to

\begin{equation*}
\sum_{n=0}^{\infty} \left[ n \left( n - 1 \right) c_{n} - 4 c_{n-2}  + n c_{n} - 3 c_{n} \right] x^{n}  = 0,
\end{equation*}

What's $c_{0}$? Let's find out:

\begin{equation*}
- 3 c_{0} = 0 \Longrightarrow c_{0} = 0.
\end{equation*}

What's $c_{1}$ ?

\begin{equation*}
c_{1} - 3c_{1} = - 2 c_{1} = 0 \Longrightarrow c_{1} = 0.
\end{equation*}

No recursion unfolds because the seeding coefficients are singular. There exists not a power series for $y(x)$ close to the origin - it is not analytic at $x=0$. Thus, we don't have a solution. But we insist on one! Well then use the generalized power series. It was Frobenius's idea to do so and therefore this method is called /Frobenius method for singular coefficients/. Moving on, let's feed in the ansatz $y(x) = x^{s} \sum_{n=0}^{\infty} c_{n} x^{n}$ now to obtain

\begin{equation*}
 \sum_{k=2}^{\infty} \left( k + s \right) \left( k + s - 1 \right) c_{k} x^{k+s} + \sum_{l=1}^{\infty} \left( l + s \right) c_{l} x^{l+s} + 4  \sum_{m=0}^{\infty} c_{m} x^{m+s+2} - 3 \sum_{n=0}^{\infty} c_{n} x^{n+s} = 0,
\end{equation*}

which after the usual song and dance yields

\begin{equation*}
\sum_{n=0}^{\infty} \left[ \left( n + s \right) \left( n + s - 1 \right) c_{n} + 4 c_{n-2}  + \left( n + s \right) c_{n} - 3 c_{n} \right] x^{n+s}  = 0.
\end{equation*}

Set the coefficient of $x^{s}$ $(n=0)$ to $0$ to find

\begin{equation*}
s \left( s - 1  \right) c_{0} + s c_{0}- 3 c_{0} = \left( s^{2} - 3 \right)c_{0} = 0.
\end{equation*}

Solving for $s^{2} - 3 = 0$, $s = \pm \sqrt{3}$. Singular no more. Choose $c_{0}$ as you please, none shall interfere. This is called the /indicial equation/ by the way.

Set the coefficient of $x^{s+1}$  $(n = 1)$ to $0$ to find

\begin{equation*}
s \left( 1 + s \right) c_{1} + \left( 1 + s \right) c_{1} - 3 c_{1} = \left[ \left( 1 + s \right)^{2} - 3 \right] c_{1} = 0.
\end{equation*}

With $s = \pm \sqrt{3}$, $c_{1} = 0$. No matter, $c_{0}$ survives and we can still unroll /two/ recurrences - corresponding to $s = \sqrt{3}$ and $s = -\sqrt{3}$. Let's now find the recurrence by demanding vanishment of $\left[ \left( n + s \right) \left( n + s - 1 \right) c_{n} + 4 c_{n-2}  + \left( n + s \right) c_{n} - 3 c_{n} \right]$ for $n \geq 2$, or $\left[ \left( n + s + 2 \right) \left( n + s + 1 \right) c_{n+2} + 4 c_{n}  + \left( n + s + 2 \right) c_{n+2} - 3 c_{n+2} \right]$ for $n \geq 0$.

\begin{equation*}
c_{n+2} = -\dfrac{4}{\left( n + 2 + s \right)^{2} - 3} c_{n}.
\end{equation*}

Let's use $s = \sqrt{3}$ and $c_{0} = 1$. Then

\begin{equation*}
c_{2} = - \dfrac{1}{1 + \sqrt{3}}, \quad c_{4} = \dfrac{1}{2\left(2 + \sqrt{3}\right)\left(1 + \sqrt{3}\right)}.
\end{equation*}

Because $c_{1} = 0$, $c_{3}, \thinspace c_{5}, \thinspace \dotso = 0$. Thus the final solution upto $c_{5}$ is

\begin{equation*}
y_{1}(x) = x^{\sqrt{3}} \left[ 1 - \dfrac{x^{2}}{1 + \sqrt{3}} + \dfrac{x^{4}}{2\left(2 + \sqrt{3}\right)\left(1 + \sqrt{3}\right)} \right], \quad x \neq 0.
\end{equation*}

Why the $x \neq 0$ qualifier? Because a derivative of $y_{1}(x)$ will pull down an $x$ to the denominator, which would diverge at $x = 0$. With $s = - \sqrt{3}$ and $c_{0} = 1$ we have the other solution 

\begin{align*}
y_{2}(x) &= \dfrac{1}{x^{\sqrt{3}}} \left[ 1 - \dfrac{x^{2}}{1 - \sqrt{3}} + \dfrac{x^{4}}{2\left(2 - \sqrt{3}\right)\left(1 - \sqrt{3}\right)} \right], \quad x \neq 0.
\end{align*}

There's no answer for this at the back of the book. How about we use this as an opportunity, for once in our life, to /actually/ plug the solution back in the differential equation and see if it holds up? Fine. I'll do it only for $s = \sqrt{3}$ though.

\begin{equation*}
y_{1} = x^{\sqrt{3}} \left[ 1 - \dfrac{x^{2}}{1 + \sqrt{3}} + \dfrac{x^{4}}{2\left(2 + \sqrt{3}\right)\left(1 + \sqrt{3}\right)} \right], \quad x \neq 0,
\end{equation*}

\begin{equation*}
y_{1}^{\prime} = x^{\sqrt{3}} \left[ \dfrac{\sqrt{3}}{x} - \left(\dfrac{2 + \sqrt{3}}{1 + \sqrt{3}}\right)x  + \left( \dfrac{4 + \sqrt{3}}{2 \left( 2 + \sqrt{3} \right) \left( 1 + \sqrt{3} \right)} \right) x^{3} \right], \quad x \neq 0,
\end{equation*}

\begin{equation*}
y_{1}^{\prime \prime} = x^{\sqrt{3}} \left[ \left( \dfrac{15 + 7 \sqrt{3}}{2 (2 + \sqrt{3}) (1 + \sqrt{3})} \right) x^{2} - \left( 2 + \sqrt{3} \right) + \dfrac{3 - \sqrt{3}}{x^{2}}} \right], \quad x \neq 0.
\end{equation*}

Moment of truth.

\begin{align*}
0 &\stackrel{?}{=} x^{2} y^{\prime \prime} + x y^{\prime} + \left( 4 x^{2} - 3 \right)y \\
&\stackrel{?}{=} \left[ \left( \dfrac{15 + 7 \sqrt{3}}{2 (2 + \sqrt{3}) (1 + \sqrt{3})} \right) x^{4} - \left( 2 + \sqrt{3} \right) x^{2} + \left(3 - \sqrt{3}\right)\right] \\
&+ \left[ \sqrt{3} - \left(\dfrac{2 + \sqrt{3}}{1 + \sqrt{3}}\right) x^{2}  + \left( \dfrac{4 + \sqrt{3}}{2 \left( 2 + \sqrt{3} \right) \left( 1 + \sqrt{3} \right)} \right) x^{4} \right] \\
&+ \left[ 4 x^{2} - \dfrac{4 x^{4}}{1 + \sqrt{3}} + \dfrac{4 x^{6}}{2\left(2 + \sqrt{3}\right)\left(1 + \sqrt{3}\right)} \right] \\
&+ \left[ -3 + \dfrac{3 x^{2}}{1 + \sqrt{3}} - \dfrac{3 x^{4}}{2\left(2 + \sqrt{3}\right)\left(1 + \sqrt{3}\right)} \right] \\
&\stackrel{?}{=} \left[\left(3 - \sqrt{3}\right) + \sqrt{3} - 3\right] \\
&+ \left[ - \left( 2 + \sqrt{3} \right)  - \left( \dfrac{2 + \sqrt{3}}{1 + \sqrt{3}} \right) + 4 + \left(\dfrac{3}{1 + \sqrt{3}} \right)\right]x^{2} \\
&+ \left[ \left( \dfrac{15 + 7 \sqrt{3}}{2 (2 + \sqrt{3}) (1 + \sqrt{3})} \right) + \left( \dfrac{4 + \sqrt{3}}{2 \left( 2 + \sqrt{3} \right) \left( 1 + \sqrt{3} \right)} \right) - \left( \dfrac{4}{1 + \sqrt{3}} \right) - \left(\dfrac{3 }{2\left(2 + \sqrt{3}\right)\left(1 + \sqrt{3}\right)}  \right)\right] x^{4} = 0.
\end{align*}

Yes indeed, we have ourselves a solution.

** SOLVED Intermezzo 2
CLOSED: [2022-10-24 Mon 09:52]

Alright, I'm confused with how $(10.4.60)$ follows from $10.4.59$. Is

\begin{equation*}
x^{2} y^{\prime \prime} + x y^{\prime} \stackrel{?}{=} x \dfrac{d}{dx} [x y^{\prime}],
\end{equation*}

true? Let's find out.

\begin{equation*}
x \dfrac{d}{dx} \left[ x y^{\prime} \right] = x \left[ x y^{\prime \prime} + y^{\prime}  \right] = x^{2} y^{\prime \prime} + x y^{\prime}.
\end{equation*}

Yes it is. How about

\begin{equation*}
x^{2} y^{\prime \prime} + p(x) y^{\prime} \stackrel{?}{=} x \dfrac{d}{dx} \left[ p(x) y^{\prime} \right].
\end{equation*}

Obviously not.

** SOLVED Problem 10.4.7
CLOSED: [2022-10-24 Mon 09:51]
:LOGBOOK:
CLOCK: [2022-10-23 Sun 23:21]--[2022-10-24 Mon 09:44] => 10:23
:END:
*Consider the case where $p(x) = \sum_{n} p_{n} x^{n}, \quad q(x) = \sum_{n} q_{n} x^{n}$ in Eqn. (10.4.54). Solve for the indicial equation and the recursion relation for $a_{m}$ for the smaller root and verify the claims made in the previous discussion.*

The differential equation of concern is

\begin{equation*}
x^{2} y^{\prime \prime} + x p(x) y^{\prime} + q(x) y = 0,
\end{equation*}

where $p(x) = \sum_{n} p_{n} x^{n}$ and $q(x) = \sum_{n} q_{n} x^{n}$ are convergent power series for $p(x)$ and $q(x)$ about $x = 0$. We will need a generalized power series as ansatz (because the question asks for the indicial equation). Let it be $y(x) = x^{s} \sum_{n=0}^{\infty} c_{n} x^{n}$. Upon feeding

\begin{align*}
0 &= x^{2} \left( \sum_{n=0}^{\infty} \left( n + s \right) \left( n + s - 1 \right) c_{n} x^{n + s - 2} \right)  + x \left(\sum_{m=0}^{\infty} p_{m} x^{m}\right) \left(\sum_{n = 0}^{\infty} \left( n + s \right) c_{n} x^{n+s-1}\right)  + \left(\sum_{m=0}^{\infty} q_{m} x^{m}\right) \left(\sum_{n = 0}^{\infty} c_{n} x^{n + s}\right).
\end{align*}

The indicial equations are obtained by equating the coefficient of $x^{s}$ and $x^{s+1}$ to $0$, i.e.,

\begin{equation*}
\left[ s \left( s - 1 \right) + s p_{0} + q_{0} \right] c_{0} = 0, \Longrightarrow s^{2} + \left( p_{0} - 1 \right) s + q_{0} = 0 \quad \text{or} \quad c_{0} = 0,
\end{equation*}

and

\begin{equation*}
\left[ s \left( s + 1 \right) + \left( 1 + s \right) p_{0} + q_{0} \right] c_{1} = 0 \Longrightarrow s^{2} + \left( p_{0} + 1 \right) s + p_{0} + q_{0} = 0 \quad \text{or} \quad c_{1} = 0.
\end{equation*}

If $s^{2} + \left( p_{0} - 1 \right) s + q_{0} = 0$, the roots are

\begin{equation*}
s_{1} = \dfrac{\left( 1 - p_{0} \right) + \sqrt{(1 - p_{0})^{2} - 4 q_{0}}}{2}, \quad s_{2} = \dfrac{\left( 1 - p_{0} \right) - \sqrt{(1 - p_{0})^{2} - 4 q_{0}}}{2},
\end{equation*}

and if $s^{2} + \left( p_{0} + 1 \right) s + p_{0} + q_{0} = 0$, the roots are

\begin{equation*}
s_{1} = \dfrac{-(1 + p_{0}) + \sqrt{(1 - p_{0})^{2} - 4 q_{0}}}{2}, \quad s_{2} = \dfrac{-(1 + p_{0}) - \sqrt{(1 - p_{0})^{2} - 4 q_{0}}}{2}.
\end{equation*}

Because $s^{2} + \left( p_{0} - 1 \right) s + q_{0} \neq s^{2} + \left( p_{0} + 1 \right) s + p_{0} + q_{0}$,

\begin{equation*}
s^{2} + \left( p_{0} - 1 \right) s + q_{0} = 0 \Longrightarrow c_{1} = 0,
\end{equation*}

and

\begin{equation*}
s^{2} + \left( p_{0} + 1 \right) s + p_{0} + q_{0} = 0 \Longrightarrow c_{0} = 0.
\end{equation*}

On now to the finding the recurrence relation for $c_{n}$. First appreciate that for convergent power series $\sum_{n = 0}^{\infty} a_{n} x^{n}$ and $\sum_{m = 0}^{\infty} b_{m} x^{m}$

\begin{equation*}
\left(\sum_{n = 0}^{\infty} a_{n} x^{n} \right) \left(\sum_{m = 0}^{\infty} b_{m} x^{m}\right) = \sum_{n=0}^{\infty} \sum_{m=0}^{\infty} a_{n} b_{m} x^{n+m} = \sum_{n=0}^{\infty} \left( \sum_{m=0}^{n} a_{m} b_{n-m} \right) x^{n},
\end{equation*}

so in our case

\begin{align*}
0 &= x^{2} \left( \sum_{n=0}^{\infty} \left( n + s \right) \left( n + s - 1 \right) c_{n} x^{n + s - 2} \right)  + x \left(\sum_{m=0}^{\infty} p_{m} x^{m}\right) \left(\sum_{n = 0}^{\infty} \left( n + s \right) c_{n} x^{n+s-1}\right)  + \left(\sum_{m=0}^{\infty} q_{m} x^{m}\right) \left(\sum_{n = 0}^{\infty} c_{n} x^{n + s}\right) \\
&= \sum_{n=0}^{\infty} \left[ \left( n + s \right) \left( n + s - 1 \right) c_{n} \right]  x^{n + s}   +  \left(\sum_{n = 0}^{\infty} \left( n + s \right) c_{n} x^{n+s}\right) \left(\sum_{m=0}^{\infty} p_{m} x^{m}\right)  + \left(\sum_{n = 0}^{\infty} c_{n} x^{n + s}\right) \left(\sum_{m=0}^{\infty} q_{m} x^{m}\right) \\
&= \sum_{n=0}^{\infty} \left[\left( n + s \right) \left( n + s - 1 \right) c_{n}\right]  x^{n + s}   +  \sum_{n = 0}^{\infty} \left( \sum_{m=0}^{n} \left( m + s \right) c_{m} p_{n-m} \right)  x^{n+s} + \sum_{n = 0}^{\infty} \left(\sum_{m=0}^{n} c_{m} q_{n-m} \right)  x^{n + s} \\
&= \sum_{n=0}^{\infty} \left[\left( n + s \right) \left( n + s - 1 \right) c_{n} + \sum_{m=0}^{n} \left[ \left( m + s \right) p_{n-m} + q_{n-m} \right] c_{m} \right] x^{n + s}.
\end{align*}

Demanding vanishment of the coefficient of $x^{n+s}$ furnishes the recurrence

\begin{equation*}
c_{n} = -\dfrac{\sum_{m=0}^{n} c_{m} \left[ \left( m + s \right)p_{n-m} + q_{n-m} \right]}{\left( n + s \right)\left( n + s - 1 \right)}.
\end{equation*}

In the general case, the $n$th coefficient depends on /all/ of the coefficients before it (half of which would have vanished given roots exist for one of the indicial equations).

1) If /distinct/ roots exist for either of the indicial equations, then either the odd or the even coefficients of $y(x)$ vanish. We can however obtain two solutions by unrolling the recurrence for each root.

2) If the roots are /degenerate/, either the odd or the even coefficients of $y(x)$ vanish all the same. We can however only obtain a single solution by unrolling the recurrence since they are now identical. To find a second solution that is linearly independent to the first, suppose that the two roots differ by small $\epsilon > 0$, i.e., $s_{1} - s_{2} = \epsilon$. For finite $\epsilon$

\begin{equation*}
y_{2} = \dfrac{x^{s_{1}} \sum_{n=0}^{\infty} c_{n}(s_{1}) x^{n} - x^{s_{1} - \epsilon} \sum_{n=0}^{\infty} c_{n}(s_{1} - \epsilon) x^{n}}{\epsilon}
\end{equation*}

is a solution. In the limit $\epsilon \to 0$, $s_{2} \to s_{1} \equiv s$ the roots are degenerate but the right hand side may be identified as the derivative so

\begin{equation*}
y_{2} = \dfrac{d}{ds} \left[x^{2} \sum_{n=0}^{\infty} a_{n}(s) x^{n} \right] = \ln x \thinspace x^{s} \sum_{n=0}^{\infty} a_{n} (s) x^{n} + x^{s} \sum_{n = 0}^{\infty} \dfrac{d a_{n}(s)}{ds} x^{n} = \ln x \thinspace x^{s} \sum_{n=0}^{\infty} a_{n} (s) x^{n} + x^{s} \sum_{n = 0}^{\infty} b_{n} (s) x^{n}
\end{equation*}

with $b_{n} \equiv \frac{d a_{n}(s)}{ds}$ is the second solution, as alluded to in the book.

3) If the roots are distinct but $s_{1} - s_{2} \equiv k$ is an integer, then the recurrence for $s_{2}$ is

\begin{equation*}
\left( n + s_{2} \right) \left( n + s_{2} - 1 \right) c_{n} = -\sum_{m=0}^{n} c_{m} \left[ \left( m + s_{2} \right)p_{n-m} + q_{n-m} \right],
\end{equation*}
or
\begin{equation*}
\left[\left( n + s_{2} \right) \left( n + s_{2} - 1 \right) + \left( n + s_{2}\right) p_{0} + q_{0}\right] c_{n} = - \sum_{m=0}^{n-1} c_{m} \left[ \left( m + s_{2} \right) p_{n-m} + q_{n-m} \right].
\end{equation*}

With the substitution $s_{2} = s_{1} - k$,

\begin{equation*}
\left[\left( n + s_{1} - k \right) \left( n + s_{1} - k - 1 \right) + \left( n + s_{1} - k\right) p_{0} + q_{0}\right] c_{n} = - \sum_{m=0}^{n-1} c_{m} \left[ \left( m + s_{2} \right) p_{n-m} + q_{n-m} \right].
\end{equation*}

When $n = k$, 

\begin{equation*}
\left[s_{1} \left( s_{1} - 1 \right) +  s_{1} p_{0} + q_{0}\right] c_{k} = - \sum_{m=0}^{k-1} c_{m} \left[ \left( m + s_{2} \right) p_{k-m} + q_{k-m} \right],
\end{equation*}

so that

\begin{equation*}
c_{k} = - \dfrac{\sum_{m=0}^{k-1} c_{m} \left[ \left( m + s_{2} \right) p_{k-m} + q_{k-m} \right]}{s_{1} \left( s_{1} - 1 \right) +  s_{1} p_{0} + q_{0}}.
\end{equation*}

We have in the denominator the left hand side of the indicial equation. $c_{k}$ diverges if $\sum_{m=0}^{k-1} c_{m} \left[ \left( m + s_{2} \right) p_{k-m} + q_{k-m} \right] \neq 0$. In that case the first term of $y_{2}$ is proportional to $x^{s_{2} + k}$, same as $y_{1}$ (proportional to $x^{s_{1}} = x^{s_{2} + k}$). In this case as well, even though the roots are distincts, both recurrences unroll to the same solution. The stunt we pulled for the degenerate root case won't work here since $s_{1} - s_{2} = k$ is finite and cannot be "sent to zero" as a limit. To obtain a solution $y_{2}$ that is linearly independent to $y_{1}$ use an ansatz of the form

\begin{equation*}
y_{2} (x) = A y_{1}(x) \ln x + x^{s_{2}} \sum_{n = 0}^{\infty} c_{n} x^{n},
\end{equation*}

and feed it into the differential equation to solve for $A$ and $c_{n}$. How was this obtained? Not sure right now, will come back to this later. Moving forward, for the /Bessel equation/

\begin{equation*}
x^{2} y^{\prime \prime} + x y^{\prime} + \left( x^{2} - \nu^{2} \right) y = 0,
\end{equation*}

$p_{0} = 1, \thinspace p_{i} = 0, i \neq 0$, $q_{0} = - \nu^{2}, \quad q_{2} = 1, \quad q_{j} = 0, j \neq 0, 2$.

The indicial equation reduces to

\begin{equation*}
s^{2} - \nu^{2} = 0,
\end{equation*}

so that

\begin{equation*}
s = \pm \nu,
\end{equation*}

same as $(10.4.61)$.

The recurrence reduces to

\begin{equation*}
c_{n} = \dfrac{c_{n-2} + c_{n} [(n + s) - \nu^{2}] }{\left( n + s \right)\left( n + s - 1 \right)},
\end{equation*}

which we juggle a bit

\begin{equation*}
c_{n} \left[ (n + s) - \nu^{2} + \left( n + s \right) \left( n + s - 1 \right)\right] = c_{n} \left[ (n + s) - \nu^{2} + \left( n + s \right)^{2} - \left( n + s \right) \right] = c_{n} \left[ (n + s)^{2} - \nu^{2} \right] =  c_{n-2},
\end{equation*}

and reduce to

\begin{equation*}
c_{n} = \dfrac{c_{n-2}}{ \left( n + s \right)^{2} - \nu^{2}},
\end{equation*}

same as $(10.4.62)$. If $\nu$ is not an integer, then the two recurrences yield two linearly independent solutions which are infact the Bessel functions of the first kind $J_{\pm \nu}(x)$ of order $\pm \nu$. If $\nu$ is an integer, then for the smaller root $s = - \nu$, $c_{n}$ diverges when $n = 2 \nu$. Thus the first term of $y_{2}$ must necessarily be proportional to $x^{- \nu} x^{2 \nu} = x^{\nu}$, same as the first term of $y_{1}$.
** TOSOLVE Problem 10.4.8
*Solve the following equations by the method of indicial equations. Go up to $x^{4}$ or to all orders if a pattern is found.*

\begin{equation*}
a: x \left( x + 1 \right)^{2} y^{\prime \prime} + \left( 1 - x^{2} \right) y^{\prime} + \left( x - 1 \right)y = 0,
\end{equation*}

\begin{equation*}
b: x(1 - x) y^{\prime \prime} + 2 \left( 1 - 2x \right) y^{\prime} - 2y = 0,
\end{equation*}

\begin{equation*}
c: x^{2} y^{\prime \prime} + x y^{\prime} - 9y = 0,
\end{equation*}

\begin{equation*}
d: x y^{\prime \prime} + \dfrac{1}{2} y^{\prime} + 2y = 0,
\end{equation*}

\begin{equation*}
e: x^{2} y^{\prime \prime} - x y^{\prime} + y = 0,
\end{equation*}

\begin{equation*}
f: 2 x y^{\prime \prime} - y^{\prime} + 2y = 0,
\end{equation*}

\begin{equation*}
g: x y^{\prime \prime} + x y^{\prime} - 2y = 0,
\end{equation*}

\begin{equation*}
h: x \left( x - 1 \right)^{2} y^{\prime \prime} - 2y = 0.
\end{equation*}

** TOSOLVE Problem 10.4.9
*Solve the following equations both ways: using Eqn. (10.3.11) as well as generalized power series:*

\begin{equation*}
\left( i \right): y^{\prime} - \dfrac{2y}{x} - x^{2} = 0,
\end{equation*}

\begin{equation*}
\left( ii \right): y^{\prime} + \dfrac{2y}{x} - x^{3} = 0.
\end{equation*}

** SOLVED Problem 10.4.10
CLOSED: [2022-10-24 Mon 11:31]
:LOGBOOK:
CLOCK: [2022-10-24 Mon 10:01]--[2022-10-24 Mon 11:31] =>  1:30
:END:
*Solve Laguerre's Equation which enters the solution of the hydrogen atom problem in quantum mechanics (after premultiplying both sides by $x$)*

\begin{equation*}
x y^{\prime \prime} + \left( 1 - x \right) y^{\prime} + m y = 0.
\end{equation*}

*by the power series method. Show that there is a repeated root and focus on the solution which is regular at the origin. Show that this reduces to a polynomial when $m$ is an integer. These are the Laguerre polynomials $L_{m}$. Find the first four polynomials choosing $c_{0} = 1$. Show that $L_{1}$ and $L_{2}$ are orthogonal in the interval $0 \leq x \leq \infty$ with a weight function $\exp \left \lbrace -x  \right \rbrace$. (Recall the gamma function.)*

Premultiplying both sides by $x$

\begin{equation*}
x^{2} y^{\prime \prime} + x \left( 1 - x \right) y^{\prime} + m x y = 0.
\end{equation*}

Feed the ansatz $y(x) = \sum_{n=0}^{\infty} c_{n} x^{n}$ to obtain

\begin{equation*}
\sum_{n=0}^{\infty} n \left( n - 1 \right) c_{n} x^{n} + \sum_{n=0}^{\infty} n c_{n} x^{n} - \sum_{n=0}^{\infty} n c_{n} x^{n+1} + \sum_{n=0}^{\infty} m c_{n} x^{n+1} = 0,
\end{equation*}

which on simplification yields

\begin{equation*}
\sum_{n=0}^{\infty} \left[ n (n + 1) c_{n + 1} + (n + 1) c_{n + 1} - n c_{n} + m c_{n} \right] x^{n+1} = 0
\end{equation*}

Match coefficient 

\begin{equation*}
\left( n + 1 \right)^{2} \thinspace c_{n+1} = \left( n - m \right) c_{n},
\end{equation*}

so the recurrence is

\begin{equation*}
c_{n+1} =  \left(\dfrac{n - m}{\left(n + 1\right)^{2}}\right) c_{n}.
\end{equation*}

The recurrence vanishes for $n = m$. We write down the /Laguerre Polynomials/ $L_{m}$:

for $m = 0$, $c_{0} = 1$, $c_{1} = 0$, so that $L_{0} = 1$,

for $m = 1$, $c_{0} = 1$, $c_{1} = - 1$, so that $L_{1} = 1 - x$,

for $m = 2$, $c_{0} = 1$, $c_{1} = - 2$, $c_{2} = \frac{1}{2}$, so that $L_{2} = 1 - 2 x + \dfrac{1}{2} x^{2}$,

for $m = 3$, $c_{0}=1$, $c_{1}= - 3$, $c_{2} = \frac{3}{2}$, $c_{3} = - \frac{1}{6}$, so that $L_{3} = 1 -  3 x + \dfrac{3}{2} x^{2} - \dfrac{1}{6} x^{3}$,

and for $m = 4$, $c_{0} = 1$, $c_{1} = - 4$, $c_{2} = 3$, $c_{3} = - \dfrac{2}{3}$, and $c_{4} = \dfrac{1}{24}$, so that $L_{4} = 1 - 4 x + 3 x^{2} - \dfrac{2}{3} x^3 + \dfrac{1}{24} x^{4}$.

Let me show orthogonality of $L_{1}$ and $L_{2}$ for now. I'll come back after I'm done proving the Rodriguez formula and write out the general case.

\begin{align*}
0 &\stackrel{?}{=} \int_{0}^{\infty} dx \thinspace \exp \left \lbrace - x  \right \rbrace \left( 1 - x \right) \left( 1 - 2 x + \dfrac{1}{2} x^{2} \right) \\
&\stackrel{?}{=} \left(\int_{0}^{\infty} 1 \exp \left \lbrace - x  \right \rbrace \thinspace dx \right) - 3 \left(\int_{0}^{\infty} \exp \left \lbrace - x  \right \rbrace x \thinspace dx \right)  + \dfrac{5}{2} \left( \int_{0}^{\infty} \exp \left \lbrace - x  \right \rbrace x^2 \thinspace dx \right)  - \dfrac{1}{2} \left( \int_{0}^{\infty} \exp \left \lbrace - x  \right \rbrace x^3 \thinspace dx \right).
\end{align*}

Identify each of these as the integral representation of the /Gamma function/ to write

\begin{align*}
0 \stackrel{?}{=} \Gamma \left( 1 \right) - 3 \Gamma \left( 2 \right) + \dfrac{5}{2} \Gamma \left( 3 \right) - \dfrac{1}{2} \Gamma \left( 4 \right) = 1 \times 0! - 3 \times 1! + \dfrac{5}{2} \times 2! - \dfrac{1}{2} \times 3! = 0,
\end{align*}

which should satisfy the ask.

** SOLVED Problem 10.5.1
CLOSED: [2022-10-24 Mon 13:22]
:LOGBOOK:
CLOCK: [2022-10-24 Mon 12:46]--[2022-10-24 Mon 13:22] =>  0:36
:END:
*Solve the PDE*

\begin{equation*}
\dfrac{1}{v^2} \dfrac{\partial^2 \psi}{\partial t^2} = \dfrac{\partial^{2} \psi}{\partial x^{2}} + \dfrac{\partial^{2} \psi}{\partial y^{2}} \equiv \nabla^{2} \psi
\end{equation*}

*subject to boundary condition*

\begin{equation*}
\psi \left( x = 0, y, t \right) = \psi \left( x = L, y, t \right) = \psi \left( x, y = 0, t \right) = \psi \left( x, y = L, t \right) = 0.
\end{equation*}

*using the method of separation of variables.*

Feed into the PDE the ansatz

\begin{equation*}
\psi \left( x, y, t \right) = T(t) X(x) Y(y)
\end{equation*}

and divide by $T(t) X(x) Y(y)$ to obtain

\begin{equation*}
\dfrac{1}{v^{2}} \dfrac{1}{T} \dfrac{d^{2} T}{d t^{2}} = \dfrac{1}{X}\dfrac{d^{2} X}{d x^{2}} + \dfrac{1}{Y} \dfrac{d^{2} Y}{d y^{2}}.
\end{equation*}

Each side must be a constant. Let this constant, by definition, be $- \left(k_{x}^{2} +  k_{y}^{2}\right)$, such that

\begin{equation*}
\dfrac{1}{X}\dfrac{d^{2} X}{d x^{2}} = - k_{x}^{2},
\end{equation*}

\begin{equation*}
\dfrac{1}{Y} \dfrac{d^{2} Y}{d y^{2}} = - k_{y}^{2},
\end{equation*}

\begin{equation*}
\dfrac{1}{T} \dfrac{d^{2} T}{d t^{2}} = - v^{2} \left( k_{x}^{2} + k_{y}^{2} \right) \equiv - \omega^{2}.
\end{equation*}

The solutions to the first, second, and third equation that are consistent with the specified boundary condition are

\begin{equation*}
X = \sin \left(  \dfrac{n \pi x}{L} \right), \quad k_{x} = \dfrac{n \pi}{L},
\end{equation*}

and

\begin{equation*}
Y = \sin \left( \dfrac{m \pi y}{L} \right), \quad k_{y} = \dfrac{m \pi}{L},
\end{equation*}

\begin{equation*}
T = A_{nm}\cos \left( \omega t \right) = A_{nm} \cos \left( v \sqrt{ \left( n \pi /L \right)^{2} + \left( m \pi / L \right)^{2}} t \right).
\end{equation*}

respectively. In the solution for $T$ we have merely used $k_{x} = n \pi /L$, $k_{y} = m \pi /L$, and the definition of $\omega$. Further, with the foresight that these functions are about to be multiplied, we have lumped together the constants of integration from $X$ and $Y$ which in turn depend on $n$ and $m$ respectively and tacked it in front of $T$. Let's multiply them

\begin{equation*}
\psi_{nm} \left( x, y , t \right) = A_{nm} \sin \left( \dfrac{n \pi x}{L} \right) \sin \left( \dfrac{m \pi y}{L} \right) \cos \left( t \sqrt{ (n \pi / L)^{2} + (m \pi / L )^{2} } t \right).
\end{equation*}

The most general solution is a superposition over $\psi_{nm}$, i.e.,

\begin{equation*}
\psi \left( x, y, t \right) = \sum_{n = 1}^{\infty} \sum_{m = 1}^{\infty} A_{nm} \sin \left( \dfrac{n \pi x}{L} \right) \sin \left( \dfrac{m \pi y}{L} \right) \cos \left( t \sqrt{ (n \pi / L)^{2} + (m \pi / L )^{2} } t \right),
\end{equation*}

which now fulfills the ask.

** SOLVED Problem 10.5.2
CLOSED: [2022-10-24 Mon 16:30]
:LOGBOOK:
CLOCK: [2022-10-24 Mon 13:27]--[2022-10-24 Mon 16:29] =>  3:02
:END:
*Find $A_{nm}$ of the previous question.*

We have

\begin{equation*}
\sum_{n^{\prime} = 1}^{\infty} \sum_{m^{\prime} = 1}^{\infty} A_{n^{\prime}m^{\prime}} \sin \left( \dfrac{n^{\prime} \pi x}{L} \right) \sin \left( \dfrac{m^{\prime} \pi y}{L} \right) = \psi \left( x, y, 0 \right) = x \left( L - x \right) y \left( L - y \right).
\end{equation*}

Multiplying both sides with $\sin \left( \dfrac{n \pi x}{L} \right) \sin \left( \dfrac{m \pi y}{L} \right)$ and integrating from $x = 0$ to $x = L$  and $y = 0$ to $y = L$,

\begin{align*}
A_{nm} \int_{0}^{L} dx \thinspace \sin^{2} \left( \dfrac{n \pi x}{L} \right) \int_{0}^{L} dy \thinspace \sin^{2} \left( \dfrac{m \pi y}{L} \right) = \int_{0}^{L} dx \thinspace x \left( L - x \right) \thinspace \sin \left( \dfrac{n \pi x}{L} \right)  \int_{0}^{L} \thinspace dy \thinspace  y \left( L - y \right) \sin \left( \dfrac{m \pi y}{L} \right).
\end{align*}

Where'd the double sum go? We used the result

\begin{equation*}
\int_{0}^{L} dx \sin \left( n \pi x / L \right) \sin \left( n^{\prime} \pi x / L \right) =
\begin{cases}
0, \quad \text{if} \quad n^{\prime} \neq n, \\
L/2, \quad \text{if} \quad n^{\prime} = n.
\end{cases},
\end{equation*}

known as the /orthogonality relation of the trigonometric functions/. Hold your nose, I'm proving it. Suppose that $n \neq n^{\prime}$.

\begin{align*}
&\int_{0}^{L} dx \sin \left( n \pi x / L \right) \sin \left( n^{\prime} \pi x / L \right) \xrightarrow[dx = L dx^{\prime}/ \pi]{ \pi x / L= x^{\prime}} \dfrac{L}{\pi} \int_{0}^{\pi} dx^{\prime} \sin n x^{\prime} \sin n^{\prime} x^{\prime} \\
&= \dfrac{L}{2 \pi} \int_{0}^{\pi} dx^{\prime} \left(\cos \left[ (n - n^{\prime}) x^{\prime} \right] - \cos \left[ (n + n^{\prime}) x^{\prime} \right] \right) \\
&=  \dfrac{L}{2 \pi} \left[ \sin \left( \left[ n - n^{\prime} \right] x \right) \right]_{0}^{\pi} - \dfrac{L}{2 \pi} \left[ \sin \left( \left[ n + n^{\prime} \right] x \right) \right]_{0}^{\pi} = 0.
\end{align*}

What happened there in the second step? Get the hell out of here! We'll prove the $n = n^{\prime}$ case in a little while, we'll need it anyway. Refocus back again on

\begin{align*}
A_{nm} \int_{0}^{L} dx \thinspace \sin^{2} \left( \dfrac{n \pi x}{L} \right) \int_{0}^{L} dy \thinspace \sin^{2} \left( \dfrac{m \pi y}{L} \right) = \int_{0}^{L} dx \thinspace x \left( L - x \right) \thinspace \sin \left( \dfrac{n \pi x}{L} \right)  \int_{0}^{L} \thinspace dy \thinspace  y \left( L - y \right) \sin \left( \dfrac{m \pi y}{L} \right).
\end{align*}

Let's do the integrals one by one, starting with

\begin{align*}
I_{1} &= \int_{0}^{L} \thinspace dy \thinspace  y \left( L - y \right) \sin \left( \dfrac{m \pi y}{L} \right) = L \int_{0}^{L} \thinspace dy \thinspace  y  \sin \left( \dfrac{m \pi y}{L} \right) - \int_{0}^{L} \thinspace dy \thinspace y^{2} \thinspace \sin \left( \dfrac{m \pi y}{L} \right) \\
& \xrightarrow[y = (L/m \pi) y^{\prime} , \quad (L/m \pi) d y^{\prime} = dy]{y^{\prime} = m \pi y / L} \dfrac{L^{3}}{m^{2} \pi^{2}} \int_{0}^{m \pi} \thinspace dy^{\prime} \thinspace  y^{\prime}  \sin \left( y^{\prime} \right) - \dfrac{L^{3}}{m^{3} \pi^{3}} \int_{0}^{m \pi} \thinspace dy^{\prime} \thinspace y^{\prime}^{2} \thinspace \sin \left( y^{\prime} \right).
\end{align*}
 
Let $I_{1^{\prime}} = \int_{0}^{m \pi} \thinspace dy^{\prime} \thinspace  y^{\prime}  \sin \left( y^{\prime} \right)$ and $I_{1^{\prime \prime}} = \int_{0}^{m \pi} \thinspace dy^{\prime} \thinspace  y^{\prime}^{2}  \sin \left( y^{\prime} \right)$. Integrating by parts

\begin{align*}
I_{1^{\prime}} &= - \int_{0}^{m \pi} \thinspace dy^{\prime} \thinspace  y^{\prime} \left( \dfrac{d \cos y^{\prime}}{dy^{\prime}} \right) = - \left[ y^{\prime} \cos y^{\prime} \right]_{0}^{m \pi} + \int_{0}^{m \pi} dy^{\prime} \thinspace \cos y^{\prime} \\
&= - \left[ y^{\prime} \cos y^{\prime} \right]_{0}^{m \pi} + \left[ \sin y^{\prime}  \right]_{0}^{m \pi} \\
&= - m \pi, \quad m \quad \text{odd}.
\end{align*}

and

\begin{align*}
I_{1^{\prime \prime}} &= \int_{0}^{m \pi} \thinspace dy^{\prime} \thinspace  y^{\prime}^{2} \left( \dfrac{d \cos y^{\prime}}{dy^{\prime}} \right) =  \left[ y^{\prime}^{2} \cos y^{\prime} \right]_{0}^{m \pi} - 2 \int_{0}^{m \pi} dy^{\prime} \thinspace y^{\prime} \cos y^{\prime} \\
&= \left[ y^{\prime}^{2} \cos y^{\prime} \right]_{0}^{m \pi} - 2 \int_{0}^{m \pi} dy^{\prime} \thinspace y^{\prime} \left( \dfrac{d \sin y^{\prime} }{dy^{\prime}} \right) \\
&= \left[ y^{\prime}^{2} \cos y^{\prime} \right]_{0}^{m \pi} - 2 \left[ y^{\prime} \sin y^{\prime} \right]_{0}^{m \pi} + 2 \int_{0}^{m \pi} dy^{\prime} \thinspace \sin y^{\prime} \\
&= \left[ y^{\prime}^{2} \cos y^{\prime} \right]_{0}^{m \pi} - 2 \left[ y^{\prime} \sin y^{\prime} \right]_{0}^{m \pi} - 2 \left[ \cos y^{\prime} \right]_{0}^{m \pi} \\
&= - m^{2} \pi^{2} - 4, \quad m \quad \text{odd}.
\end{align*}

Thus

\begin{equation*}
I_{1} = - \dfrac{L^{3}}{m^{2} \pi^{2}} m \pi - \dfrac{L^{3}}{m^{3} \pi^{3}} \left( - m^{2} \pi^{2} - 4\right) = \dfrac{4L^{3}}{m^{3} \pi^{3}} \quad m \quad \text{odd}.
\end{equation*}

Similarly


\begin{equation*}
I_{2} = \int_{0}^{L} dx \thinspace x \left( L - x \right) \thinspace \sin \left( \dfrac{n \pi x}{L} \right) = \dfrac{4 L^{3}}{n^{3} \pi^{3}}, \quad n \quad \text{odd}.
\end{equation*}

Now let's take up $I_{3} &= \int_{0}^{L} dy \thinspace \sin^{2} \left( \frac{m \pi y}{L} \right)$.

\begin{align*}
I_{3} &= \int_{0}^{L} dy \thinspace \sin^{2} \left( \dfrac{m \pi y}{L} \right) \\
& \xrightarrow[y = (L/m \pi) y^{\prime} , \quad (L/m \pi) d y^{\prime} = dy]{y^{\prime} = m \pi y / L} \dfrac{L}{m \pi} \int_{0}^{m \pi} dy^{\prime} \sin^{2} \left( y^{\prime} \right) \\
&= \dfrac{L}{2 m \pi} \int_{0}^{m \pi} dy^{\prime} \left[ 1 - \cos 2 y^{\prime}\right] = \dfrac{L}{2 m \pi} \int_{0}^{m \pi} dy^{\prime} - \dfrac{L}{2 m \pi} \int_{0}^{m \pi} dy^{\prime} \cos 2 y^{\prime} \\
&\xrightarrow[dy^{\prime} = dy^{\prime \prime}/2]{y^{\prime} = y^{\prime \prime}/2} \dfrac{L}{2} - \dfrac{L}{4 m \pi} \int_{0}^{2 m \pi} d y^{\prime \prime} \cos y^{\prime \prime} = \dfrac{L}{2} - \dfrac{L}{4 m \pi} \left[ \sin y^{\prime \prime} \right]_{0}^{2 m \pi} = \dfrac{L}{2}.
\end{align*}

Similarly

\begin{equation*}
I_{4} = \int_{0}^{L} dx \thinspace \sin^{2} \left( \dfrac{n \pi x}{L} \right) = \dfrac{L}{2}.
\end{equation*}

This by the way also completes the proof for the orthogonality relation of trigonometic functions. Plugging these back into the equation from where we leaped into this grind

\begin{align*}
A_{nm} \left[ \dfrac{L}{2} \right]^{2} = \dfrac{4L^{3}}{n^{3} \pi^{3}} \dfrac{4L^{3}}{m^{3} \pi^{3}}, \quad n, m \quad \text{odd},
\end{align*}

which should satisfy the ask.

For the given initial condition, the solution is

\begin{equation*}
\psi \left( x, y, t \right) = \left(\dfrac{64 \thinspace L^{4}}{\pi^{6}}\right) \sum_{m} \sum_{n} \left(\dfrac{1}{n^{3} \thinspace m^{3}}\right)  \sin \left(\dfrac{n \thinspace \pi \thinspace x}{L}\right) \sin \left(\dfrac{m \thinspace \pi \thinspace y}{L}\right)\cos \left( \dfrac{\pi v t}{L} \sqrt{m^{2} + n^{2}}\right) \quad n, m \quad \text{odd}.
\end{equation*}

I'm gonna stare at this for a moment.

** SOLVED Problem 10.5.3
CLOSED: [2022-10-25 Tue 22:26]
:LOGBOOK:
CLOCK: [2022-10-25 Tue 22:12]--[2022-10-25 Tue 22:26] =>  0:14
:END:
*What is the lowest frequency of vibration in the square membrane desribed above? What is the corresponding $\psi(x, y, t)$? Is there any degeneracy in the frequencies?*

The lowest frequency is when $n$ and $m$ take the smallest possible (odd) value, i.e., $n = 1$ and $m = 1$. The angular frequency then is $\sqrt{2} \pi v / L$ and the frequency is $\dfrac{v}{\sqrt{2}L}$. the corresponding $\psi \left( x, y , t \right)$ is

\begin{equation*}
\psi \left( x, y, t \right) = \left(\dfrac{64 \thinspace L^{4}}{\pi^{6}}\right) \sin \left(\dfrac{\thinspace \pi \thinspace x}{L}\right) \sin \left(\dfrac{\thinspace \pi \thinspace y}{L}\right)\cos \left(\dfrac{\sqrt{2} \pi v t}{L} \right) \quad n, m \quad \text{odd}.
\end{equation*}

Yes there are degeneracies in the frequencies. Swapping the values of $m$ and $n$ leads to the same frequency but is clearly a different solution for example.
** SOLVED Problem 10.5.4
CLOSED: [2022-10-26 Wed 01:01]
:LOGBOOK:
CLOCK: [2022-10-25 Tue 23:30]--[2022-10-26 Wed 00:12] =>  0:42
:END:
*First and second-order Bessel functions. Show, using*

\begin{equation*}
a_{n} = - \dfrac{a_{n-2}}{\left( n + s \right)^{2} - \nu^{2}}
\end{equation*}

*that*

\begin{equation*}
J_{0}(x) = \sum_{n = 0}^{\infty} \left( -1 \right)^{n} \dfrac{1}{\left( n! \right)^{2}} \left( \dfrac{x}{2} \right)^{2n},
\end{equation*}

\begin{equation*}
J_{1}(x) = \sum_{n = 0}^{\infty} \left( -1 \right)^{n} \dfrac{1}{n! \left( n + 1 \right)!} \left( \dfrac{x}{2}\right)^{2n + 1}.
\end{equation*}

*Observe that $J_{0}$ does not vanish at the origin and is even while $J_{1}$ is odd and hence vanishes at the origin.*

The recurrence for $J_{0}$ is

\begin{align*}
a_{2n} &= - \dfrac{a_{2(n-1)}}{(2n)^{2}}.
\end{align*}

Thus

\begin{equation*}
J_{0}(x) = \sum_{n = 0}^{\infty} a_{2n} x^{2n} = \sum_{n = 0}^{\infty} \left( -1 \right)^{n} \dfrac{1}{2^{2n} (n!)^{2}}  x^{2n} = \sum_{n = 0}^{\infty} \left( -1 \right)^{n} \dfrac{1}{(n!)^{2}} \left(\dfrac{x}{2}\right)^{2n},
\end{equation*}

where we have chosen $a_{0} = 1$.

Note that $a_{0} = 0$ for $J_{1}$ and $a_{1}$ is the seed from which the recurrence unrolls. Thus the recurrence for $J_{1}$ is

\begin{equation*}
a_{2n+1} = - \dfrac{a_{2n-3}}{(2n+1)^{2} - 1^{2}} = - \dfrac{a_{2n-3}}{(2n)^{2} + 1^{2} + 4n  - 1^{2}} = - \dfrac{a_{2n-3}}{2^{2}n(n + 1)}, \quad n \geq 2.
\end{equation*}

Thus

\begin{equation*}
J_{1}(x) = \sum_{n=0}^{\infty} a_{2n+1} x^{2n+1} = \sum_{n = 0}^{\infty} \left( -1 \right)^{n} \dfrac{a_{1}}{2^{2n}n!(n + 1)!} x^{2n+1}.
\end{equation*}

Rather unoriginally, we choose $a_{1} = \frac{1}{2}$ to write

\begin{equation*}
J_{1}(x) = \sum_{n = 0}^{\infty} \left( -1 \right)^{n} \dfrac{1}{2^{2n+1}n!(n + 1)!} x^{2n+1} = \sum_{n = 0}^{\infty} \left( -1 \right)^{n} \dfrac{1}{n!(n + 1)!} \left( \dfrac{x}{2} \right)^{2n+1}.
\end{equation*}

** SOLVED Problem 10.5.5
CLOSED: [2022-10-26 Wed 03:20]
:LOGBOOK:
CLOCK: [2022-10-26 Wed 01:10]--[2022-10-26 Wed 03:05] =>  1:55
:END:
*In quantum mechanics one defines a box as a region to which the particle is confined. Consider a square box in two dimensions of sides $L$ with its lower left corner at the origin. The allowed energies for the particle of mass $m$ are given by $E$, which is defined by the following eigenvalue equation:*

\begin{equation*}
- \dfrac{\hbar^2}{2m} \nabla^2 \psi = E \psi,
\end{equation*}

*and the boundary condition that $\psi = 0$ at the walls of the box. Show that the allowed energies are $E = \frac{\hbar^{2} \pi^{2} \left( n_{x}^{2} + n_{y}^{2} \right)}{2 m L^{2}}$, where $n$'s are positive integers. Find the lowest energy state in this box. Assuming the solution is rotationally invariant, show that the lowest energy state in a circular box of radius $a$ is $E = \frac{\hbar^{2} x_{0}^{2}}{2 m a^{2}}$, where $x_{0} \simeq 2.4$ is the first zero of $J_{0}(x)$.*

Starting with the two dimensional square box of sides with length $L$, the eigenvalue equation in cartesian coordinates after some rearrangement is

\begin{equation*}
\dfrac{\partial^{2} \psi}{\partial x^{2}} + \dfrac{\partial^{2} \psi}{\partial y^{2}} + \left(\dfrac{2 m E}{\hbar^{2}}\right) \psi = 0.
\end{equation*}

We /assume/ an ansatz of the form $\psi = X(x) Y(y)$

\begin{equation*}
Y \dfrac{\partial^{2} X}{\partial x^{2}} + X \dfrac{\partial^{2} Y}{\partial y^{2}} + \left( \dfrac{2mE}{\hbar^{2}} \right) X Y = 0,
\end{equation*}

which on dividing throughout by $X(x) Y(y)$ yields

\begin{equation*}
\dfrac{1}{X} \dfrac{\partial^{2} X}{\partial x^{2}} + \dfrac{1}{Y} \dfrac{\partial^{2} Y}{\partial y^{2}} + \left( \dfrac{2mE}{\hbar^{2}} \right) = 0.
\end{equation*}

We /assume/ our interest is in solutions for which

\begin{equation*}
\dfrac{1}{X} \dfrac{\partial^{2} X}{\partial x^{2}} = - k^{2},
\end{equation*}

\begin{equation*}
\dfrac{1}{Y} \dfrac{\partial^{2} Y}{\partial y^{2}} = - l^{2}.
\end{equation*}

The individual solutions, subject to vanishment at $x = y = 0$ and $x = y = L$ are

\begin{equation*}
X = A_{k} \sin k x, \quad Y = A_{l} \sin l y, \quad k L = n_{x} \pi, \quad l L = n_{x} \pi,
\end{equation*}

where $n_{x}$ and $n_{y}$ are (strictly) positive integers. Why "strictly"? Because $n_{x} = n_{y} = 0$ is the poor trivial solution that never gets anybody's interest. The full solution is the following superposition

\begin{equation*}
\psi = \sum_{n_{x} = 1}^{\infty} \sum_{n_{y} = 1}^{\infty} A_{n_{x} n_{y}} \sin \left( \dfrac{n_{x} \pi x}{L} \right) \sin \left( \dfrac{n_{y} \pi y}{L} \right).
\end{equation*}

Wait a second. How about I let one of them be zero and not the other? Sure, that works why not. The change is trivial though and here we will work with $n_{x}$ and $n_{y}$ stictly positive. Feeding this into the differential equation

\begin{equation*}
\dfrac{\hbar^{2}}{2m} \left( \dfrac{n_{x}^{2} \pi^{2}}{L^{2}} + \dfrac{n_{x}^{2} \pi^{2}}{L^{2}} \right) = E = \dfrac{\hbar^{2} \pi^{2} \left( n_{x}^{2} + n_{y}^{2} \right)}{2 m L^{2}},
\end{equation*}

same as demanded. The lowest energy state is when both $n_{x} = n_{y} = 1$ so that

\begin{equation*}
E = \dfrac{\hbar^{2} \pi^{2}}{m L^{2}}.
\end{equation*}

Moving on now to the circular box of radius $a$, we are on hunt for a rotationally invariant solution. Of course, we will use polar coordinates in which the differential equation takes the form

\begin{equation*}
- \dfrac{\hbar^{2}}{2m} \left( \dfrac{1}{r} \dfrac{\partial }{\partial r} \left( r \dfrac{\partial }{\partial r} \right) + \dfrac{1}{r^2} \dfrac{\partial^{2} }{\partial \phi^{2}} \right) \psi = E \psi.
\end{equation*}

Laplacian scares. Marching on, we /assume/ a solution of the form $\psi(r,\phi) = R(r) \Phi(\phi)$. Feeding in the ansatz

\begin{equation*}
\Phi \dfrac{1}{r} \dfrac{\partial}{\partial r} \left( r \dfrac{\partial R}{\partial r} \right) + \dfrac{R}{r^{2}} \dfrac{\partial^{2} \Phi}{\partial x^{2}} = - \dfrac{2 m E}{\hbar^{2}} R(r) \Phi \left( \phi \right),
\end{equation*}

dividing throughout by $R \Phi$ and rearranging we get

\begin{equation*}
\dfrac{1}{R r} \dfrac{\partial}{\partial r} \left( r \dfrac{\partial R}{\partial r} \right) + \dfrac{1}{\Phi r^{2}} \dfrac{\partial^{2} \Phi}{\partial x^{2}} + \dfrac{2 m E}{\hbar^{2}} = 0.
\end{equation*}

Can't be letting that $r^{2}$ causing trouble in the denominator so multiply throughout with $r^{2}$ to arrive at

\begin{equation*}
\left(\dfrac{r}{R} \dfrac{\partial}{\partial r} \left( r \dfrac{\partial R}{\partial r} \right) + \dfrac{2 m r^{2} E}{\hbar^{2}}\right) + \left(\dfrac{1}{\Phi} \dfrac{\partial^{2} \Phi}{\partial x^{2}}\right) = 0.
\end{equation*}

Separation of variables is complete. We further /assume/ our interest is in solutions for which the separated equation look like

\begin{equation*}
r^{2} R^{\prime \prime} + r R^{\prime} + \left( \dfrac{2mEr^{2}}{\hbar^{2}} - \nu^{2} \right)R = 0,
\end{equation*}

\begin{equation*}
\Phi^{\prime \prime} = - \nu^{2} \Phi.
\end{equation*}

The solution for $\Phi$ is

\begin{equation*}
\Phi = A_{\nu} \exp \left \lbrace i \nu \phi \right \rbrace + B_{\nu} \exp \left \lbrace - i \nu \phi  \right \rbrace.
\end{equation*}

Rotational invariance ties our hands into demanding that $\nu$ be an integer. Note that the ODE for $R$ with a change of scale $x = \left(\frac{r}{\hbar}\right) \sqrt{2mE}$ reduces to

\begin{equation*}
x^{2} \dfrac{d^{2} R}{dx^{2}} + x \dfrac{d R}{dx} + \left( x^{2} - \nu^{2} \right) = 0,
\end{equation*}

which we identify as the /Bessel equation/. The roots of the indicial equation are $s = \pm \nu$ but by invoking /regularity condition/ at $r = 0$ we discard the Neumann function solution corresponding to $s = - \nu$ and keep only the Bessel function of the first kind of order $\nu$,

\begin{equation*}
J_{\nu}(x) = \sum_{n=0}^{\infty} \dfrac{(-1)^{n}}{n! \Gamma \left( n + \nu + 1 \right) } \left( \dfrac{x}{2} \right)^{2n+\nu} = \sum_{n=0}^{\infty} \dfrac{(-1)^{n}}{n! \left( n + \nu\right)!} \left( \dfrac{x}{2} \right)^{2n+\nu}.
\end{equation*}

In the second equality we have used the fact that $\nu$ is required to be an integer. On substituting $x = \left( \frac{r}{\hbar} \right) \sqrt{2 m E}$ yields

\begin{equation*}
J_{\nu} \left(\sqrt{\frac{2mE}{\hbar^{2}}}r\right) = \sum_{n=0}^{\infty} \dfrac{(-1)^{n}}{n! \left( n + \nu\right)!} \left(r\sqrt{\dfrac{mE}{2 \hbar^{2}}}\right)^{2n+\nu},
\end{equation*}

The general solution is a superposition over the Bessel functions

\begin{equation*}
\psi(r, \phi) = \sum_{\nu} \left( A_{\nu} \exp \left \lbrace i \nu \phi  \right \rbrace + B_{\nu} \exp \left \lbrace - i \nu \phi  \right \rbrace \right) \sum_{n=0}^{\infty} \dfrac{(-1)^{n}}{n! \left( n + \nu\right)!} \left(r\sqrt{\dfrac{mE}{2 \hbar^{2}}}\right)^{2n+\nu}.
\end{equation*}

Now we invoke the /boundary conditions/ to demand that $r = a$ coincide with the roots (which we label with $x_{\nu j}$) of the Bessel function above. This demand translates to an equation for the energy levels $E$

\begin{equation*}
E = \dfrac{\hbar^{2} x_{\nu j}^{2}}{2 m a^{2}}.
\end{equation*}

The full solution becomes

\begin{equation*}
\psi(r, \phi) = \sum_{\nu} \left( A_{\nu} \exp \left \lbrace i \nu \phi  \right \rbrace + B_{\nu} \exp \left \lbrace - i \nu \phi  \right \rbrace \right) \sum_{n=0}^{\infty} \dfrac{(-1)^{n}}{n! \left( n + \nu\right)!} \left( \dfrac{rx_{\nu j}}{2a} \right)^{2n+\nu}.
\end{equation*}

The smallest possible value of  $x_{\nu j}$ is for the first zero $x_{0} \simeq 2.4$ of $J_{0}$. Thus the ground state energy is

\begin{equation*}
E = \dfrac{\hbar^{2} x_{0}^{2}}{2 m a^{2}} \simeq 2.4 \dfrac{\hbar^{2}}{2 m a^{2}},
\end{equation*}

as demanded and we are done.

** TOSOLVE Problem 10.5.6
*Consider a bar of length $L$. Given $u(x, 0)$ rises linearly from $0$ at the left end to $U$ at the middle and comes down linearly to zero at the right end, find $u(x, t)$ for all future times. (The ends are jammed into ice trays that hold them fixed at zero degrees.) Note that the sines are not normalized to unity in $(10.5.57)$.*
** TOSOLVE Problem 10.5.7
*Consider once again the initial temperature given in problem $(10.5.6)$ (isosceles triangle of height $U$) but assume that at $t = 0$ the reservoirs connected at the left and right ends are at $u = 0$ and $u = 2 U$ respectively. Find the temperature for all future times. Remember that the sines are not normalized to unity in $(10.5.66)$.*
** TOSOLVE Problem 10.5.8
*Consider the problem of the semi-infinite strip of width $L$ given above. (i) Given $u(x,0) = x$, and $u = 0$ on the other three sides, find $u$ at all interior points. (ii) Repeat with $u(x, 0) = \cos \left( x \pi / L \right)$. Hint: In integrating trigonometric functions, write them in terms of exponentials, use $\sin x = \Im \exp \left \lbrace i x  \right \rbrace$.*
** TOSOLVE Problem 10.5.9
*Consider a sheet of width $a$ along the $x$ axis and $b$ along the $y$-axis, with its lower left corner at the origin. You are given that the edge at $y = b$ is at $u (x, b) = 100$ and that the other edges are at zero degrees. Find the temperature in the interior. Here are the suggestions: Use separation of variables, in the $y$-direction admits a superposition of hyperbolic functions and finally kill the $\cosh y$ part using the conditions at $y = 0$ to reach an expansion of the form*

\begin{equation*}
u(x, y) = \sum A_{n} \sin \left( n \pi x / a \right) \sinh \left(  n \pi y /b \right).
\end{equation*}

*Set $y = b$, use the boundary conditions and solve for $A_{n}$.*
** TOSOLVE Problem 10.5.10
*So far we have only considered cases where the sheet has nonzero temperature on only one of its edges. What about a plate which has $T \neq 0$ on all four edges? We simply solve four problems in each of which $T \neq 0$ on just one edge and then we add these solutions. It is clear that the sum has the right boundary conditions and satisfies the heat equations due to linearity.*
** TOSOLVE Problem 10.5.11
*Derive the above equation by trying a series solution. Only one term will survive. Notice that the equation has scale invariance: it is unaffected by $r \to \alpha r$. This means that if $R(r)$ is a solution, so is $R(\alpha r)$. Pure powers have this property. Analyze the special case $m = 0$ separately. Show that the general solution is $(A + B \ln r) (C + D \theta)$. In other words, besides $r^{0}$, $\ln r$ is a possible solution for the radial function $R(r)$ and likewise, the angular function for $m = 0$ is not just $\exp \left \lbrace 0  \right \rbrace$ but of the form $A + B \theta$. Why must we choose $B = D = 0$?*
** TOSOLVE Problem 10.5.12
*Do the sum over the geometric series to obtain the above formula.*
** TOSOLVE Problem 10.5.13
*Provide the missing steps between $(10.5.81)$ and the Poisson formula.*
** TOSOLVE Problem 10.6.1
*Provide the missing steps involving contour integrations that link $(10.6.18)$ to $(10.6.19)$. First write the integral in question as the imaginary part of an integral with $\exp \left \lbrace i k r  \right \rbrace$ instead of $\sin kr$. Note that for large $k$, the integrand behaves as $\frac{\exp \left \lbrace i k r \right \rbrace}{kr}$. Argue that on a semi-circle in the upper-half-plane of radius $R \to \infty$, the exponential kills the integrand except in an infinitesimal angular range near the ral axis which makes an infinitesimal contribution.*
