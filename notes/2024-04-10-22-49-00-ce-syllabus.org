:PROPERTIES:
:ID:       ecf10cac-8fe8-417c-84de-c4a85d17c464
:END:
#+TITLE: CE syllabus
#+FILETAGS: :fleeting: :phd:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org
#+BEGIN: clocktable :maxlevel 2 :scope nil :emphasize nil
#+CAPTION: Clock summary at [2024-06-24 Mon 03:53]
| Headline                                  | Time      |       |
|-------------------------------------------+-----------+-------|
| *Total time*                              | *5d 4:03* |       |
|-------------------------------------------+-----------+-------|
| PH 202: Statistical Mechanics             | 18:13     |       |
| \_  Legendre transformations              |           |  0:42 |
| \_  Probability                           |           | 10:20 |
| \_  Thermodynamics                        |           |  1:30 |
| \_  Classical statistical mechanics       |           |  4:48 |
| \_  Ising model                           |           |  0:53 |
| E0 270: Machine Learning                  | 1d 18:13  |       |
| \_  Probability distributions             |           |  6:55 |
| \_  Maximum likelihood estimation (MLE)   |           |  0:49 |
| \_  Gradient descent (GD)                 |           |  1:08 |
| \_  Linear regression                     |           |  2:37 |
| \_  Logistic regression                   |           |  0:17 |
| \_  Perceptrons                           |           |  0:35 |
| \_  Artificial neural networks (ANN)      |           |  3:06 |
| \_  Markov random fields (MRFs)           |           |  3:10 |
| \_  Markov chain Monte Carlo (MCMC)       |           |  0:21 |
| \_  Restricted Boltzmann machines (RBMs)  |           |  9:59 |
| \_  Expectation maximization (EM)         |           | 11:02 |
| \_  Mixture models                        |           |  2:14 |
| PH 354: Computational Physics             | 2d 15:37  |       |
| \_  Machine Representation, Precision,... |           |  6:36 |
| \_  Roots of Equations                    |           |  8:56 |
| \_  Fourier Methods                       |           |  6:10 |
| \_  Numerical Linear Algebra              |           |  6:33 |
| \_  Random Numbers and Monte Carlo        |           | 10:36 |
| \_  Quadrature                            |           | 11:03 |
| \_  Ordinary Differential Equations       |           | 13:43 |
#+END
* PH 202: Statistical Mechanics
** Legendre transformations
:LOGBOOK:
CLOCK: [2024-06-18 Tue 06:31]--[2024-06-18 Tue 06:48] =>  0:17
CLOCK: [2024-06-14 Fri 16:33]--[2024-06-14 Fri 16:58] =>  0:25
:END:
*** Preliminaries
#+NAME: Cartesian product
#+begin_definition latex
Given sets \(X\) and \(Y\), the Cartesian product \(X \times Y\) is defined as \(\{(x, y) \mid x \in X\) and \(y \in Y\}\).
#+end_definition
#+NAME: Binary relation
#+begin_definition latex
A binary relation \(R\) over sets \(X\) and \(Y\) is a subset of its Cartesian product \(X \times Y\).
#+end_definition
In a binary relation, the order of the elements is important; if \(x \neq y\) then \(y \, R \, x\) can be true or false independently of \(x \, R \, y\). As an example, 3 divides 9 , but 9 does not divide 3.
#+NAME: Homogeneous relation
#+begin_definition latex
Let \(R\) be a binary relation over sets \(X\) and \(Y\). When \(X=Y\), the binary relation \( R \) is called a homogeneous relation or endorelation.
#+end_definition
#+NAME: Partial order
#+begin_definition latex
A partial order, is a homogeneous relation \(\leq\) on a set \(P\) that is reflexive, anti-symmetric, and transitive. That is, for all \(a, b, c \in P\), it must satisfy:
1. Reflexivity: \(a \leq a\), i.e. every element is related to itself.
2. Anti-symmetry: if \(a \leq b\) and \(b \leq a\) then \(a=b\), i.e. no two distinct elements precede each other.
3. Transitivity: if \(a \leq b\) and \(b \leq c\) then \(a \leq c\).
#+end_definition
#+NAME: Upper bound
#+begin_definition latex
The upper bound of a subset \(S\) of a partially ordered set \((P, \leq)\) is an element \(b\) of \(P\) such that \(b \geq x\) for all \(x \in S\).
#+end_definition
#+NAME: Lower bound
#+begin_definition latex
Let \(S\) be a subset of a partially ordered set \((P, \leq)\). An upper bound \(b\) of \(S\) is called a supremum (or least upper bound, or join) of \(S\) if for all upper bounds \(z\) of \(S\) in \(P, z \geq b\), i.e., \(b\) is less than any other upper bound.
#+end_definition
+ Given a set \(S\) of real numbers, the supremum or least upper bound of \(S\) is the smallest real number that is greater than or equal to every number in \(S\).
+ The supremum may or may not be an element of the set \(S\). For example, consider the set \(S=\{x \in\) \(\mathbb{R} \mid x<3\}\). \(\sup S=3\), even though it is not an element of \(S\).
+ If the supremum is an element of the set, it is also called the maximum. For example, consider the set \(S=\) \(\{1,2,3\}\). \(\sup S=3\), which is also the maximum of \(S\).
#+NAME: Convex set
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Let \(S\) be a vector space over the real numbers. A subset \(C\) of \(S\) is convex if, for all \(x\) and \(y\) in \(C\), the line segment connecting \(x\) and \(y\) is included in \(C\) i.e., \((1-t) x+ ty\) belongs to \(C\) for all \(x, y\) in \(C\) and \(t\) in the interval \([0,1]\).
#+end_definition
#+begin_definition latex
#+NAME: Convex function
Let \(X\) be a convex subset of a real vector space and let \(f: X \rightarrow \mathbb{R}\) be a function.
Then \(f\) is called convex if and only if any of the following equivalent conditions hold:
1. For all \(0 \leq t \leq 1\) and all \(x_1, x_2 \in X\): \( f\left(t x_1+(1-t) x_2\right) \leq t f\left(x_1\right)+(1-t) f\left(x_2\right) \)
2. For all \(0<t<1\) and all \(x_1, x_2 \in X\) such that \(x_1 \neq x_2\) : \( f\left(t x_1+(1-t) x_2\right) \leq t f\left(x_1\right)+(1-t) f\left(x_2\right) \)
#+end_definition
#+BEGIN_COMMENT
The right hand side represents the straight line between \(\left(x_1, f\left(x_1\right)\right)\) and \(\left(x_2, f\left(x_2\right)\right)\) in the graph of \(f\) as a function of \(t\); increasing \(t\) from 0 to 1 or decreasing \(t\) from 1 to 0 sweeps this line. Similarly, the argument of the function \(f\) in the left hand side represents the straight line between \(x_1\) and \(x_2\) in \(X\) or the \(x\)-axis of the graph of \(f\). So, this condition requires that the straight line between any pair of points on the curve of \(f\) to be above or just meets the graph.
#+END_COMMENT
#+BEGIN_COMMENT
The difference of this second condition with respect to the first condition above is that this condition does not include the intersection points (for example, \(\left(x_1, f\left(x_1\right)\right)\) and \(\left(x_2, f\left(x_2\right)\right)\) ) between the straight line passing through a pair of points on the curve of \(f\) (the straight line is represented by the right hand side of this condition) and the curve of \(f\); the first condition includes the intersection points as it becomes \(f\left(x_1\right) \leq f\left(x_1\right)\) or \(f\left(x_2\right) \leq f\left(x_2\right)\) at \(t=0\) or 1 , or \(x_1=x_2\). In fact, the intersection points do not need to be considered in a condition of convex using \( f\left(t x_1+(1-t) x_2\right) \leq t f\left(x_1\right)+(1-t) f\left(x_2\right) \) because \(f\left(x_1\right) \leq f\left(x_1\right)\) and \(f\left(x_2\right) \leq f\left(x_2\right)\) are always true (so not useful to be a part of a condition).
#+END_COMMENT
*** Motivation
Suppose \(f\) is a function of \(x\). The variations of \( f \) satisfy
\begin{align*}
\mathrm{d} f=\frac{\mathrm{d} f}{\mathrm{~d} x} \mathrm{~d} x .
\end{align*}
\( x \) is the /independent variable/ above. Performing a *Legendre transformation* on this function means that we take \(y=\mathrm{D}_x f\) as the /independent variable/, so that the above expression can be written as
\begin{align*}
\mathrm{d} f = y \mathrm{~d} x.
\end{align*}
Using the chain rule of differentiation \(\mathrm{d}(u v)=u \mathrm{~d} v+v \mathrm{~d} u\)
\begin{align*}
\mathrm{d}(x y-f)=x \mathrm{~d} y.
\end{align*}
If we define \(g = x y - f\), we have \(\mathrm{d} g = x \mathrm{~d} y\), which means
\begin{align*}
x = \mathrm{D}_y g.
\end{align*}
Similarly, suppose \(f\) is a function of \(n\) variables \(x_1, x_2, \cdots, x_n\). Then we can perform the Legendre transformation on each one or several variables: we have
\begin{align*}
\mathrm{d} f=y_1 \mathrm{~d} x_1+y_2 \mathrm{~d} x_2+\cdots+y_n \mathrm{~d} x_n
\end{align*}
where \(y_i=\partial_{x_i} f\) for \( i = 1, \ldots, n \). If we want to perform Legendre transformation on, e.g. \(x_1\), then we take \(y_1 = \partial_{x_1} f\) together with \(x_2, \cdots, x_n\) as independent variables, and with the chain rule we have
\begin{align*}
\mathrm{d}\left(f-x_1 y_1\right)=-x_1 \mathrm{~d} y_1+y_2 \mathrm{~d} x_2+\cdots+y_n \mathrm{~d} x_n .
\end{align*}
so for function \(h\left(y_1, x_2, \cdots, x_n\right)=f\left(x_1, x_2, \cdots, x_n\right)-x_1 y_1\), we have
\begin{align*}
(\partial_{y_1} h) = -x_1, \quad (\partial_{x_2} h) = y_2, \quad \cdots, \quad (\partial_{x_n} h) = y_n .
\end{align*}
We can also do this transformation for variables \(x_2, \cdots, x_n\). If we do it to all the variables, then we have
\begin{align*}
\mathrm{d} h = -x_1 \mathrm{~d} y_1 - x_2 \mathrm{~d} y_2 - \cdots - x_n \mathrm{~d} y_n \text { where } h = f - x_1 y_1 - x_2 y_2 - \cdots - x_n y_n .
\end{align*}
*** Definitions
#+NAME: Legendre transform of a function of single variable
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Let \(I \subset \mathbb{R}\) be an interval, and \(f: I \rightarrow \mathbb{R}\) a convex function; then the Legendre transform of \(f\) is the function \(g: I^* \rightarrow \mathbb{R}\) defined by
\begin{align*}
g\left(y\right)=\sup _{x \in I}\left(y x-f(x)\right), \quad I^*=\left\{y \in \mathbb{R}: g\left(y\right)<\infty\right\}
\end{align*}
where \( \sup \) denotes the supremum over \(I\), e.g., \(x\) in \(I\) is chosen such that \(y x-f(x)\) is maximized at each \(y\), or \(y\) is such that \(y x-f(x)\) as a bounded value throughout \(x\) exists (e.g., when \(f(x)\) is a linear function).
#+end_definition
+ The transform is always well-defined when \(f(x)\) is a convex function.
+ The definition above requires \(y x-f(x)\) to be bounded from above in \(I\) in order for the supremum to exist.
#+NAME: Legendre transform of a function of multiple variables
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Let \(X \subset \mathbb{R}^n\) be a convex set, and \(f: X \rightarrow \mathbb{R}\) be a convex function; then the Legendre transform of \( f \) is a function \(g: Y \rightarrow \mathbb{R}\) that has domain
\begin{align*}
Y=\left\{y \in \mathbb{R}^n: \sup _{x \in X}\left(\left\langle y ,\, x\right\rangle-f(x)\right)<\infty\right\}
\end{align*}
and is defined by
\begin{align*}
g\left(y\right)=\sup _{x \in X}\left(\left\langle y ,\, x\right\rangle-f(x)\right), \quad y \in Y,
\end{align*}
where \(\left\langle y ,\, x\right\rangle\) denotes the dot product of \(y\) and \(x\).
#+end_definition
*** Properties
**** Legendre transform of convex function
#+NAME: Legendre transform of convex function
#+begin_theorem latex
The Legendre transform of a convex function, of which double derivative values are all positive, is also a convex function of which double derivative values are all positive.
#+end_theorem
#+NAME: Legendre transform of convex function
#+begin_proof latex
For a fixed \(y\), let \(\bar{x}\) maximize or make the function \(p x-f(x)\) bounded over \(x\).
Then the Legendre transformation of \(f\) is \(g(y)=y \bar{x}-f(\bar{x})\). Note that \(\bar{x}\) depends on \(y\).
\(\mathrm{D}_x(y x-f(x))=p-\mathrm{D}_x f(x)=0\) yields \( \mathrm{D}_x f (\bar{x})=y \). Assuming \(h \equiv\left(\mathrm{D}_x f\right)^{-1}\) exists, \(\mathrm{D}_x f (h(y))=y\) or \(\bar{x}=h(y)\).
\(h\) is differentiable \( \mathrm{D}_y h(y) = 1/\mathrm{D}_x^2 f(h(y)) \). Thus, \(g(y)=y h(y)-f(h(y))\) is also differentiable.
We have \( \mathrm{D}_y g = h(y) + [y-\mathrm{D}_x f (h(y))] \cdot \mathrm{D}_y h (y) = h (y) \) where we have used \(\bar{x}=h(y)\).
It follows that \( \mathrm{D}_y^2 g = \mathrm{D}_y h(y) = 1/\mathrm{D}_x^2 f (h(y)) > 0 \). Thus \(g\) is convex with its double derivatives are all positive.
#+end_proof
**** Identities
#+NAME: Identities
#+begin_corollary latex
Let \( g(y) \) be the Legendre transform of \( f(x) \). Suppose that \(h = (\mathrm{D}_x f)^{-1}\) exists. The following identities hold:
1) \(\mathrm{D}_x f(\bar{x})= y\),
2) \(\bar{x}=h(y)\),
3) \(\mathrm{D}_y g(y)=h(y)\).
#+end_corollary
**** The Legendre transformation is an involution transform.
#+NAME: Legendre transform is an involution transform
#+begin_theorem latex
If \( g(y) \) is the Legendre transform of \( f(x) \), then the Legendre transform of \( g(y) \) is \( f(x) \).
#+end_theorem
#+NAME: Legendre transform is an involution transform
#+begin_proof latex
By using the identities \(\mathrm{D}_x f(\bar{x})= y\), \(\bar{x}=h(y)\), and \(\mathrm{D}_y g(y)=h(y)\)
\begin{align*}
g{\ast}(z) & = (z \cdot \bar{y} - g(\bar{y})) \rvert_{\mathrm{D}_y g (\bar{h}) = z} \\
& =h(\bar{y}) \cdot \bar{y}-g(\bar{y}) \\
& =h(\bar{y}) \cdot \bar{y}-(\bar{y} h(\bar{y})-f(h(\bar{y}))) \\
& =f(h(\bar{y})) \\
& =f(z)
\end{align*}
#+end_proof
*** Examples
**** Example 1: \(f(x)=\exp(x)\).
\(f(x)=\exp(x)\) is a convex continuous differential function on the open interval \( \mathbb{R} \). By definition, the Legendre transform is
\begin{align*}
g\left(y\right)=\sup _{x \in \mathbb{R}}\left(y x-\exp(x)\right), \quad y \in I^*
\end{align*}
where \(I^*\) remains to be determined. To evaluate the supremum, compute the derivative of \(y x-\exp(x)\) with respect to \(x\) and set equal to zero
\begin{align*}
\mathrm{D}_x \left(y x-\exp(x)\right)=y-\exp(x)=0 .
\end{align*}
The second derivative \( \mathrm{D}_x^2 \left(y x-\exp(x)\right) = -\exp(x)\) is negative everywhere, so the maximal value is achieved at \(x=\ln \left(y\right)\). Therefore
\begin{align*}
g\left(y\right)=y \ln \left(y\right)-\exp(\ln \left(y\right))=y \ln (y) - y
\end{align*}
and has domain \(I^*=(0, \infty)\). Notice that while \( f(x) \) is defined over the domain \( \mathbb{R} \), \( g(y) \) is defined over the domain \( (0,\,\infty) \).
**** Example 2: \(f(x)=k x^2 / 2, \qquad k > 0\)
\(f(x)=k x^2/2\) is a convex continuous differential function on the open interval \( \mathbb{R} \). By definition, the Legendre transform is
\begin{align*}
g\left(y\right)=\sup _{x \in \mathbb{R}}\left(y x-k x^2/2\right), \quad y \in I^*
\end{align*}
where \(I^*\) remains to be determined. To evaluate the supremum, compute the derivative of \(y x- k x^2/2\) with respect to \(x\) and set equal to zero
\begin{align*}
\mathrm{D}_x \left(y x- k x^2 /2\right) = y - k x = 0.
\end{align*}
The second derivative \( \mathrm{D}_x^2 \left(y x-k x^2/2\right) = - k\) is negative everywhere, so the maximal value is achieved at \(x=y/k\).
Therefore
\begin{align*}
g\left(y\right)=y \cdot y/k - k \, (y/k)^2 /2 = y^2/2k
\end{align*}
and has domain \(I^*= \mathbb{R}\).
**** Example 3: \(f(x)=x^p / p, \qquad x > 0, \, p \geq 2\)
\(f(x)= x^p/p\) is a convex continuous differential function on the open interval \((0,\,\infty)\). By definition, the Legendre transform is
\begin{align*}
g\left(y\right)=\sup _{x \in (0,\,\infty)}\left(y x- x^p/p\right), \quad y \in I^*
\end{align*}
where \(I^*\) remains to be determined. With \( y \) fixed, to evaluate the supremum, compute the derivative of \(y x- x^p/p\) with respect to \(x\) and set equal to zero
\begin{align*}
\mathrm{D}_x \left(y x- x^p /p\right) = y - x^{p-1} = 0.
\end{align*}
The second derivative \( \mathrm{D}_x^2 \left(y x- x^p/p\right) = - (p-1) x^{p-2}\) is negative for \( x > 0 \) (recall \( p \geq 2 \)), so the maximal value is achieved at \(x=y^{1/(p-1)}\). It follows that \( I^{\ast} = (0,\, \infty) \). Therefore
\begin{align*}
g\left(y\right)=y \cdot y^{1/(p-1)} - y^{p/(p-1)} / p = \frac{p-1}{p} \cdot y^{p/(p-1)} = y^q / q
\end{align*}
where \( q = p/(p-1) \) or \( 1/p + 1/q = 1 \). \( g(y) \) and has domain \(I^*= (0, \infty)\).
**** Example 4: \(f(x)=x^2\) for \(x \in I=[2,3]\)
\(f(x)=x^2\) is a convex continuous differential function on the closed interval  \(I = [2,\,3]\). With \( y \) fixed, \( yx - x^2 \) is continuous in the closed interval \(I\), hence \( yx - x^2 \) has a supremum in \(I\) as a consequence of the /least upper bound property/. It follows that \( g(y) < \infty \) for all \( y \in \mathbb{R} \) i.e.,  \(I^*= \mathbb{R}\). By definition, the Legendre transform is
\begin{align*}
g\left(y\right)=\sup_{x \in [2,3]}\left(y x- x^2\right), \quad y \in I^*
\end{align*}
To evaluate the supremum, compute the derivative of \(y x- x^2\) with respect to \(x\) and set equal to zero
\begin{align*}
\mathrm{D}_x \left(y x- x^2\right) = y - 2 x = 0.
\end{align*}
The second derivative \( \mathrm{D}_x^2 \left(y x- x^2\right) = - 2\) is negative for \(x \in I=[2,3]\), so the maximal value is achieved at \(x=y/2\) if and only if \(4 \leq y \leq 6\). We have
\[ \sup_{x \in [2,3]}\left(y x- x^2\right) = 2 y - 4, \qquad \text{for} \quad y < 4, \]
and
\[ \sup_{x \in [2,3]}\left(y x- x^2\right) = 3 y - 9, \qquad \text{for} \quad y > 6. \]
Therefore
\begin{align*}
g \left(y\right) =
\begin{cases}
2 y-4, & y<4 \\
y^2/4, & 4 \leq y \leq 6 \\
3 y-9, & y>6
\end{cases}.
\end{align*}
**** Example 5: \(f(x)=c x\)
\(f(x)=c x\) is a is a convex continuous differential function on the open interval \( \mathbb{R} \). By definition, the Legendre transform is
\begin{align*}
g\left(y\right)=\sup _{x \in \mathbb{R}}\left(y x- c x \right), \quad y \in I^*
\end{align*}
where \(I^*\) remains to be determined. By definition, the Legendre transform requires the existence of the supremum. Clearly \(y x-f(x)=\left(y-c\right) x\) does not have a supremum for \(x \in \mathbb{R}\) unless \(y-c=0\).  Hence \(g\) is defined on \(I^*=\{c\}\) and \(g (c) = 0\). We can do the inverse Legendre transform to recover \( f(x) = cx \): with \( x \) fixed, \(xy-g(y)\) has a supremum for \(y \in \{c\}\), hence \(I=\mathbb{R}\). Therefore
\begin{align*}
f(x) = \sup _{y \in\{c\}}\left(x y - g(y) \right) = c x, \qquad x \in \mathbb{R}.
\end{align*}
**** Example 6: \(f(x)=|x|\)
\(f(x)=|x|\) is a is a convex continuous function that is *not* everywhere differentiable. By definition, the Legendre transform is
\begin{align*}
g(y) = \sup_{x \in \mathbb{R}} \left(yx - |x| \right), \qquad y \in I^{\ast}
\end{align*}
where \(I^*\) remains to be determined. By definition, the Legendre transform requires the existence of the supremum. Clearly \(yx - |x|\) does not have a supremum for \( x \in \mathbb{R} \) unless \( y \in [-1,\,1] \). Hence \( g \) is defined on \( I^{\ast} = [-1,\,1] \) and \( g(y) = 0 \) for \( y \in I^{\ast} \). We can do the inverse Legendre transform to recover \( f(x) = \lvert x \rvert \): with \( x \) fixed, \(xy-g(y)\) has a supremum for \(y \in [-1,\,1]\), hence \(I=\mathbb{R}\). Therefore
\begin{align*}
f(x) = \sup _{y \in [-1, \, 1]}\left(x y - g(y) \right) =
\begin{cases}
x, & x \geq 0 \\
-x, & x < 0
\end{cases}.
\end{align*}
**** Example 7: \( f(\mathbf{x})=\langle \mathbf{x} ,\, A \mathbf{x} \rangle + c\) for \(\mathbf{x} \in \mathbb{R}^n\) and \( A \) is positive-definite.
\(f(\mathbf{x}) = \langle \mathbf{x},\,  A \mathbf{x} \rangle + c\) is a convex function (follows from the positive-definiteness) of the matrix \( A \)) (\(\left\langle y ,\, x\right\rangle\) denotes the inner product of \(y\) and \(x\)). By definition, the Legendre transform is
\begin{align*}
g\left(y\right)=\sup_{\mathbf{x} \in X}\left(\left\langle y, \ x\right\rangle-\langle \mathbf{x} ,\, A \mathbf{x} \rangle - c\right), \quad y \in Y,
\end{align*}
where \(Y\) remains to be determined. To evaluate the supremum, compute the gradient of \(\left\langle y ,\, x\right\rangle-\langle \mathbf{x} ,\, A \mathbf{x} \rangle - c\) with respect to \(\mathbf{x}\) and set equal to zero
\begin{align*}
\nabla_{\mathbf{x}} \left(\left\langle y ,\, x\right\rangle-\langle \mathbf{x} ,\, A \mathbf{x} \rangle - c\right) = \mathbf{y} - 2 A \mathbf{x} = \mathbf{0}.
\end{align*}
The Hessian \( \nabla_{\mathbf{x}}^{2} \left(\left\langle y ,\, x\right\rangle-\langle \mathbf{x} ,\, A \mathbf{x} \rangle - c\right) = - 2 A\) is negative everywhere, so the maximal value is achieved at \(\mathbf{x} = A^{-1} \mathbf{y}/2\) where \( A^{-1} \) is the inverse matrix of \( A \) (a positive-definite matrix is invertible). Therefore
\begin{align*}
g(\mathbf{y})=\frac{1}{4}\left\langle \mathbf{y}, A^{-1} \mathbf{y}\right\rangle-c.
\end{align*}
and has domain \(Y= \mathbb{R}^{n}\).
** Probability
:LOGBOOK:
CLOCK: [2024-06-18 Tue 05:55]--[2024-06-18 Tue 06:31] =>  0:36
CLOCK: [2024-06-17 Mon 20:50]--[2024-06-18 Tue 05:10] =>  8:20
CLOCK: [2024-06-17 Mon 19:54]--[2024-06-17 Mon 20:49] =>  0:55
CLOCK: [2024-06-14 Fri 19:32]--[2024-06-14 Fri 20:01] =>  0:29
:END:
*** General definitions
#+NAME: Random variable
#+begin_definition latex
A random variable \(x\) is defined as a variable that for each observation randomly takes a value from among a set of finite or countably infinite possible outcomes \(\mathcal{S} \equiv\left\{x_{1}, x_{2}, \cdots\right\}\).
The outcomes may be discrete as in the case of a coin toss, \(S_{\text {coin }}=\{\) head, tail \(\}\), or a dice throw, \(\mathcal{S}_{\text {dice }}=\{1,2,3,4,5,6\}\), or continuous as for the velocity of a particle in a gas, \(\mathcal{S}_{\vec{v}}=\left\{-\infty<v_{x}, v_{y}, v_{z}<\infty\right\}\), or the energy of an electron in a metal at zero temperature, \(\mathcal{S}_{\epsilon}=\left\{0 \leq \epsilon \leq \epsilon_{F}\right\}\).
#+end_definition
#+NAME: Event
#+begin_definition latex
Let \( \mathcal{S} \) denote the set of outcomes associated with a random variable \( x \). An event \( E \) is any subset of the outcomes, i.e., \(E \subset \mathcal{S}\).
#+end_definition
#+NAME: Probability
#+begin_definition latex
Let \( \mathcal{S} \) denote the set of outcomes associated with a random variable \( x \). Given the set \( \mathcal{S} \), let \( p \) be a function that maps all possible event \( E \) to a real number \(p(E) \in \mathbb{R}\). The function is said to give the probability of the event \( E \), for example \(p_{\text {dice }}(\{1\})=1 / 6\) or \(p_{\text {dice }}(\{1,3\})=1 / 3\), if the following hold for all \( E \):
1) Positivity. \(p(E) \geq 0\), that is, all probabilities must be real and non-negative.
2) Additivity. If \(A\) and \(B\) are disconnected events, i.e., \( A \cap B = \varnothing \), then \(p(A \text{ or } B)=p(A)+p(B)\).
3) Normalization. \(p(\mathcal{S})=1\), that is, the random variable must have some outcome in \(\mathcal{S}\).
#+end_definition
#+NAME: Objective probability
#+begin_definition latex
Objective probabilities refer to the relative frequency of the occurrence of an outcome across several observations of the value of the random variable. If the random process that generates the values assumed by the random variable is repeated \(N\) times, and the event \(A\) occurs \(N_{A}\) times, then
\[
p(A)=\lim _{N \rightarrow \infty} \frac{N_{A}}{N}.
\]
For example, a series of \(N=100,200,300\) throws of a dice may result in \(N_{1}=19,30,48\) occurrences of 1 . The ratios \(0.19,0.15,0.16\) provide an increasingly more reliable estimate of the probability \(p_{\text {dice }}(\{1\})\).
#+end_definition
#+NAME: Subjective probability
#+begin_definition latex
Subjective probabilities provide a theoretical estimate based on the uncertainties related to lack of precise knowledge of outcomes.
For example, the assessment \(p_{\text {dice }}(\{1\})=1 / 6\) is based on the knowledge that there are six possible outcomes to a dice throw, and that in the absence of any prior reason to believe that the dice is biased, all six are equally likely.
#+end_definition
All assignments of probability in statistical mechanics are subjectively based. The consequences of such subjective assignments of probability have to be checked against measurements, and they may need to be modified as more information about the outcomes becomes available.
**** One random variable
As the properties of a discrete random variable are rather well known, here we focus on continuous random variables, which are more relevant to our purposes. Consider a random variable \(x\), whose outcomes are real numbers, that is, \(\mathcal{S}_{x}=\{-\infty<x<\infty\}\).
***** Definitions
****** Cumulative probability function (CPF)
#+NAME: Cumulative probability function (CPF)
#+begin_definition latex
The cumulative probability function (CPF) \(P(x)\) is the probability of an outcome with any value less than \(x\), that is, \(P(x)=\operatorname{prob}(E \in (-\infty,\,x])\). \(P(x)\) must be a monotonically increasing function of \(x\), with \(P(-\infty)=0\) and \(P(+\infty)=1\).
#+end_definition
#+ATTR_HTML: :width 300px
[[file:~/.local/images/kardar-spop-2.1.jpg]]
#+CAPTION: A typical cumulative probability function.
****** Probability density function (PDF)
#+NAME: Probability density function (PDF)
#+begin_definition latex
The probability density function (PDF) \( p(x) \) is defined by 
\[
\mathrm{d}P(x) = \mathrm{D}_{x} P(x) \, \mathrm{d} x \equiv p(x) \, \mathrm{d}x \Longrightarrow p(x) = \mathrm{D}_{x} P(x).
\]
#+end_definition
#+begin_corollary latex
The PCF and PDF are related by \(P(x) = \int_{-\infty}^{x} p(x) \, \mathrm{d}x \).
#+end_corollary
#+begin_proof latex
This follows immediately from the definition of the CPF and the PDF
\[
P(x) = \operatorname{prob} (E \in[-\infty, \, x]) = \int_{-\infty}^{x} \mathrm{D}_{x} P(x) \, \mathrm{d}x = \int_{-\infty}^{x} p(x) \, \mathrm{d}x.
\]
#+end_proof
#+begin_corollary latex
The probability of the event \( E \in (a, \, b] \) is given by \(p(E) = \int_{a}^{b} p(x)\, \mathrm{d}x = P(b) - P(a)\).
#+end_corollary
#+begin_proof latex
Consider the probabilities \( \operatorname{prob}(E \in (-\infty, \, a]) \) and \( \operatorname{prob}(E \in (a, \, b]) \). By definition, we have 
\[\operatorname{prob} (E \in (-\infty, \, a] \text{ or } E \in (a, \, b]) = \operatorname{prob}(E \in (-\infty, \, a]) + \operatorname{prob}(E \in (a, \, b]).
\]
The event \( E \in (-\infty, \, a] \text{ or } E \in (a, \, b]) \) simplifies to \( E \in (-\infty, \, b] \) so that 
\[
\operatorname{prob}(E \in (a, \, b]) = \operatorname{prob} (E \in (-\infty, \, b)) - \operatorname{prob}(E \in (-\infty, \, a]).
\]
Since \( P(x) = \int_{-\infty}^{x} p(x) \, \mathrm{d}x = \operatorname{prob} (E \in (-\infty, \, x))\), we have
\[
\operatorname{prob}(E \in (a, \, b]) = \int_{-\infty}^{b} p(x) \, \mathrm{d}x - \int_{-\infty}^{a} p(x) \, \mathrm{d}x = \int_{a}^{b} p(x) \, \mathrm{d}x.
\]
It is easy to verify that \( p(E) = \int_{a}^{b} p(x) \, \mathrm{d}x \) is a probability. By definition, \( P(x) \) is a monotonically increasing function of \( x \); we must have \( \mathrm{D}_{x} P(x) = p(x) \geq 0 \) for all \( x \). Now, consider two arbitrary events \( A \in [a,\, a + \Delta a] \) and \( B \in [b,\, b + \Delta b] \). Further suppose that \( A \cap B = \varnothing \), i.e., the intervals \( (a, \, a+ \Delta a] \) and \( (b, \, b + \Delta b] \) are non-overlapping. By definition \(\operatorname{prob} (A \text{ or } B) = \operatorname{prob}(A) + \operatorname{prob}(B)\). Since \( \int_{a}^{b} p(x) \, \mathrm{d}x = \operatorname{prob} (E \in (a, \, b])\), we have \(p(A \text{ or } B) = \int_{a}^{a + \Delta a} p(x) \mathrm{d}x + \int_{b}^{b + \Delta b} p(x) \mathrm{d}x = p(A) + p(B) \). Finally, since \(P(x) = \int_{-\infty}^{x} p(x) \, \mathrm{d}x  \), it follows that \(\int_{-\infty}^{\infty} p(x) \, \mathrm{d} x = P(\infty) = 1\). But \(P(\infty) = \operatorname{prob} (E \in(-\infty, \, \infty)) =  \operatorname{prob}(\mathcal{S})\). It follows that \(\operatorname{prob} (\mathcal{S})  = 1 \). Thus the quantity \(\int_{a}^{b} p(x) \, \mathrm{d}x \) satisfies the condition of positivity, additivity, and normalization.
#+end_proof
Since \(p(x)\) is a probability density, it has dimensions of \([x]^{-1}\), and changes its value if the units measuring \(x\) are modified. Unlike the CPF \(P(x)\), the PDF has no upper bound, that is, \(0<p(x)<\infty\), and may contain divergences as long as they are integrable.
#+end_definition
#+ATTR_HTML: :width 300px
[[file:~/.local/images/kardar-spop-2.2.jpg]]
#+CAPTION: A typical probability density function.
****** Expectation value
#+NAME: Expectation value
#+begin_definition latex
The expectation value \( \langle F(x) \rangle \) of any function, \(F(x)\), of the random variable is
\[
\langle F(x)\rangle \equiv \int_{-\infty}^{\infty} \mathrm{d} x p(x) F(x).
\]
#+end_definition
The function \(F(x)\) is itself a random variable, with an associated PDF of \(p_{F}(f) \mathrm{d} f=\operatorname{prob}(F(x) \in[f, f+\mathrm{d} f])\). There may be multiple solutions \(x_{i}\) to the equation \(F(x)=f\), and
\[
p_{F}(f) \mathrm{d} f=\sum_{i} p\left(x_{i}\right) \mathrm{d} x_{i}, \quad \Longrightarrow \quad p_{F}(f)=\sum_{i} p\left(x_{i}\right)\left|\frac{\mathrm{d} x}{\mathrm{~d} F}\right|_{x=x_{i}}
\]
The factors of \(|\mathrm{d} x / \mathrm{d} F|\) are the Jacobians associated with the change of variables from \(x\) to \(F\). For example, consider \(p(x)=\lambda \exp (-\lambda|x|) / 2\), and the function \( F(x) = x^2 \). There are two solutions to \(F(x)=f\), located at \(x_{ \pm}= \pm \sqrt{f}\), with corresponding Jacobians \(\lvert \pm f^{-1 / 2} / 2\rvert\). Hence,
\[
P_F(f)=\frac{\lambda}{2} \exp (-\lambda \sqrt{f})\left(\left|\frac{1}{2 \sqrt{f}}\right|+\left|\frac{-1}{2 \sqrt{f}}\right|\right)=\frac{\lambda \exp (-\lambda \sqrt{f})}{2 \sqrt{f}},
\]
for \(f>0\), and \(p_F(f)=0\) for \(f<0\). Note that \(p_F(f)\) has an (integrable) divergence at \(f=0\).
#+ATTR_HTML: :width 300px
[[file:~/.local/images/kardar-spop-2.3.jpg]]
#+CAPTION: Obtaining the PDF for the function F(x).
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-2.4.jpg]]
#+CAPTION: Probability density functions for x, and F(x) = x^2.
****** Characteristic function
#+NAME: Characteristic function
#+begin_definition latex
The characteristic function is simply the Fourier transform of the PDF or, equivalently, the expectation value of \( \exp (- \mathrm{i} k x) \):
\[
\tilde{p}(k) \equiv \int \mathrm{d} x p(x) \exp (- \mathrm{i} k x) = \langle \exp (- \mathrm{i} k x) \rangle.
\]
#+end_definition
The PDF can be recovered from the characteristic function through the inverse Fourier transform
\[
p(x)=\frac{1}{2 \pi} \int \mathrm{d} k \tilde{p}(k) \exp (+ \mathrm{i} k x).
\]
****** Moment
#+NAME: Moments
#+begin_definition latex
The \(n\)-th moment is
\[
m_{n} \equiv \langle x^{n} \rangle=\int \mathrm{d} x p(x) x^{n}.
\]
Moments of the PDF are expectation values for powers of the random variable. The characteristic function is the generator of moments of the distribution.
#+end_definition
Moments of the distribution are obtained by expanding \(\tilde{p}(k)\) in powers of \(k\)
\[
\tilde{p}(k)=\left\langle\sum_{n=0}^{\infty} \frac{(-\mathrm{i} k)^{n}}{n!} x^{n}\right\rangle=\sum_{n=0}^{\infty} \frac{(-\mathrm{i} k)^{n}}{n!}\left\langle x^{n}\right\rangle.
\]
Moments of the PDF around any point \(x_{0}\) can also be generated by expanding
\[
\exp (\mathrm{i} k x_{0}) \tilde{p}(k) = \bigg \langle \exp (- \mathrm{i} k (x - x_{0})) \bigg \rangle  = \sum_{n=0}^{\infty} \frac{(- \mathrm{i} k)^{n}}{n!} \langle (x-x_{0}\right)^{n} \rangle.
\]
****** Cumulant generating function
#+NAME: Cumulant generating function
#+begin_definition latex
The cumulant generating function is the logarithm of the characteristic function:
\[
\ln \tilde{p}(k)= \ln \langle \exp (- \mathrm{i} k x) \rangle = \ln \bigg \langle \sum_{n=1}^{\infty} \frac{(-\mathrm{i} k)^{n}}{n!} x^{n} \bigg \rangle.
\]
#+end_definition
****** Cumulant
#+NAME: Cumulants
#+begin_definition latex
The expansion of the cumulant generating function defines the cumulants of the distribution:
\[
\ln \tilde{p}(k)=\sum_{n=1}^{\infty} \frac{(-\mathrm{i} k)^{n}}{n!} \langle x^{n} \rangle_{c}.
\]
#+end_definition
The cumulants provide a useful and compact way of describing a PDF. The first four cumulants are called the mean, variance, skewness, and curtosis (or kurtosis) of the distribution.
***** Cumulants from moments
Cumulants can be expressed in terms of the moments by by expanding the logarithm of the characteristic function \(\tilde{p}(k)\) and using
v\[
\ln (1+\epsilon)=\sum_{n=1}^{\infty}(-1)^{n+1} \frac{\epsilon^{n}}{n}.
\]
The first four cumulants are obtained from the moments as
\begin{align*}
\langle x\rangle_c & =\langle x \rangle \\
\langle x^2 \rangle_c & =\langle x^2 \rangle-\langle x \rangle^2 \\
\langle x^3 \rangle_c & =\langle x^3 \rangle-3 \langle x^2 \rangle \langle x \rangle + 2 \langle x \rangle^3 \\
\langle x^4 \rangle_c & =\langle x^4 \rangle-4 \langle x^3 \rangle \langle x \rangle - 3 \langle x^2 \rangle^2 + 12 \langle x^2 \rangle \langle x \rangle^2 - 6 \langle x\rangle^4
\end{align*}
***** Moments from cumulants: cluster expansion
Moments can be expressed in terms of the cumulants by using the cluster expansion.
#+NAME: Cluster expansion
#+begin_theorem latex
The moment \( \langle x^{m} \rangle \) is related to the cumulants \( \langle x^{n} \rangle_{c}\) by
\[
\langle x^{m} \rangle=\sum_{\left\{p_{n}\right\}}^{\prime} m!\prod_{n} \frac{1}{p_{n}!(n!)^{p_{n}}} \langle x^{n} \rangle_{c}^{p_{n}}, \quad  \sum n p_{n}=m.
\]
The name cluster theorem derives from the following graphical interpretation: represent the cumulant \( \langle x^{n} \rangle_{c}\) graphically as a connected cluster of \(n\) points. The moment \( \langle x^{m} \rangle \) is then obtained by summing all possible subdivisions of \(m\) points into groupings of smaller (connected or disconnected) clusters. The contribution of each subdivision to the sum is the product of the connected cumulants that it represents.
#+end_theorem
#+NAME: Cluster expansion
#+begin_proof latex
We have
\[
\sum_{m=0}^{\infty} \frac{(-\mathrm{i} k)^{m}}{m!} \langle x^{m} \rangle=\exp \left[\sum_{n=1}^{\infty} \frac{(-\mathrm{i} k)^{n}}{n!} \langle x^{n} \rangle_{c}\right]=\prod_{n} \sum_{p_{n}}\left[\frac{(-\mathrm{i} k)^{n p_{n}}}{p_{n}!}\left(\frac{ \langle x^{n} \rangle_{c}}{n!}\right)^{p_{n}}\right]
\]
where we have used the definition of moments and the cumulant generating function along with the elementary identity \( x = \exp (\ln x) \). Matching the powers of \((-\mathrm{i} k)^{m}\) on the two sides of the above expression leads to
\[
\langle x^{m} \rangle=\sum_{\left\{p_{n}\right\}}^{\prime} m!\prod_{n} \frac{1}{p_{n}!(n!)^{p_{n}}} \langle x^{n} \rangle_{c}^{p_{n}}.
\]
The sum is restricted such that \(\sum n p_{n}=m\) and leads to the graphical interpretation given above, as the numerical factor is simply the number of ways of breaking \(m\) points into \(\left\{p_{n}\right\}\) clusters of \(n\) points.
#+end_proof
Using the cluster theorem, the first four moments are computed graphically:
#+NAME: Cluster expansion example
#+begin_src latex :file ~/.local/images/cluster-expansion-example.png :results file graphics
  \begin{tikzpicture}
    % <x>
    \node at (-2,0) {\(\langle x \rangle\)};
    \node at (-1,0) {\(=\)};
    \node at (0,0) {\(\bullet\)};
    % <x^2>
    \node at (-2,-1) {\(\langle x^2 \rangle\)};
    \node at (-1,-1) {\(=\)};
    \node[circle, draw, dashed, inner sep=0.15cm] (circle1) at (0,-1) {};
    \node at (-0.15,-1) {\(\bullet\)};
    \node at (0.15,-1) {\(\bullet\)};
    \node at (1,-1) {\(+\)};
    \node at (2,-1) {\(\bullet \ \bullet\)};
    % <x^3>
    \node at (-2,-2) {\(\langle x^3 \rangle\)};
    \node at (-1,-2) {\(=\)};
    \node[regular polygon, regular polygon sides=3, draw, dashed, inner sep=0.15cm] (triangle1) at (0,-2) {};
    \node at (-0.15,-2.15) {\(\bullet\)};
    \node at (0,-1.85) {\(\bullet\)};
    \node at (0.15,-2.15) {\(\bullet\)};
    \node at (1,-2) {\(+\)};
    \node[circle, draw, dashed, inner sep=0.15cm] (circle1) at (2,-2.2) {};
    \node at (1.5,-2) {\(3\)};
    \node at (1.85,-2.15) {\(\bullet\)};
    \node at (2.0,-1.85) {\(\bullet\)};
    \node at (2.15,-2.15) {\(\bullet\)};
    \node at (3.0,-2) {\(+\)};
    \node at (3.85,-2.10) {\(\bullet\)};
    \node at (4,-1.85) {\(\bullet\)};
    \node at (4.15,-2.10) {\(\bullet\)};
    % <x^4>
    \node at (-2,-3) {\(\langle x^4 \rangle\)};
    \node at (-1,-3) {\(=\)};
    \node[regular polygon, regular polygon sides=4, draw, dashed, inner sep=0.2cm] (triangle1) at (0,-3) {};
    \node at (-0.15,-2.85) {\(\bullet\)};
    \node at (-0.15,-3.15) {\(\bullet\)};
    \node at (0.15,-2.85) {\(\bullet\)};
    \node at (0.15,-3.15) {\(\bullet\)};
    \node at (1,-3) {\(+\)};
    \node[regular polygon, regular polygon sides=3, draw, dashed, inner sep=0.15cm] (triangle1) at (2,-3) {};
    \node at (1.5,-3) {\(4\)};
    \node at (1.85,-3.15) {\(\bullet\)};
    \node at (2.0,-2.85) {\(\bullet\)};
    \node at (2.15,-3.15) {\(\bullet\)};
    \node at (2.3,-2.85) {\(\bullet\)};
    \node at (3.0,-3) {\(+\)};
    \node[circle, draw, dashed, inner sep=0.15cm] (circle1) at (4,-2.85) {};
    \node[circle, draw, dashed, inner sep=0.15cm] (circle1) at (4,-3.15) {};
    \node at (3.5,-3) {\(3\)};
    \node at (3.85,-2.85) {\(\bullet\)};
    \node at (4.15,-2.85) {\(\bullet\)};
    \node at (3.85,-3.15) {\(\bullet\)};
    \node at (4.15,-3.15) {\(\bullet\)};
    \node at (5.0,-3) {\(+\)};
    \node[circle, draw, dashed, inner sep=0.15cm] (circle1) at (6,-3.15) {};
    \node at (5.5,-3) {\(6\)};
    \node at (5.85,-2.85) {\(\bullet\)};
    \node at (6.15,-2.85) {\(\bullet\)};
    \node at (5.85,-3.15) {\(\bullet\)};
    \node at (6.15,-3.15) {\(\bullet\)};
    \node at (7.0,-3) {\(+\)};
    \node at (7.85,-2.85) {\(\bullet\)};
    \node at (7.85,-3.15) {\(\bullet\)};
    \node at (8.15,-2.85) {\(\bullet\)};
    \node at (8.15,-3.15) {\(\bullet\)};
  \end{tikzpicture}
#+end_src
#+RESULTS: Cluster expansion example
[[file:~/.local/images/cluster-expansion-example.png]]
#+CAPTION: Graphical computation of the first four moments using the first four cumulants.
The corresponding algebraic expressions are
\begin{align*}
& \langle x\rangle=\langle x\rangle_c \\
& \langle x^2 \rangle = \langle x^2 \rangle_c+\langle x\rangle_c^2 \\
& \langle x^3 \rangle = \langle x^3 \rangle_c+3\langle x^2 \rangle_c\langle x\rangle_c+\langle x\rangle_c^3 \\
& \langle x^4 \rangle = \langle x^4 \rangle_c+4\langle x^3 \rangle_c\langle x\rangle_c+3\langle x^2 \rangle_c^2+6\langle x^2 \rangle_c\langle x\rangle_c^2+\langle x\rangle_c^4
\end{align*}
This theorem is the starting point for various diagrammatic computations in statistical mechanics and field theory. It allows easy computation of moments in terms of the cumulants.
**** Many random variables
With more than one random variable, the set of outcomes is an \(N\)-dimensional space, \(\mathcal{S}_{\mathbf{x}}=\left\{-\infty<x_{1}, x_{2}, \cdots, x_{N}<\infty\right\}\). For example, describing the location and velocity of a gas particle requires six coordinates.
***** Definitions
****** Joint probability density
#+NAME: Joint probability density
#+begin_definition latex
The joint PDF \(p(\mathbf{x})\) is the probability density of an outcome in a volume element \(\mathrm{d}^{N} \mathbf{x}=\prod_{i=1}^{N} \mathrm{~d} x_{i}\) around the point \(\mathbf{x}=\left\{x_{1}, x_{2}, \cdots, x_{N}\right\}\). The joint PDF is normalized such that
\[
p_{\mathbf{x}}(\mathcal{S})=\int \mathrm{d}^{N} \mathbf{x} p(\mathbf{x})=1 \tag{2.32}
\]
#+end_definition
#+begin_corollary latex
If, and only if, the \(N\) random variables are independent, the joint PDF is the product of individual PDFs,
\[
p(\mathbf{x})=\prod_{i=1}^{N} p_{i}\left(x_{i}\right).
\]
#+end_corollary
****** The unconditional PDF
#+NAME: Unconditional PDF
#+begin_definition latex
The unconditional PDF describes the behavior of a subset of random variables, independent of the values of the others
\[
p (x_{1}, \cdots, x_{m})= \int \prod_{i=m+1}^{N} \mathrm{~d} x_{i} p (x_{1}, \cdots, x_{N}).
\]
#+end_definition
For example, if we are interested only in the location of a gas particle, an unconditional PDF can be constructed by integrating over all velocities at a given location, \(p(\vec{x})=\) \(\int \mathrm{d}^{3} \vec{v} p(\vec{x}, \vec{v})\).
****** The conditional PDF
#+NAME: Conditional PDF
#+begin_definition latex
The conditional PDF describes the behavior of a subset of random variables, for specified values of the others. In general, the conditional PDFs are obtained from the joint probability density and the unconditional PDF by using Bayes' theorem as
\[
p\left(x_{1}, \cdots, x_{m} \mid x_{m+1}, \cdots, x_{N}\right)=\frac{p\left(x_{1}, \cdots, x_{N}\right)}{p\left(x_{m+1}, \cdots, x_{N}\right)}.
\]
#+end_definition
For example, the PDF for the velocity of a particle at a particular location \(\vec{x}\), denoted by \(p(\vec{v} \mid \vec{x})\), is proportional to the joint PDF \(p(\vec{v} \mid \vec{x})=p(\vec{x}, \vec{v}) / \mathcal{N}\). The constant of proportionality, obtained by normalizing \(p(\vec{v} \mid \vec{x})\), is
\[
\mathcal{N}=\int \mathrm{d}^{3} \vec{v} p(\vec{x}, \vec{v})=p(\vec{x})
\]
the unconditional PDF for a particle at \(\vec{x}\).
#+begin_corollary latex
If the random variables are independent, the unconditional PDF is equal to the conditional PDF.
#+end_corollary
****** Expectation value
#+NAME: Expectation value
#+begin_definition latex
The expectation value \( \langle F(\mathbf{x}) \rangle \) of any function, \(F(\mathbf{x})\), of the random variable is
\[
\langle F(\mathbf{x})\rangle=\int \mathrm{d}^{N} \mathbf{x} p(\mathbf{x}) F(\mathbf{x}).
\]
#+end_definition
****** Joint characteristic function
#+NAME: Joint characteristic function
#+begin_definition latex
The joint characteristic function is the \(N\)-dimensional Fourier transformation of the joint PDF
\[
\tilde{p}(\mathbf{k})= \bigg \langle \exp \bigg( -\mathrm{i} \sum_{j=1}^{N} k_{j} x_{j} \bigg) \bigg \rangle.
\]
#+end_definition
****** Joint moments
#+NAME: Joint moments
#+begin_definition latex
The joint moments are generated by \(\tilde{p}(\mathbf{k})\)
\[
\langle x_{1}^{n_{1}} x_{2}^{n_{2}} \cdots x_{N}^{n_{N}} \rangle = \big(\partial_{-\mathrm{i} k_{1}}^{n_{1}} \, \partial_{-\mathrm{i} k_{2}}^{n_{2}} \, \cdots \, \partial_{-\mathrm{i} k_{N}}^{n_{N}}\big) \tilde{p}(\mathbf{k}=\mathbf{0}).
\]
#+end_definition
****** Joint cumulants
#+NAME: Joint cumulants
#+begin_definition latex
The joint cumulants are generated by \(\ln \tilde{p}(\mathbf{k})\), respectively, as
\[
\langle x_{1}^{n_{1}} * x_{2}^{n_{2}} * \cdots x_{N}^{n_{N}} \rangle_{c} & = \big(\partial_{-\mathrm{i} k_{1}}^{n_{1}} \, \partial_{-\mathrm{i} k_{2}}^{n_{2}} \, \cdots \, \partial_{-\mathrm{i} k_{N}}^{n_{N}}\big) \ln \tilde{p}(\mathbf{k}=\mathbf{0})
\]
#+end_definition
****** Connected correlation
#+NAME: Connected correlation
#+begin_definition latex
The connected correlation between the random variables \( x_{\alpha} \) and \( x_{\beta} \) is the joint cumulant \(\left\langle x_{\alpha} * x_{\beta}\right\rangle_{c}\).
#+end_definition
#+begin_theorem latex
If \( x_{\alpha} \) and \( x_{\beta} \) are independent random variables, the connected correlation \(\left\langle x_{\alpha} * x_{\beta}\right\rangle_{c}\) vanishes.
#+end_theorem
***** Moments from cumulants: cluster expansion
The graphical relation between joint moments (all clusters of labeled points) and joint cumulant (connected clusters) is still applicable. From
#+NAME: Cluster expansion example (multivariable)
#+begin_src latex :file ~/.local/images/multi-cluster-expansion-example.png :results file graphics
  \begin{tikzpicture}
    % <x1 x2>
    \node at (-2,-1) {\(\langle x_{1} x_{2} \rangle\)};
    \node at (-1,-1) {\(=\)};
    \node[circle, draw, dashed, inner sep=0.15cm] (circle1) at (0,-1) {};
    \node at (-0.2,-1) {\({}_{1} \bullet\)};
    \node at (0.2,-1) {\(\bullet {}_{2}\)};
    \node at (1,-1) {\(+\)};
    \node at (1.85,-1) {\({}_{1} \bullet\)};
    \node at (2.15,-1) {\(\bullet {}_{2}\)};
    % <x1^2 x2>
    \node at (-2,-2) {\(\langle x_{1}^{2} x_{2} \rangle\)};
    \node at (-1,-2) {\(=\)};
    \node[regular polygon, regular polygon sides=3, draw, dashed, inner sep=0.125cm] (triangle1) at (0,-2) {};
    \node at (-0.25,-2.2) {\({}_{1} \bullet\)};
    \node at (0.08,-1.75) {\(\bullet^{2}\)};
    \node at (0.25,-2.2) {\(\bullet {}_{1}\)};
    \node at (1,-2) {\(+\)};
    \node[circle, draw, dashed, inner sep=0.15cm] (circle1) at (2,-2.2) {};
    \node at (1.75,-2.2) {\({}_{1} \bullet\)};
    \node at (2.08,-1.75) {\(\bullet^{2}\)};
    \node at (2.25,-2.2) {\(\bullet {}_{1}\)};
    \node at (3,-2) {\(+\)};
    \node[circle, draw, dashed, inner sep=0.15cm] (circle1) at (4,-2.2) {};
    \node at (3.5,-2) {\(2\)};
    \node at (3.75,-2.2) {\({}_{1} \bullet\)};
    \node at (4.08,-1.75) {\(\bullet^{1}\)};
    \node at (4.25,-2.2) {\(\bullet {}_{2}\)};
    \node at (5,-2) {\(+\)};
    \node at (5.75,-2.2) {\({}_{1} \bullet\)};
    \node at (6.08,-1.75) {\(\bullet^{2}\)};
    \node at (6.25,-2.2) {\(\bullet {}_{1}\)};
  \end{tikzpicture}
#+end_src
#+RESULTS: Cluster expansion example (multivariable)
[[file:~/.local/images/multi-cluster-expansion-example.png]]
we get
\begin{align*}
 \langle x_{1} x_{2} \rangle & = \langle x_{1} \rangle_{c} \langle x_{2} \rangle_{c}+ \langle x_{1} * x_{2} \rangle_{c}, \\
 \langle x_{1}^{2} x_{2} \rangle & = \langle x_{1} \rangle_{c}^{2} \langle x_{2} \rangle_{c}+ \langle x_{1}^{2} \rangle_{c} \langle x_{2} \rangle_{c}+2 \langle x_{1} * x_{2} \rangle_{c} \langle x_{1} \rangle_{c}+ \langle x_{1}^{2} * x_{2} \rangle_{c}.
\end{align*}
Compare the above result with
\begin{align*}
& \langle x^2 \rangle = \langle x\rangle_c^2 + \langle x^2 \rangle_c\\
& \langle x^3 \rangle = \langle x\rangle_c^3 + 3\langle x^2 \rangle_c\langle x\rangle_c + \langle x^3 \rangle_c.
\end{align*}
*** Probability distributions
**** Normal distribution
The normal distribution serves as the starting point for most perturbative computations in field theory. 
***** PDF
The normal (Gaussian) distribution describes a continuous real random variable \(x\), with
\[
p(x)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left[-\frac{(x-\lambda)^{2}}{2 \sigma^{2}}\right].
\]
***** Characteristic function
The corresponding characteristic function also has a Gaussian form,
\[
\tilde{p}(k)=\int_{-\infty}^{\infty} \mathrm{d} x \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left[-\frac{(x-\lambda)^{2}}{2 \sigma^{2}}-\mathrm{i} k x\right]=\exp \left[-\mathrm{i} k \lambda-\frac{k^{2} \sigma^{2}}{2}\right].
\]
***** Cumulant generating function
\[
\ln \tilde{p}(k)=\ln \exp \left[-\mathrm{i} k \lambda-\frac{k^{2} \sigma^{2}}{2}\right] = -\mathrm{i} k \lambda-\frac{k^{2} \sigma^{2}}{2}.
\]
***** Cumulants
Cumulants of the distribution can be identified from \(\ln \tilde{p}(k)=-\mathrm{i} k \lambda-k^{2} \sigma^{2} / 2\), using the expansion of the cumulant generating function, as
\[
\langle x\rangle_{c} = \lambda, \quad \langle x^{2} \rangle_{c} = \sigma^{2}, \quad \langle x^{3} \rangle_{c} =  \langle x^{4} \rangle_{c} = \cdots = 0.
\]
The normal distribution is thus completely specified by its two first cumulants.
***** Moments
Because the normal distribution is thus completely specified by its two first cumulants, computation of moments using the cluster expansion is particularly simple:
\begin{align*}
& \langle x\rangle = \lambda, \\
&  \langle x^2 \rangle = \sigma^2+\lambda^2, \\
&  \langle x^3 \rangle = 3 \sigma^2 \lambda+\lambda^3, \\
&  \langle x^4 \rangle = 3 \sigma^4+6 \sigma^2 \lambda^2+\lambda^4, \\
& \ldots
\end{align*}
The vanishing of higher cumulants implies that all graphical computations involve only products of one-point, and two-point (known as propagators), clusters.
***** Standard deviation
#+NAME: Standard deviation
#+begin_definition latex
The standard deviation is the square root of the variance, i.e., \( \sqrt{ \langle x \rangle_{c} } \).
#+end_definition
***** Relative uncertainty
#+NAME: Relative uncertainty
#+begin_definition latex
The relative uncertainty is the ratio of the mean \(\langle x \rangle_{c}\) and the standard deviation \( \sqrt{ \langle x \rangle_{c} } \), i.e., \(  \langle x \rangle_{c} / \sqrt{ \langle x \rangle_{c} } \).
#+end_definition
**** Binomial distribution
Consider a random variable with two outcomes \(A\) and \(B\) (e.g., a coin toss) of relative probabilities \(p_{A}\) and \(p_{B}=1-p_{A}\). The probability that in \(N\) trials the event \(A\) occurs exactly \(N_{A}\) times (e.g., 5 heads in 12 coin tosses) is given by the /binomial distribution/.
***** PDF
\[
p_{N}\left(N_{A}\right)=\binom{N}{N_{A}} p_{A}^{N_{A}} p_{B}^{N-N_{A}}.
\]
The prefactor,
\[
\binom{N}{N_{A}}=\frac{N!}{N_{A}!\left(N-N_{A}\right)!}
\]
is just the coefficient obtained in the binomial expansion of \(\left(p_{A}+p_{B}\right)^{N}\), and gives the number of possible orderings of \(N_{A}\) events \(A\) and \(N_{B}=N-N_{A}\) events \(B\).
***** Characteristic function
The characteristic function for this discrete distribution is given by
\[
\tilde{p}_{N}(k) = \langle \exp (-\mathrm{i} k N_{A}) \rangle = \sum_{N_{A}=0}^{N} \frac{N!}{N_{A}! (N-N_{A})!} \cdot p_{A}^{N_{A}} \cdot p_{B}^{N-N_{A}} \cdot \exp (-\mathrm{i} k N_{A}) = (p_{A} \cdot \exp (-\mathrm{i} k) + p_{B})^{N}.
\]
***** Cumulant generating function
The cumulant generating function is
\[
\ln \tilde{p}_{N}(k)=N \ln [p_{A} \exp (-\mathrm{i} k) + p_{B}] = N \ln \tilde{p}_{1}(k),
\]
where \(\ln \tilde{p}_{1}(k)\) is the cumulant generating function for a single trial.
***** Cumulants
The cumulants after \(N\) trials are simply \(N\) times the cumulants in a single trial. In each trial, the allowed values of \(N_{A}\) are \( 0 \) and \( 1 \) with respective probabilities \(p_{B}\) and \(p_{A}\), leading to \(\langle N_{A}^{\ell}\rangle=p_{A}\), for all \(\ell\). After \(N\) trials the first two cumulants are
\[
\left\langle N_{A}\right\rangle_{c}=N p_{A}, \quad \langle N_{A}^{2} \rangle_{c} = N (p_{A}-p_{A}^{2})=N p_{A} p_{B}.
\]
***** Moments
The moments after \( N \) trials can be obtained from the cumulants by using the cluster expansion. We have
\[
\langle N_{A} \rangle = \langle N_{A} \rangle_{c} = N p_{A}, \quad \langle N_{A}^{2} \rangle = \langle N_{A}^{2} \rangle_{c} - \langle N \rangle_{c}^{2} = N p_{A} (1 - p_{A}) - N^{2} p_{A}^{2}.
\]
***** Relative uncertainty
While the mean of the binomial distribution scales as \(N\), its standard deviation only grows as \(\sqrt{N}\). Hence, the relative uncertainty grows like \( 1/ \sqrt{N} \).
**** Multinomial distribution
Consider a random variable with outcomes  \(\{A, B, \cdots, M\}\) of relative probabilities  \(\left\{p_{A}, p_{B}, \cdots, p_{M}\right\}\). The probability of finding outcomes \(\left\{N_{A}, N_{B}, \cdots, N_{M}\right\}\) in a total of \(N=N_{A}+N_{B}+\cdots+N_{M}\) trials is given by the /multinomial distribution/.
***** PDF
\[
p_{N}\left(\left\{N_{A}, N_{B}, \cdots, N_{M}\right\}\right)=\frac{N!}{N_{A}!N_{B}!\cdots N_{M}!} p_{A}^{N_{A}} p_{B}^{N_{B}} \cdots p_{M}^{N_{M}}.
\]
The prefactor \(\frac{N!}{N_{A}!N_{B}!\cdots N_{M}!}\) is just the coefficient obtained in the multinomial expansion of \(\left(p_{A}+p_{B}+ \ldots + p_{M}\right)^{N}\), and gives the number of possible orderings of \(N_{A}\) events \(A\), \(N_{B}\) events \(B\), and so on till \(N_{M}\) events \(M\).
**** The Poisson distribution
#+ATTR_HTML: :width 300px
[[file:~/.local/images/kardar-spop-2.6.jpg]]
#+CAPTION: Subdividing the time interval into small segments of size dt.
#+NAME: Poisson process
#+begin_definition latex
Let \( M \) be a random variable that denotes the number of times an event \( E \) occurs in the interval \([t, t+T]\). Each occurence of the event \( E \) is called an arrival. \( M \) is said to model a Poisson process if and only if:
1) The probability \( p(M = 1) \) of one and only one arrival in the interval \([t, t+\mathrm{d} t]\) is proportional to \(\mathrm{d} t\) as \(\mathrm{d} t \rightarrow 0\),
2) The probabilities of events at different intervals are independent of each other.
#+end_definition
Suppose that \( M \) models a Poisson process. The /Poisson distribution/ gives the probability \( p(M) \) of observing exactly \( M \) arrival in the interval \( T \).
***** Characteristic function
The Poisson distribution is obtained as a limit of the binomial distribution by subdividing the interval into \(N=T / \mathrm{d} t \gg 1\) segments of size \(\mathrm{d} t\). In each segment, an event occurs with probability \(p=\alpha \mathrm{d} t\), and there is no event with probability \(q=1-\alpha \mathrm{d} t\). As the probability of more than one event in \(\mathrm{d} t\) is too small to consider, the process is equivalent to a binomial one. Using the expression of the characteristic function of the binomial distribution we have
\[
\tilde{p}(k)= (p \exp (-\mathrm{i} k) + q)^{n}=\lim _{\mathrm{d} t  \to 0} [1+\alpha \mathrm{d} t (\exp (-\mathrm{i} k) - 1 ) ]^{T / \mathrm{d} t} = \exp  \{\alpha \, T \, [\exp (-\mathrm{i} k)-1] \}.
\]
***** PDF
The Poisson PDF is obtained from the inverse Fourier transform its characteristic function as
\[
p(x)=\int_{-\infty}^{\infty} \frac{\mathrm{d} k}{2 \pi} \exp \left[\alpha\left(\mathrm{e}^{-i k}-1\right) T+\mathrm{i} k x\right]=\mathrm{e}^{-\alpha T} \int_{-\infty}^{\infty} \frac{\mathrm{d} k}{2 \pi} \mathrm{e}^{\mathrm{i} k x} \sum_{M=0}^{\infty} \frac{(\alpha T)^{M}}{M!} \mathrm{e}^{-i k M}
\]
using the power series for the exponential. The integral over \(k\) is
\[
\int_{-\infty}^{\infty} \frac{\mathrm{d} k}{2 \pi} \mathrm{e}^{\mathrm{i} k(x-M)}=\delta(x-M)
\]
leading to
\[
p_{\alpha T} (x) = \sum_{M=0}^{\infty} \mathrm{e}^{-\alpha T} \frac{(\alpha T)^{M}}{M!} \delta(x-M).
\]
This answer clearly realizes that the only possible values of \(x\) are integers \(M\). The probability of \(M\) events is thus \(p_{\alpha T}(M)=\mathrm{e}^{-\alpha T}(\alpha T)^{M} / M!\). 
***** Cumulant generation function
The Cumulant generation function 
\[
\ln \tilde{p}_{\alpha T}(k)=\alpha T\left(\mathrm{e}^{-\mathrm{i} k}-1\right)=\alpha T \sum_{n=1}^{\infty} \frac{(-\mathrm{i} k)^{n}}{n!}
\]
***** Cumulants
Since \( \ln \tilde{p}_{\alpha T}(k)= \alpha T \sum_{n=1}^{\infty} \frac{(-\mathrm{i} k)^{n}}{n!} \), all the cumulants have the same value
\[
\langle M^{n} \rangle_{c}=\alpha T.
\]
***** Moments
The moments are obtained from the cumulants using the cluster expansion:
\[
\langle M\rangle=(\alpha T), \quad\left\langle M^{2}\right\rangle=(\alpha T)^{2}+(\alpha T), \quad\left\langle M^{3}\right\rangle=(\alpha T)^{3}+3(\alpha T)^{2}+(\alpha T).
\]
***** Example
Assuming that stars are randomly distributed in the Galaxy (clearly unjustified) with a density \(n\), what is the probability that the nearest star is at a distance \(R\)? Since the probability of finding a star in a small volume \(\mathrm{d} V\) is \(n \mathrm{~d} V\), and they are assumed to be independent, the number of stars in a volume \(V\) is described by a Poisson process with \(\alpha=n\). The probability \(p(R)\) of encountering the first star at a distance \(R\) is the product of the probabilities \(p_{n V}(0)\) of finding zero stars in the volume \(V=4 \pi R^{3} / 3\) around the origin, and \(p_{n \mathrm{~d} V}(1)\) of finding one star in the shell of volume \(\mathrm{d} V=4 \pi R^{2} \mathrm{~d} R\) at a distance \(R\). Both \(p_{n V}(0)\) and \(p_{n \mathrm{~d} V}(1)\) can be calculated from
\[
p_{\alpha T} (x) = \sum_{M=0}^{\infty} \mathrm{e}^{-\alpha T} \frac{(\alpha T)^{M}}{M!} \delta(x-M),
\]
and
\begin{align*}
p(R) \mathrm{d} R= & p_{n V}(0) p_{n \mathrm{~d} V}(1)=\mathrm{e}^{-4 \pi R^{3} n / 3} \mathrm{e}^{-4 \pi R^{2} n \mathrm{~d} R} 4 \pi R^{2} n \mathrm{~d} R \\
& \Longrightarrow \quad p(R)=4 \pi R^{2} n \exp \left(-\frac{4 \pi}{3} R^{3} n\right).
\end{align*}
**** The joint Gaussian distribution
***** PDF
The joint Gaussian distribution is the generalization of the normal distribution to \(N\) random variables, as
\[
p(\mathbf{x}) = \frac{1}{\sqrt{(2 \pi)^{N} \operatorname{det}[C]}} \exp  \bigg \{-\frac{1}{2} \sum_{m n} (C^{-1} )_{m n} (x_{m}-\lambda_{m} ) (x_{n}-\lambda_{n} )\bigg \}
\]
where \(C\) is a symmetric matrix, and \(C^{-1}\) is its inverse. The simplest way to get the normalization factor is to make a linear transformation from the variables \(y_{j}=x_{j}-\lambda_{j}\), using the unitary matrix that diagonalizes \(C\). This reduces the normalization to that of the product of \(N\) Gaussians whose variances are determined by the eigenvalues of \(C\). The product of the eigenvalues is the determinant \(\operatorname{det}[C]\). (This also indicates that the matrix \(C\) must be positive definite.)
***** Joint characteristic function
The corresponding joint characteristic function is obtained by similar manipulations, and is given by
\[
\tilde{p}(\mathbf{k}) = \exp \bigg \{-\mathrm{i} k_{m} \lambda_{m}-\frac{1}{2} C_{m n} k_{m} k_{n} \bigg \}.
\]
where the summation convention (implicit summation over a repeated index) is used.
***** Joint cumulants
The joint cumulants of the Gaussian are then obtained from \(\ln \tilde{p}(\mathbf{k})\) as
\[
\langle x_{m} \rangle_{c} = \lambda_{m}, \quad \langle x_{m} * x_{n} \rangle_{c}=C_{m n}
\]
with all higher cumulants equal to zero.
***** Wick's theorem
#+NAME: Wick's theorem
#+begin_theorem latex
Consider the joint Gaussian distribution
\[
p(\mathbf{x}) = \frac{1}{\sqrt{(2 \pi)^{N} \operatorname{det}[C]}} \exp  \bigg \{-\frac{1}{2} \sum_{m n} (C^{-1} )_{m n} \, x_{m} x_{n} \bigg \}
\]
where \(C\) is a symmetric matrix, and \(C^{-1}\) is its inverse. All odd moments vanish and any even moment is obtained by summing over all ways of grouping the involved random variables into pairs.
#+end_theorem
As an example \(\langle x_{a} x_{b} x_{c} x_{d} \rangle = C_{a b} C_{c d}+C_{a c} C_{b d}+C_{a d} C_{b c}\).
*** Central limit theorems (CLT)
**** Central limit theorem
#+NAME: Central limit theorem
#+begin_theorem latex
Consider the sum \(X=\sum_{i=1}^{N} x_{i}\), where \(x_{i}\) are random variables with a joint PDF of \(p(\mathbf{x})\). Now consider the random variable \(y=(X- N \langle x\rangle_{c}) / \sqrt{N}\). If the random variables \( \{x_{i}\} \) are independent then \( \langle y \rangle_{c} = 0 \) and \(\left\langle y^{n}\right\rangle_{c} \propto N^{1-n / 2}\). As \(N \rightarrow \infty\) the PDF for \(y\) converges to the normal distribution,
\[
\lim_{N  \to \infty} p \bigg(y= \frac{\sum_{i=1}^{N} x_{i} - N \langle x\rangle_{c}}{\sqrt{N}} \bigg) = \frac{1}{\sqrt{2 \pi \langle x^{2} \rangle_{c}}} \exp  (-\frac{y^{2}}{2 \langle x^{2} \rangle_{c}}).
\]
#+end_theorem
#+NAME: Central limit theorem
#+begin_proof latex
Consider the sum \(X=\sum_{i=1}^{N} x_{i}\), where \(x_{i}\) are random variables with a joint PDF of \(p(\mathbf{x})\). 
The PDF for \(X\) is
\[
p_{X}(x)=\int \mathrm{d}^{N} \mathbf{x} p(\mathbf{x}) \delta (x-\sum x_{i} )=\int \prod_{i=1}^{N-1} \mathrm{~d} x_{i} p (x_{1}, \cdots, x_{N-1}, x-x_{1} \cdots-x_{N-1} )
\]
and the corresponding characteristic function is given by
\[
\tilde{p}_{X}(k)= \langle\exp  (-\mathrm{i} k \sum_{j=1}^{N} x_{j} ) \rangle=\tilde{p} (k_{1}=k_{2}=\cdots=k_{N}=k )
\]
Cumulants of the sum are obtained by expanding \(\ln \tilde{p}_{X}(k)\),
\[
\ln \tilde{p} (k_{1}=k_{2}=\cdots=k_{N}=k )=-\mathrm{i} k \sum_{i_{1}=1}^{N} \langle x_{i_{1}} \rangle_{c}+\frac{(-\mathrm{i} k)^{2}}{2} \sum_{i_{1}, i_{2}}^{N} \langle x_{i_{1}} x_{i_{2}} \rangle_{c}+\cdots,
\]
as
\[
\langle X\rangle_{c}=\sum_{i=1}^{N} \langle x_{i} \rangle_{c}, \quad \langle X^{2} \rangle_{c}=\sum_{i, j}^{N} \langle x_{i} x_{j} \rangle_{c}, \cdots.
\]
If the random variables are independent, \(p(\mathbf{x})=\prod p_{i}\left(x_{i}\right)\), and \(\tilde{p}_{X}(k)=\prod \tilde{p}_{i}(k)\), the cross cumulants in the equation above vanish, and the \(n\)-th cumulant of \(X\) is simply the sum of the individual cumulants, \(\left\langle X^{n}\right\rangle_{c}=\sum_{i=1}^{N}\left\langle x_{i}^{n}\right\rangle_{c}\). When all the \(N\) random variables are independently taken from the same distribution \(p(x)\), this implies \(\left\langle X^{n}\right\rangle_{c}=N\left\langle x^{n}\right\rangle_{c}\), generalizing the result obtained previously for the binomial distribution. For large values of \(N\), the average value of the sum is proportional to \(N\), while fluctuations around the mean, as measured by the standard deviation, grow only as \(\sqrt{N}\). The random variable \(y=(X-\) \(\left.N\langle x\rangle_{c}\right) / \sqrt{N}\) has zero mean, and cumulants that scale as \(\left\langle y^{n}\right\rangle_{c} \propto N^{1-n / 2}\). As \(N \rightarrow \infty\), only the second cumulant survives, and the PDF for \(y\) converges to the normal distribution,
\[
\lim _{N \rightarrow \infty} p\left(y=\frac{\sum_{i=1}^{N} x_{i}-N\langle x\rangle_{c}}{\sqrt{N}}\right)=\frac{1}{\sqrt{2 \pi\left\langle x^{2}\right\rangle_{c}}} \exp \left(-\frac{y^{2}}{2\left\langle x^{2}\right\rangle_{c}}\right).
\]
Note that the Gaussian distribution is the only distribution with only first and second cumulants.
#+end_proof
The convergence of the PDF for the sum of many random variables to a normal distribution is an essential result in the context of statistical mechanics where such sums are frequently encountered.
**** Central limit theorem (general)
#+NAME: Central limit theorem (general)
#+begin_theorem latex
Consider the sum \(X=\sum_{i=1}^{N} x_{i}\), where \(x_{i}\) are random variables with a joint PDF of \(p(\mathbf{x})\). Now consider the random variable \(y=(X- N \langle x\rangle_{c}) / \sqrt{N}\). If the random variables \( \{x_{i}\} \) satisfy the condition \(\sum_{i_{1}, \cdots, i_{m}}^{N}\left\langle x_{i_{1}} \cdots x_{i_{m}}\right\rangle_{c} \ll \mathcal{O}\left(N^{m / 2}\right)\) then \( \langle y \rangle_{c} = 0 \) and \(\left\langle y^{n}\right\rangle_{c} \propto N^{1-n / 2}\). As \(N \rightarrow \infty\) the PDF for \(y\) converges to the normal distribution,
\[
\lim_{N  \to \infty} p \bigg(y= \frac{\sum_{i=1}^{N} x_{i} - N \langle x\rangle_{c}}{\sqrt{N}} \bigg) = \frac{1}{\sqrt{2 \pi \langle x^{2} \rangle_{c}}} \exp  (-\frac{y^{2}}{2 \langle x^{2} \rangle_{c}}).
\]
#+end_theorem
This is a stronger CLT than the previous one because it is not necessary for the random variables to be independent; the condition \(\sum_{i_{1}, \cdots, i_{m}}^{N}\left\langle x_{i_{1}} \cdots x_{i_{m}}\right\rangle_{c} \ll \mathcal{O}\left(N^{m / 2}\right)\) is sufficient.
**** Central limit theorem (Levy distribution)
The previous two CLTs implicitly assumes that the cumulants of the individual random variables \( \{x_{i}\} \) are finite. What happens if this is not the case, that is, when the variables are taken from a very wide PDF? The sum may still converge to a so-called Levy distribution.
#+NAME: Central limit theorem (Levy distribution)
#+begin_theorem latex
Consider the sum \(X=\sum_{i=1}^{N} x_{i}\), where \(\{x_{i}\}\) are random variables with a joint PDF of \(p(\mathbf{x})\). Further assume that \( \langle x_{i} \rangle = 0 \) for all \( i \). Now consider the random variable \(y=X / N^{1 / \alpha}\). If the random variables \(\{x_{i}\}\) are independent, and identically distributed random variables, with then
\[
p_{\alpha}(y)=\frac{1}{\pi} \sum_{n=1}^{\infty}(-1)^{n+1} \sin \left(\frac{n \pi}{2} \alpha\right) \frac{\Gamma(1+n \alpha)}{n!} \frac{a^{n}}{y^{1+n \alpha}}.
\]
#+end_theorem
#+NAME: Central limit theorem (Levy distribution)
#+begin_proof latex
Consider a sum of \(N\) independent, identically distributed random variables, with the mean set to zero for convenience. The variance does not exist if the individual PDF falls off slowly at large values as \(p_{i}(x)=p_{1}(x) \propto 1 /|x|^{1+\alpha}\), with \(0<\alpha \leq 2\). \((\alpha>0\) is required to make sure that the distribution is normalizable; while for \(\alpha>2\) the variance is finite.) The behavior of \(p_{1}(x)\) at large \(x\) determines the behavior of \(\tilde{p}_{1}(k)\) at small \(k\), and simple power counting indicates that the expansion of \(\tilde{p}_{1}(k)\) is singular, starting with \(|k|^{\alpha}\). Based on this argument we conclude that
\[
\ln \tilde{p}_{X}(k)=N \ln \tilde{p}_{1}(k)=N\left[-a|k|^{\alpha}+\text { higher order terms }\right].
\]
As before we can define a rescaled variable \(y=X / N^{1 / \alpha}\) to get rid of the \(N\) dependence of the leading term in the above equation, resulting in
\[
\lim _{N \rightarrow \infty} \tilde{p}_{y}(k)=-a|k|^{\alpha}.
\]
The higher-order terms appearing in Eq. (2.50) scale with negative powers of \(N\) and vanish as \(N \rightarrow \infty\). The simplest example of a Levy distribution is obtained for \(\alpha=1\), and corresponds to \(p_{y}=a /\left[\pi\left(y^{2}+a^{2}\right)\right]\). (This is the Cauchy distribution discussed in the problems section.) For other values of \(\alpha\) the distribution does not have a simple closed form, but can be written as the asymptotic series
\[
p_{\alpha}(y)=\frac{1}{\pi} \sum_{n=1}^{\infty}(-1)^{n+1} \sin \left(\frac{n \pi}{2} \alpha\right) \frac{\Gamma(1+n \alpha)}{n!} \frac{a^{n}}{y^{1+n \alpha}}.
\]
Such distributions describe phenomena with large rare events, characterized here by a tail that falls off slowly as \(p_{\alpha}(y \rightarrow \infty) \sim y^{-1-\alpha}\).
#+end_proof
*** Rules for large numbers
To describe equilibrium properties of macroscopic bodies, statistical mechanics has to deal with the very large number \(N\) of microscopic degrees of freedom. Actually, taking the thermodynamic limit of \(N \rightarrow \infty\) leads to a number of simplifications, some of which are described in this section.
There are typically three types of \(N\) dependence encountered in the thermodynamic limit:
(a) /Intensive quantities/, such as temperature \(T\), and generalized forces, for example, pressure \(P\), and magnetic field \(\vec{B}\), are independent of \(N\), that is, \(\mathcal{O}\left(N^0\right)\).
(b) /Extensive quantities/, such as energy \(E\), entropy \(S\), and generalized displacements, for example, volume \(V\), and magnetization \(\vec{M}\), are proportional to \(N\), that is, \(\mathcal{O}\left(N^1\right)\).
(c) /Exponential dependence/, that is, \(\mathcal{O}(\exp (N \phi))\), is encountered in enumerating discrete micro-states, or computing available volumes in phase space.
Other asymptotic dependencies are certainly not ruled out a priori. For example, the Coulomb energy of \(N\) ions at fixed density scales as \(Q^2 / R \sim N^{5 / 3}\). Such dependencies are rarely encountered in everyday physics. The Coulomb interaction of ions is quickly screened by counter-ions, resulting in an extensive overall energy. (This is not the case in astrophysical problems since the gravitational energy is not screened. For example, the entropy of a black hole is proportional to the square of its mass.)
In statistical mechanics we frequently encounter sums or integrals of exponential variables. Performing such sums in the thermodynamic limit is considerably simplified due to the following results.
**** Summation of exponential quantities
Consider the sum
\begin{align*}
\mathcal{S}=\sum_{i=1}^{\mathcal{N}} \mathcal{E}_i
\end{align*}
where each term is positive, with an exponential dependence on \(N\), that is,
\begin{align*}
0 \leq \mathcal{E}_i \sim \mathcal{O}\left(\exp \left(N \phi_i\right)\right),
\end{align*}
and the number of terms \(\mathcal{N}\) is proportional to some power of \(N\).
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-2.7.jpg]]
#+CAPTION: A sum with number of terms exponential in N is dominated by the largest term.
Such a sum can be approximated by its largest term \(\mathcal{E}_{\text {max }}\), in the following sense. Since for each term in the sum, \(0 \leq \mathcal{E}_i \leq \mathcal{E}_{\text {max }}\),
\[
\mathcal{E}_{\max} \leq \mathcal{S} \leq \mathcal{N} \mathcal{E}_{\text{max}}.
\]
An intensive quantity can be constructed from \(\ln \mathcal{S} / N\), which is bounded by
\[
\frac{\ln \mathcal{E}_{\max }}{N} \leq \frac{\ln \mathcal{S}}{N} \leq \frac{\ln \mathcal{E}_{\max }}{N}+\frac{\ln \mathcal{N}}{N}.
\]
For \(\mathcal{N} \propto N^p\), the ratio \(\ln \mathcal{N} / N\) vanishes in the large \(N\) limit, and
\[
\lim_{N \rightarrow \infty} \frac{\ln \mathcal{S}}{N} = \frac{\ln \mathcal{E}_{\max }}{N}=\phi_{\max }.
\]
**** Saddle point integration
Similarly, an integral of the form
\[
\mathcal{J}=\int \mathrm{d} x \exp (N \phi(x)) \tag{1}
\]
can be approximated by the maximum value of the integrand, obtained at a point \(x_{\max }\) that maximizes the exponent \(\phi(x)\). Expanding the exponent around this point gives
\[
\mathcal{J}=\int \mathrm{d} x \exp \left\{N\left[\phi\left(x_{\max }\right)-\frac{1}{2}\left|\phi^{\prime \prime}\left(x_{\max }\right)\right|\left(x-x_{\max }\right)^2+\cdots\right]\right\}.
\]
Note that at the maximum, the first derivative \(\phi^{\prime}\left(x_{\max }\right)\) is zero, while the second derivative \(\phi^{\prime \prime}\left(x_{\max }\right)\) is negative. Terminating the series at the quadratic order results in
\[
\mathcal{J} \approx \exp \{ N \phi (x_{\max }) \} \int \mathrm{d} x \exp \left[-\frac{N}{2}\left|\phi^{\prime \prime}\left(x_{\max }\right)\right|\left(x-x_{\max }\right)^2\right] \approx \sqrt{\frac{2 \pi}{N\left|\phi^{\prime \prime}\left(x_{\max }\right)\right|}} \mathrm{e}^{N \phi\left(x_{\max }\right)}, \tag{2}
\]
where the range of integration has been extended to \([-\infty, \infty]\). The latter is justified since the integrand is negligibly small outside the neighborhood of \(x_{\max }\).
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-2.8.jpg]]
#+CAPTION: Saddle point evaluation of an "exponential" integral.
There are two types of correction to the above result. Firstly, there are higher-order terms in the expansion of \(\phi(x)\) around \(x_{\max }\). These corrections can be looked at perturbatively, and lead to a series in powers of \(1 / N\). Secondly, there may be additional local maxima for the function. A maximum at \(x_{\max }^{\prime}\) leads to a similar Gaussian integral that can be added to (2). Clearly such contributions are smaller by \(\mathcal{O}\left(\exp \left\{-N\left[\phi\left(x_{\max }\right)-\phi\left(x_{\max }^{\prime}\right)\right]\right\}\right)\). Since all these corrections vanish in the thermodynamic limit,
\[
\lim _{N \rightarrow \infty} \frac{\ln \mathcal{J}}{N}=\lim _{N \rightarrow \infty}\left[\phi\left(x_{\max }\right)-\frac{1}{2 N} \ln \left(\frac{N\left|\phi^{\prime \prime}\left(x_{\max }\right)\right|}{2 \pi}\right)+\mathcal{O}\left(\frac{1}{N^2}\right)\right]=\phi\left(x_{\max }\right).
\]
The saddle point method for evaluating integrals is the extension of the above result to more general integrands, and integration paths in the complex plane. (The appropriate extremum in the complex plane is a saddle point.) The simplified version presented above is sufficient for our needs.
***** Stirling's formula
Stirling's approximation for \(N\) ! at large \(N\) can be obtained by saddle point integration. In order to get an integral representation of \(N!\), start with the result
\[
\int_0^{\infty} \mathrm{d} x \exp \{-\alpha x \} = 1 / \alpha.
\]
Repeated differentiation of both sides of the above equation with respect to \(\alpha\) leads to
\[
\int_0^{\infty} \mathrm{d} x x^N \exp \{-\alpha x \} = \frac{N!}{\alpha^{N+1}}.
\]
Although the above derivation only applies to integer \(N\), it is possible to define by analytical continuation a function
\[
\Gamma(N+1) \equiv N!=\int_0^{\infty} \mathrm{d} x x^N \exp (-x), \tag{3}
\]
for all \(N\). While the integral in (3) is not exactly in the form of (1), it can still be evaluated by a similar method. The integrand can be written as \(\exp (N \phi(x))\), with \(\phi(x) = \ln x-x / N\). The exponent has a maximum at \(x_{\max }=N\), with \(\phi\left(x_{\max }\right)=\ln N-1\), and \(\phi^{\prime \prime}\left(x_{\max }\right)=-1 / N^2\). Expanding the integrand in Eq. (3) around this point yields
\[
N!\approx \int \mathrm{d} x \exp \left[N \ln N-N-\frac{1}{2 N}(x-N)^2\right] \approx N^N \mathrm{e}^{-N} \sqrt{2 \pi N}, \tag{4}
\]
where the integral is evaluated by extending its limits to \((-\infty, \, \infty)\). Stirling's formula is obtained by taking the logarithm of (4) as
\[
\ln N!=N \ln N-N+\frac{1}{2} \ln (2 \pi N)+\mathcal{O} (1/N).
\]
*** Information, entropy, and estimation
**** Information
In the context of /information theory/ there is a precise meaning to the information content of a probability distribution.
#+NAME: Shannon's theorem
#+begin_theorem latex
The minimum number of bits necessary to ensure that the percentage of errors in \(N\) trials vanishes in the \(N \rightarrow \infty\) limit is \(\ln _{2} g\). For any non-uniform distribution, this is less than the \(N \ln _{2} M\) bits needed in the absence of any information on relative probabilities.
#+end_theorem
Shannon's theorem proves that the minimum number of bits necessary to ensure that the percentage of errors in \(N\) trials vanishes in the \(N \rightarrow \infty\) limit is \(\ln _{2} g\). For any non-uniform distribution, this is less than the \(N \ln _{2} M\) bits needed in the absence of any information on relative probabilities. The *difference per trial* is thus attributed to the information content of the probability distribution, which motivates the definition presented above.
#+NAME: Information
#+begin_definition latex
Consider a random variable with a discrete set of outcomes \(\mathcal{S}=\left\{x_{i}\right\}\), occurring with probabilities \(\{p(i)\}\), for \(i=1, \cdots, M\). The information content of the probability distribution is given by
\[
I\left[\left\{p_{i}\right\}\right]=\ln_{2} M+\sum_{i=1}^{M} p(i) \ln_{2} p(i)
\]
#+end_definition
***** Motivation
Consider a random variable with a discrete set of outcomes \(\mathcal{S}=\left\{x_{i}\right\}\), occurring with probabilities \(\{p(i)\}\), for \(i=1, \cdots, M\). Let us construct a message from \(N\) independent and identically outcomes of the random variable. Since there are \(M\) possibilities for each character in this message, it has an apparent *information content* of \(N \ln _{2} M\) bits, that is, \(N \ln _{2} M\) binary bits of information have to be transmitted to convey the message precisely. However, the probabilities \(\{p(i)\}\) limit the types of messages that are likely. In particular, we expect the probability of finding any \(N_{i}\) that is different from \(N p_{i}\) by more than \(\mathcal{O}(\sqrt{N})\) becomes exponentially small in \(N\) as \(N \rightarrow \infty\). The number of typical messages thus corresponds to the number of ways of rearranging the \(\left\{N_{i}\right\}\) occurrences of \(\left\{x_{i}\right\}\), and is given by the multinomial coefficient
\[
g=\frac{N !}{\prod_{i=1}^{M} N_{i} !} \ll M^N.
\]
To specify one out of \(g\) possible sequences requires
\[
\ln _{2} g \approx-N \sum_{i=1}^{M} p_{i} \ln _{2} p_{i} \quad(\text { for } N \rightarrow \infty)
\]
The last result can be obtained by applying Stirling's approximation for \(\ln N!\) or by
\[
1=\bigg(\sum_{i} p_{i}\bigg)^{N}=\sum_{\left\{N_{i}\right\}} \frac{N !}{\prod_{i=1}^{M} N_{i} !} \prod_{i=1}^{M} p_{i}^{N_{i}} \approx g \prod_{i=1}^{M} p_{i}^{N p_{i}}
\]
where the sum has been replaced by its largest term, which is justified for \( N \to \infty \).
**** Shannon Entropy
#+NAME: Shannon entropy
#+begin_definition latex
Consider a random variable with a discrete set of outcomes \(\mathcal{S}=\left\{x_{i}\right\}\), occurring with probabilities \(\{p(i)\}\), for \(i=1, \cdots, M\). The Shannon entropy of the probability distribution is given by
\[
S\left[\left\{p_{i}\right\}\right]=-\sum_{i=1}^{M} p(i) \ln_2 p(i)=-\langle\ln_2 p(i)\rangle
\]
#+end_definition
Shannon's entropy formula differs from Boltzmann's entropy formula only in the lack of the proportionality constant, the Boltzmann's constant, introduced in the latter for physical reasons.
***** Motivation
The multinomial coefficient
\[
g=\frac{N !}{\prod_{i=1}^{M} N_{i} !}
\]
which is fundamental to Shannon's theorem and the definition of information is encountered frequently in classical statistical mechanics in the context of mixing \(M\) distinct components; its natural logarithm is related to the entropy of mixing. This motivates the general definition of entropy of a probability distribution presented above.
***** Properties
1) The Shannon entropy \( S \) takes a /minimum value/ of zero for the Dirac delta function \(p(i)=\delta_{ij}\),
2) The Shannon entropy \( S \) takes a /maximum value/ of \(\ln M\) for the uniform distribution \(p(i)=M^{-1}\),
3) The Shannon entropy \( S \) is thus a measure of /dispersity (disorder) of the distribution/, and *does not depend* on the values of the random variables \(\left\{x_{i}\right\}\),
4) A /one-to-one mapping/ to \(f_{i}=F\left(x_{i}\right)\) leaves the Shannon entropy unchanged,
5) A /many-to-one mapping/ makes the distribution more ordered and decreases \(S\). For example, if the two values, \(x_{1}\) and \(x_{2}\), are mapped onto the same \(f\), the change in the Shannon entropy is
   \[
   \Delta S\left(x_{1}, x_{2} \rightarrow f\right)=\left[p_{1} \ln \frac{p_{1}}{p_{1}+p_{2}}+p_{2} \ln \frac{p_{2}}{p_{1}+p_{2}}\right]<0.
   \]
***** Generalization
The Shannon entropy formula above is for a random variable with a discrete set of outcomes. We can define an entropy for a continuous random variable \(\left(\mathcal{S}_{x}=\{-\infty<x<\infty\}\right)\) as
\[
S=-\int \mathrm{d} x p(x) \ln p(x)=-\langle\ln p(x)\rangle.
\]
There are, however, problems with this definition, as for example \(S\) is not invariant under a one-to-one mapping. (After a change of variable to \(f=F(x)\), the entropy is changed by \(\left\langle\left|F^{\prime}(x)\right|\right\rangle\).)
Canonically conjugate pairs offer a suitable choice of coordinates in classical statistical mechanics, since the Jacobian of a canonical transformation is unity. In such situations, the definition above can be used.
**** Estimation
The entropy \(S\) can also be used to quantify subjective estimates of probabilities. In the absence of any information, the best unbiased estimate is that all \(M\) outcomes are equally likely. This is the distribution of maximum entropy. If additional information is available, the unbiased estimate is obtained by maximizing the entropy subject to the constraints imposed by this information. For example, if it is known that \(\langle F(x)\rangle=f\), we can maximize
\[
S\left(\alpha, \beta,\left\{p_{i}\right\}\right)=-\sum_{i} p(i) \ln p(i)-\alpha\left(\sum_{i} p(i)-1\right)-\beta\left(\sum_{i} p(i) F\left(x_{i}\right)-f\right)
\]
where the Lagrange multipliers \(\alpha\) and \(\beta\) are introduced to impose the constraints of normalization, and \(\langle F(x)\rangle=f\), respectively. The result of the optimization is a distribution \(p_{i} \propto \exp \left(-\beta F\left(x_{i}\right)\right)\), where the value of \(\beta\) is fixed by the constraint. This process can be generalized to an arbitrary number of conditions. It is easy to see that if the first \(n=2 k\) moments (and hence \(n\) cumulants) of a distribution are specified, the unbiased estimate is the exponential of an \(n\)-th order polynomial.
In analogy with Eq. (2.71), we can define an entropy for a continuous random variable \(\left(\mathcal{S}_{x}=\{-\infty<x<\infty\}\right.\) ) as
\[
S=-\int \mathrm{d} x p(x) \ln p(x)=-\langle\ln p(x)\rangle.
\]
There are, however, problems with this definition, as for example \(S\) is not invariant under a one-to-one mapping. (After a change of variable to \(f=F(x)\), the entropy is changed by \(\left\langle\left|F^{\prime}(x)\right|\right\rangle\).) As discussed in the following chapter, canonically conjugate pairs offer a suitable choice of coordinates in classical statistical mechanics, since the Jacobian of a canonical transformation is unity. The ambiguities are also removed if the continuous variable is discretized. This happens quite naturally in quantum statistical mechanics where it is usually possible to work with a discrete ladder of states. The appropriate volume for discretization of phase space is then set by Planck's constant \(\hbar\).
** Thermodynamics
:LOGBOOK:
CLOCK: [2024-06-14 Fri 17:01]--[2024-06-14 Fri 18:31] =>  1:30
:END:
*** Prelude
*Thermodynamics is a phenomenological description of properties of macroscopic systems in thermal equilibrium.*
+ "Imagine yourself as a post-Newtonian physicist intent on understanding the behavior of such a simple system as a container of gas. How would you proceed?"
  The prototype of a successful physical theory is classical mechanics, which describes the intricate motions of particles starting from simple basic laws and employing the mathematical machinery of calculus. By analogy, you may proceed as follows:"
  1) _Idealize the system_
     + Closed Systems :: Start with closed systems insulated by *adiabatic walls* to prevent heat exchange. 
     + Open Systems :: Study systems with *diathermic walls* allowing heat exchange.
  2) _Describe the system in terms of thermodynamic coordinates and state functions_
     + Analogous to coordinates and momenta for point particles, macroscopic systems are described by *thermodynamic coordinates*.
     + Examples: Pressure and volume for fluids, surface tension and area for films, tension and length for wires, electric field and polarization for dielectrics.
     + State functions are defined in thermal equilibrium, where properties are time-invariant over observation intervals.
     + Equilibrium is observation-time-dependent (e.g., glass flows over millennia but is solid over decades).
  3) _Describe the co-evolution of thermodynamic coordinates using phenomenology_
     + Relationships between state functions are governed by thermodynamic laws, based on empirical observations.
     + A logical and mathematical framework is built on these observations to derive useful concepts and testable relationships.
+ The practical impetus for development of the science of thermodynamics in the nineteenth century was not the undertaking of a post-Newtonian theorist, it was the enterprise of scientists and engineers concerned with the advent of heat engines. There are 4 laws of thermodynamics:
  + The zeroth law :: The zeroth law of thermodynamics describes the transitive nature of thermal equilibrium. It states: If two systems, \(A\) and \(B\), are separately in equilibrium with a third system, \(C\), then they are also in equilibrium with one another.
  + The first law :: The amount of work required to change the state of an otherwise adiabatically isolated system depends only on the initial and final states, and not on the means by which the work is performed, or on the intermediate stages through which the system passes.
  + The second law :: There is a number of different formulations of the second law, such as the following two statements: 
    + Kelvin's statement :: No process is possible whose sole result is the complete conversion of heat into work. 
    + Clausius's statement :: No process is possible whose sole result is the transfer of heat from a colder to a hotter body.
  + The third law :: The entropy of all systems at zero absolute temperature is a universal constant that can be taken to be zero. The above statement actually implies that \(\lim _{T \rightarrow 0} S(\mathbf{X}, T)=0\) which is a stronger requirement than the vanishing of the differences \(\Delta S(\mathbf{X}, T)\).
*** Thermal equilibrium
A thermodynamic system is said to be in a state of *thermal equilibrium* when the thermodynamic coordinates that describe it are invariant over observation timescales. Equilibrium is observation-time-dependent (e.g., glass flows over millennia but is solid over decades).
*** The zeroth law
*The zeroth law of thermodynamics describes the transitive nature of thermal equilibrium*.
#+NAME: The zeroth law of thermodynamics
#+begin_law latex
*If two systems,* \(A\) *and* \(B\), *are separately in equilibrium with a third system,* \(C\), *then they are also in equilibrium with one another.*
#+end_law
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-1.1.png]]
#+NAME: The empirical temperature
#+begin_corollary latex
A consequence of the zeroth law is the existence of a state function, the empirical temperature \(\Theta\), such that systems in equilibrium are at the same temperature.
#+end_corollary
#+NAME: The empirical temperature
#+begin_proof latex
Let the equilibrium state of systems \(A\), \(B\), and \(C\) be described by the coordinates \(\{A_{1}, A_{2}, \cdots\}\), \(\{B_{1}, B_{2}, \cdots\}\), and \(\{C_{1}, C_{2}, \cdots\}\), respectively. If \(A\) and \(C\) are in equilibrium, there is a constraint between their coordinates, implying:
\[
f_{A C}\left(A_{1}, A_{2}, \cdots ; C_{1}, C_{2}, \cdots\right)=0
\]
Similarly, the equilibrium between \(B\) and \(C\) implies:
\[
f_{B C}\left(B_{1}, B_{2}, \cdots ; C_{1}, C_{2}, \cdots\right)=0
\]
Assuming mechanical equilibrium, the constraints can be expressed as:
\begin{align*}
C_{1} &= F_{A C}\left(A_{1}, A_{2}, \cdots ; C_{2}, \cdots\right), \\
C_{1} &= F_{B C}\left(B_{1}, B_{2}, \cdots ; C_{2}, \cdots\right)
\end{align*}
Thus, if \(C\) is in equilibrium with both \(A\) and \(B\), we must have:
\[
F_{A C}\left(A_{1}, A_{2}, \cdots ; C_{2}, \cdots\right) = F_{B C}\left(B_{1}, B_{2}, \cdots ; C_{2}, \cdots\right)
\]
According to the zeroth law, \(A\) and \(B\) are also in equilibrium, implying:
\[
f_{A B}\left(A_{1}, A_{2}, \cdots ; B_{1}, B_{2}, \cdots\right)=0
\]
By selecting parameters \(\{A, B\}\) that satisfy this equation and substituting into the equilibrium condition for \(C\), we can simplify and eliminate \(C\)'s coordinates, resulting in:
\[
\Theta_{A}\left(A_{1}, A_{2}, \cdots\right) = \Theta_{B}\left(B_{1}, B_{2}, \cdots\right)
\]
Thus, equilibrium is characterized by a function \(\Theta\) of thermodynamic coordinates, defining the equation of state. Isotherms are described by \(\Theta_{A}\left(A_{1}, A_{2}, \cdots\right) = \Theta\).
#+end_proof
**** The ideal gas temperature scale
While many potential choices for \(\Theta\) exist, the key point is the existence of a state function that constrains the parameters of each system in thermal equilibrium. The function \(\Theta\) is analogous to the force in a mechanical system. For instance, in two connected springs, equilibrium occurs when the forces are equal. This transitive mechanical equilibrium can serve as an analogy for deducing the existence of a mechanical force.
#+NAME: Equilibria of a gas (A) and a magnet (B), and a gas (A) and a wire (C).
#+begin_example latex
As an example, let us consider the following three systems: 
A) a wire of length \(L\) with tension \(F\), 
B) a paramagnet of magnetization \(M\) in a magnetic field \(B\), and 
C) a gas of volume \(V\) at pressure \(P\).
Observations indicate that when these systems are in equilibrium, the following constraints are satisfied between their coordinates:
\begin{gathered}
\left(P+\frac{a}{V^{2}}\right)(V-b)\left(L-L_{0}\right)-c\left[F-K\left(L-L_{0}\right)\right]=0, \\
\left(P+\frac{a}{V^{2}}\right)(V-b) M-d B=0 .
\end{gathered}
The two conditions can be organized into three empirical temperature functions as
\[
\Theta \propto\left(P+\frac{a}{V^{2}}\right)(V-b)=c\left(\frac{F}{L-L_{0}}-K\right)=d \frac{B}{M} .
\]
Note that the zeroth law severely constrains the form of the constraint equation describing the equilibrium between two bodies. Any arbitrary function cannot necessarily be organized into an equality of two empirical temperature functions.
The constraints used in the above example were in fact chosen to reproduce three well-known equations of state that will be encountered and discussed later in this book. In their more familiar forms they are written as
\begin{align*}
(P+a / V^{2})(V-b)=N k_{B} T & \qquad \text {(van der Waals gas)} \\
M=(N \mu_{B}^{2} B) /(3 k_{B} T) & \qquad \text {(Curie paramagnet)} \\
F=(K+D T)(L-L_{0}) & \qquad \text {(Hooke's law for rubber)}
\end{align*}
#+end_example
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-1.2.png]]
#+CAPTION: Equilibria of a gas (A) and a magnet (B), and a gas (A) and a wire (C).
Note that we have employed the symbol for Kelvin temperature \(T\), in place of the more general empirical temperature \(\Theta\). While the zeroth law merely states the presence of isotherms, to set up a practical temperature scale at this stage, a reference system is necessary. This concrete temperature scale can be constructed by using the properties of the ideal gas. *Empirical observations indicate that the product of pressure and volume is constant along the isotherms of any gas that is sufficiently dilute.* The /ideal gas/ refers to this dilute limit of real gases, and the ideal gas temperature is proportional to the product. The constant of proportionality is determined by reference to the temperature of the triple point of the ice-water-gas system, which was set to 273.16 (degrees) kelvin (K) by the 10th General Conference on Weights and Measures in 1954. Using a dilute gas (i.e., as \(P \rightarrow 0\) ) as thermometer, the temperature of a system can be obtained from
\[
T(\mathrm{~K}) \equiv 273.16 \times\left(\lim _{P \rightarrow 0}(P V)_{\text {system }} / \lim _{P \rightarrow 0}(P V)_{\text {ice-water-gas }}\right)
\]
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-1.3.jpg]]
*** The first law
Observations indicate that a principle similar to the conservation of (mechanical) energy operates at the level of macroscopic bodies provided that the system is properly insulated, that is, when the only sources of energy are of mechanical origin.
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-1.4.jpg]]
#+CAPTION: The two adiabatic paths for changing macroscopic coordinates between the initial and final point result in the same change in internal energy.
#+NAME: The internal energy
#+begin_definition latex
The amount of work required to change the state of an otherwise adiabatically isolated system depends only on the initial and final states, and not on the means by which the work is performed, or on the intermediate stages through which the system passes. A consequence of this observation is the existence of a state function, the internal energy \(E(\mathbf{X})\), which can be obtained from the amount of work \(\Delta W\) needed for an adiabatic transformation from an initial state \(\mathbf{X}_{\mathbf{i}}\) to a final state \(\mathbf{X}_{\mathbf{f}}\), using
\[
\Delta W=E\left(\mathbf{X}_{\mathbf{f}}\right)-E\left(\mathbf{X}_{\mathbf{i}}\right)
\]
#+end_definition
Observations indicate that once the adiabatic constraint is removed, the amount of work is no longer equal to the change in the internal energy. 
#+NAME: Heat
#+begin_definition latex
The difference \(\Delta Q=\Delta E-\Delta W\) is defined as the heat intake of the system from its surroundings. Clearly, in such transformations, \(\Delta Q\) and \(\Delta W\) are not separately functions of state in that they depend on external factors such as the means of applying work, and not only on the final states. To emphasize this, for a differential transformation we write
\[
\mathrm{} Q=\mathrm{d} E-\mathrm{} W
\]
where \(\mathrm{d} E=\sum_{i} \partial_{i} E \mathrm{~d} X_{i}\) can be obtained by differentiation, while \(\mathrm{} Q\) and \(\mathrm{} W\) generally cannot.
#+end_definition
#+BEGIN_COMMENT
By convention the signs of work and heat indicate the energy added to the system, and not vice versa. 
#+END_COMMENT
The first law is formulated in terms of the internal energy and heat. It rules out *perpetual motion machines of the first kind*, that is, engines that produce work without consuming any energy.
#+NAME: The first law of thermodynamics
#+begin_law latex
The first law of thermodynamics states that to change the state of a system we need a fixed amount of energy, which can be in the form of mechanical work or heat
\[
\mathrm{d} E= \mathrm{} Q + \mathrm{} W.
\]
This can also be regarded as a way of defining and quantifying the exchanged heat.
#+end_law
**** Quasi-static transformation
A *quasi-static transformation* is one that is performed sufficiently slowly so that the system is always in thermal equilibrium. Thus, at any stage of the process, the state functions of the thermodynamic system exist and can in principle be computed. For such transformations, the work done on the system (equal in magnitude but opposite in sign to the work done by the system) can be related to changes in these coordinates.
#+NAME: Quasi-static transformations
#+begin_example latex
As a mechanical example, consider the stretching of a spring or rubber band. To construct the potential energy of the system as a function of its length \(L\), we can pull the spring sufficiently slowly so that at each stage the external force is matched by the internal force \(F\) from the spring. For such a quasi-static process, the change in the potential energy of the spring is \(\int F \mathrm{~d} L\). If the spring is pulled abruptly, some of the external work is converted into kinetic energy and eventually lost as the spring comes to rest.
#+end_example
Generalizing from the above example, one can typically divide the state functions \(\{\mathbf{X}\}\) into a set of generalized displacements \(\{\mathbf{x}\}\), and their conjugate generalized forces \(\{\mathbf{J}\}\), such that for an infinitesimal quasi-static transformation
\[
\mathrm{} W=\sum_{i} J_{i} \mathrm{~d} x_{i}
\]
The table below provides some common examples of such conjugate coordinates. Note that pressure \(P\) is by convention calculated from the force exerted by the system on the walls, as opposed to the force on a spring, which is exerted in the opposite direction. This is the origin of the negative sign that accompanies hydrostatic work.
#+NAME: Generalized forces and displacements
| System            | Force              |    | Displacement    |   |
|-------------------+--------------------+----+-----------------+---|
| Wire              | tension            | F  | length          | L |
| Film              | surface tension    | S  | area            | A |
| Fluid             | pressure           | -P | volume          | V |
| Magnet            | magnetic field     | H  | magnetization   | M |
| Dielectric        | electric field     | E  | polarization    | P |
| Chemical reaction | chemical potential |   | particle number | N |
The displacements are usually extensive quantities, that is, proportional to system size, while the forces are intensive quantities, that is, independent of size.
**** Joule's free expansion experiment
Measurements indicate that if an ideal gas expands adiabatically (but not necessarily quasi-statically), from a volume \(V_{i}\) to \(V_{f}\), the initial and final temperatures are the same.
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-1.5.jpg]]
#+NAME: Internal energy of an ideal gas
#+begin_corollary latex
Internal energy of an ideal gas depends only on temperature.
#+end_corollary
#+NAME: Internal energy of an ideal gas
#+begin_proof latex
As the transformation is adiabatic \((\Delta Q=0)\) and there is no external work done on the system \((\Delta W=\) \(0)\), the internal energy of the gas is unchanged. Since the pressure and volume of the gas change in the process, but its temperature does not, we conclude that the internal energy depends only on temperature, that is, \(E(V, T)=E(T)\).
#+end_proof
This property of the ideal gas is in fact a consequence of the form of its equation of state.
**** Response functions
*Response functions* are the usual method for characterizing the macroscopic behavior of a thermodynamic system. They are experimentally measured from the changes of thermodynamic coordinates with external probes. Some common response functions are /heat capacities/, /force constants/, and /thermal responses/.
***** Heat capacities
*Heat capacities* are obtained from the change in temperature upon addition of heat to the system.
Since heat is not a function of state, the path by which it is supplied must also be specified. For a gas we can calculate 
1) the heat capacities at *constant volume*, denoted by \(C_{V}=\mathrm{} Q /\left.\mathrm{d} T\right|_{V}\)
\[
C_{V}=\left.\frac{\mathrm{} Q}{\mathrm{~d} T}\right|_{V}=\left.\frac{\mathrm{d} E-\mathrm{} W}{\mathrm{~d} T}\right|_{V}=\left.\frac{\mathrm{d} E+P \mathrm{~d} V}{\mathrm{~d} T}\right|_{V}=(\partial_T E)_{V}. \tag{1}
\]
2) the heat capacities at constant pressure, denoted by \(C_{V}=\mathrm{} Q /\left.\mathrm{d} T\right|_{P}\)
\[
C_{P}=\left.\frac{\mathrm{} Q}{\mathrm{~d} T}\right|_{P}=\left.\frac{\mathrm{d} E-\mathrm{} W}{\mathrm{~d} T}\right|_{P}=\left.\frac{\mathrm{d} E+P \mathrm{~d} V}{\mathrm{~d} T}\right|_{P}=(\partial_T E)_{P} + P (\partial_T V)_{P}. \tag{2}
\]
The latter is larger since some of the heat is used up in the work done in changes of volume.
Since the internal energy of an ideal gas depends only on its temperature (see Joule's free expansion experiment), \((\partial_T E)_{V} = (\partial_T E)_{P}=\mathrm{D}_T E\), and (2) simplifies to
\[
C_{P} - C_{V} = P (\partial_T V)_{P} = P V \alpha_{P} = \frac{P V}{T} \equiv N k_{B}.
\]
The last equality follows from /extensivity/: for a given amount of ideal gas, the constant \(P V / T\) is proportional to \(N\), the number of particles in the gas; the ratio is /Boltzmann's constant/ with a value of \(k_{B} \approx 1.4 \times 10^{-23} \mathrm{JK}^{-1}\).
***** Force constants
*Force constants* measure the (infinitesimal) ratio of displacement to force and are generalizations of the spring constant. As examples, we have
1) *isothermal compressibility of a gas* 
\[\kappa_{T} = - V^{-1} (\partial_P V)_{T} \]
2) *susceptibility of a magnet* 
\[\chi_{T} = V^{-1} (\partial_B M)_{T}\]
From the equation of state of an ideal gas \(P V \propto T\), we obtain its isothermal compressibility \(\kappa_{T}=1 / P\).
***** Thermal responses
*Thermal responses* probe the change in the thermodynamic coordinates with temperature.
1) the *expansivity* of a gas is given by 
\[
\alpha_{P}= V^{-1} (\partial_T V / \partial T)_{P}.
\]
Using the equation of state of an ideal gas, its expansivity is \(1 / T\).
**** Reversible transformation
A *reversible transformation* is one that can be run backward in time by simply reversing its inputs and outputs. /It is the thermodynamic equivalent of frictionless motion in mechanics/. All reversible transformations are quasi-static transformations. The converse is not true in general.
#+NAME: Reversible transformations
#+begin_theorem latex
If a transformation is reversible, then it is quasi-static too.
#+end_theorem
#+NAME: Reversible transformations
#+begin_proof latex
1) Time reversibility at all stages of the transformation implies thermal equilibrium at all stages of the transformation.
2) A quasi-static transformation, by definition, is one in which the system is always in thermal equilibrium.
3) From (1) and (2) it follows that if a transformation is reversible, then it is quasi-static.
#+end_proof
#+NAME: Quasi-static transformation
#+begin_theorem latex
If a transformation is quasi-static, then it need not necessarily be reversible.
#+end_theorem
#+NAME: Quasi-static transformations
#+begin_example latex
Consider a non-adiabatic expansion of an ideal gas into a colder medium. The expansions occurs sufficiently slowly that the transformation is quasi-static. Suppose that the process was reversible. The sole result of the process is a transfer of heat from a hotter body (the gas) to a colder body (the surrounding). The reverse process would be one whose sole result is the transfer of heat from a colder body (the surroundings) to a hotter body (the gas). This is a violation of Clausius's statement of the second law. The process thus, despite being quasi-static, cannot have been reversible.
#+end_example
*** The second law
An *ideal heat engine* works by taking in a certain amount of heat \(Q_{H}\), from a heat source (for example a coal fire), converting a portion of it to work \(W\), and dumping the remaining heat \(Q_{C}\) into a heat sink (e.g., atmosphere). The *efficiency* of the engine is calculated from
\[
\eta=\frac{W}{Q_{H}}=\frac{Q_{H}-Q_{C}}{Q_{H}} \leq 1 .
\]
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-1.6.jpg]]
An *ideal refrigerator* is like an ideal heat engine running backward, that is, using work \(W\) to extract heat \(Q_{C}\) from a cold system, and dumping heat \(Q_{H}\) at a higher temperature. The *efficiency* of a refrigerator is calculated from
\[
\omega=\frac{Q_{C}}{W}=\frac{Q_{C}}{Q_{H}-Q_{C}}
\]
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-1.6.jpg]]
The observation that the natural direction for the flow of heat is from hotter to colder bodies is the essence of *the second law of thermodynamics*. We work with two equivalent formulations of the second law:
#+NAME: The second law of thermodynamics (Kelvin's statement)
#+begin_postulate latex
No process is possible whose sole result is the complete conversion of heat into work. (K)
#+end_postulate
#+NAME: The second law of thermodynamics (Clausius's statement)
#+begin_postulate latex
No process is possible whose sole result is the transfer of heat from a colder to a hotter body. (C)
#+end_postulate
The first law rules out *perpetual motion machines of the first kind*. It does not however forbid the existence of an engine that produces work by converting water to ice. Such a *perpetual motion machines of the second kind* is ruled out by the second law of thermodynamics: an: ideal heat engines are ruled out by /Kelvin's statement/, an ideal refrigerator is ruled out by /Clausius's statement/.
#+NAME: (K) <=> (C)
#+begin_lemma latex
Kelvin's statement of the second law is equivalent to Clausius's statement of the second law.
#+end_lemma
#+NAME: (K) <=> (C)
#+begin_proof latex
(1) \(\neg (C) \Longrightarrow \neg (K)\)
+ Assume a machine that violates Clausius's statement (C) by taking heat \(Q\) from a cooler region to a hotter one.
+ Now consider an engine operating between these two regions, taking heat \(Q_{H}\) from the hotter one and dumping \(Q_{C}\) at the colder sink.
+ The combined system takes \(Q_{H}-Q\) from the hot source, produces work equal to \(Q_{H}-Q_{C}\), and dumps \(Q_{C}-Q\)  at the cold sink.
+ If we adjust the engine output such that \(Q_{C}=Q\), the net result is a \(100 \%\) efficient engine, in violation of Kelvin's statement.
(2) \(\neg (K) \Longrightarrow \neg (C)\)
+ Assume a machine that violates Kelvin's statement (K) by taking heat \(Q\) and converting it completely to work.
+ The work output of this machine can be used to run a refrigerator, with the net outcome of transferring heat from a colder to a hotter body, in violation of Clausius's statement.
(3) \( (K) \Longleftrightarrow (C) \) (follows from (1) and (2)).
#+end_proof
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-1.7.jpg]]
#+CAPTION: A machine violating Clausius's statement (C) can be connected to an engine, resulting in a combined device (K) that violates Kelvin's statement.
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-1.8.jpg]]
#+CAPTION: A machine violating Kelvin's statement (K) can be connected to a refrigerator, resulting in violation of Clausius's statement (C).
*** Carnot engines
A *Carnot engine* is defined in terms of a reversible, cyclic transformation. /The distinguishing characteristic of the Carnot engine is that heat exchanges with the surroundings are carried out only at two temperatures./
#+NAME: Carnot engine
#+begin_definition latex
A Carnot engine is any engine that is /reversible/, runs in a /cycle/, with all of its heat exchanges taking place at a source temperature \(T_{H}\), and a sink temperature \(T_{C}\)
#+end_definition
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-1.9.jpg]]
#+CAPTION: A Carnot engine operates between temperatures T_H and T_C, with no other heat exchanges.
#+BEGIN_COMMENT
The zeroth law allows us to select two isotherms at temperatures \(T_{H}\) and \(T_{C}\) for these heat exchanges. To complete the Carnot cycle we have to connect these isotherms by reversible adiabatic paths in the coordinate space. However, since heat is not a function of state, we don't know how to construct such paths in general.
#+END_COMMENT
**** Example: Ideal gas carnot engine
Let us compute the adiabatic curves for a Carnot engine using a mono-atomic ideal gas as working substance. A mono-atomic ideal gas has internal energy
\[
E=\frac{3}{2} N k_{B} T=\frac{3}{2} P V
\]
Along a quasi-static path (all reversible transformations are quasi-static transformations), the first law implies
\[
\mathrm{} Q=\mathrm{d} E-\mathrm{d} W=\mathrm{d}\left(\frac{3}{2} P V\right)+P \mathrm{~d} V=\frac{5}{2} P \mathrm{~d} V+\frac{3}{2} V \mathrm{~d} P
\]
Since the curves are /adiabatics/ \(\mathrm{} Q =0\) so that
\[
\frac{\mathrm{d} P}{P}+\frac{5}{3} \frac{\mathrm{d} V}{V}=0, \Longrightarrow P V^{\gamma}=\text { constant }
\]
with \(\gamma=5 / 3\).
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-1.10.jpg]]
#+CAPTION: The Carnot cycle for an ideal gas, with isothermal and adiabatic paths indicated by solid and dashed lines, respectively.
The adiabatic curves are clearly distinct from the isotherms, and we can select two such curves to intersect our isotherms, thereby completing a *Carnot cycle*. The assumption of \(E \propto T\) is not necessary. A similar construction is possible for any two-parameter system with \(E(J, x)\).
**** Carnot's theorem
*Carnot's theorem* is a statement about the /efficiency/ of a Carnot engine.
#+NAME: Carnot's theorem
#+begin_theorem latex
No engine operating between two reservoirs (at temperatures \(T_{H}\) and \(T_{C}\)) is more efficient than a Carnot engine operating between them.
#+end_theorem
The proof of Carnot's theorem relies on:
1) Reversibility of the Carnot engine which allows operating it as an ideal refrigerator by use of a non-Carnot engine.
2) Clausius's statement of the second law of thermodynamics.
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-1.11.jpg]]
#+CAPTION: A generic engine is used to run a Carnot engine in reverse.
#+NAME: Carnot's theorem
#+begin_proof latex
+ Since a Carnot engine is reversible, it can be run backward as a refrigerator. Use the non-Carnot engine to run the Carnot engine backward.
+ Let \(Q_{H}, Q_{C}\), and \(Q_{H}^{\prime}\), \(Q_{C}^{\prime}\) denote the heat exchanges of the non-Carnot and Carnot engines by respectively. 
+ The net effect of the two engines is to transfer heat equal to \(Q_{H}-Q_{H}^{\prime}=Q_{C}-Q_{C}^{\prime}\) from \(T_{H}\) to \(T_{C}\). 
+ Clausius's statement of the second law implies \(Q_{H} \geq Q_{H}^{\prime}\) (the quantity of transferred heat cannot be negative).
+ Since the same quantity of work \(W\) is involved in this process, it follows
\[
\frac{W}{Q_{H}^{\prime}} \geq \frac{W}{Q_{H}} \quad \Longrightarrow \quad \eta_{\text {Carnot}} \geq \eta_{\text {non-Carnot}}.
\]
#+end_proof
#+NAME: Universal efficiency of Carnot engines
#+begin_corollary latex
All reversible (Carnot) engines have the same universal efficiency \(\eta\left(T_{H}, T_{C}\right)\).
#+end_corollary
#+NAME: Universal efficiency of Carnot engines
#+begin_proof latex
The corollary follows immediately since each can be used to run any other one backward.
#+end_proof
**** Efficiency of a Carnot engine
1) Consider two Carnot engiens running in series, one between temperatures \(T_{1}\) and \(T_{2}\), and the other between \(T_{2}\) and \(T_{3}\left(T_{1}>T_{2}>T_{3}\right)\).
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-1.12.jpg]]
#+CAPTION: Two Carnot engines connected in series are equivalent to a third.
2) Denote the heat exchanges, and work outputs, of the two engines by \(Q_{1}, Q_{2}, W_{12}\), and \(Q_{2}\), \(Q_{3}, W_{23}\), respectively.
3) The heat dumped by the first engine is taken in by the second, so that the combined effect is another Carnot engine (since each component is reversible) with heat exchanges \(Q_{1}, Q_{3}\), and work output \(W_{13}=W_{12}+W_{23}\).
Using the corollary on the universal efficiency of Carnot engines, the three heat exchanges are related by
\[
\left\{\begin{array}{l}
Q_{2}=Q_{1}-W_{12}=Q_{1}\left[1-\eta\left(T_{1}, T_{2}\right)\right], \\
Q_{3}=Q_{2}-W_{23}=Q_{2}\left[1-\eta\left(T_{2}, T_{3}\right)\right]=Q_{1}\left[1-\eta\left(T_{1}, T_{2}\right)\right]\left[1-\eta\left(T_{2}, T_{3}\right)\right], \\
Q_{3}=Q_{1}-W_{13}=Q_{1}\left[1-\eta\left(T_{1}, T_{3}\right)\right] .
\end{array}\right.
\]
Comparison of the final two expressions yields
\[
1-\eta(T_{1}, T_{2}) = \frac{1-\eta(T_{1}, T_{3})}{1-\eta(T_{2}, T_{3})}
\]
This property implies that \(1-\eta\left(T_{1}, T_{2}\right)\) can be written as a ratio of the form \(f\left(T_{2}\right) / f\left(T_{1}\right)\), which by convention is set to \(T_{2} / T_{1}\), that is,
\begin{align*}
1-\eta\left(T_{1}, T_{2}\right) = \frac{Q_{2}}{Q_{1}} \equiv \frac{T_{2}}{T_{1}} \Longrightarrow \eta\left(T_{H}, T_{C}\right) & =\frac{T_{H}-T_{C}}{T_{H}}.
\end{align*}
**** The thermodynamic temperature scale
The universal efficiency of Carnot engines allows for the construction of the *thermodynamic temperature scale*: it is at least theoretically possible to construct a Carnot engine using an ideal gas (or any other two-parameter system) as working substance. A corollary of Carnot's theorem is that independent of the material used, and design and construction, all such cyclic and reversible engines have the same maximum theoretical efficiency.
Since this maximum efficiency is only dependent on the two temperatures, it can be used to construct a temperature scale. Such a temperature scale has the attractive property of being independent of the properties of any material (e.g., the ideal gas).
To construct such a scale we first obtained a constraint on the form of the efficiency \(\eta\left(T_{H}, T_{C}\right)\) of a Carnot engine:
\begin{align*}
\eta\left(T_{H}, T_{C}\right) & =\frac{T_{H}-T_{C}}{T_{H}}.
\end{align*}
The equation above defines temperature up to a constant of proportionality, which is again set by choosing the triple point of water, ice, and steam to \(273.16 \mathrm{~K}\). In fact, by running a Carnot cycle for a perfect gas, it can be proved that the ideal gas temperature scale and the thermodynamic temperature scales are equivalent. Clearly, the thermodynamic scale is not useful from a practical standpoint; its advantage is conceptual, in that it is independent of the properties of any substance.
#+NAME: Positivity of thermodynamic temperatures
#+begin_corollary latex
All thermodynamic temperatures are positive.
#+end_corollary
#+NAME: Positivity of thermodynamic temperatures
#+begin_proof latex
According to 
\begin{align*}
\eta\left(T_{H}, T_{C}\right) & =\frac{T_{H}-T_{C}}{T_{H}}.
\end{align*}
the heat extracted from a temperature \(T\) is proportional to it. If a negative temperature existed, an engine operating between it and a positive temperature would extract heat from both reservoirs and convert the sum total to work, in violation of Kelvin's statement of the second law.
#+end_proof
*** Entropy
*Clausius's theorem* is a statement about total heat increments supplied to a system at temperature \( T \) during a cyclic transformation. The existence of a state function named entropy is a consequence of Clausius's theorem.
#+NAME: Clausius's theorem
#+begin_theorem latex
For any cyclic transformation (reversible or not), \(\oint \mathrm{} Q / T \leq 0\), where \(\mathrm{} Q\) is the heat increment supplied to the system at temperature \(T\).
#+end_theorem
The proof of Clausius's theorem relies on
#+NAME: Clausius's theorem
#+begin_proof latex
+ Subdivide the cycle into a series of infinitesimal transformations (not necessarily quasi-static) in which the system receives energy in the form of heat \(\mathrm{} Q\) and work \(\mathrm{} W\).
+ Direct all the heat exchanges of the system to one port of a Carnot engine, which has another reservoir at a fixed temperature \(T_{0}\). 
+ Since the sign of \(\mathrm{} Q\) is not specified, the Carnot engine must operate a series of infinitesimal cycles in either direction.
+ To deliver heat \(\mathrm{} Q\) to the system at some stage, the engine has to extract heat \(\mathrm{} Q_{R}\) from the fixed reservoir. If the heat is delivered to a part of the system that is locally at a temperature \(T\), then according to
\begin{align*}
1-\eta\left(T_{1}, T_{2}\right) = \frac{Q_{2}}{Q_{1}} \equiv \frac{T_{2}}{T_{1}},
\end{align*}
\[
\mathrm{} Q_{R}=T_{0} \frac{\mathrm{} Q}{T}.
\]
+ After the cycle is completed, the system and the Carnot engine return to their original states. The net effect of the combined process is extracting heat \(Q_{R}=\oint \mathrm{} Q_R\) from the reservoir and converting it to external work \(W\).
+ The work \(W=Q_{R}\) is the sum total of the work elements done by the Carnot engine, and the work performed by the system in the complete cycle.
+ By Kelvin's statement of the second law, \(Q_{R}=W \leq 0\), that is,
\[
T_{0} \oint \frac{\text { Q }}{T} \leq 0, \quad \Longrightarrow \oint \frac{\text { Q }}{T} \leq 0 \tag{1}
\]
since \(T_{0}>0\). 
#+end_proof
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-1.13.jpg]]
#+CAPTION: The heat exchanges of the system are directed to a Carnot engine with a reservoir at T_0.
#+BEGIN_COMMENT
Note that \(T\) in (1) above refers to the temperature of the whole system only for quasi-static processes in which it can be uniquely defined throughout the cycle. Otherwise, it is just a local temperature (say at a boundary of the system) at which the Carnot engine deposits the element of heat.
#+END_COMMENT
+ Consequences of Clausius's theorem:
  1. For a reversible cycle \(\oint \mathrm{} Q_{\mathrm{rev}} / T=0\), since by running the cycle in the opposite direction \(\mathrm{} Q_{\text {rev}} \rightarrow-\mathrm{} Q_{\mathrm{rev}}\), and by Clausius's theorem \(\mathrm{} Q_{\mathrm{rev}} / T\) is both non-negative and non-positive, hence zero. This result implies that the integral of \(\mathrm{} Q_{\text {rev }} / T\) between any two points \(A\) and \(B\) is independent of path, since for two paths (1) and (2)
  \[
  \int_{A}^{B} \frac{\mathrm{} Q_{\mathrm{rev}}^{(1)}}{T_{1}}+\int_{B}^{A} \frac{\mathrm{} Q_{\mathrm{rev}}^{(2)}}{T_{2}}=0, \quad \Longrightarrow \int_{A}^{B} \frac{\mathrm{} Q_{\mathrm{rev}}^{(1)}}{T_{1}}=\int_{A}^{B} \frac{\mathrm{} Q_{\mathrm{rev}}^{(2)}}{T_{2}}. \tag{1}
  \]
  2. Using (1) we can construct yet another function of state, the *entropy* \(S\). 
  #+NAME: Entropy
  #+begin_definition latex
  The integral \( \int_{A}^{B} \mathrm{} Q_{\mathrm{rev}}/T \) between any two arbitrary points \(A\) and \(B\) is path independent, a consequence of Clausius's theorem. This allows us to define a state function called the entropy as follows
  \[
  S(B)-S(A) \equiv \int_{A}^{B} \frac{\mathrm{} Q_{\mathrm{rev}}}{T}. \tag{2}
  \]
  #+end_definition
  Note that (2) only defines the entropy up to an overall constant.
  #+ATTR_HTML: :width 500px
  [[file:~/.local/images/kardar-spop-1.14.png]]
  #+CAPTION: (a) A reversible cycle. (b) Two reversible paths between A and B. (c) The cycle formed from a generic path between A and B, and a reversible one.
  For reversible processes, we can now compute the heat from \(\mathrm{} Q_{\mathrm{rev}}=T \mathrm{~d} S\). This allows us to construct adiabatic curves for a general (multi-variable) system from the condition of constant \(S\).
  3. For a reversible (hence quasi-static) transformation, \(\mathrm{} Q=T \mathrm{~d} S\) and \(\mathrm{} W=\sum_{i} J_{i} \mathrm{~d} x_{i} + \sum_{n} \mu_{n} \mathrm{~d} N_{n} \). The first law
  \[
  \mathrm{d} E=\mathrm{} Q+\mathrm{} W=T \mathrm{~d} S + \mathbf{J} \cdot \mathrm{d} \mathbf{x} + \boldsymbol{\mu} \cdot \mathrm{d} \mathbf{N}. \\tag{3}
  \]
  We can see that in this equation \(S\) and \(T\) appear as conjugate variables, with \(S\) playing the role of a displacement, and \(T\) as the corresponding force. This identification allows us to make the correspondence between mechanical and thermal exchanges more precise, although we should keep in mind that unlike its mechanical analog, temperature is always positive. While to obtain (3) we had to appeal to reversible transformations, it is important to emphasize that it is a relation between functions of state. (3) is the most /fundamental identity of thermodynamics/.
  4. The number of independent variables necessary to describe a thermodynamic system also follows from (3). If there are \(n\) methods of doing work on a system, represented by \(n\) conjugate pairs \(\left(J_{i}, x_{i}\right)\), then \(n+1\) independent coordinates are necessary to describe the system. (We shall ignore possible constraints between the mechanical coordinates.) For example, choosing \(\left(E,\left\{x_{i}\right\}\right)\) as coordinates, it follows from (3) that
  \[
  (\partial_E S)_{\mathbf{x}}= T^{-1} \quad \text { and } (\partial_{x_{i}} S)_{E,\,x_{j \neq i}} = -J_{i}/T.
  \]
  Note that \(\mathbf{x}\) and \(\mathbf{J}\) are shorthand notations for the parameter sets \(\left\{x_{i}\right\}\) and \(\left\{J_{i}\right\}\) (also see The Gibbs phase rule).
  5. Consider an irreversible change from \(A\) to \(B\). Make a complete cycle by returning from \(B\) to \(A\) along a reversible path. Then
  \[
  \int_{A}^{B} \frac{\mathrm{} Q}{T}+\int_{B}^{A} \frac{\mathrm{} Q_{\mathrm{rev}}}{T} \leq 0, \quad \Longrightarrow \int_{A}^{B} \frac{\mathrm{} Q}{T} \leq S(B)-S(A) .
  \]
  In differential form, this implies that \(\mathrm{d} S \geq \mathrm{} Q / T\) for any transformation. In particular, consider adiabatically isolating a number of subsystems, each initially separately in equilibrium. As they come to a state of joint equilibrium, since the net \(\mathrm{}Q=0\), we must have \(\delta S \geq 0\).
  #+ATTR_HTML: :width 200px
  [[file:~/.local/images/kardar-spop-1.15.jpg]]
  #+CAPTION: The initially isolated subsystems are allowed to interact, resulting in an increase of entropy.
  Thus an adiabatic system attains a maximum value of entropy in equilibrium since spontaneous internal changes can only increase \(S\). The direction of increasing entropy thus points out the arrow of time, and the path to equilibrium. It is thus the appropriate thermodynamic potential that describes the approach to equilibrium of adiabatic transformations with with no mechanical work.
  The mechanical analog is a point mass placed on a surface, with gravity providing a downward force. As various constraints are removed, the particle will settle down at locations of decreasing height. The statement about the increase of entropy is thus no more mysterious than the observation that objects tend to fall down under gravity!
*** Thermodynamic potentials
The time evolution of an adiabatically isolated system (\(\mathrm{} Q=0\)) with no mechanical work (\(\mathrm{} W=0\)) toward thermal equilibrium is governed by the second law of thermodynamics: the entropy \( S \) must increase in any spontaneous change and reaches a maximum in equilibrium, i.e., the non-equilibrium variations of the entropy obey the inequality \( \delta S \geq 0 \) (see entropy). For out-of-equilibrium systems that are not adiabatically isolated, and may also be subject to external mechanical work, it is usually possible to define other thermodynamic potentials that are extremized when the system is in equilibrium. Four commonly encountered thermodynamic potentials are:
+ Enthalpy :: adiabatic transformations (\(\mathrm{} Q=0\)) involving mechanical work (\(\mathrm{} W\neq0\)) at constant external force \( J \) and in the absence of chemical work.
   \[
   \delta H \leq 0, \quad \text { where } \quad H(S,\, \mathbf{J})=E(S,\, \mathbf{x})-\mathbf{J} \cdot \mathbf{x} \tag{1}
   \]
+ Helmholtz free energy :: isothermal transformations in the absence of mechanical work (\(\mathrm{} W=0\)) and chemical work.
   \[
   \delta F \leq 0, \quad \text { where } F(T,\,\mathbf{x}) = E(S,\, \mathbf{x})-T S \tag{2}
   \]
+ Gibbs free energy :: isothermal transformations involving mechanical work (\(\mathrm{} W\neq0\)) at constant external force \( J \) and in the absence of chemical work.
  \[
  \delta G \leq 0, \quad \text { where } \quad G(T, \mathbf{J}) = H(S, \mathbf{J}) - T S = E(S,\, \mathbf{x})-\mathbf{J} \cdot \mathbf{x}-T S \tag{3}
  \]
+ Grand potential :: isothermal transformations in the absence of mechanical work (\(\mathrm{} W=0\)) and involving chemical work at constant chemical potential \( \mu \).
   \[
   \delta \mathcal{G} \leq 0, \quad \text { where } \mathcal{G}(T,\,\mathbf{x}, \boldsymbol{\mu})=E(S,\, \mathbf{x},\, \mathbf{N})-T S-\boldsymbol{\mu} \cdot \mathbf{N} \tag{4}
   \]
#+BEGIN_COMMENT
Chemical potentials measures the work necessary to add additional particles to the system.
#+END_COMMENT
Equations (1), (2), (3) and (4) are examples of *Legendre transforms*, used to change variables to the most natural set of coordinates for describing a particular situation.  The strategy behind the use of Legendre transforms in thermodynamics is to shift from a function that depends on a variable to a new (conjugate) function that depends on a new variable, the conjugate of the original one. The new variable is the partial derivative of the original function with respect to the original variable (recall that from the the fundamental identity of thermodynamics \( \mathrm{d} E=T \mathrm{~d} S+\mathbf{J} \cdot \mathrm{d} \mathbf{x} + \boldsymbol{\mu} \cdot \mathrm{d} \mathbf{N} \) it follows that \( T = \partial_{S} E(S, \mathbf{x}, \mathbf{N}) \), \( \mathbf{J} = \nabla_{\mathbf{x}} E(S, \mathbf{x}, \mathbf{N})\), and \(  \)\(\boldsymbol{\mu} = \nabla_{\boldsymbol{N}} E(S,\,\mathbf{x},\,\mathbf{N})\)). The new function is the difference between the original function and the product of the old and new variables. Typically, this transformation is useful because it shifts the dependence of, e.g., the energy from an extensive variable to its conjugate intensive variable, which can often be controlled more easily in a physical experiment. 
The table below summarizes the inequalities satisfied by the above thermodynamic functions.
#+NAME: Inequalities satisfied by thermodynamic potentials
|            | Q=0   | constant T |
|------------+--------+------------|
| W=0       | S  0 | F  0     |
| constant J | H  0 | G  0     |
**** Enthalpy
+ *Enthalpy* is useful for *adiabatic transformations* \((\mathrm{} Q=0)\) *involving mechanical work under the action of a a constant external force.*
+ The coordinate set \((S, \mathbf{J})\) (the quantities kept constant during an adiabatic transformation with constant external force) is the most suitable for describing the enthalpy.
+ For any set of displacements \(\mathbf{x}\), at constant (externally applied) generalized forces \(\mathbf{J}\), the work input to the system is \(\mathrm{W} \leq \mathbf{J} \cdot \delta \mathbf{x}\). (Equality is achieved for a quasi-static change with \(\mathbf{J}=\mathbf{J}_{i}\), but there is generally some loss of the external work to dissipation. Since \(\mathrm{} Q=0\), using the first law \(\delta E \leq \mathbf{J} \cdot \delta \mathbf{x}\), and
  \[
  \delta H \leq 0, \quad \text { where } \quad H=E-\mathbf{J} \cdot \mathbf{x} \tag{1}
  \]
  is the /Enthalpy/. The *minimum enthalpy principle* merely formulates the observation that stable mechanical equilibrium is obtained by minimizing the net potential energy of the system plus the external agent.
+ The variations of \(H\) in thermal equilibrium are given by
  \[
  \mathrm{d} H=\mathrm{d} E-\mathrm{d}(\mathbf{J} \cdot \mathbf{x})=T \mathrm{~d} S+\mathbf{J} \cdot \mathrm{d} \mathbf{x}-\mathbf{x} \cdot \mathrm{d} \mathbf{J}-\mathbf{J} \cdot \mathrm{d} \mathbf{x}=T \mathrm{~d} S-\mathbf{x} \cdot \mathrm{d} \mathbf{J}. \tag{2}
  \]
  #+BEGIN_COMMENT
  The equality in (2), and the inequality in (1), are a possible source of confusion. Equation (1) refers to variations of \(H\) on approaching equilibrium as some parameter that is not a function of state is varied (e.g., the velocity of the particle joined to the spring in the above example). By contrast, Eq. (2) describes a relation between equilibrium coordinates. To differentiate the two cases, the former (non-equilibrium) variations is denoted using \(\delta\).
  #+END_COMMENT
+ The following /Maxwell's relation/ follows from (2)
  \[
  x_{i}= - (\partial_{J_{i}} H)_{S, J_{j \neq i}}
  \]
+ Variations of the enthalpy with temperature are related to heat capacities at constant force
  \[
  C_{P}=\left.\frac{\mathrm{d} Q}{\mathrm{~d} T}\right|_{P}=\left.\frac{\mathrm{d} E+P \mathrm{~d} V}{\mathrm{~d} T}\right|_{P}=\left.\frac{\mathrm{d}(E+P V)}{\mathrm{d} T}\right|_{P}=\left.\frac{\mathrm{d} H}{\mathrm{~d} T}\right|_{P} .
  \]
  Note, however, that a change of variables is necessary to express \(H\) in terms of \(T\), rather than the more natural variable \(S\).
***** Example
Consider a spring of natural extension \(L_{0}\) and spring constant \(K\), subject to the force \(J=m g\) exerted by a particle of mass \(m\). For an extension \(x=L-L_{0}\), the internal potential energy of the spring is \(U(x)=K x^{2} / 2\), while there is a change of \(-m g x\) in the potential energy of the particle. Mechanical equilibrium is obtained by minimizing \(K x^{2} / 2-m g x\) at an extension \(x_{\mathrm{eq}}=\) \(m g / K\). The spring at any other value of the displacement initially oscillates before coming to rest at \(x_{\mathrm{eq}}\) due to friction. For a more general potential energy \(U(x)\), the internally generated force \(J_{i}=-\mathrm{d} U / \mathrm{d} x\) has to be balanced with the external force \(J\) at the equilibrium point.
**** Helmholtz free energy
+ *Helmholtz free energy* is useful for *isothermal transformations in the absence of mechanical work* \((\mathrm{} W=0)\).
+ The coordinate set \((T, \mathbf{x})\) (the quantities kept constant during an isothermal transformation with no work) is most suitable for describing the free energy.
+ It is rather similar to enthalpy, with \(T\) taking the place of \(J\). From Clausius's theorem, the heat intake of a system at a constant temperature \(T\) satisfies \( \mathrm{} Q \leq T \delta S\). Hence \(\delta E=\) \(\mathrm{} Q+\mathrm{} W \leq T \delta S\), and
  \[
  \delta F \leq 0, \quad \text { where } F=E-T S \tag{1}
  \]
  is the /Helmholtz free energy/.
+ The variations of \(F\) in thermal equilibrium are given by
  \[
  \mathrm{d} F=\mathrm{d} E-\mathrm{d}(T S)=T \mathrm{~d} S+\mathbf{J} \cdot \mathrm{d} \mathbf{x}-S \mathrm{~d} T-T \mathrm{~d} S=-S \mathrm{~d} T+\mathbf{J} \cdot \mathrm{d} \mathbf{x} \tag{2}
  \]
 + The following Maxwell relations follow from (2)
  \[
  J_{i}= (\partial_{x_i} F)_{T,\,x_{j \neq i}} \qquad S = - (\partial_{T} F)_{\mathbf{x}} \tag{3}
  \]
+ The internal energy can also be calculated from \(F\) using
\[
E=F+T S=F- T (\partial_T F)_{\mathbf{x}} = -T^{2} (\partial_{T} (F / T))_{\mathbf{x}}. \tag{4}
\]
***** Example
Consider \(N\) particles of supersaturated steam in a container of volume \(V\) at a temperature \(T\). How can we describe the approach of steam to an equilibrium mixture with \(N_{w}\) particles in the liquid and \(N_{s}\) particles in the gas phase? The fixed coordinates describing this system are \(V, T\), and \(N\). The appropriate thermodynamic potential is the Helmholtz free energy \(F(V, T, N)\), whose variations satisfy
\[
\mathrm{d} F=\mathrm{d}(E-T S)=-S \mathrm{~d} T-P \mathrm{~d} V+\mu \mathrm{d} N .
\]
#+ATTR_HTML: :width 200px
[[file:~/.local/images/kardar-spop-1.17.jpg]]
#+CAPTION: Condensation of water from supersaturated steam.
Before the system reaches equilibrium at a particular value of \(N_{w}\), it goes through a series of non-equilibrium states with smaller amounts of water. If the process is sufficiently slow, we can construct an out-of-equilibrium value for \(F\) as
\[
F\left(V, T, N \mid N_{w}\right)=F_{w}\left(T, N_{w}\right)+F_{s}\left(V, T, N-N_{w}\right)
\]
which depends on an additional variable \(N_{w}\). (It is assumed that the volume occupied by water is small and irrelevant.) According to (1), the point of ermal equilibrium is obtained by minimizing \(F\) with respect to this variable. Since
\[
\delta F= (\partial_{N_{w}} F_{w})_{T, V}\,\delta N_{w} - (\partial_{N_{s}} F_{s})_{T, V}\,\delta N_{w},
\]
and \((\partial_N F)_{T, V}=\mu\) from (2), the equilibrium condition can be obtained by equating the chemical potentials, that is, from \(\mu_{w}(V, T)=\mu_{s}(V, T)\).
#+ATTR_HTML: :width 300px
[[file:~/.local/images/kardar-spop-1.18.jpg]]
#+CAPTION: The net free energy has a minimum as a function of the amount of water.steam.
The identity of chemical potentials is the condition for chemical equilibrium. Naturally, to proceed further we need expressions for \(\mu_{w}\) and \(\mu_{s}\) (see Gibbs-Duhem relation).
**** Gibbs free energy
+ *Gibbs free energy* is useful for *isothermal transformations involving mechanical work at constant external force.*
+ The coordinate set \((T, \mathbf{J})\) (the quantities kept constant during an isothermal transformations involving mechanical work at constant external force) is the most suitable for describing the Gibbs free energy.
+ The natural inequalities for work and heat input into the system are given by \(\mathrm{} W \leq \mathbf{J} \cdot \delta \mathbf{x}\) and \(\mathrm{} Q \leq T \delta S\). Hence \(\delta E \leq T \delta S+\mathbf{J} \cdot \delta \mathbf{x}\), leading to
  \[
  \delta G \leq 0, \quad \text { where } \quad G=E-T S-\mathbf{J} \cdot \mathbf{x} \tag{1}
  \]
  is the /Gibbs free energy/.
+ The variations of \(G\) in thermal equilibrium are given by
\begin{align*}
\mathrm{d} G=\mathrm{d} E-\mathrm{d}(T S)-\mathrm{d}(\mathbf{J} \cdot \mathbf{x}) & =T \mathrm{~d} S+\mathbf{J} \cdot \mathrm{d} \mathbf{x}-S \mathrm{~d} T-T \mathrm{~d} S-\mathbf{x} \cdot \mathrm{d} \mathbf{J}-\mathbf{J} \cdot \mathrm{d} \mathbf{x} \\
& =-S \mathrm{~d} T-\mathbf{x} \cdot \mathrm{d} \mathbf{J}. \tag{2}
\end{align*}
**** Grand potential
+ *Grand potential* is useful for *isothermal transformations in the absence of mechanical work* (\(\mathrm{} W=0\)) *but at constant chemical potential* \( \mu \).
+ Chemical potentials measures the work necessary to add additional particles to the system.
+ The coordinate set \((T,\,\mu,\,\mathbf{x})\) (the quantities kept constant during an isothermal transformation with no mechanical work and chemical work at constant chemical potential) is most suitable for describing the Grand potential.
+ In chemical reactions, and in equilibrium between two phases, the number of particles in a given constituent may change. The change in the number of particles necessarily involves changes in the internal energy, which is expressed in terms of a chemical work \(\mathrm{} W=\mu \cdot \mathrm{d} \mathbf{N}\). Here \(\mathbf{N}=\left\{N_{1}, N_{2}, \cdots\right\}\) lists the number of particles of each species, and \(\mu=\left\{\mu_{1}, \mu_{2}, \cdots\right\}\) the associated chemical potentials that measure the work necessary to add additional particles to the system. 
  \[
  \delta \mathcal{G} \leq 0, \quad \text { where } \mathcal{G}=E-T S-\mu \cdot \mathbf{N} \tag{1}
  \]
  is the /Grand potential/.
  + The variations of \( \mathcal{G} \) in thermal equilibrium are given by
  \[
  \mathrm{d} \mathcal{G}=-S \mathrm{~d} T+\mathbf{J} \cdot \mathrm{d} \mathbf{x}-\mathbf{N} \cdot \mathrm{d} \mu. \tag{2}
  \]
*** Constraints on thermodynamic parameters
If there are \(n\) ways of doing mechanical work, \(n+1\) independent parameters suffice to characterize an equilibrium state. There must thus be various constraints and relations between the thermodynamic parameters.
**** Gibbs-Duhem relationship
#+NAME: Extensivity
#+begin_definition latex
Extensivity refers to a property of a function of state, say \( f(X_1, X_2, \ldots, X_n) \), for which the following constraint holds
\[
f(\lambda X_1, \lambda X_2, \ldots, \lambda X_n)= \lambda f(X_1, X_2, \ldots, X_n),
\]
#+end_definition
The fundamental identity of thermodynamics is
\[
\mathrm{d} E=T \mathrm{~d} S+\mathbf{J} \cdot \mathrm{d} \mathbf{x}+\mu \cdot \mathrm{d} \mathbf{N} \tag{1}
\]
For fixed intensive coordinates, the extensive quantities are simply proportional to size or to the number of particles
\[
E(\lambda S, \lambda \mathbf{x}, \lambda \mathbf{N})=\lambda E(S, \mathbf{x}, \mathbf{N})
\]
Evaluating the derivative of the above equation with respect to \(\lambda\) at \(\lambda=1\) results in
\[
(\partial_{S} E )_{\mathbf{x}, \mathbf{N}}\,S + \sum_{i} (\partial_{x_i} E)_{S, x_{j \neq i}, \mathbf{N}}\,x_{i} + \sum_{\alpha} (\partial_{N_{\alpha}} E)_{S, \mathbf{x}, N_{\beta \neq \alpha}}\, N_{\alpha} = E(S, \mathbf{x}, \mathbf{N}). \tag{2}
\]
The partial derivatives in the above equation can be identified from (1) as \(T, J_{i}\), and \(\mu_{\alpha}\), respectively. Substituting these values into (2) leads to
\[
E=T S+\mathbf{J} \cdot \mathbf{x}+\mu \cdot \mathbf{N}. \tag{3}
\]
Combining the variations of (3) with (1) leads to a constraint between the variations of intensive coordinates
\[
S \mathrm{~d} T+\mathbf{x} \cdot \mathrm{d} \mathbf{J}+\mathbf{N} \cdot \mathrm{d} \mu=0
\]
known as the *Gibbs-Duhem relation*.
#+BEGIN_COMMENT
While (3) is sometimes referred to as the "fundamental equation of thermodynamics," (1) is the more fundamental. The reason is that extensivity is an additional assumption, and in fact relies on short-range interactions between constituents of the system. It is violated for a large system controlled by gravity, such as a galaxy, while  (1) remains valid.
#+END_COMMENT
#+NAME: Gibbs-Duhem relation
#+begin_example latex
For a fixed amount of gas, variations of the chemical potential along an isotherm can be calculated as follows. Since \(\mathrm{d} T=0\), the Gibbs-Duhem relation gives \(-V \mathrm{~d} P+N \mathrm{~d} \mu=0\), and
\[
\mathrm{d} \mu=\frac{V}{N} \mathrm{~d} P=k_{B} T \frac{\mathrm{d} P}{P}
\]
where we have used the ideal gas equation of state \(P V=N k_{B} T\). Integrating the above equation gives
\[
\mu=\mu_{0}+k_{B} T \ln \frac{P}{P_{0}}=\mu_{0}-k_{B} T \ln \frac{V}{V_{0}}
\]
where \(\left(P_{0}, V_{0}, \mu_{0}\right)\) refer to the coordinates of a reference point.
#+end_example
**** Maxwell's relations
Combining the mathematical rules of differentiation with thermodynamic relationships leads to several useful results. The most important of these are Maxwell's relations, which follow from the commutative property \(\left[\partial_{x} \partial_{y} f(x, y)=\partial_{y} \partial_{x} f(x, y)\right]\) of derivatives. For example, it follows from the fundamental identity of thermodynamics
\begin{align*}
\mathrm{d} E=T \mathrm{~d} S+\mathbf{J} \cdot \mathrm{d} \mathbf{x}+\mu \cdot \mathrm{d} \mathbf{N}
\end{align*}
that
\[
(\partial_{S} E)_{\mathbf{x}, \mathbf{N}} = T, \qquad (\partial_{x_i} E)_{S, x_{j \neq i}, \mathbf{N}} = J_{i}.
\]
The joint second derivative of \(E\) is then given by
\[
\partial_{S} \partial_{x_i} E = (\partial_{x_i} T)_{S} = (\partial_{S} J_{i})_{x_{i}} = \partial_{x_i} \partial_{S} E .
\]
Since \(\partial_x y = (\partial_y x)^{-1}\), the above equation can be inverted to give
\[
\boxed{
(\partial_{J_{i}} S)_{x_{i}} = (\partial_{T} x_{i})_{S}.
}
\]
Similar identities can be obtained from the variations of other state functions. Supposing that we are interested in finding an identity involving \((\partial_{x} S)_{T}\). We would like to find a state function whose variations include \(S \, dT\) and \(J \, dx\). The correct choice is \(dF = d(E - TS) = -S \, dT + J \, dx\). Looking at the second derivative of \(F\) yields the Maxwell relation
\[
\boxed{
-(\partial_{x} S)_{T} = (\partial_{T} J)_{x}.
}
\]
To calculate \((\partial_{J} S)_{T}\), consider \(d(E - TS - Jx) = -S \, dT - x \, dJ\), which leads to the identity
\[
\boxed{
(\partial_{J} S)_{T} = (\partial_{T} x)_{J}.
}
\]
#+NAME: Maxwell relation
#+begin_example latex
To obtain \((\partial_{P} \mu)_{N, T}\) for an ideal gas, start with \(d(E - TS + PV) = -S \, dT + V \, dP + \mu \, dN\). Clearly
\[
(\partial_{P} \mu)_{N, T} = (\partial_{N} V)_{T, P} = \frac{V}{N} = \frac{k_{B} T}{P}.
\]
From \( \mathrm{d} E=T \mathrm{~d} S+\mathbf{J} \cdot \mathrm{d} \mathbf{x}+\mu \cdot \mathrm{d} \mathbf{N} \) it also follows that
\[
(\partial_{V} S)_{E, N} = \frac{P}{T} = -\frac{(\partial_{V} E)_{S, N}}{(\partial_{S} E)_{V, N}}.
\]
The above equation can be rearranged into
\[
(\partial_{V} S)_{E, N} \cdot (\partial_{S} E)_{V, N} \cdot (\partial_{E} V)_{S, N} = -1,
\]
which is an illustration of the chain rule of differentiation.
#+end_example
**** Gibbs phase rule
#+NAME: The Gibbs phase rule
#+begin_rule latex
The Gibbs phase rule states that quite generally, if there are \(p\) phases in coexistence, the dimensionality (number of degrees of freedom) of the corresponding loci in the space of intensive variables is
\[
\boxed{
f=n+c+1-p
}.
\]
#+end_rule
If there are \(n\) ways of performing work on a system that can also change its internal energy by transformations between \(c\) chemical constituents, the number of independent intensive parameters is \(n+c\). Of course, including thermal exchanges there are \(n+c+1\) displacement-like variables in the fundamental identity of thermodynamics
\begin{align*}
\mathrm{d} E=T \mathrm{~d} S+\mathbf{J} \cdot \mathrm{d} \mathbf{x}+\mu \cdot \mathrm{d} \mathbf{N}.
\end{align*}
But the intensive variables are constrained by the Gibbs-Duhem relationship
\[
S \mathrm{~d} T+\mathbf{x} \cdot \mathrm{d} \mathbf{J}+\mathbf{N} \cdot \mathrm{d} \mu=0.
\]
It follows that at least one of the parameters characterizing the system must be extensive. The /phase diagram/ depicted in /the ideal gas temperature scale/ corresponds to a one-component system (water) with only one means of doing work (hydrostatic), and is thus described by two independent intensive coordinates, for example, \((P, T)\) or \((\mu, T)\). In a situation such as depicted in Fig. 1.17, where two phases (liquid and gas) coexist, there is an additional constraint on the intensive parameters, as the chemical potentials must be equal in the two phases. This constraint reduces the number of independent parameters by 1 , and the corresponding manifold for coexisting phases in Fig. 1.3 is onedimensional. At the triple point, where three phases coexist, we have to impose another constraint so that all three chemical potentials are equal. 
The triple point of pure water thus occurs at a single point \((f=1+1+1-3=0)\) in the space of intensive parameters. If there are additional constituents, for example, a concentration of salt in the solution, the number of intensive quantities increases and the triple point can drift along a corresponding manifold.
*** Stability conditions
The *stability condition* for a thermodynamic system is given by
\[
\boxed{
\delta T \, \delta S+\sum_{i} \delta J_{i} \, \delta x_{i}+\sum_{\alpha} \delta \mu_{\alpha} \, \delta N_{\alpha}>0
}
\]
For the consequences of the condition above, see Consequences of the stability condition.
**** Derivation 1 (by analogy with a mechanical system)
Consider a particle moving freely in an external potential \(U(x)\) dissipates energy and settles to equilibrium at a minimum value of \(U\). The vanishing of the force \(J_{i}=-\mathrm{D}_{x} U\) is not by itself sufficient to ensure stability, as we must check that it occurs at a minimum of the potential energy, such that \(\mathrm{D}_{x}^{2} U>0\). In the presence of an external force \(J\), we must minimize the enthalpy \(H=U-J x\), which amounts to tilting the potential. At the new equilibrium point \(x_{\mathrm{eq}}(J)\), we must require \(\mathrm{D}_{x}^{2} H =\mathrm{D}_{x}^{2} U >0\). *Thus only the convex portions of the potential \(U(x)\) are physically accessible.*
#+ATTR_HTML: :width 300px
[[file:~/.local/images/kardar-spop-1.19.jpg]]
#+CAPTION: Possible types of mechanical equilibrium for a particle in a potential. The convex portions (solid line) of the potential can be explored with a finite force J, while the concave (dashed line) portion around the unstable point is not accessible.
With more than one mechanical coordinate, the requirement that any change \(\delta \mathbf{x}\) results in an increase in energy (or enthalpy) can be written as
\[
\sum_{i j} (\partial_{x_{i}} \partial_{x_{j}} U)\, \delta x_{i}\, \delta x_{j}>0 . \tag{1}
\]
The corresponding change in forces is given by
\[
\delta J_{i}=\delta\left(\frac{\partial U}{\partial x_{i}}\right)=\sum_{j} (\partial_{x_{i}} \partial_{x_{j}} U)\, \delta x_{j} \tag{2}
\]
Thus (1) is equivalent to
\[
\sum_{i} \delta J_{i} \, \delta x_{i}>0
\]
When dealing with a thermodynamic system, we should allow for thermal and chemical inputs to the internal energy of the system. Including the corresponding pairs of conjugate coordinates, the condition for mechanical stability should generalize to
\[
\delta T \, \delta S+\sum_{i} \delta J_{i} \, \delta x_{i}+\sum_{\alpha} \delta \mu_{\alpha} \, \delta N_{\alpha}>0
\]
**** Derivation 2 (by appeal to the uniformity of an extended thermodynamic body)
Consider a homogeneous system at thermal equilibrium, characterized by intensive state functions \((T, \mathbf{J}, \mu)\), and extensive variables \((E, \mathbf{x}, \mathbf{N})\). Now imagine that the system is arbitrarily divided into two equal parts, and that one part spontaneously transfers some energy to the other part in the form of work or heat. The two subsystems, \(A\) and \(B\), initially have the same values for the intensive variables, while their extensive coordinates satisfy \(E_{A}+E_{B}=E\), \(\mathbf{x}_{A}+\mathbf{x}_{B}=\mathbf{x}\), and \(\mathbf{N}_{A}+\mathbf{N}_{B}=\mathbf{N}\). After the exchange of energy between the two subsystems, the coordinates of \(A\) change to
\[
\left(E_{A}+\delta E, \mathbf{x}_{A}+\delta \mathbf{x}, \mathbf{N}_{A}+\delta \mathbf{N}\right) \text { and }\left(T_{A}+\delta T_{A}, \mathbf{J}_{A}+\delta \mathbf{J}_{A}, \mu_{A}+\delta \mu_{A}\right), \tag{1}
\]
and those of \(B\) to
\[
\left(E_{B}-\delta E, \mathbf{x}_{B}-\delta \mathbf{x}, \mathbf{N}_{B}-\delta \mathbf{N}\right) \text { and }\left(T_{B}+\delta T_{B}, \mathbf{J}_{B}+\delta \mathbf{J}_{B}, \mu_{B}+\delta \mu_{B}\right) . \tag{2}
\]
#+ATTR_HTML: :width 300px
[[file:~/.local/images/kardar-spop-1.20.jpg]]
#+CAPTION: Spontaneous change between two halves of a homogeneous system.
Note that the overall system is maintained at constant \(E, \mathbf{x}\), and \(\mathbf{N}\). Since the intensive variables are themselves functions of the extensive coordinates, to first order in the variations of \((E, \mathbf{x}, \mathbf{N})\), we have
\[
\delta T_{A}=-\delta T_{B} \equiv \delta T, \quad \delta \mathbf{J}_{A}=-\delta \mathbf{J}_{B} \equiv \delta \mathbf{J}, \quad \delta \mu_{A}=-\delta \mu_{B} \equiv \delta \mu. \tag{3}
\]
Using \(E=T S+\mathbf{J} \cdot \mathbf{x}+\mu \cdot \mathbf{N}\) (see Gibbs-Duhem relation) the entropy of the system can be written as
\[
S=S_{A}+S_{B}=\left(\frac{E_{A}}{T_{A}}-\frac{\mathbf{J}_{A}}{T_{A}} \cdot \mathbf{x}_{A}-\frac{\mu_{A}}{T_{A}} \cdot \mathbf{N}_{A}\right)+\left(\frac{E_{B}}{T_{B}}-\frac{\mathbf{J}_{B}}{T_{B}} \cdot \mathbf{x}_{B}-\frac{\mu_{B}}{T_{B}} \cdot \mathbf{N}_{B}\right) .
\]
Since, by assumption, we are expanding about the equilibrium point, the /first order/ changes vanish, and to second order
\[
\delta S=\delta S_{A}+\delta S_{B}=2\left[\delta\left(\frac{1}{T_{A}}\right) \delta E_{A}-\delta\left(\frac{\mathbf{J}_{A}}{T_{A}}\right) \cdot \delta \mathbf{x}_{A}-\delta\left(\frac{\mu_{A}}{T_{A}}\right) \cdot \delta \mathbf{N}_{A}\right] \tag{4}
\]
(We have used (3) to note that the second-order contribution of \(B\) is the same as \(A\).) Equation (4) can be rearranged to
\begin{align*}
\delta S & =-\frac{2}{T_{A}}\left[\delta T_{A}\left(\frac{\delta E_{A}-\mathbf{J}_{A} \cdot \delta \mathbf{x}_{A}-\mu_{A} \cdot \delta \mathbf{N}_{A}}{T_{A}}\right)+\delta \mathbf{J}_{A} \cdot \delta \mathbf{x}_{A}+\delta \mu_{A} \cdot \delta \mathbf{N}_{A}\right] \\
& =-\frac{2}{T_{A}}\left[\delta T_{A} \delta S_{A}+\delta \mathbf{J}_{A} \cdot \delta \mathbf{x}_{A}+\delta \mu_{A} \cdot \delta \mathbf{N}_{A}\right]. \tag{5}
\end{align*}
The condition for stable equilibrium is that any change should lead to a decrease in entropy, and hence we must have
\[
\delta T \delta S+\delta \mathbf{J} \cdot \delta \mathbf{x}+\delta \mu \cdot \delta \mathbf{N} \geq 0
\]
We have now removed the subscript \(A\), as the equation above must apply to the whole system as well as to any part of it.
**** Consequences of the stability conditions
The condition above holds under constant \(E, \mathbf{x}\), and \(\mathbf{N}\) constraints, and is symmetric across different sets of constraints. For variations in \(\delta T\) and \(\delta \mathbf{x}\) with \(\delta \mathbf{N} = 0\):
\begin{aligned}
\delta S &= (\partial_{T} S)_{\mathbf{x}} \delta T + (\partial_{x_{i}} S)_{T} \delta x_{i} \\
\delta J_{i} &= (\partial_{T} J_{i})_{\mathbf{x}} \delta T + (\partial_{x_{j}} J_{i})_{T} \delta x_{j}
\end{aligned}
Substituting these into the stability condition:
\[
(\partial_{T} S)_{\mathbf{x}} (\delta T)^{2} + (\partial_{x_{j}} J_{i})_{T} \delta x_{i} \delta x_{j} \geq 0
\]
The cross terms \(\delta T \delta x_{i}\) cancel due to Maxwell relations, leading to a quadratic form which must be positive for all \(\delta T\) and \(\delta \mathbf{x}\). 
For \(\delta T \neq 0\), we obtain \((\partial_{T} S)_{\mathbf{x}} \geq 0\), implying positive heat capacity:
\[
C_{\mathbf{x}} = \left(\frac{\mathrm{d} Q}{\mathrm{d} T}\right)_{\mathbf{x}} = T (\partial_{T} S)_{\mathbf{x}} \geq 0
\]
For \(\delta x_{i} \neq 0\), the corresponding response function \((\partial_{J_{i}} x_{i})_{T, x_{j \neq i}}\) must be positive. More generally, the matrix \((\partial_{x_{j}} J_{i})_{T}\) must be positive definite, implying all eigenvalues are positive.
Including chemical work for a gas, the relevant matrix is:
\begin{pmatrix}
-(\partial_{V} P)_{T, N} & -(\partial_{N} P)_{T, V} \\
(\partial_{V} \mu)_{T, N} & (\partial_{N} \mu)_{T, V}
\end{pmatrix}
Positive definiteness requires the determinant:
\[
(\partial_{N} P)_{T, V} (\partial_{V} \mu)_{T, N} - (\partial_{V} P)_{T, N} (\partial_{N} \mu)_{T, V} \geq 0
\]
At the critical point, \((\partial_{V} P)_{T_{c}, N}=0\). Expanding the critical isotherm:
\[
\delta P (T=T_{c}) = (\partial_{V} P)_{T_{c}, N} \delta V + \frac{1}{2} (\partial_{V}^{2} P)_{T_{c}, N} \delta V^{2} + \frac{1}{6} (\partial_{V}^{3} P)_{T_{c}, N} \delta V^{3} + \cdots
\]
The condition \(-\delta P \delta V \geq 0\) implies \((\partial_{V}^{2} P)_{T_{c}, N} = 0\), and the third derivative must be negative if the first derivative vanishes. This helps determine the critical point from isotherm approximations, such as with the van der Waals equation, though a Taylor expansion is usually not justified.
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-1.21.jpg]]
#+CAPTION: Stability condition at criticality, illustrated for van der Waals isotherms.
*** The third law
Differences in entropy between two states can be computed using the second law, from \(\Delta S=\int \mathrm{} Q_{\mathrm{rev}} / T\). Low-temperature experiments indicate that \(\Delta S(\mathbf{X}, T)\) vanishes as \(T\) goes to zero for any set of coordinates \(\mathbf{X}\). This observation is independent of the other laws of thermodynamics, leading to the formulation of a *third law* by Nernst, which states:
#+NAME: The third law of thermodynamics
#+begin_law latex
The entropy of all systems at zero absolute temperature is a universal constant that can be taken to be zero.
#+end_law
#+NAME: The third law of thermodynamics
#+begin_corollary latex
The third law of thermodynamics, as stated above, implies that
\[
\lim _{T \rightarrow 0} S(\mathbf{X}, T)=0
\]
which is a stronger requirement than the vanishing of the differences \(\Delta S(\mathbf{X}, T)\). 
#+end_corollary
Suppose that
1) Under *slow cooling* from a high temperature equilibrium phase \(A\), the phase makes a transition at a temperature \(T^{\ast}\) to phase \(B\), releasing latent heat \(L\).
2) Under *rapid cooling* from a high temperature equilibrium phase \( B \), the transition is avoided, and phase \(A\) persists in metastable equilibrium.
#+ATTR_HTML: :width 300px
[[file:~/.local/images/kardar-spop-1.22.jpg]]
#+CAPTION: Heat capacity measurements on allotropes of the same material.
The entropies in the two phases can be calculated by measuring the heat capacities \(C_{A}(T)\) and \(C_{B}(T)\). Starting from \(T=0\), the entropy at a temperature slightly above \(T^{\ast}\) can be computed along the two possible paths as
\[
S\left(T^{*}+\epsilon\right)=S_{A}(0)+\int_{0}^{T^{*}} \mathrm{~d} T^{\prime} \frac{C_{A}\left(T^{\prime}\right)}{T^{\prime}}=S_{B}(0)+\int_{0}^{T^{*}} \mathrm{~d} T^{\prime} \frac{C_{B}\left(T^{\prime}\right)}{T^{\prime}}+\frac{L}{T^{*}}
\]
By such measurements we can indeed verify that \(S_{A}(0)=S_{B}(0) \equiv 0\).
#+BEGIN_COMMENT
This extended condition has been tested for metastable phases of a substance. Certain materials, such as sulfur or phosphine, can exist in a number of rather similar crystalline structures (*allotropes*). Of course, at a given temperature only one of these structures is truly stable.
#+END_COMMENT
**** Consequences of the third law
1) Since \(S(T=0, \mathbf{X})=0\) for all coordinates \(\mathbf{X}\),
\[
\lim_{T \rightarrow 0}(\partial_{\mathbf{X}} S)_{T} = 0
\]
2) Heat capacities must vanish as \(T \rightarrow 0\) since
\[
S(T, \mathbf{X})-S(0, \mathbf{X})=\int_{0}^{T} \mathrm{~d} T^{\prime} \frac{C_{\mathbf{X}}\left(T^{\prime}\right)}{T^{\prime}}
\]
and the integral diverges as \(T \rightarrow 0\) unless
\[
\lim _{T \rightarrow 0} C_{\mathbf{X}}(T)=0
\]
3) Thermal responses such as the *thermal expansivities* also vanish as \(T \rightarrow 0\) since
\[
\alpha_{J}=\left.\frac{1}{x} \frac{\partial x}{\partial T}\right|_{J}=\left.\frac{1}{x} \frac{\partial S}{\partial J}\right|_{T}
\]
The second equality follows from the Maxwell relation \( (\partial_{J} S)_{T} = (\partial_{T} x)_{J} \). The vanishing of the latter is guaranteed by \(\lim_{T \rightarrow 0}(\partial_{\mathbf{X}} S)_{T} = 0\). 
4) It is impossible to cool any system to absolute zero temperature in a finite number of steps.
For example, we can cool a gas by an adiabatic reduction in pressure. Since the curves of \(S\) versus \(T\) for different pressures must join at \(T=0\), successive steps involve progressively smaller changes, in \(S\) and in \(T\), on approaching zero temperature. Alternatively, that zero temperatures is unattainable implies that \(S(T=0, P)\) is independent of \(P\). This is a weaker statement of the third law, which also implies the equality of zero temperature entropy for different substances.
#+ATTR_HTML: :width 300px
[[file:~/.local/images/kardar-spop-1.23.jpg]]
#+CAPTION: An infinite number of steps is required to cool a gas to T=0 by a series of isothermal decompressions.
** Classical statistical mechanics
:LOGBOOK:
CLOCK: [2024-06-14 Fri 20:02]--[2024-06-14 Fri 23:49] =>  3:47
CLOCK: [2024-06-14 Fri 18:31]--[2024-06-14 Fri 19:32] =>  1:01
:END:
*** Prelude
*Statistical mechanics is a probabilistic approach to equilibrium macroscopic properties of large numbers of degrees of freedom.*
+ The laws of thermodynamics give a phenomenological description of properties of macroscopic bodies at thermal equilibrium. The macrostate \(M\) depends on a relatively small number of thermodynamic coordinates. The microstate \(\mu\) depends on an enormously large number of degrees of freedom and the corresponding time evolution, governed by the canonical equations, is usually quite complicated.
+ Rather than following the evolution of an individual (pure) microstate, statistical mechanics examines an /equilibrium ensemble/ of microstates corresponding to a given (mixed) macrostate. It aims to provide the probabilities \(p_{M}(\mu)\) for the equilibrium ensemble. Such assignment of probabilities is subjective.
+ Liouville's theorem justifies the /fundamental postulate of thermodynamics/ - the assumption that all accessible microstates are equally likely in an equilibrium ensemble.
+ An important part of the enterprise of *classical equilibrium statistical mechanics* is to recover the laws of thermodynamics from the fundamental postulate of statistical mechanics:
  1) The zeroth law from the fundamental postulate
  2) The first law from the fundamental postulate
  3) The second law from the fundamental postulate
  4) The stability conditions from the fundamental postulate
+ Another equally important part of the enterprise of classical equilibrium statistical mechanics is to provide unbiased estimates of \(p_{M}(\mu)\) for a number of different equilibrium ensembles such as
  1) The microcanonical ensemble
  2) The canonical ensemble
  3) The Gibbs canonical ensemble
  4) The grand canonical ensemble
  for a variety of systems such as the:
  1) Two-level systems
  2) The ideal gas
  3) Spins in an external magnetic field
  and then use these probabilities \(p_{M}(\mu)\) to make testable predictions about the equilibrium macroscopic properties of these systems comprising a large number of degrees of freedom that are consistent with the predictions of the phenomenological laws of thermodynamics.
+ A central conclusion of classical equilibrium statistical mechanics is that in /the thermodynamic limit/, with large numbers of degrees of freedom, all of the above equilibrium ensembles are in fact equivalent.
+ Classical equilibrium statistical mechanics has limits of applicability:
    1) It does not give an adequate description of identical particles as is exemplified by mixing entropy and the Gibbs paradox. Classical equilibrium statistical mechanics's solution is to make an additional assumption of *indistinguishability of identical particles*. This in fact is an additional postulate of classical equilibrium statistical mechanics which we will dub as the /ad-hoc postulate of statistical mechanics/. It allows a switch to a non-Haecceisstic phase space via the introduction of correct Boltzmann counting.
    2) The fundamental postulate of statistical mechanics relates phase space volumes to the number of microstates. The volume of phase space involves products \(pq\), of coordinates and conjugate momenta, and hence can change by an arbitrary factor if we change the units in which \(p\) and \(q\) are measured. It follows that the number of microstates depends on the units of measurement which is nonsense.
    3) Classical equilibrium statistical mechanics leaves out the question of how various systems evolve to a state of thermal equilibrium.
    (1) and (2) are resolved in quantum statistical mechanics whereas (3) is resolved by kinetic theory of gases and non-equilibrium statistical mechanics.
*** Definitions and postulates
**** Macrostate
At any time \(t\), the *macrostate* \( M \) of a system of \(N\) particles is described by specifying a small number of state functions, for example the energy \(E\), the temperature \(T\), the pressure \(P\), and the particle number \(N\).
**** Microstate
At any time \(t\), the *microstate* of a system of \(N\) particles is described by specifying the positions \(\vec{q}_i(t)\), and momenta \(\vec{p}_i(t)\), of all particles. The microstate thus corresponds to a point \(\mu(t)\), in the \(6 N\)-dimensional phase space \(\Gamma=\prod_{i=1}^N\left\{\vec{q}_i,\,\vec{p}_i\right\}\). The time evolution of this point is governed by the [[id:6b9b1aff-8bc3-4b9f-b616-b8a3940d520c][canonical equations]]
\[
\mathrm{D}_t\vec{q}_i  = \partial_{\vec{p}_i}\,H,
\qquad
\mathrm{D}_t\vec{p}_i = - \partial_{\vec{q}_i}\,H \tag{1}
\]
where the [[id:28fb5913-91f6-4c6d-adec-6355e4b2fa80][Hamiltonian]] \(H (\mathbf{p},\,\mathbf{q})\) describes the total energy in terms of the [[id:ba8c624f-7b21-41b3-b070-a5f191c1a4ab][degrees of freedom]] \(\mathbf{q} \equiv \{\vec{q}_1,\,\vec{q}_2,\,\cdots,\,\vec{q}_N\}\), called coordinates of the particles and \(\mathbf{p} \equiv \{\vec{p}_1,\,\vec{p}_2,\,\cdots,\,\vec{p}_N\}\), their associated momenta.
#+NAME: Invariance of H(p, q) under T(p, q) -> (-p, q) implies q(t) = q(-t).
#+ATTR_LATEX: :environment theorem
#+begin_theorem latex
Invariance of \(H(\mathbf{p},\,\mathbf{q})\) under \( T(\mathbf{p},\,\mathbf{q}) \to (-\mathbf{p},\,\mathbf{q})\), i.e., \(H(\mathbf{p},\,\mathbf{q}) = H(-\mathbf{p},\,\mathbf{q})\) implies \textbf{time reversal symmetry}, i.e., \(\mathbf{q} (t) = \mathbf{q}(-t)\).
\hfill \blacksquare
#+end_theorem
#+ATTR_LATEX: :environment proof
#+begin_proof latex
From the canonical equations (q), we have \( \vec{q}_i (t) = \int_0^{t} \mathrm{d}t^{\prime}\,\partial_{\vec{p}_i} H (\mathbf{p}(t^{\prime}),\,\mathbf{q}(t^{\prime})) \). Under the transformation \(  \)\(T(\mathbf{p},\,\mathbf{q}) \rightarrow(-\mathbf{p},\,\mathbf{q})\) and \( t^{\prime} \to -t^{\prime} \), we have \( \vec{q}_i (-t) = \int_0^{-t} \mathrm{d}(-t^{\prime})\,\partial_{-\vec{p}_i} H (-\mathbf{p}(-t^{\prime}),\,\mathbf{q}(-t^{\prime}))\). If the Hamiltonian is invariant under the transformation \(T(\mathbf{p},\,\mathbf{q}) \rightarrow(-\mathbf{p},\,\mathbf{q})\), then \( \partial_{- \vec{p}_i} H (-\mathbf{p}(t^{\prime}),\,\mathbf{q}(t^{\prime})) = - \partial_{\vec{p}_i} H (\mathbf{p}(t^{\prime}),\,\mathbf{q}(t^{\prime})) \) and therefore \( \vec{q}_i (-t) = -\int_0^{-t} \mathrm{d}(-t^{\prime}) \,\partial_{\vec{p}_i} H (\mathbf{p}(-t^{\prime}),\,\mathbf{q}(-t^{\prime})) \). But \(- \int_0^{-t} \mathrm{d}(-t^{\prime})\,\partial_{\vec{p}_i} H (\mathbf{p}(-t^{\prime}),\,\mathbf{q}(-t^{\prime})) = \int_0^{t} \mathrm{d}t^{\prime}\,\partial_{\vec{p}_i} H (\mathbf{p}(t^{\prime}),\,\mathbf{q}(t^{\prime})) \equiv \vec{q}_i (t)\). Thus \(\mathbf{q}(t)=\mathbf{q}(-t)\).
\hfill \blacksquare
#+end_proof
The microscopic equations of motion have /time-reversal symmetry/, that is, if all the momenta are suddenly reversed, \(\mathbf{p} \rightarrow-\mathbf{p}\), at \(t=0\), the particles retrace their previous trajectory, \(\mathbf{q}(t)=\mathbf{q}(-t)\). As illustrated above, this follows from the invariance of \(H\) under the transformation \(T(\mathbf{p},\,\mathbf{q}) \rightarrow(-\mathbf{p},\,\mathbf{q})\).
**** Phase space density
For a system of \( N \) particles, the space of macrostates is considerably smaller than the phase space spanned by microstates. Therefore, there must be a very large number of microstates \(\mu\) corresponding to the same macrostate \(M\). This many-to-one correspondence suggests the introduction of a /statistical ensemble/ of microstates. Doing so natural leads to the concept of a *phase space density*, a properly normalized probability density function in phase space.
#+NAME: Phase space density
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Consider \(\mathcal{N}\) copies of a particular macrostate \( M \), each described by a different representative point \(\mu(t)\) in the phase space \(\Gamma=\prod_{i=1}^N\left\{\vec{q}_i,\,\vec{p}_i\right\}\). Let \(\mathrm{d} \mathcal{N}(\mathbf{p},\,\mathbf{q},\,t)\) equal the number of representative points in an infinitesimal volume \(\mathrm{d} \Gamma=\prod_{i=1}^N \mathrm{~d}^3 \vec{p}_i\,\mathrm{~d}^3 \vec{q}_i\) around the point \((\mathbf{p},\,\mathbf{q})\). We define a phase space density \(\rho(\mathbf{p},\,\mathbf{q},\,t)\)
\begin{equation*}
\mathrm{d} \Gamma\,\rho(\mathbf{p},\,\mathbf{q},\,t) = \lim_{\mathcal{N} \rightarrow \infty} \frac{\mathrm{d} \mathcal{N}(\mathbf{p},\,\mathbf{q},\,t)}{\mathcal{N}}, \qquad \int \mathrm{d}\Gamma\,\rho=1.  
\end{equation*}
\hfill \blacksquare
#+end_definition
**** Liouville's theorem
#+NAME: Liouville's theorem
#+ATTR_LATEX: :environment theorem
#+begin_theorem latex
A Hamiltonian evolution is such that \( \partial_{q_{\alpha}} \partial_{p_{\alpha}} H (\mathbf{p},\,\mathbf{q}) = \partial_{p_{\alpha}} \partial_{q_{\alpha}} H (\mathbf{p},\,\mathbf{q}) \). Let \( \rho (\mathbf{p},\,\mathbf{q},\,t) \) be the phase space density around the point \((\mathbf{p},\,\mathbf{q})\). Let \(\mathrm{d} \mathcal{N}\) be the number of pure states in an infinitesimal volume \(\mathrm{d} \Gamma=\prod_{i=1}^N \mathrm{~d}^3 \vec{p}_i \mathrm{~d}^3 \vec{q}_i\) around the point \((\mathbf{p},\,\mathbf{q})\). Suppose that after a time interval \(\delta t\) these states have moved to the vicinity of another point, say \((\mathbf{p}^{\prime},\,\mathbf{q}^{\prime})\). The phase space density around the point \((\mathbf{p}^{\prime},\,\mathbf{q}^{\prime})\) is \( \rho (\mathbf{p}^{\prime},\,\mathbf{q}^{\prime},\,t+\delta t) \). The following relation holds
\begin{equation*}
\rho (\mathbf{p}^{\prime},\,\mathbf{q}^{\prime},\,t+\delta t) = \rho (\mathbf{p},\,\mathbf{q},\,t).
\end{equation*}
Thus, the phase space density \( \rho (\Gamma,\,t) \) behaves like an \textit{incompressible fluid}.
\hfill \blacksquare
#+end_theorem
#+ATTR_LATEX: :environment proof
#+begin_proof latex
According to the canonical equations, the \(\mathrm{d} \mathcal{N}\) pure states in an infinitesimal volume \(\mathrm{d} \Gamma=\) \(\prod_{i=1}^N \mathrm{~d}^3 \vec{p}_i \mathrm{~d}^3 \vec{q}_i\) around the point \((\mathbf{p},\,\mathbf{q})\) have moved to the vicinity of another point \(\left(\mathbf{p}^{\prime},\,\mathbf{q}^{\prime}\right)\) after an interval \(\delta t\) where
\begin{align*}
q_\alpha^{\prime} (t+\delta t) = q_\alpha (t) + \mathrm{D}_{t} q_\alpha(t)\,\delta t + \mathcal{O} (\delta t^2),
\quad
p_\alpha^{\prime} (t+\delta t) = p_\alpha (t) + \mathrm{D}_t p_{\alpha} (t)\,\delta t + \mathcal{O}(\delta t^2).
\end{align*}
In the above expression, the \(q_\alpha(t)\) and \(p_\alpha(t)\) refer to any of the \(6 N\) coordinates and momenta, and \(\mathrm{D}_t q_\alpha (t)\) and \(\mathrm{D}_t p_\alpha (t)\) are the corresponding velocities at time \( t \). The original volume element \(\mathrm{d} \Gamma\) is in the shape of a hypercube of sides \(\mathrm{d} p_\alpha (t)\) and \(\mathrm{d} q_\alpha (t)\). In the time interval \(\delta t\) it gets distorted, and the projected sides of the new volume element are given by
\begin{align*}
\mathrm{d} q_\alpha^{\prime} (t+\delta t) = \mathrm{d} q_\alpha (t) + \partial_{q_{\alpha}} (\mathrm{D}_t q_\alpha(t))\,\delta t + \mathcal{O} (\delta t^2),
\quad
\mathrm{d} p_\alpha^{\prime} (t+\delta t) = \mathrm{d} p_\alpha (t) + \partial_{p_{\alpha}}(\mathrm{D}_t p_{\alpha} (t))\,\delta t + \mathcal{O}(\delta t^2).
\end{align*}
To order of \(\delta t^2\), the new volume element is \(\mathrm{d} \Gamma^{\prime}=\prod_{i=1}^N \mathrm{~d}^3 \vec{p}_i{ }^{\prime} \mathrm{d}^3 \vec{q}_i{ }^{\prime}\). It follows that for each pair of conjugate coordinates
\begin{align*}
\mathrm{d} q_\alpha^{\prime} \cdot \mathrm{d} p_\alpha^{\prime}=\mathrm{d} q_\alpha \cdot \mathrm{d} p_\alpha
\big[1 + \big\{\partial_{q_{\alpha}} \mathrm{D}_t q_\alpha(t) + \partial_{p_{\alpha}} \mathrm{D}_t p_\alpha(t) \big\} \delta t + \mathcal{O}(\delta t^2)\big].
\end{align*}
But since the time evolution of coordinates and momenta are governed by the canonical equations
\begin{equation*}
\mathrm{D}_t q_\alpha  = \partial_{p_\alpha}\,H,
\qquad
\mathrm{D}_t p_\alpha = - \partial_{q_\alpha}\,H,
\end{equation*}
it follows that
\begin{align*}
\mathrm{d} q_\alpha^{\prime} \cdot \mathrm{d} p_\alpha^{\prime}=\mathrm{d} q_\alpha \cdot \mathrm{d} p_\alpha
\big[1 + \big\{\partial_{q_{\alpha}} \partial_{p_{\alpha}} H (\mathbf{p},\,\mathbf{q}) - \partial_{p_{\alpha}} \partial_{q_{\alpha}} H (\mathbf{p},\,\mathbf{q}) \big\}\,\delta t + \mathcal{O}(\delta t^2)\big].
\end{align*}
Given the equality of mixed derivative, i.e., \( \partial_{q_{\alpha}} \partial_{p_{\alpha}} H (\mathbf{p},\,\mathbf{q}) = \partial_{p_{\alpha}} \partial_{q_{\alpha}} H (\mathbf{p},\,\mathbf{q}) \), we have for all \( \alpha \), to order \( \mathcal{O} (\delta t^2) \),  \( \mathrm{d} q_\alpha^{\prime} \cdot \mathrm{d} p_\alpha^{\prime}=\mathrm{d} q_\alpha \cdot \mathrm{d} p_\alpha \). It follows that the volume element is unaffected, i.e.,
\begin{equation*}
\mathrm{d} \Gamma^{\prime} = \mathrm{d} \Gamma,
\end{equation*}
All the pure states \(\mathrm{d} \mathcal{N}\) originally in the vicinity of \((\mathbf{p},\,\mathbf{q})\) are transported to the neighborhood of \(\left(\mathbf{p}^{\prime},\,\mathbf{q}^{\prime}\right)\), but occupy exactly the same volume. The ratio \(\mathrm{d} \mathcal{N} / \mathrm{d} \Gamma\) is time-independent, and \(\rho(\Gamma,\,t)\) behaves like the density of an incompressible fluid.
\hfill \blacksquare
#+end_proof
**** Liouville's equation
#+NAME: Poisson's bracket
#+ATTR_LATEX: :environment definition
#+begin_definition latex
The \textit{Poisson bracket} of two functions \( A(\mathbf{p},\,\mathbf{q}) \) and \( B(\mathbf{p},\,\mathbf{q}) \) in phase space \(\Gamma=\prod_{i=1}^N\left\{\vec{q}_i,\,\vec{p}_i\right\}\) is defined as
\begin{align*}
\{A, B\} \equiv \sum_{\alpha=1}^{3 N}\big(\partial_{q_\alpha} A \cdot \partial_{p_\alpha} B - \partial_{p_\alpha} A \cdot \partial_{q_\alpha} B \big).
\end{align*}
\hfill \blacksquare
#+end_definition
#+NAME: Poisson's bracket
#+begin_corollary latex
From the definition of the Poisson bracket, it immediately follows that \( \{A, B\}   = -\{B, A\} \).
#+end_corollary
*Liouville's equation* is a consequence of Liouville's theorem concerning the phase space density. The incompressibility condition \(\rho (\mathbf{p}^{\prime},\,\mathbf{q}^{\prime},\,t+\delta t)=\rho(\mathbf{p},\,\mathbf{q},\,t)\) can be written in differential form as
\begin{align*}
\mathrm{D}_t \rho (\Gamma,\,t) = \partial_t \rho (\Gamma,\,t) + \sum_{\alpha=1}^{3 N} 
\bigg(
\partial_{p_\alpha} \rho (\Gamma,\,t) \cdot \mathrm{D}_t p_\alpha (t) + \partial_{q_\alpha} \rho (\Gamma,\,t) \cdot \mathrm{D}_t q_\alpha (t) 
\bigg) = 0. \tag{1}
\end{align*}
Note the distinction between \(\partial_t \rho (\Gamma,\,t)\) and \(\mathrm{D}_t \rho (\Gamma,\,t)\): the former partial derivative refers to the changes in \(\rho\) at a particular location in phase space, while the latter total derivative follows the evolution of a volume of fluid as it moves in phase space. Using the canonical equations
\begin{align*}
\sum_{\alpha=1}^{3 N} 
&\bigg(
\partial_{p_\alpha} \rho (\Gamma,\,t) \cdot \mathrm{D}_t p_\alpha (t) + \partial_{q_\alpha} \rho (\Gamma,\,t) \cdot \mathrm{D}_t q_\alpha (t) 
\bigg) \\
&\qquad =
- \sum_{\alpha=1}^{3 N} 
\bigg(
\partial_{q_\alpha} \rho (\Gamma,\,t) \cdot \partial_{p_\alpha} H (\mathbf{p},\,\mathbf{q}) - \partial_{p_\alpha} \rho (\Gamma,\,t) \cdot \partial_{q_\alpha} H (\mathbf{p},\,\mathbf{q})
\bigg) = - \{\rho,\,H\}
\end{align*}
where we have identified the Poisson bracket. Substituting in (1), we are left with Liouville's equation
\begin{equation*}
\partial_t \rho (\Gamma,\,t) = - \{\rho,\,H\}.
\end{equation*}
**** Pure states and mixed states
When the exact microstate \( \mu \) is specified, the system is said to be in a *pure state*. When our knowledge of the system is probabilistic, in the sense of its being taken from an ensemble with density \(\rho(\Gamma)\), it is said to belong to a *mixed state*. It is difficult to describe equilibrium (using a appropriate set of thermodynamic coordinates forming the system's macrostate) in the context of a /pure state/, since \(\mu(t)\), governed by the canonical equations, is time-dependent. Equilibrium is more conveniently described for /mixed states/ by examining the time evolution of the phase space density, governed by the Liouville's equation.
**** Principle of maximum entropy
Shannon's entropy \(S\) can be used to quantify subjective estimates of probabilities. In the absence of any information, the best unbiased estimate is that all \(M\) outcomes are equally likely. This is codified by the *principle of maximum entropy*.
#+NAME: Principle of maximum entropy
#+begin_theorem latex
Consider a random variable with a discrete set of outcomes \(\mathcal{S}=\left\{x_{i}\right\}\) for \(i=1, \cdots, M\). In the absence of any information, the best unbiased estimate is that all \(M\) outcomes are equally likely. This is the distribution of maximum entropy.
#+end_theorem
If additional information is available, the unbiased estimate is obtained by maximizing the entropy subject to the constraints imposed by this information (using the method of Lagrange multipliers).
#+NAME: Principle of maximum entropy
#+begin_example latex
For example, if it is known that \(\langle F(x)\rangle=f\), we can maximize
\[
S\left(\alpha, \beta,\left\{p_{i}\right\}\right)=-\sum_{i} p(i) \ln p(i)-\alpha\left(\sum_{i} p(i)-1\right)-\beta\left(\sum_{i} p(i) F\left(x_{i}\right)-f\right),
\]
where the Lagrange multipliers \(\alpha\) and \(\beta\) are introduced to impose the constraints of normalization, and \(\langle F(x)\rangle=f\), respectively.
The result of the optimization is a distribution \(p_{i} \propto \exp \left(-\beta F\left(x_{i}\right)\right)\), where the value of \(\beta\) is fixed by the constraint.
This process can be generalized to an arbitrary number of conditions. It is easy to see that if the first \(n=2 k\) moments (and hence \(n\) cumulants) of a distribution are specified, the unbiased estimate is the exponential of an \(n\)th-order polynomial.
#+end_example
The principle of maximum entropy forms the basis of the fundamental postulate of statistical mechanics.
**** Fundamental postulate of statistical mechanics
In classical statistical mechanics, microstates are defined by points in phase space, their time evolution governed by a Hamiltonian \(H(\mu)\). Since the canonical equations conserve the total energy of a given system, all microstates are confined to the surface \(H(\mu)=E\) in phase space. Let us assume that there are no other conserved quantities, so that all points on this surface are mutually accessible. The *fundamental postulate of statistical mechanics* states that the /equilibrium probability distribution/ of the microcanonical ensemble is given by
\[
\boxed{
p_{(E, \mathbf{x})}(\mu)=\frac{1}{\Omega(E, \mathbf{x})} \cdot \begin{cases}1 & \text { for } H(\mu)=E \\ 0 & \text { otherwise }\end{cases}
} \tag{1}
\]
\( p_{(E, \mathbf{x})}(\mu) \) *is in fact the unbiased probability estimate in phase space subject to the constraint of constant energy.* This is why the fundamental postulate of statistical mechanics is also called /Boltzmann's assumption of equal a priori equilibrium probabilities/. It is an application of the principle of maximum entropy.
#+BEGIN_COMMENT
This assignment is consistent with, but not required by, Liouville's theorem. Note that the phase space specifying the microstates \(\mu\) must be composed of canonically conjugate pairs. Under a canonical change of variables, \(\mu \rightarrow \mu^{\prime}\), volumes in phase space are left invariant. The Jacobian of such transformations is unity, and the transformed probability, \(p\left(\mu^{\prime}\right)=p(\mu)\left|\partial \mu / \partial \mu^{\prime}\right|\), is again uniform on the surface of constant energy.
#+END_COMMENT
#+BEGIN_COMMENT
The normalization factor \(\Omega(E, \mathbf{x})\) is the area of the surface of constant energy \(E\) in phase space. To avoid subtleties associated with densities that are non-zero only on a surface, it is sometimes more convenient to define the microcanonical ensemble by requiring \(E-\Delta \leq H(\mu) \leq E+\Delta\), that is, constraining the energy of the ensemble up to an uncertainty of \(\Delta\). In this case, the accessible phase space forms a shell of thickness \(2 \Delta\) around the surface of energy \(E\). The normalization is now the volume of the shell, \(\Omega^{\prime} \approx 2 \Delta \Omega\). Since \(\Omega\) typically depends exponentially on \(E\), as long as \(\Delta \sim \mathcal{O}\left(E^{0}\right)\) (or even \(\left.\mathcal{O}\left(E^{1}\right)\right)\), the difference between the surface and volume of the shell is negligible in the \(E \propto N \rightarrow \infty\) limit, and we can use \(\Omega\) and \(\Omega^{\prime}\) interchangeably.
#+END_COMMENT
The laws of thermodynamics now follow from (1), provided that we consider macroscopic systems with many degrees of freedom (see the zeroth law from the fundamental postulate, the first law from the fundamental postulate, the second law from the fundamental postulate, and the stability conditions from the fundamental postulate).
**** Shannon's entropy formula
#+NAME: Shannon entropy
#+begin_definition latex
Consider a random variable with a discrete set of outcomes \(\mathcal{S}=\left\{x_{i}\right\}\), occurring with probabilities \(\{p(i)\}\), for \(i=1, \cdots, M\). The Shannon entropy of the probability distribution is given by
\[
S\left[\left\{p_{i}\right\}\right]=-\sum_{i=1}^{M} p(i) \ln_2 p(i)=-\langle\ln_2 p(i)\rangle
\]
#+end_definition
***** Motivation
Consider a random variable with a discrete set of outcomes \(\mathcal{S}=\left\{x_{i}\right\}\), occurring with probabilities \(\{p(i)\}\), for \(i=1, \cdots, M\). Let us construct a message from \(N\) independent and identically distributed outcomes of the random variable.
Since there are \(M\) possibilities for each character in this message, it has an apparent *information content* of \(N \ln _{2} M\) bits, that is, \(N \ln _{2} M\) binary bits of information have to be transmitted to convey the message precisely.
However, the probabilities \(\{p(i)\}\) limit the types of messages that are likely. In particular, we expect the probability of finding any \(N_{i}\) that is different from \(N p_{i}\) by more than \(\mathcal{O}(\sqrt{N})\) becomes exponentially small in \(N\) as \(N \rightarrow \infty\). 
The number of typical messages thus corresponds to the number of ways of rearranging the \(\left\{N_{i}\right\}\) occurrences of \(\left\{x_{i}\right\}\), and is given by the multinomial coefficient 
\[
g=\frac{N !}{\prod_{i=1}^{M} N_{i} !} \ll M^N
\]
To specify one out of \(g\) possible sequences requires
\[
\ln _{2} g \approx-N \sum_{i=1}^{M} p_{i} \ln _{2} p_{i} \quad(\text { for } N \rightarrow \infty)
\]
The last result can be obtained by applying Stirling's approximation for \(\ln N!\) or by
\[
1=\bigg(\sum_{i} p_{i}\bigg)^{N}=\sum_{\left\{N_{i}\right\}} \frac{N !}{\prod_{i=1}^{M} N_{i} !} \prod_{i=1}^{M} p_{i}^{N_{i}} \approx g \prod_{i=1}^{M} p_{i}^{N p_{i}}
\]
The multinomial coefficient 
\[
g=\frac{N !}{\prod_{i=1}^{M} N_{i} !}
\]
which is fundamental to Shannon's theorem and the definition of information is encountered frequently in classical statistical mechanics in the context of mixing \(M\) distinct components; its natural logarithm is related to the entropy of mixing (see mixing entropy and the Gibbs paradox). This motivates a general definition of entropy of a probability distribution which is presented above as the Shannon entropy formula.
***** Properties
1) The Shannon entropy \( S \) takes a /minimum value/ of zero for the Dirac delta distribution \(p(i)=\delta_{ij}\),
2) The Shannon entropy \( S \) takes a /maximum value/ of \(\ln M\) for the Uniform distribution \(p(i)=M^{-1}\),
3) The Shannon entropy \( S \) is thus a measure of /dispersity (disorder) of the distribution/, and *does not depend* on the values of the random variables \(\left\{x_{i}\right\}\),
4) A /one-to-one mapping/ to \(f_{i}=F\left(x_{i}\right)\) leaves the Shannon entropy unchanged,
5) A /many-to-one mapping/ makes the distribution more ordered and decreases \(S\). For example, if the two values, \(x_{1}\) and \(x_{2}\), are mapped onto the same \(f\), the change in the Shannon entropy is
   \[
   \Delta S\left(x_{1}, x_{2} \rightarrow f\right)=\left[p_{1} \ln \frac{p_{1}}{p_{1}+p_{2}}+p_{2} \ln \frac{p_{2}}{p_{1}+p_{2}}\right]<0
   \]
***** Generalization
The Shannon entropy formula above is for a random variable with a discrete set of outcomes. We can define an entropy for a continuous random variable (\(\mathcal{S}_{x}=\{-\infty<x<\infty\}\)) as
\[
S=-\int \mathrm{d} x p(x) \ln p(x)=-\langle\ln p(x)\rangle
\]
There are, however, problems with this definition, as for example \(S\) is not invariant under a one-to-one mapping. (After a change of variable to \(f=F(x)\), the entropy is changed by \(\left\langle\left|F^{\prime}(x)\right|\right\rangle\).)
Canonically conjugate pairs offer a suitable choice of coordinates in classical statistical mechanics, since the Jacobian of a canonical transformation is unity. In such situations, the definition above can be used.
**** Boltzmann's entropy formula
The *Boltzmann's entropy formula* refers to the entropy of the uniform probability distribution that (according to the fundamental postulate of statistical mechanics) gives unbiased estimates of the /equilibrium probability distribution/ 
\[
\boxed{
p_{(E, \mathbf{x})}(\mu)=\frac{1}{\Omega(E, \mathbf{x})} \cdot \begin{cases}1 & \text { for } \mathcal{H}(\mu)=E \\ 0 & \text { otherwise }\end{cases}
}
\]
for microcanonical ensemble. It takes the form
\[
S(E, \mathbf{x})=k_{B} \ln \Omega(E, \mathbf{x})
\]
+ The factor of \(k_{B}\), called the /Boltzmann's constant/, is introduced, in contrast to Shannon's entropy formula, so that the entropy has the correct dimensions of energy per degree Kelvin, used in thermodynamics.
+ \(\Omega\) and \(S\) are not changed by a /canonical transformation/ in phase space. For a collection of independent systems, the overall allowed phase space is the product of individual ones, that is, \(\Omega_{\text {Total }}=\prod_{i} \Omega_{i}\). The resulting entropy is thus additive, as expected for an extensive thermodynamic coordinate.
*** The microcanonical ensemble
#+NAME: Microcanonical ensemble
#+begin_definition latex
In the absence of heat or work input to the system, the internal energy \( E \), and the generalized coordinates \(\mathbf{x}\), are fixed, specifying a macrostate \(M \equiv(E, \mathbf{x})\). The corresponding set of /mixed microstates/ forms the /microcanonical ensemble/.
#+end_definition
In classical statistical mechanics, these microstates are defined by points in phase space, their time evolution governed by a Hamiltonian \(H(\mu)\). Since the canonical equations conserve the total energy of a given system, all microstates are confined to the surface \(H(\mu)=E\) in phase space. Let us assume that there are no other conserved quantities, so that all points on this surface are mutually accessible. 
#+NAME: Equilibrium probability distribution (microcanonical ensemble)
#+begin_postulate latex
The fundamental postulate of statistical mechanics, by assumption, prescribes the unbiased estimate of the /equilibrium probability distribution/ in the microcanonical ensemble by
\[
\boxed{
p_{(E, \mathbf{x})}(\mu)=\frac{1}{\Omega(E, \mathbf{x})} \cdot \begin{cases}1 & \text { for } \mathcal{H}(\mu)=E, \\ 0 & \text { otherwise. }\end{cases}
}
\]
\( p_{(E, \mathbf{x})}(\mu) \) is in fact the unbiased probability estimate in phase space subject to the constraint of constant energy. It is an application of the principle of maximum entropy. Hence, the fundamental postulate of statistical mechanics is also called /Boltzmann's assumption of equal a priori equilibrium probabilities/.
#+end_postulate
#+NAME: Entropy (microcanonical ensemble)
#+begin_definition latex
The /entropy/ of the equilibrium probability distribution of the microcanonical ensemble is given by Boltzmann's entropy formula
\[
S(E, \mathbf{x})=k_{B} \ln \Omega(E, \mathbf{x})
\]
#+end_definition
#+begin_remark latex
This assignment is consistent with, but not required by, Liouville's theorem. Note that the phase space specifying the microstates \(\mu\) must be composed of canonically conjugate pairs. Under a canonical change of variables, \(\mu \rightarrow \mu^{\prime}\), volumes in phase space are left invariant. The Jacobian of such transformations is unity, and the transformed probability, \(p\left(\mu^{\prime}\right)=p(\mu) \lvert \partial \mu^{\prime} \mu\rvert \), is again uniform on the surface of constant energy.
#+end_remark
#+begin_remark latex
The normalization factor \(\Omega(E, \mathbf{x})\) is the area of the surface of constant energy \(E\) in phase space. To avoid subtleties associated with densities that are non-zero only on a surface, it is sometimes more convenient to define the microcanonical ensemble by requiring \(E-\Delta \leq H(\mu) \leq E+\Delta\), that is, constraining the energy of the ensemble up to an uncertainty of \(\Delta\). In this case, the accessible phase space forms a shell of thickness \(2 \Delta\) around the surface of energy \(E\). The normalization is now the volume of the shell, \(\Omega^{\prime} \approx 2 \Delta \Omega\). Since \(\Omega\) typically depends exponentially on \(E\), as long as \(\Delta \sim \mathcal{O}\left(E^{0}\right)\) (or even \(\left.\mathcal{O}\left(E^{1}\right)\right)\), the difference between the surface and volume of the shell is negligible in the \(E \propto N \rightarrow \infty\) limit, and we can use \(\Omega\) and \(\Omega^{\prime}\) interchangeably.
#+end_remark
**** Connection with thermodynamics
The laws of thermodynamics follow from the form of the equilibrium probability distribution of the microcanonical ensemble provided that we consider macroscopic systems with many degrees of freedom.
***** The zeroth law
Equilibrium properties are discussed in thermodynamics by placing two previously isolated systems in contact and allowing them to exchange heat. We can similarly bring together two microcanonical systems and allow them to exchange energy, but not work.
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-4.1.jpg]]
#+CAPTION: The exchange of energy between two isolated systems.
If the original systems have energies \(E_{1}\) and \(E_{2}\), respectively, the combined system has energy \(E=E_{1}+E_{2}\). Assuming that interactions between the two parts are small, each microstate of the joint system corresponds to a pair of microstates of the two components, that is, \(\mu=\mu_{1} \otimes \mu_{2}\), and \(H\left(\mu_{1} \otimes \mu_{2}\right)=H_{1}\left(\mu_{1}\right)+H_{2}\left(\mu_{2}\right)\). As the joint system is in a microcanonical ensemble of energy \(E=E_{1}+E_{2}\), in thermal equilibrium
\[
p_{E}\left(\mu_{1} \otimes \mu_{2}\right)=\frac{1}{\Omega(E)} \cdot\left\{\begin{array}{ll}
1 & \text { for } H_{1}\left(\mu_{1}\right)+H_{2}\left(\mu_{2}\right)=E \\
0 & \text { otherwise }
\end{array} .\right. \tag{1}
\]
Since only the overall energy is fixed, the total allowed phase space is computed from
\[
\Omega(E)=\int \mathrm{d} E_{1} \Omega_{1}\left(E_{1}\right) \Omega_{2}\left(E-E_{1}\right)=\int \mathrm{d} E_{1} \exp \left[\frac{S_{1}\left(E_{1}\right)+S_{2}\left(E-E_{1}\right)}{k_{B}}\right]. \tag{2}
\]
Extensivity of entropy suggests that \(S_{1}\) and \(S_{2}\) are proportional to the numbers of particles in each system, making the integrand above an exponentially large quantity. Hence the integral can be equated (by appeal to the Laplace approximation) to the maximum value of the integrand, obtained for energies \(E_{1}^{\ast}\) and \(E_{2}^{\ast}=E-E_{1}^{\ast}\), that is,
\[
\Omega(E) \approx \exp \bigg(\frac{S_{1}(E_{1}^{\ast})+S_{2}(E_{2}^{\ast})}{k_{B}}\bigg). \tag{3}
\]
Using Boltzmann's entropy formula, we have
\[
S(E)=k_{B} \ln \Omega(E) \approx S_{1}\left(E_{1}^{*}\right)+S_{2}\left(E_{2}^{*}\right). \tag{4}
\]
The position of the maximum is obtained by extremizing the exponent in (3) with respect to \(E_{1}\), resulting in the condition
\[
(\partial_{E_1} S_{1})_{\mathbf{x}_{1}} = (\partial_{E_2} S_{2})_{\mathbf{x}_{\mathbf{2}}}. \tag{5}
\]
*Although all joint microstates are equally likely, the above results indicate that there is an exponentially larger number of states in the vicinity of* \(\left(E_{1}^{\ast}, E_{2}^{\ast}\right)\).
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-4.2.jpg]]
#+CAPTION: The joint number of states of two systems in contact is overwhelmingly larger at the "equilibrium" energies E_1* and E_2*.
Originally, the joint system starts in the vicinity of some point \(\left(E_{1}^{0}, E_{2}^{0}\right)\). After the exchange of energy takes place, the combined system explores a whole set of new microstates. The probabilistic arguments provide no information on the dynamics of evolution amongst these microstates, or on the amount of time needed to establish thermal equilibrium. However, once sufficient time has elapsed so that the fundamental postulate of statistical mechanics is again valid, the system is overwhelmingly likely to be at a state with internal energy \(\left(E_{1}^{\ast}, E_{2}^{\ast}\right)\). At this equilibrium point, condition (5) is satisfied, specifying a relation between two state functions. These state functions are thus equivalent to empirical temperatures, and, indeed, consistent with the zeroth law of thermodynamics
\[
\boxed{
T = (\partial_E S)_{\mathbf{x}}^{-1}
}
\]
***** The first law
The first law of thermodynamics states that to change the state of a system we need a fixed amount of energy, which can be in the form of mechanical work or heat. We similarly take an adiabatically isolated microcanonical system, and do work on the system by changing the coordinates by \(\delta \mathbf{x}\) under a constant force \(\mathbf{J}\) via a reversible transformation.
Consider the variations of \(S(E, \mathbf{x})\). The amount of work done on the system is \(\mathrm{} W=\mathbf{J} \cdot \delta \mathbf{x}\), which changes the internal energy to \(E+\mathbf{J} \cdot \delta \mathbf{x}\). The first-order change in entropy is
\[
\delta S=S(E+\mathbf{J} \cdot \delta \mathbf{x}, \mathbf{x}+\delta \mathbf{x})-S(E, \mathbf{x})=\left[(\partial_E S)_{\mathbf{x}} \, \mathbf{J} + (\partial_{\mathbf{x}} S)_{E}\right] \cdot \delta \mathbf{x} \tag{1}
\]
*This change will occur spontaneously, taking the system into a more probable state, unless the quantity in brackets is zero.* 
Using the Maxwell relation \( T = (\partial_E S)_{\mathbf{x}}^{-1} \) (see also the zeroth law from the fundamental postulate) this allows us to identify the derivatives
\[
(\partial_{x_{i}} S)_{E,\,x_{j \neq i}} = - J_{i}/T.
\]
Substituting the partial derivatives into the expression for the equilibrium variations of \(S\) we have
\[
\mathrm{d} S(E, \mathbf{x})= (\partial_E S)_{\mathbf{x}} \, \mathbf{J} \cdot \mathrm{d} \mathbf{x} + (\partial_{\mathbf{x}} S)_{E} \cdot \mathrm{d} \mathbf{x} = T^{-1} (\mathrm{d} E - \mathbf{J} \cdot \mathrm{d} \mathbf{x}) \quad \Longrightarrow \quad \mathrm{d} E=T \mathrm{~d} S+\mathbf{J} \cdot \mathrm{d} \mathbf{x}.
\]
Finally, we identify the heat input \(\mathrm{} \mathrm{Q}=T \mathrm{~d} S\).
***** The second law
We use the same setup with which we recovered the zeroth law from the fundamental postulate: we bring together two microcanonical systems, and allow them to exchange energy, but not work.
We have seen in the zeroth law from the fundamental postulate that once sufficient time has elapsed so that the fundamental postulate of statistical mechanics is again valid, the system is overwhelmingly likely to be at a state with internal energy \(\left(E_{1}^{\ast}, E_{2}^{\ast}\right)\). Clearly, the above statistical definition of equilibrium rests on the presence of many degrees of freedom \(N \gg 1\), which make it exponentially unlikely in \(N\) that the combined systems are found with component energies different from \(\left(E_{1}^{*}, E_{2}^{*}\right)\).
By this construction, the equilibrium point has a larger number of accessible microstates than any starting point, that is,
\[
\Omega_{1}\left(E_{1}^{*}, \mathbf{x}_{1}\right) \Omega_{2}\left(E_{2}^{*}, \mathbf{x}_{2}\right) \geq \Omega_{1}\left(E_{1}, \mathbf{x}_{1}\right) \Omega_{2}\left(E_{2}, \mathbf{x}_{2}\right)
\]
In the process of evolving to the more likely (and more densely populated) regions, there is an irreversible loss of information accompanied by an increase in entropy
\[
\delta S=S_{1}\left(E_{1}^{*}\right)+S_{2}\left(E_{2}^{*}\right)-S_{1}\left(E_{1}\right)-S_{2}\left(E_{2}\right) \geq 0
\]
as required by the second law of thermodynamics.
When the two microcanonical systems are first brought into contact, the equality derived in the zeroth law from the fundamental postulate
\[
(\partial_{E_1} S_{1})_{\mathbf{x}_{1}} = (\partial_{E_2} S_{2})_{\mathbf{x}_{\mathbf{2}}}
\]
*does not* hold. The change in entropy is such that
\[
\delta S=\left[(\partial_{E_1} S_{1})_{\mathbf{x}_{1}} - (\partial_{E_2} S_{2})_{\mathbf{x}_{\mathbf{2}}}\right] \, \delta E_{1} = [T_{1}^{-1} - T_{2}^{-1} ] \, \delta E_{1} \geq 0
\]
that is, heat (energy) flows from the hotter to the colder body, as in Clausius's statement of the second law.
***** Stability conditions
We use the same setup with which we recovered the zeroth law from the fundamental postulate and the second law from the fundamental postulate: we bring together two microcanonical systems, and allow them to exchange energy, but not work.
Since the point \(\left(E_{1}^{\ast}, E_{2}^{\ast}\right)\) is a maximum, the second derivative of \(S_{1}\left(E_{1}\right)+S_{2}\left(E_{2}\right)\) must be negative at this point, that is,
\[
(\partial_{E_{1}}^{2} S_{1})_{\mathbf{x}_{1}} + (\partial_{E_{2}}^{2} S_{2})_{\mathbf{x}_{2}} \leq 0
\]
Applying the above condition to two parts of the same system, the condition of thermal stability, \(C_{\mathbf{x}} \geq 0\), as obtained in consequences of the stability condition is regained.
In the first law from the fundamental postulate we obtained the first-order change in entropy in an adiabatically isolated microcanonical system, on which we did work by changing the coordinates by \(\delta \mathbf{x}\) under a constant force \(\mathbf{J}\) via a reversible transformation:
\[
\delta S=S(E+\mathbf{J} \cdot \delta \mathbf{x}, \mathbf{x}+\delta \mathbf{x})-S(E, \mathbf{x})=\left[(\partial_E S)_{\mathbf{x}} \, \mathbf{J} + (\partial_{\mathbf{x}} S)_{E}\right] \cdot \delta \mathbf{x}
\]
The second-order changes in the above equation must be *negative*, requiring that the Hessian matrix \((\partial_{x_{i}} \, \partial_{x_{j}} \, S)_{E}\) be *positive definite*, same as what was obtained in consequences of the stability condition.
**** Examples
***** The ideal gas
Microstates of a gas of \(N\) particles correspond to points \(\mu \equiv\left\{\vec{p}_i, \vec{q}_i\right\}\) in the \(6 N\)-dimensional phase space. Ignoring the potential energy of interactions, the particles are subject to a Hamiltonian
\begin{align*}
\mathcal{H}=\sum_{i=1}^N\left[\frac{\vec{p}_i{ }^2}{2 m}+U\left(\vec{q}_i\right)\right],
\end{align*}
where \(U(\vec{q})\) describes the potential imposed by a box of volume \(V\). A microcanonical ensemble is specified by its energy, volume, and number of particles, \(M \equiv(E, V, N)\). The joint PDF for a microstate is
\[
p(\mu)=\frac{1}{\Omega(E, V, N)} \cdot 
\begin{array}{ll}
1 & \text { for } \vec{q}_{i} \in \text { box, and } \sum_{i} \vec{p}_{i}{ }^{2} / 2 m=E \quad ( \pm \Delta_{E}) \\
0 & \text { otherwise }
\end{array}. \tag{1}
\]
In the allowed microstates, coordinates of the particles must be within the box, while the momenta are constrained to the surface of the (hyper-)sphere \(\sum_{i=1}^{N} \vec{p}_{i}^{2}=2 m E\). The allowed phase space is thus the product of a contribution \(V^{N}\) from the coordinates, with the surface area of a \(3 N\)-dimensional sphere of radius \(\sqrt{2 m E}\) from the momenta. (If the microstate energies are accepted in the energy interval \(E \pm \Delta_{E}\), the corresponding volume in momentum space is that of a (hyper-)spherical shell of thickness \(\Delta_{R}=\sqrt{2 m / E} \Delta E\).) The area of a \(d\)-dimensional sphere is \(\mathcal{A}_{d}=S_{d} R^{d-1}\), where \(S_{d}\) is the generalized solid angle.
A simple way to calculate the \(d\)-dimensional solid angle is to consider the product of \(d\) Gaussian integrals,
\[
I_{d} \equiv\left(\int_{-\infty}^{\infty} \mathrm{d} x \mathrm{e}^{-x^{2}}\right)^{d}=\pi^{d / 2} \tag{2}
\]
Alternatively, we may consider \(I_{d}\) as an integral over the entire \(d\)-dimensional space, that is,
\[
I_{d}=\int \prod_{i=1}^{d} \mathrm{~d} x_{i} \exp \left(-x_{i}^{2}\right)
\]
The integrand is spherically symmetric, and we can change coordinates to \(R^{2}=\sum_{i} x_{i}^{2}\). Noting that the corresponding volume element in these coordinates is \(\mathrm{d} V_{d}=S_{d} R^{d-1} \mathrm{~d} R\)
\[
I_{d}=\int_{0}^{\infty} \mathrm{d} R S_{d} R^{d-1} \mathrm{e}^{-R^{2}}=\frac{S_{d}}{2} \int_{0}^{\infty} \mathrm{d} y y^{d / 2-1} \mathrm{e}^{-y}=\frac{S_{d}}{2}(d / 2-1) ! \tag{3}
\]
where we have first made a change of variables to \(y=R^{2}\), and then used the integral representation of \(n!\) in
\begin{align*}
\int_0^{\infty} \mathrm{d} x x^N \mathrm{e}^{-\alpha x}=\frac{N!}{\alpha^{N+1}}.
\end{align*}
Equating expressions (2) and (3) for \(I_{d}\) gives the final result for the solid angle,
\[
S_{d}=\frac{2 \pi^{d / 2}}{(d / 2-1) !}
\]
The volume of the available phase space is thus given by
\[
\Omega(E, V, N)=V^{N} \frac{2 \pi^{3 N / 2}}{(3 N / 2-1) !}(2 m E)^{(3 N-1) / 2} \Delta_{R} \tag{4}
\]
The entropy is obtained from the logarithm of the above expression. Using Stirling's formula, and neglecting terms of order of 1 or \(\ln E \sim \ln N\) in the large \(N\) limit, results in
\begin{align*}
S(E, V, N) & =k_{B}\left[N \ln V+\frac{3 N}{2} \ln (2 \pi m E)-\frac{3 N}{2} \ln \frac{3 N}{2}+\frac{3 N}{2}\right] \\
& =N k_{B} \ln \left[V\left(\frac{4 \pi \mathrm{e} m E}{3 N}\right)^{3 / 2}\right]
\end{align*}
Properties of the ideal gas can now be recovered from \(T \mathrm{~d} S=\mathrm{d} E+P \mathrm{~d} V-\mu \mathrm{d} N\),
\[
\frac{1}{T}=\left.\frac{\partial S}{\partial E}\right|_{N, V}=\frac{3}{2} \frac{N k_{B}}{E} .
\]
The internal energy \(E=3 N k_{B} T / 2\) is only a function of \(T\), and the heat capacity \(C_{V}=3 N k_{B} / 2\) is a constant. The equation of state is obtained from
\[
\frac{P}{T}=\left.\frac{\partial S}{\partial V}\right|_{N, E}=\frac{N k_{B}}{V}, \quad \Longrightarrow \quad P V=N k_{B} T .
\]
The unconditional probability of finding a particle of momentum \(\vec{p}_{1}\) in the gas can be calculated from the joint PDF in (1), by integrating over all other variables,
\begin{align*}
p\left(\vec{p}_{1}\right) & =\int \mathrm{d}^{3} \vec{q}_{1} \prod_{i=2}^{N} \mathrm{~d}^{3} \vec{q}_{i} \mathrm{~d}^{3} \vec{p}_{i} p\left(\left\{\vec{q}_{i}, \vec{p}_{i}\right\}\right) \\
& =\frac{V \Omega\left(E-\vec{p}_{1}^{2} / 2 m, V, N-1\right)}{\Omega(E, V, N)}
\end{align*}
The final expression indicates that once the kinetic energy of one particle is specified, the remaining energy must be shared amongst the other \(N-1\). Using Eq. (4)
\begin{align*}
p\left(\vec{p}_{1}\right) & =\frac{V^{N} \pi^{3(N-1) / 2}\left(2 m E-\vec{p}_{1}^{2}\right)^{(3 N-4) / 2}}{(3(N-1) / 2-1) !} \cdot \frac{(3 N / 2-1) !}{V^{N} \pi^{3 N / 2}(2 m E)^{(3 N-1) / 2}} \\
& =\left(1-\frac{\vec{p}_{1}^{2}}{2 m E}\right)^{3 N / 2-2} \frac{1}{(2 \pi m E)^{3 / 2}} \frac{(3 N / 2-1) !}{(3(N-1) / 2-1) !} .
\end{align*}
From Stirling's formula, the ratio of \((3 N / 2-1)\) ! to \((3(N-1) / 2-1)\) ! is approximately \((3 N / 2)^{3 / 2}\), and in the large \(E\) limit,
\[
p\left(\vec{p}_{1}\right)=\left(\frac{3 N}{4 \pi m E}\right)^{3 / 2} \exp \left(-\frac{3 N}{2} \frac{\vec{p}_{1}^{2}}{2 m E}\right)
\]
This is a properly normalized Maxwell-Boltzmann distribution, which can be displayed in its more familiar form after the substitution \(E=3 N k_{B} T / 2\),
\[
p\left(\vec{p}_{1}\right)=\frac{1}{\left(2 \pi m k_{B} T\right)^{3 / 2}} \exp \left(-\frac{\vec{p}_{1}^{2}}{2 m k_{B} T}\right)
\]
***** Two-level system
Consider \(N\) impurity atoms trapped in a solid matrix. Each impurity can be in one of two states, with energies 0 and \(\epsilon\), respectively. This example is somewhat different from the situations considered so far, in that the allowed microstates are discrete. Liouville's theorem applies to Hamiltonian evolution in a continuous phase space. Although there is less ambiguity in enumeration of discrete states, the dynamics that ensures that all allowed microstates are equally accessed will remain unspecified for the moment. (An example from quantum mechanical evolution will be presented later on.)
The microstates of the two-level system are specified by the set of occupation numbers \(\left\{n_i\right\}\), where \(n_i=0\) or 1 depending on whether the \(i\) th impurity is in its ground state or excited. The overall energy is
\begin{align*}
\mathcal{H}\left(\left\{n_i\right\}\right)=\epsilon \sum_{i=1}^N n_i \equiv \epsilon N_1,
\end{align*}
where \(N_1\) is the total number of excited impurities. The macrostate is specified by the total energy \(E\), and the number of impurities \(N\). The microcanonical probability is thus
\begin{align*}
p\left(\left\{n_i\right\}\right)=\frac{1}{\Omega(E, N)} \delta_{\epsilon \sum_i n_i, E}. \tag{1}
\end{align*}
As there are \(N_{1}=E / \epsilon\) excited impurities, the normalization \(\Omega\) is the number of ways of choosing \(N_{1}\) excited levels among the available \(N\), and given by the binomial coefficient
\[
\Omega(E, N)=\frac{N !}{N_{1} !\left(N-N_{1}\right) !} \tag{2}
\]
The entropy
\[
S(E, N)=k_{B} \ln \frac{N !}{N_{1} !\left(N-N_{1}\right) !}
\]
can be simplified by /Stirling's formula/ in the limit of \(N_{1}, N \gg 1\) to
\begin{align*}
S(E, N) & \approx-N k_{B}\left[\frac{N_{1}}{N} \ln \frac{N_{1}}{N}+\frac{N-N_{1}}{N} \ln \frac{N-N_{1}}{N}\right] \\
& =-N k_{B}\left[\left(\frac{E}{N \epsilon}\right) \ln \left(\frac{E}{N \epsilon}\right)+\left(1-\frac{E}{N \epsilon}\right) \ln \left(1-\frac{E}{N \epsilon}\right)\right] .
\end{align*}
The equilibrium temperature can now be calculated from the Maxwell's relation \( (\partial_{E} S)_{\mathbf{x}} = T^{-1} \) as
\[
\frac{1}{T}= (\partial_{E} S)_{N} = - \frac{k_{B}}{\epsilon} \ln \bigg(\frac{E}{N \epsilon-E}\bigg). \tag{3}
\]
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-4.3.jpg]]
#+CAPTION: The internal energy of a two-level system, as a function of its temperature T.
Alternatively, the internal energy at a temperature \(T\) is given by
\[
E(T)=\frac{N \epsilon}{\exp \left(\frac{\epsilon}{k_{B} T}\right)+1}. \tag{4}
\]
The internal energy is a monotonic function of temperature, increasing from a minimum value of 0 at \(T=0\) to a maximum value of \(N \epsilon / 2\) at infinite temperature. It is, however, possible to start with energies larger than \(N \epsilon / 2\), which correspond to negative temperatures from (3). The origin of the negative temperature is the decrease in the number of microstates with increasing energy, the opposite of what happens in most systems. Two-level systems have an upper bound on their energy, and very few microstates close to this maximal energy. Hence, increased energy leads to more order in the system. However, once a negative temperature system is brought into contact with the rest of the Universe (or any portion of it without an upper bound in energy), it loses its excess energy and comes to equilibrium at a positive temperature. The world of negative temperatures is quite unusual in that systems can be cooled by adding heat, and heated by removing it. There are physical examples of systems temporarily prepared at a metastable equilibrium of negative temperature in lasers, and for magnetic spins.
The heat capacity of the system, given by
\[
C=\frac{\mathrm{d} E}{\mathrm{~d} T}=N k_{B}\left(\frac{\epsilon}{k_{B} T}\right)^{2} \exp \left(\frac{\epsilon}{k_{B} T}\right)\left[\exp \left(\frac{\epsilon}{k_{B} T}\right)+1\right]^{-2},
\]
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-4.4.jpg]]
#+CAPTION: The heat capacity of a two-level system goes to zero at both high and low temperatures T.
vanishes at both low and high temperatures. The vanishing of \(C\) as \(\exp \left(-\epsilon / k_{B} T\right)\) at low temperatures is characteristic of all systems with an energy gap separating the ground state and lowest excited states. The vanishing of \(C\) at high temperatures is a saturation effect, common to systems with a maximum in the number of states as a function of energy. In between, the heat capacity exhibits a peak at a characteristic temperature of \(T_{\epsilon} \propto \epsilon / k_{B}\).
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-4.5.jpg]]
#+CAPTION: The probabilities for finding a single impurity in its ground state (top), or excited state (bottom), as a function of temperature T.
Statistical mechanics provides much more than just macroscopic information for quantities such as energy and heat capacity. (1) is a complete joint probability distribution with considerable information on the microstates. For example, the unconditional probability for exciting a particular impurity is obtained from
\[
p\left(n_{1}\right)=\sum_{\left\{n_{2}, \cdots, n_{N}\right\}} p\left(\left\{n_{i}\right\}\right)=\frac{\Omega\left(E-n_{1} \epsilon, N-1\right)}{\Omega(E, N)} .
\]
The second equality is obtained by noting that once the energy taken by the first impurity is specified, the remaining energy must be distributed among the other \(N-1\) impurities. Using (2),
\[
p\left(n_{1}=0\right)=\frac{\Omega(E, N-1)}{\Omega(E, N)}=\frac{(N-1) !}{N_{1} !\left(N-N_{1}-1\right) !} \cdot \frac{N_{1} !\left(N-N_{1}\right) !}{N !}=1-\frac{N_{1}}{N}
\]
and \(p\left(n_{1}=1\right)=1-p\left(n_{1}=0\right)=N_{1} / N\). Using \(N_{1}=E / \epsilon\), and (4), the occupation probabilities at a temperature \(T\) are
\[
p(0)=\frac{1}{1+\exp \left(-\frac{\epsilon}{k_{B} T}\right)}, \quad \text { and } \quad p(1)=\frac{\exp \left(-\frac{\epsilon}{k_{B} T}\right)}{1+\exp \left(-\frac{\epsilon}{k_{B} T}\right)}
\]
*** Mixing entropy and Gibbs paradox
**** Mixing entropy
#+NAME: Mixing entropy
#+begin_definition latex
Consider two distinct gases, initially occupying volumes \(V_{1}\) and \(V_{2}\) at the same temperature \(T\). Further suppose that the entropy of the combined system while the partition is in place is given by \(S_{i}\). The partition between them is removed, and they are allowed to expand and occupy the combined volume \(V=V_{1}+V_{2}\). Suppose the entropy of the combined system of the mixed gas is \(S_{f}\). The mixing entropy is defined as \( \Delta S_{\text{Mix}} = S_{f} - S_{i} \).
#+end_definition
#+NAME: Mixing entropy (ideal gases)
#+begin_lemma latex
The mixing entropy of \(n\) ideal gases is given by
\[
\Delta S_{\mathrm{Mix}}= -N k_{B} \sum_{\alpha=1}^{n} \left(N_{\alpha} / N\right) \ln \left(V_{\alpha} / V\right).
\]
#+end_lemma
#+NAME: Mixing entropy (ideal gases)
#+begin_proof latex
Consider two distinct gases, initially occupying volumes \(V_{1}\) and \(V_{2}\) at the same temperature \(T\). The partition between them is removed, and they are allowed to expand and occupy the combined volume \(V=V_{1}+V_{2}\). The mixing process is clearly irreversible, and must be accompanied by an increase in entropy, calculated as follows. According to
\begin{align*}
S(E, V, N) & =k_{B}\left[N \ln V+\frac{3 N}{2} \ln (2 \pi m E)-\frac{3 N}{2} \ln \frac{3 N}{2}+\frac{3 N}{2}\right] \\
& =N k_{B} \ln \left[V\left(\frac{4 \pi \mathrm{e} m E}{3 N}\right)^{3 / 2}\right],
\end{align*}
obtained for an ideal gas in a microcanonical ensemble, the initial entropy is
\[
S_{i}=S_{1}+S_{2}=N_{1} k_{B}\left(\ln V_{1}+\sigma_{1}\right)+N_{2} k_{B}\left(\ln V_{2}+\sigma_{2}\right)
\]
where
\[
\sigma_{\alpha}=\ln \left(\frac{4 \pi \mathrm{e} m_{\alpha}}{3} \cdot \frac{E_{\alpha}}{N_{\alpha}}\right)^{3 / 2}
\]
is the momentum contribution to the entropy of the gas \(\alpha\). Since \(E_{\alpha} / N_{\alpha}=\) \(3 k_{B} T / 2\) for a monotonic gas,
\[
\sigma_{\alpha}(T)=\frac{3}{2} \ln \left(2 \pi \mathrm{e}_{\alpha} k_{B} T\right).
\]
The temperature of the gas is unchanged by mixing, since
\[
\frac{3}{2} k_{B} T_{f}=\frac{E_{1}+E_{2}}{N_{1}+N_{2}}=\frac{E_{1}}{N_{1}}=\frac{E_{2}}{N_{2}}=\frac{3}{2} k_{B} T.
\]
The final entropy of the mixed gas is
\[
S_{f}=N_{1} k_{B} \ln \left(V_{1}+V_{2}\right)+N_{2} k_{B} \ln \left(V_{1}+V_{2}\right)+k_{B}\left(N_{1} \sigma_{1}+N_{2} \sigma_{2}\right).
\]
There is no change in the contribution from the momenta, which depends only on temperature. The mixing entropy,
[
\Delta S_{\mathrm{Mix}}=S_{f}-S_{i}=N_{1} k_{B} \ln \frac{V}{V_{1}}+N_{2} k_{B} \ln \frac{V}{V_{2}}=-N k_{B}\left[\frac{N_{1}}{N} \ln \frac{V_{1}}{V}+\frac{N_{2}}{N} \ln \frac{V_{2}}{V}\right],
\]
is solely from the contribution of the coordinates. The above expression is easily generalized to the mixing of many components \[
\Delta S_{\mathrm{Mix}}= -N k_{B} \sum_{\alpha=1}^{n} \left(N_{\alpha} / N\right) \ln \left(V_{\alpha} / V\right).
\]
#+end_proof
#+ATTR_HTML: :width 500px
[[file:~/.local/images/kardar-spop-4.6.jpg]]
#+CAPTION: A mixing entropy results from removing the partition separating two gases.
**** Gibbs paradox
While considering the ideal gas in the microcanonical ensemble, we obtained
\begin{align*}
S(E, V, N) & =k_{B}\left[N \ln V+\frac{3 N}{2} \ln (2 \pi m E)-\frac{3 N}{2} \ln \frac{3 N}{2}+\frac{3 N}{2}\right] \\
& =N k_{B} \ln \left[V\left(\frac{4 \pi \mathrm{e} m E}{3 N}\right)^{3 / 2}\right].
\end{align*}
This expression for the entropy of the ideal gas has a major shortcoming in that it is not extensive. Under the transformation \((E, V, N) \rightarrow\) \((\lambda E, \lambda V, \lambda N)\), the entropy changes to \(\lambda\left(S+N k_{B} \ln \lambda\right)\). The additional term comes from the contribution \(V^{N}\) of the coordinates to the available phase space.  This difficulty is described by the /Gibbs paradox/ using the concept of the /mixing entropy/ of two gases. 
The Gibbs paradox is related to what happens when the two gases, initially on the two sides of the partition, are identical with the same density, \(n=\) \(N_{1} / V_{1}=N_{2} / V_{2}\). Since removing or inserting the partition does not change the state of the system, there should be no entropy of mixing, while the previously obtained formula for the mixing entropy of \( n \) ideal gases
\[
\Delta S_{\mathrm{Mix}}= -N k_{B} \sum_{\alpha=1}^{n} \left(N_{\alpha} / N\right) \ln \left(V_{\alpha} / V\right).
\]
does predict such a change \( \Delta S_{\mathrm{Mix}} \neq 0 \).
**** Correct Boltzmann counting
For the resolution of this paradox, note that while after removing and reinserting the partition, the macroscopic system does return to its initial configuration, the actual particles that occupy the two chambers are not the same. But as the particles are by assumption identical, these configurations cannot be distinguished. In other words, while the exchange of distinct particles leads to two configurations, a similar exchange has no effect on identical particles. For a gas with \(N\) identical particles, we have over-counted the phase space by \(N!\), the number of permutations leading to indistinguishable microstates. Therefore, we introduce a correction for the volume of the available phase space
\[
\Omega(E, V, N)=V^{N} \frac{2 \pi^{3 N / 2}}{(3 N / 2-1) !}(2 m E)^{(3 N-1) / 2} \Delta_{R} \tag{4}
\]
by modifying it to
\[
\Omega(N, E, V)=\frac{V^{N}}{N !} \frac{2 \pi^{3 N / 2}}{(3 N / 2-1) !}(2 m E)^{(3 N-1) / 2} \Delta_{R}.
\]
The modified \( \Omega (N, E, V) \) results in a modified entropy
\[
S=k_{B} \ln \Omega=k_{B}[N \ln V-N \ln N+N \ln \mathrm{e}]+N k_{B} \sigma=N k_{B}\left[\ln \frac{\mathrm{e} V}{N}+\sigma\right].
\]
As the argument of the logarithm has changed from \(V\) to \(V / N\), the final expression is now properly extensive. The mixing entropies can be recalculated using this modified formula for the entropy.
For the mixing of /distinct gases/,
\begin{align*}
\Delta S_{\text {Mix }}=S_{f}-S_{i} & =N_{1} k_{B} \ln \frac{V}{N_{1}}+N_{2} k_{B} \ln \frac{V}{N_{2}}-N_{1} k_{B} \ln \frac{V_{1}}{N_{1}}-N_{2} k_{B} \ln \frac{V_{2}}{N_{2}} \\
& =N_{1} k_{B} \ln \left(\frac{V}{N_{1}} \cdot \frac{N_{1}}{V_{1}}\right)+N_{2} k_{B} \ln \left(\frac{V}{N_{2}} \cdot \frac{N_{2}}{V_{2}}\right) \\
& =-N k_{B}\left[\frac{N_{1}}{N} \ln \frac{V_{1}}{V}+\frac{N_{2}}{N} \ln \frac{V_{2}}{V}\right]
\end{align*}
exactly as obtained before.
For the "mixing" of two /identical gases/,
\[
\Delta S_{\text {Mix}}=S_{f}-S_{i}=\left(N_{1}+N_{2}\right) k_{B} \ln \frac{V_{1}+V_{2}}{N_{1}+N_{2}}-N_{1} k_{B} \ln \frac{V_{1}}{N_{1}}-N_{2} k_{B} \ln \frac{V_{2}}{N_{2}}=0
\]
where we have used \(N_{1} / V_{1}=N_{2} / V_{2}= (N_{1}+N_{2}) / (V_{1}+V_{2})\), true for identical gases. The Gibb's paradox is resolved because \( \Delta S_{\mathrm{Mix}} = 0 \). Note that the resolution of the paradox required an additional postulate: /permutations of identical particles leads to indistinguishable microstates/.
The correction to the available phase space volume under the assumption of identical particles is called /correct Boltzmann counting/. Thus we say that /correct Boltzmann counting resolves the Gibbs paradox/. Note that after taking the permutations of identical particles into account, the available coordinate volume in the final state is \(V^{N_{1}+N_{2}} / N_{1} ! N_{2}\) ! for distinct particles, and \(V^{N_{1}+N_{2}} /\left(N_{1}+N_{2}\right)\) ! for identical particles.
***** Notes
1) In the example of two-level impurities in a solid matrix, the volume of the available phase space
\[
\Omega(E, N)=\frac{N !}{N_{1} !\left(N-N_{1}\right) !} \tag{2}
\]
does *not* require a correction for over-counting by a factor of \(N!\) because there is no over-counting: defects are never identical as they can be distinguished by their locations.
2) The corrected formula for the ideal gas entropy in
  \[
  S=k_{B} \ln \Omega=k_{B}[N \ln V-N \ln N+N \ln \mathrm{e}]+N k_{B} \sigma=N k_{B}\left[\ln \frac{\mathrm{e} V}{N}+\sigma\right].
  \]
 does not affect the computations of energy in
  \[
  \frac{1}{T}=\left.\frac{\partial S}{\partial E}\right|_{N, V}=\frac{3}{2} \frac{N k_{B}}{E},
  \]
  and the pressure in
  \[
  \frac{P}{T}=\left.\frac{\partial S}{\partial V}\right|_{N, E}=\frac{N k_{B}}{V}, \quad \Longrightarrow \quad P V=N k_{B} T.
  \]
  It is essential to obtaining an intensive chemical potential,
  \[
  \frac{\mu}{T}=-\left.\frac{\partial S}{\partial N}\right|_{E, V}=-\frac{S}{N}+\frac{5}{2} k_{B}=k_{B} \ln \left[\frac{V}{N}\left(\frac{4 \pi m E}{3 N}\right)^{3 / 2}\right].
  \]
3) The above treatment of identical particles is somewhat artificial. This is because the concept of identical particles does not easily fit within the framework of classical mechanics. To implement the Hamiltonian equations of motion on a computer, one has to keep track of the coordinates of the \(N\) particles. The computer will have no difficulty in distinguishing exchanged particles. The indistinguishability of their phase spaces is in a sense an additional postulate of classical statistical mechanics. This problem is elegantly resolved within the framework of quantum statistical mechanics. Description of identical particles in quantum mechanics requires proper symmetrization of the wave function. The corresponding quantum microstates naturally yield the \(N!\) factor.
4) The volume of phase space involves products \( pq \), of coordinates and conjugate momenta, and hence can change by an arbitrary factor if we change the units in which \( p \) and \( q \) are measured. It follows that the number of microstates depends on the units of measurement which is nonsense. The volume of phase space involves products \(p q\), of coordinates and conjugate momenta, and hence has dimensions of (action) \({ }^{N}\). Quantum mechanics provides the appropriate measure of action in Planck's constant \(h\). Anticipating these quantum results, we shall henceforth set the measure of phase space for identical particles to
  \[
  \mathrm{d} \Gamma_{N}=\frac{1}{h^{3 N} N !} \prod_{i=1}^{N} \mathrm{~d}^{3} \vec{q}_{i} \mathrm{~d}^{3} \vec{p}_{i}.
  \]
*** The canonical ensemble
Our starting point in thermodynamics was a mechanically and adiabatically isolated system which specified a macrostate \(M \equiv(E, \mathbf{x})\). The corresponding set of mixed microstates formed the microcanonical ensemble. We now allow the input of heat into the system but no external work. The corresponding set of mixed microstates forms the canonical ensemble.
#+NAME: Canonical ensemble
#+begin_definition latex
For isothermal transformations in the absence of mechanical work \((\mathrm{} W=0)\), the temperature \( T \) and the generalized coordinates \(\mathbf{x}\), are fixed, which specifies a macrostate \(M \equiv(T, \mathbf{x})\)
The corresponding set of /mixed microstates/ forms the /canonical ensemble/.
#+end_definition
#+NAME: Equilibrium probability distribution (canonical ensemble)
#+begin_theorem latex
From the fundamental postulate of statistical mechanics it follows that the unbiased estimate of equilibrium probability distribution in the canonical ensemble is given by
\[
\boxed{
p_{(T,\,\mathbf{x})} (\mu) = \exp(-\beta H(\mu)) / Z(T, \mathbf{x}), \qquad Z(T, \mathbf{x})=\sum_{\{\mu\}} \exp(-\beta H(\mu))
}
\]
where the normalization \( Z(T, \mathbf{x})=\sum_{\{\mu\}} \exp(-\beta H(\mu)) \) is called the partition function, and \(\beta \equiv 1 / k_{B} T\). \( p_{(T,\,\mathbf{x})} (\mu) \) is in fact the unbiased probability estimate in phase space subject to the constraint of a constant average energy.
#+end_theorem
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-4.7.jpg]]
#+CAPTION: The system S can be maintained at a temperature T through heat exchanges with the reservoir R.
#+NAME: Equilibrium probability distribution (canonical ensemble)
#+begin_proof latex
Let \(S\) be the system that is maintained at a constant temperature through contact with a reservoir \(\mathrm{R}\). The reservoir is another macroscopic system that is sufficiently large so that its temperature is not changed due to interactions with \(\mathrm{S}\). In this setup, the temperature \( T \) and the generalized coordinates \(\mathbf{x}\), are fixed, which specifies a macrostate \(M \equiv(T, \mathbf{x})\). The fundamental postulate of statistical mechanics postulates that the unbiased estimate of equilibrium probability distribution for the joint system \(\mathrm{S} \oplus \mathrm{R}\) in a microcanonical ensemble of energy \( E_{\text{Tot}} \gg E_{S} \) is given by
\[
\boxed{
p_{(T,\,\mathbf{x})}\left(\mu_{\mathrm{S}} \otimes \mu_{\mathrm{R}}\right)=\frac{1}{\Omega_{\mathrm{S} \oplus \mathrm{R}}\left(E_{\mathrm{Tot}}\right)} \cdot \begin{cases}1 & \text { for } H_{\mathrm{S}}\left(\mu_{\mathrm{S}}\right)+H_{\mathrm{R}}\left(\mu_{\mathrm{R}}\right)=E_{\mathrm{Tot}} \\ 0 & \text { otherwise }\end{cases}
}
\]
The unconditional probability for microstates of \( S \) is obtained via marginalization
\[
p_{(T,\,\mathbf{x})} \left(\mu_{\mathrm{S}}\right)=\sum_{\left\{\mu_{\mathrm{R}}\right\}} p\left(\mu_{\mathrm{S}} \otimes \mu_{\mathrm{R}}\right).
\]
Once \(\mu_{\mathrm{S}}\) is specified, the above sum is restricted to microstates of the reservoir with energy \(E_{\text {Tot }}-\mathcal{H}_{\mathrm{S}}\left(\mu_{\mathrm{S}}\right)\). The number of such states is related to the entropy of the reservoir, and leads to
\[
p\left(\mu_{\mathrm{S}}\right)=\frac{\Omega_{\mathrm{R}}\left(E_{\mathrm{Tot}}-H_{\mathrm{S}}\left(\mu_{\mathrm{S}}\right)\right)}{\Omega_{\mathrm{S} \oplus \mathrm{R}}\left(E_{\mathrm{Tot}}\right)} \propto \exp \left[\frac{1}{k_{B}} S_{\mathrm{R}}\left(E_{\mathrm{Tot}}-H_{\mathrm{S}}\left(\mu_{\mathrm{S}}\right)\right)\right]
\]
Since, by assumption, the energy of the system is insignificant compared with that of the reservoir,
\[
S_{\mathrm{R}}\left(E_{\mathrm{Tot}}-H_{\mathrm{S}}\left(\mu_{\mathrm{S}}\right)\right) \approx S_{\mathrm{R}}\left(E_{\mathrm{Tot}}\right)-H_{\mathrm{S}}\left(\mu_{\mathrm{S}}\right)\,\partial_{E_{\mathrm{R}}} S_{\mathrm{R}} =S_{\mathrm{R}}\left(E_{\mathrm{Tot}}\right)-\frac{H_{\mathrm{S}}\left(\mu_{\mathrm{S}}\right)}{T}
\]
Dropping the subscript \(S\), the normalized probabilities are given by
\[
p_{(T,\,\mathbf{x})} (\mu) = \exp(-\beta H(\mu)) / Z(T, \mathbf{x}), \qquad Z(T, \mathbf{x})=\sum_{\{\mu\}} \exp(-\beta H(\mu))
\]
where the normalization \( Z(T, \mathbf{x})=\sum_{\{\mu\}} \exp(-\beta H(\mu)) \) is called the partition function, and \(\beta \equiv 1 / k_{B} T\).
#+end_proof
#+NAME: Entropy (canonical ensemble)
#+begin_definition latex
The entropy of the canonical ensemble can be be calculated directly from the equilibrium probability distribution
\[
p_{(T,\,\mathbf{x})} (\mu) = \exp(-\beta H(\mu)) / Z(T, \mathbf{x}), \qquad Z(T, \mathbf{x})=\sum_{\{\mu\}} \exp(-\beta H(\mu))
\]
by using Shannon's entropy formula
\begin{align*}
S=-k_B\langle\ln p(\mu)\rangle=-k_B\langle(-\beta \mathcal{H}-\ln Z)\rangle=\frac{E-F}{T}.
\end{align*}
In the final step, we have identified \(F(T,\,\mathbf{x}) = -k_{B} T \ln Z (T,\,\mathbf{x})\), the Helmholtz free energy.
#+end_definition
\begin{table}
\centering
\begin{tabular}{llll}
\hline
\hline
Ensemble       & Macrostate          & \(p(\mu)\)                                   & Normalization                         \\ \hline
Microcanonical & \((E, \mathbf{x})\) & \(\delta_{\Delta}( H(\mu) - E) / \Omega\)       & \(S(E, \mathbf{x}) = k_{B} \ln \Omega\) \\
Canonical      & \((T, \mathbf{x})\) & \(\exp (-\beta H(\mu)) / Z\)                 & \(F(T, \mathbf{x}) = -k_{B} T \ln Z\) \\
\hline
\hline
\end{tabular}
\end{table}
#+CAPTION: Comparison of canonical and microcanonical ensembles
**** The Helmholtz free energy
In the canonical ensemble, the energy of a system \( S \) exchanging heat with a reservoir R is a random variable. We denote it using \(\mathcal{E}\). This is in contrast with the microcanonical ensemble where the energy is fixed. The probability distribution for the energy \(p(\mathcal{E})\) can be obtained by obtained by changing variables from \(\mu\) to \(H(\mu)\) in
\[
p_{(T,\,\mathbf{x})} (\mu) = \exp(-\beta H(\mu)) / Z(T, \mathbf{x}), \qquad Z(T, \mathbf{x})=\sum_{\{\mu\}} \exp(-\beta H(\mu))
\]
resulting in
\[
p_{(T,\,\mathbf{x})}(\mathcal{E})=\sum_{\{\mu\}} p_{(T,\,\mathbf{x})}(\mu) \delta(H(\mu)-\mathcal{E})=\frac{\exp(-\beta \mathcal{E})}{Z} \sum_{\{\mu\}} \delta(H(\mu)-\mathcal{E})
\]
Since the restricted sum is just the number \(\Omega(\mathcal{E})\) of microstates of appropriate energy
\[
p_{(T,\,\mathbf{x})}(\mathcal{E})=\frac{\Omega(\mathcal{E}) \exp(-\beta \mathcal{E})}{Z}=\frac{1}{Z} \exp \left[\frac{S(\mathcal{E})}{k_{B}}-\frac{\mathcal{E}}{k_{B} T}\right]=\frac{1}{Z} \exp \left[-\frac{(\mathcal{E} - T S (\mathcal{E}))}{k_{B} T}\right],
\]
where we have used Boltzmann's entropy formula. The probability \(p(\mathcal{E})\) is sharply peaked at a /most probable energy/ \(E^{\ast}\), which minimizes \(\mathcal{E} - T S (\mathcal{E})\). Using the result on the summation of exponential exponential quantities
\begin{align*}
\lim _{N \rightarrow \infty} \frac{\ln \mathcal{S}}{N}=\frac{\ln \mathcal{E}_{\max }}{N}=\phi_{\max } .
\end{align*}
we have
\[
Z=\sum_{\{\mu\}} \exp(-\beta H(\mu)) = \sum_{\mathcal{E}} \exp(-\beta (\mathcal{E} - T S (\mathcal{E}))) \approx \exp[-\beta (E^{\ast} - T S (E^{\ast}))]. \tag{1}
\]
The /average energy/ \( \langle H \rangle \) computed using \( p_{(T,\,\mathbf{x})}(\mathcal{E}) \) is
\[
\langle H \rangle = \sum_{\mu} H(\mu) \frac{\exp(-\beta H(\mu))}{Z} = - \frac{1}{Z} \big( \partial_{\beta} \sum_{\mu} \exp(-\beta H) \big) = - \partial_{\beta} \ln Z. \tag{2}
\]
In thermodynamics, a similar expression was encountered for the energy,
\begin{align*}
E=F+T S=F-\left.T \frac{\partial F}{\partial T}\right|_{\mathbf{x}}=-T^2 \frac{\partial}{\partial T}\left(\frac{F}{T}\right)=\frac{\partial(\beta F)}{\partial \beta}.
\end{align*}
Equations (1) and (2) both suggest identifying
\[
F(T,\,\mathbf{x}) = -k_{B} T \ln Z (T,\,\mathbf{x}). \tag{3}
\]
However, note that (1) refers to the /most likely energy/, while (2) refers to the /average energy/. To determine if it is justified to treat the /average energy/ and the /most probable energy/ interchangeably, we can look at the width of the probability distribution \(p(\mathcal{E})\), by computing the variance \(\left\langle H^{2}\right\rangle_{c}\). This is most easily accomplished by noting that \(Z(\beta)\) is related to the characteristic function for \(H\) (with \(\beta\) replacing \(ik\)) and
\[
-\frac{\partial Z}{\partial \beta}=\sum_{\mu} H \mathrm{e}^{-\beta H}, \quad \text { and } \quad \frac{\partial^{2} Z}{\partial \beta^{2}}=\sum_{\mu} H^{2} \mathrm{e}^{-\beta} H \tag{4}
\]
Cumulants of \(H\) are generated by \(\ln Z(\beta)\),
\[
\langle H \rangle_{c}=\frac{1}{Z} \sum_{\mu} H \mathrm{e}^{-\beta H}=-\frac{1}{Z} \frac{\partial Z}{\partial \beta}=-\frac{\partial \ln Z}{\partial \beta} \tag{5}
\]
and
\[
\langle H^{2} \rangle_{c}= \langle H^{2} \rangle - \langle H \rangle^{2} = \frac{1}{Z} \sum_{\mu} H^{2} \exp(-\beta H)-\frac{1}{Z^{2}} \left(\sum_{\mu} H \exp(-\beta H)\right)^{2} = \partial^{2}_{\beta} \ln Z = - \partial_{\beta} \langle H \rangle \tag{6}
\]
More generally, the \(n^{\text{th}}\) cumulant of \(H\) is given by
\[
\langle H^{n} \rangle_{c} = (-1)^{n} \partial^{n}_{\beta} \ln Z \tag{7}
\]
From (6)
\[
\langle H^{2} \rangle_{c} = - \partial_{\beta} \langle H \rangle = k_{B} T^{2} (\partial_{T} \langle H \rangle)_{\mathbf{x}}, \quad \Rightarrow \quad \langle H^{2} \rangle_{c} = k_{B} T^{2} C_{\mathbf{x}} \tag{8}
\]
where we have identified the heat capacities with the thermal derivative of the average energy \(\langle H \rangle\).
Equation (8) shows that it is justified to treat the /average energy/ and the /most probable energy/ interchangeably, since the width of the distribution \(p(\mathcal{E})\) only grows as \(\sqrt{\langle H^{2} \rangle_{c}} \propto N^{1 / 2}\). The relative error, \(\sqrt{\langle H^{2} \rangle_{c}} /\langle H \rangle_{c}\), vanishes in the thermodynamic limit as \(1 / \sqrt{N}\). In fact, (7) shows that all cumulants of \(H\) are proportional to \(N\). The probability distribution for the energy in the canonical ensemble can thus be approximated by
\[
p(\mathcal{E})=\frac{1}{Z} \exp(-\beta F(\mathcal{E})) \approx \exp \left(-\frac{(\mathcal{E}-\langleH\rangle)^{2}}{2 k_{B} T^{2} C_{\mathbf{x}}}\right) \frac{1}{\sqrt{2 \pi k_{B} T^{2} C_{\mathbf{x}}}} .
\]
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-4.8.jpg]]
#+CAPTION: The probability distribution of the internal energy for a system in a canonical ensemble at a temperature T.
The above distribution is sufficiently sharp to make the internal energy in a canonical ensemble unambiguous in the \(N \rightarrow \infty\) limit. Some care is necessary if the heat capacity \(C_{\mathrm{x}}\) is divergent, as is the case in some continuous phase transitions.
**** Examples
***** The ideal gas
For the canonical macrostate \(M \equiv(T, V, N)\), the joint PDF for the microstates \(\mu \equiv\left\{\vec{p}_{i}, \vec{q}_{i}\right\}\) is
\[
p\left(\left\{\vec{p}_{i}, \vec{q}_{i}\right\}\right)=\frac{1}{Z} \exp \left[-\beta \sum_{i=1}^{N} \frac{p_{i}^{2}}{2 m}\right] \cdot\left\{\begin{array}{ll}
1 & \text { for }\left\{\vec{q}_{i}\right\} \in \text { box } \\
0 & \text { otherwise }
\end{array} .\right. \tag{1}
\]
Including the modifications to the phase space of identical particles in
  \[
  \mathrm{d} \Gamma_{N}=\frac{1}{h^{3 N} N !} \prod_{i=1}^{N} \mathrm{~d}^{3} \vec{q}_{i} \mathrm{~d}^{3} \vec{p}_{i},
  \]
the dimensionless partition function is computed as
\begin{align*}
Z(T, V, N) & =\int \frac{1}{N !} \prod_{i=1}^{N} \frac{\mathrm{d}^{3} \vec{q}_{i} \mathrm{~d}^{3} \vec{p}_{i}}{h^{3}} \exp \left[-\beta \sum_{i=1}^{N} \frac{p_{i}^{2}}{2 m}\right] \\
& =\frac{V^{N}}{N !}\left(\frac{2 \pi m k_{B} T}{h^{2}}\right)^{3 N / 2}=\frac{1}{N !}\left(\frac{V}{\lambda(T)^{3}}\right)^{N}
\end{align*}
where
\[
\lambda(T)=\frac{h}{\sqrt{2 \pi m k_{B} T}}
\]
is a characteristic length associated with the action \(h\). It shall be demonstrated later on that this length scale controls the onset of quantum mechanical effects in an ideal gas.
The free energy is given by
\begin{align*}
F & =-k_{B} T \ln Z=-N k_{B} T \ln V+N k_{B} T \ln N-N k_{B} T-\frac{3 N}{2} k_{B} T \ln \left(\frac{2 \pi m k_{B} T}{h^{2}}\right) \\
& =-N k_{B} T\left[\ln \left(\frac{V \mathrm{e}}{N}\right)+\frac{3}{2} \ln \left(\frac{2 \pi m k_{B} T}{h^{2}}\right)\right] .
\end{align*}
Various thermodynamic properties of the ideal gas can now be obtained from \(\mathrm{d} F=-S \mathrm{~d} T-P \mathrm{~d} V+\mu \mathrm{d} N\). For example, from the entropy
\[
-S=\left.\frac{\partial F}{\partial T}\right|_{V, N}=-N k_{B}\left[\ln \frac{V \mathrm{e}}{N}+\frac{3}{2} \ln \left(\frac{2 \pi m k_{B} T}{h^{2}}\right)\right]-N k_{B} T \frac{3}{2 T}=\frac{F-E}{T},
\]
we obtain the internal energy \(E=3 N k_{B} T / 2\). The equation of state is obtained from
\[
P=-\left.\frac{\partial F}{\partial V}\right|_{T, N}=\frac{N k_{B} T}{V}, \quad \Longrightarrow \quad P V=N k_{B} T
\]
and the chemical potential is given by
\[
\mu=\left.\frac{\partial F}{\partial N}\right|_{T, V}=\frac{F}{N}+k_{B} T=\frac{E-T S+P V}{N}=k_{B} T \ln \left(n \lambda^{3}\right) \text {. }
\]
Also, according to (1), the momenta of the \(N\) particles are taken from independent Maxwell-Boltzmann distributions, consistent with
\[
p\left(\vec{p}_{1}\right)=\frac{1}{\left(2 \pi m k_{B} T\right)^{3 / 2}} \exp \left(-\frac{\vec{p}_{1}^{2}}{2 m k_{B} T}\right)
\]
obtained the ideal gas in a microcanonical ensemble.
***** Two-level system
The \(N\) impurities are described by a macrostate \(M \equiv\) \((T, N)\). Subject to the Hamiltonian \(\mathcal{H}=\epsilon \sum_{i=1}^{N} n_{i}\), the canonical probabilities of the microstates \(\mu \equiv\left\{n_{i}\right\}\) are given by
\[
p\left(\left\{n_{i}\right\}\right)=\frac{1}{Z} \exp \left[-\beta \epsilon \sum_{i=1}^{N} n_{i}\right]. \tag{1}
\]
From the partition function,
\begin{align*}
Z(T, N) & =\sum_{\left\{n_{i}\right\}} \exp \left[-\beta \epsilon \sum_{i=1}^{N} n_{i}\right]=\left(\sum_{n_{1}=0}^{1} \mathrm{e}^{-\beta \epsilon n_{1}}\right) \cdots\left(\sum_{n_{N}=0}^{1} \mathrm{e}^{-\beta \epsilon n_{N}}\right) \\
& =\left(1+\mathrm{e}^{-\beta \epsilon}\right)^{N},
\end{align*}
we obtain the free energy
\[
F(T, N)=-k_{B} T \ln Z=-N k_{B} T \ln \left[1+\mathrm{e}^{-\epsilon /\left(k_{B} T\right)}\right]
\]
The entropy is now given by
\[
S=-\left.\frac{\partial F}{\partial T}\right|_{N}=\underbrace{N k_{B} \ln \left[1+\mathrm{e}^{-\epsilon /\left(k_{B} T\right)}\right]}_{-F / T}+N k_{B} T\left(\frac{\epsilon}{k_{B} T^{2}}\right) \frac{\mathrm{e}^{-\epsilon /\left(k_{B} T\right)}}{1+\mathrm{e}^{-\epsilon /\left(k_{B} T\right)}} .
\]
The internal energy,
\[
E=F+T S=\frac{N \epsilon}{1+\mathrm{e}^{\epsilon /\left(k_{B} T\right)}}
\]
can also be obtained from
\[
E=-\frac{\partial \ln Z}{\partial \beta}=\frac{N \epsilon \mathrm{e}^{-\beta \epsilon}}{1+\mathrm{e}^{-\beta \epsilon}}
\]
Since the joint probability in (1) is in the form of a product, \(p=\prod_{i} p_{i}\), the excitations of different impurities are independent of each other, with the unconditional probabilities
\[
p_{i}\left(n_{i}\right)=\frac{\mathrm{e}^{-\beta \epsilon n_{i}}}{1+\mathrm{e}^{-\beta \epsilon}}
\]
This result coincides with
\[
p(0)=\frac{1}{1+\exp \left(-\frac{\epsilon}{k_{B} T}\right)}, \quad \text { and } \quad p(1)=\frac{\exp \left(-\frac{\epsilon}{k_{B} T}\right)}{1+\exp \left(-\frac{\epsilon}{k_{B} T}\right)}
\]
obtained through a more elaborate analysis in the microcanonical ensemble. As expected, in the large \(N\) limit, the canonical and microcanonical ensembles describe exactly the same physics, both at the macroscopic and microscopic levels.
*** The Gibbs canonical ensemble
#+NAME: Gibbs canonical ensemble
#+begin_definition latex
For isothermal transformations involving mechanical work (\( \mathrm{} W \neq 0\)) at constant external force \( J \) and no chemical work, the temperature \( T \) and the generalized forces \( \mathbf{J} \), are fixed, which specifies a macrostate \(M \equiv(T, \mathbf{J})\) in which the generalized coordinates \( \mathbf{x} \) appear as random variables. The corresponding set of mixed microstates forms the Gibbs canonical ensemble.
#+end_definition
#+begin_remark latex
The system whose microstates form the Gibbs canonical ensemble is maintained at constant force through external elements (e.g., pistons or magnets). Including the work done against the forces, the energy of the combined system that includes these elements is \(\mathcal{H}-\mathbf{J} \cdot \mathbf{x}\). Note that while the work done on the system is \(+\mathbf{J} \cdot \mathbf{x}\), the energy change associated with the external elements with coordinates \(\mathbf{x}\) has the opposite sign.
#+end_remark
#+NAME: Equilibrium probability distribution (Gibbs canonical ensemble)
#+begin_theorem latex
From the fundamental postulate of statistical mechanics it follows that the unbiased estimate of equilibrium probability distribution in the (Gibbs) canonical ensemble is given by
\[
\boxed{
p_{(T,\,\mathbf{J})} (\mu,\, \mathbf{x}) = \exp(-\beta H(\mu) + \beta \mathbf{J} \cdot \mathbf{x}) / \mathcal{Z} (T,\, N, \, \mathbf{J}), \qquad \mathcal{Z} (T,\, N, \, \mathbf{J})=\sum_{\mu, \mathbf{x}} \exp(-\beta H(\mu) + \beta \mathbf{J} \cdot \mathbf{x})
}
\]
where the normalization \( \mathcal{Z} (T,\, N, \, \mathbf{J})\) is called the Gibbs partition function, and \(\beta \equiv 1 / k_{B} T\).
#+end_theorem
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-4.9.jpg]]
#+CAPTION: A system in contact with a reservoir at temperature T, and maintained at a fixed force J.
#+NAME: Entropy (Gibbs canonical ensemble)
#+begin_definition latex
The entropy of the canonical ensemble can be be calculated directly from the equilibrium probability distribution
\[
p_{(T,\,\mathbf{J})} (\mu,\, \mathbf{x}) = \exp(-\beta H(\mu) + \beta \mathbf{J} \cdot \mathbf{x}) / \mathcal{Z} (T,\, N, \, \mathbf{J}), \qquad \mathcal{Z} (T,\, N, \, \mathbf{J})=\sum_{\mu, \mathbf{x}} \exp(-\beta H(\mu) + \beta \mathbf{J} \cdot \mathbf{x})
\]
by using Shannon's entropy formula
\begin{align*}
S=-k_B\langle\ln p(\mu)\rangle=-k_B\langle(-\beta \mathcal{H} + \beta \mathbf{J} \cdot \mathbf{x} -\ln Z)\rangle=\frac{E- \mathbf{x} \cdot J - G}{T}.
\end{align*}
In the final step, we have identified \(G(N,\,T,\,\mathbf{J}) = -k_{B} T \ln \mathcal{Z} (N,\,T,\,\mathbf{J})\), the Gibbs free energy.
#+end_definition
**** Gibbs free energy
In the Gibbs canonical ensemble, the generalized coordinates of a system \( S \) - exchanging heat with a reservoir R and subject to a constant external force \( \mathbf{J} \) - are random variables. We denote it using \(\mathcal{E}\). This is in contrast with the canonical ensemble where the generalized coordinates are fixed. We can obtain the expectation value of \( \mathbf{x} \) using the Gibbs partition function
\begin{align*}
\langle\mathbf{x}\rangle=k_B T \partial_{\mathbf{J}} \ln Z,
\end{align*}
which together with the thermodynamic identity \(\mathbf{x}=-\partial G / \partial \mathbf{J}\), suggests the identification
\begin{align*}
G(N, T, \mathbf{J})=-k_B T \ln Z,
\end{align*}
where \(G=E-T S-\mathbf{x} \cdot \mathbf{J}\) is the Gibbs free energy.The /enthalpy/ \(H \equiv E-\mathbf{x} \cdot \mathbf{J}\) is easily obtained in this ensemble from
\begin{align*}
-\frac{\partial \ln Z}{\partial \beta}=\langle\mathcal{H}-\mathbf{x} \cdot \mathbf{J}\rangle=H .
\end{align*}
Note that heat capacities at constant force (which include work done against the external forces) are obtained from the enthalpy as \(C_{\mathbf{J}}=\partial H / \partial T\).
**** Examples
***** The ideal gas
The ideal gas in the isobaric ensemble is described by the macrostate \(M \equiv(N, T, P)\). A microstate \(\mu \equiv\left\{\vec{p}_{i}, \vec{q}_{i}\right\}\), with a volume \(V\), occurs with the probability
\[
p\left(\left\{\vec{p}_{i}, \vec{q}_{i}\right\}, V\right)=\frac{1}{\mathcal{Z}} \exp \left[-\beta \sum_{i=1}^{N} \frac{p_{i}^{2}}{2 m}-\beta P V\right] \cdot \begin{cases}1 & \text { for }\left\{\vec{q}_{i}\right\} \in \text { box of volume } V \\ 0 & \text { otherwise }\end{cases}
\]
The normalization factor is now
\begin{align*}
Z(N, T, P) & =\int_{0}^{\infty} \mathrm{d} V \mathrm{e}^{-\beta P V} \int \frac{1}{N !} \prod_{i=1}^{N} \frac{\mathrm{d}^{3} \vec{q}_{i} \mathrm{~d}^{3} \vec{p}_{i}}{h^{3}} \exp \left[-\beta \sum_{i=1}^{N} \frac{p_{i}^{2}}{2 m}\right] \\
& =\int_{0}^{\infty} \mathrm{d} V V^{N} \mathrm{e}^{-\beta P V} \frac{1}{N ! \lambda(T)^{3 N}}=\frac{1}{(\beta P)^{N+1} \lambda(T)^{3 N}}
\end{align*}
Ignoring non-extensive contributions, the Gibbs free energy is given by
\[
G=-k_{B} T \ln Z \approx N k_{B} T\left[\ln P-\frac{5}{2} \ln \left(k_{B} T\right)+\frac{3}{2} \ln \left(\frac{h^{2}}{2 \pi m}\right)\right]
\]
Starting from \(\mathrm{d} G=-S \mathrm{~d} T+V \mathrm{~d} P+\mu \mathrm{d} N\), the volume of the gas is obtained as
\[
V=\left.\frac{\partial G}{\partial P}\right|_{T, N}=\frac{N k_{B} T}{P}, \quad \Longrightarrow \quad P V=N k_{B} T
\]
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-4.10.jpg]]
#+CAPTION: In the isobaric ensemble, the gas in the piston is maintained at a pressure P.
The enthalpy \(H=\langle E+P V\rangle\) is easily calculated from
\[
H=-\frac{\partial \ln z}{\partial \beta}=\frac{5}{2} N k_{B} T
\]
from which we get \(C_{P}=\mathrm{d} H / \mathrm{d} T=5 / 2 N k_{B}\).
***** Spins in an external magnetic field
Adding the work done against the magnetic field to the internal Hamiltonian \(\mathcal{H}\) results in the Gibbs partition function
\[
Z(N, T, B)=\operatorname{tr}[\exp (-\beta \mathcal{H}+\beta \vec{B} \cdot \vec{M})]
\]
where \(\vec{M}\) is the net magnetization. The symbol "tr" is used to indicate the sum over all spin degrees of freedom, which in a quantum mechanical formulation are restricted to discrete values. The simplest case is spin of \(1 / 2\), with two possible projections of the spin along the magnetic field. A microstate of \(N\) spins is now described by the set of Ising variables \(\left\{\sigma_{i}= \pm 1\right\}\). The corresponding magnetization along the field direction is given by \(M=\mu_{0} \sum_{i=1}^{N} \sigma_{i}\), where \(\mu_{0}\) is a microscopic magnetic moment. Assuming that there are no interactions between spins \((\mathcal{H}=0)\), the probability of a microstate is
\[
p\left(\left\{\sigma_{i}\right\}\right)=\frac{1}{Z} \exp \left[\beta B \mu_{0} \sum_{i=1}^{N} \sigma_{i}\right] .
\]
Clearly, this is closely related to the example of two-level systems discussed in the canonical ensemble, and we can easily obtain the Gibbs partition function
\[
\mathcal{Z}(N, T, B)=\left[2 \cosh \left(\beta \mu_{0} B\right)\right]^{N}
\]
and the Gibbs free energy
\[
G=-k_{B} T \ln Z=-N k_{B} T \ln \left[2 \cosh \left(\beta \mu_{0} B\right)\right] .
\]
The average magnetization is given by
\[
M=-\frac{\partial G}{\partial B}=N \mu_{0} \tanh \left(\beta \mu_{0} B\right) .
\]
Expanding \( \tanh \left(\beta \mu_{0} B\right) \) for small \(B\) results in the well-known Curie law for magnetic susceptibility of non-interacting spins,
\[
\chi(T)=\left.\frac{\partial M}{\partial B}\right|_{B=0}=\frac{N \mu_{0}^{2}}{k_{B} T}
\]
The enthalpy is simply \(H=\langle\mathcal{H}-B M\rangle=-B M\), and \(C_{B}=-B \partial M / \partial T\).
*** The grand canonical ensemble
#+NAME: Grand canonical ensemble
#+begin_definition latex
For isothermal transformations involving chemical work at constant chemical potential \( \mu \) and no mechanical work, the temperature \( T \), the generalized coordinates \( \mathbf{x} \), and the chemical potential \( \mu \) are fixed, which specifies a macrostate \(M \equiv (T, \, \mu, \, \mathbf{x})\. The corresponding set of mixed microstates forms the grand canonical ensemble.
#+end_definition
#+begin_remark latex
The corresponding microstate \( \mu_{S} \) contain an indefinite number of particles \( N (\mu_{S}) \). As in the case of the canonical ensemble, the system \(\mathrm{S}\) can be maintained at a constant chemical potential through contact with a reservoir \(\mathrm{R}\), at temperature \(T\) and chemical potential \(\mu\). The probability distribution for the microstates of \(\mathrm{S}\) is obtained by summing over all states of the reservoir, as was done in the canonical ensemble.
#+end_remark
#+ATTR_HTML: :width 400px
[[file:~/.local/images/kardar-spop-4.11.jpg]]
#+CAPTION: A system S in contact with a reservoir R, of temperature T and  chemical potential .
#+NAME: Equilibrium probability distribution (Grand canonical ensemble)
#+begin_theorem latex
From the fundamental postulate of statistical mechanics it follows that the unbiased estimate of equilibrium probability distribution in the (grand) canonical ensemble is given by
\[
\boxed{
p_{(T, \, \mu,\, \mathbf{x})} (\mu_{\mathrm{S}} ) = \exp(\beta \, \mu \, N(\mu_{S}) - \beta H (\mu_{S})) / \mathcal{Q} (T, \, \mu,\, \mathbf{x}), \qquad \mathcal{Q} (T, \, \mu,\, \mathbf{x}) = \sum_{\mu_{S}} \exp(\beta \, \mu \, N(\mu_{S}) - \beta H (\mu_{S}))
}
\]
where the normalization \( \mathcal{Q} (T, \, \mu,\, \mathbf{x}) \) is called the grand partition function, and \(\beta \equiv 1 / k_{B} T\).
#+end_theorem
#+NAME: Entropy (Grand canonical ensemble)
#+begin_definition latex
The entropy of the grand canonical ensemble can be be calculated directly from the equilibrium probability distribution
\[
p_{(T, \, \mu,\, \mathbf{x})} (\mu_{\mathrm{S}} ) = \exp(\beta \, \mu \, N(\mu_{S}) - \beta H (\mu_{S})) / \mathcal{Q} (T, \, \mu,\, \mathbf{x}), \qquad \mathcal{Q} (T, \, \mu,\, \mathbf{x}) = \sum_{\mu_{S}} \exp(\beta \, \mu \, N(\mu_{S}) - \beta H (\mu_{S}))
\]
by using Shannon's entropy formula
\begin{align*}
S=-k_B\langle\ln p(\mu)\rangle=-k_B\langle(-\beta \mathcal{H} - \beta \mu N -\ln \mathcal{Q}) \rangle=\frac{E- \mu N - \mathcal{G}}{T}.
\end{align*}
In the final step, we have identified \(\mathcal{Q} (T,\,\mu,\, \mathbf{x}) = -k_{B} \ln \mathcal{Q}\), the grand potential.
#+end_definition
**** Grand potential
We can reorganize the summation for the grand partition function of the grand canonical ensemble by grouping together all microstates with a /given number of particles/, that is,
\[
\mathcal{Q}(T, \mu, \mathbf{x})=\sum_{N=0}^{\infty} \exp(\beta \mu N) \sum_{\left(\mu_{\mathrm{S}} \mid N\right)} \exp(-\beta H_{N}\left(\mu_{\mathrm{S}}\right)).
\]
The restricted sums in the equation above are just the \(N\)-particle partition functions. As each term in \(\mathcal{Q}\) is the total weight of all microstates of \(N\) particles, the unconditional probability of finding \(N\) particles in the system is
\[
p_{T,\,\mu,\,\mathbf{x}} (N) = \frac{\exp(\beta \mu N)\,Z(T,\, N,\, \mathbf{x})}{Q(T,\,\mu,\,\mathbf{x})}.
\]
The /average number/ of particles in the system is
\[
\langle N \rangle=\frac{1}{\mathcal{Q}} (\partial_{(\beta \mu)} \mathcal{Q}) = \partial_{(\beta \mu)} \ln \mathcal{Q}
\]
while the /number fluctuations/ are related to the variance
\[
\langle N^{2}\rangle_{c} = \langle N^{2} \rangle - \langle N \rangle^{2} = \frac{1}{\mathcal{Q}} \partial_{(\beta \mu)^{2}}^{2} \ln \mathcal{Q} - \left(\partial_{(\beta \mu)} \ln \mathcal{Q}\right)^{2} = \partial_{(\beta \mu)}^{2} \ln \mathcal{Q} = \partial_{(\beta \mu)} \langle N\rangle
\]
The variance is thus proportional to \(N\), and the relative number fluctuations vanish in the thermodynamic limit, establishing the equivalence of this ensemble to the previous ones.
Because of the sharpness of the distribution for \(N\), the sum 
\[
\mathcal{Q}(T, \mu, \mathbf{x})=\sum_{N=0}^{\infty} \exp(\beta \mu N) \sum_{\left(\mu_{\mathrm{S}} \mid N\right)} \exp(-\beta H_{N}\left(\mu_{\mathrm{S}}\right))
\]
can be approximated by its largest term at \(N=N^{\ast} \approx\langle N\rangle\), that is,
\begin{align*}
\mathcal{Q}(T, \mu, \mathbf{x}) & = \lim_{N \rightarrow \infty} \sum_{N=0}^{\infty} \exp(\beta \mu N) Z(T, N, \mathbf{x}) = \exp(\beta \mu N^{\ast}) Z (T, N^{\ast}, \mathbf{x}) = \exp (\beta \mu N^{\ast} - \beta F) \\
& = \exp (-\beta (-\mu N^{\ast} + E - T S)) = \exp(-\beta \mathcal{G}),
\end{align*}
where \(\mathcal{G}(T,\, \mu,\, \mathbf{x}) = E - T S - \mu N = -k_{B} T \ln Q (T, \mu, \mathbf{x})\) is the /grand potential/. Thermodynamic information is obtained by using \(\mathrm{d} \mathcal{G}=\) \(-S \mathrm{~d} T-N \mathrm{~d} \mu+\mathbf{J} \cdot \mathrm{d} \mathbf{x}\), as
\[
-S = (\partial_{T} \mathcal{G})_{\mu, \mathbf{x}} \qquad N = -(\partial_{\mu} \mathcal{G})_{T, \mathbf{x}} \qquad J_{i} = (\partial_{x_{i}} \mathcal{G})_{T, \mu}.
\]
**** Examples
***** The ideal gas
We compute the properties of the ideal gas of non-interacting particles in the grand canonical ensemble. The macrostate is \(M \equiv\) \((T, \mu, V)\), and the corresponding microstates \(\left\{\vec{p}_{1}, \vec{q}_{1}, \vec{p}_{2}, \vec{q}_{2}, \cdots\right\}\) have indefinite particle number. The grand partition function is given by
\begin{align*}
Q(T, \mu, V) & =\sum_{N=0}^{\infty} \mathrm{e}^{\beta \mu N} \frac{1}{N !} \int\left(\prod_{i=1}^{N} \frac{\mathrm{d}^{3} \vec{q}_{i} \mathrm{~d}^{3} \vec{p}_{i}}{h^{3}}\right) \exp \left[-\beta \sum_{i} \frac{p_{i}^{2}}{2 m}\right] \\
& =\sum_{N=0}^{\infty} \frac{\mathrm{e}^{\beta \mu N}}{N !}\left(\frac{V}{\lambda^{3}}\right)^{N} \quad\left(\text { with } \lambda=\frac{h}{\sqrt{2 \pi m k_{B} T}}\right) \\
& =\exp \left[\mathrm{e}^{\beta \mu} \frac{V}{\lambda^{3}}\right],
\end{align*}
and the grand potential is
\[
\mathcal{G}(T, \mu, V)=-k_{B} T \ln Q=-k_{B} T \mathrm{e}^{\beta \mu} \frac{V}{\lambda^{3}}.
\]
But, since \(\mathcal{G}=E-T S-\mu N=-P V\), the gas pressure can be obtained directly as
\[
P= - \mathcal{G}/V = - (\partial_{V} \mathcal{G})_{\mu,\,T} = k_{B} T \frac{\exp (\beta \, \mu)}{\lambda^{3}}. \tag{1}
\]
The particle number and the chemical potential are related by
\[
N=- (\partial_{\mu} \mathcal{G})_{T,\, V} = \frac{\exp (\beta \, \mu) \, V}{\lambda^{3}}. \tag{2}
\]
The equation of state is obtained by comparing Eqs. (1) and (2), as \(P=k_{B} T N / V\). Finally, the chemical potential is given by
\[
\mu=k_{B} T \ln \left(\lambda^{3} \frac{N}{V}\right)=k_{B} T \ln \left(\frac{P \lambda^{3}}{k_{B} T}\right)
\]
** Ising model
:LOGBOOK:
CLOCK: [2024-06-18 Tue 06:41]--[2024-06-18 Tue 07:34] =>  0:53
:END:
*** Preliminaries
#+NAME: Lattice
#+begin_definition latex
A set of integers from \(V=\{1,2, \ldots, N\} \equiv\{i\}_{i=1, \ldots, N}\) is called a lattice.
#+end_definition
#+NAME: Site
#+begin_definition latex
A site is an element \( i \) of a lattice \(V \equiv\{i\}_{i=1, \ldots, N}\).
#+end_definition
For example, a site may be the real lattice point on a crystal, or the pixel of a digital picture, or perhaps the neuron in a neural network.
#+NAME: Spin
#+begin_definition latex
A spin \(S_{i}\) is a random assigned to a site \(i\) of a lattice \(V \equiv\{i\}_{i=1, \ldots, N}\).
#+end_definition
#+NAME: Ising spin
#+begin_definition latex
A spin \(S_{i}\) is called an Ising spin if the set of possible outcomes \( \mathcal{S} \) of the random variable \( S_{i} \) is given by \( \mathcal{S} = \{1, \, - 1\}\).
#+end_definition
In the problem of magnetism, the Ising spin \(S_{i}\) represents whether the microscopic magnetic moment is pointing up or down.
#+NAME: Bond
#+begin_definition latex
A bond \( (i, \, j) \) is a pair made up of site \( i \) and site \( j \).
#+end_definition
#+NAME: Bond
#+begin_definition latex
A bond \( (i, \, j) \) is a pair made up of site \( i \) and site \( j \). We will denote an appropriate set of bonds using \( B = \{(ij)\} \).
#+end_definition
#+NAME: Interaction energy
#+begin_definition latex
For all bond \( (i, \, j) \in B \), the interaction energy is defined as \( - J S_{i} S_{j} \).
#+end_definition
#+NAME: Ferromagnetic interation
#+begin_definition latex
For Ising spins \( S_{i} \) and \( S_{j} \), the interaction energy is \(-J\) when the states of the two spins are the same \(\left(S_{i}=S_{j}\right)\) and is \(J\) otherwise \(\left(S_{i}=-S_{j}\right)\). When \( J > 0 \), the interaction is called ferromagnetic.
#+end_definition
If \( J > 0 \), then the two interacting spins tend to be oriented in the same direction (\(\uparrow \uparrow\) or \(\downarrow \downarrow\)); the positive interaction \(J>0\) is therefore called a /ferromagnetic interaction/.
#+NAME: Anti-ferromagnetic interation
#+begin_definition latex
For Ising spins \( S_{i} \) and \( S_{j} \), the interaction energy is \(-J\) when the states of the two spins are the same \(\left(S_{i}=S_{j}\right)\) and is \(J\) otherwise \(\left(S_{i}=-S_{j}\right)\). When \( J > 0 \), the interaction is called anti-ferromagnetic.
#+end_definition
If \( J < 0 \), then the two interacting spins tend to be oriented in the opposite direction (\(\uparrow \downarrow\) or \(\downarrow \downuparrow\)); the negative interaction \(J<0\) is therefore called a /anti-ferromagnetic interaction/.
#+NAME: Zeeman energy
#+begin_definition latex
The Zeeman energy is intrinsic energy of the site \( i \) and is given by \(-h S_{i}\).
#+end_definition
#+NAME: Hamiltonian of the Ising model
#+begin_definition latex
The total energy of a system therefore has the form
\[
H=-J \sum_{(i j) \in B} S_{i} S_{j}-h \sum_{i=1}^{N} S_{i}.
\]
#+end_definition
The choice of the set of bonds \(B\) depends on the type of problem one is interested in. For example, in the case of a two-dimensional crystal lattice, the set of sites \(V=\{i\}\) is a set of points with regular intervals on a two-dimensional space. The bond \((i j)\in B\) is a pair of nearest neighbor sites.
#+NAME: Spin configuration
#+begin_definition latex
A spin configuration is a set of spin states \(\mathbf{S} = \{S_{i}\} \).
#+end_definition
#+NAME: Equilibrium probability density
#+begin_definition latex
For a given Hamiltonian
\[
H=-J \sum_{(i j) \in B} S_{i} S_{j}-h \sum_{i=1}^{N} S_{i},
\]
the equilibrium probability density for the spin configuration \(\mathbf{S} = \{S_{i}\}_{i = 1 \ldots N} \) takes the form
\[
p(\mathbf{S}) = \exp (- \beta H(\mathbf{S})) / Z(T, \, \mathbf{S}).
\]
This is the Gibbs-Boltzmann distribution. \(\exp \{- \beta H (\mathbf{S})\}\) is termed the Boltzmann factor. 
#+end_definition
#+NAME: Partition function
#+begin_definition latex
We take the unit of temperature such that Boltzmann's constant \(k_{\mathrm{B}}\) is unity, and \(\beta\) is the inverse temperature \(\beta=1 / T\). The normalization factor \(Z\) is the partition function
\[
Z=\sum_{S_{1}= \pm 1} \sum_{S_{2}= \pm 1} \ldots \sum_{S_{N}= \pm 1} \mathrm{e}^{-\beta H} \equiv \sum_{S} \exp\{-\beta H\} \equiv \operatorname{tr} \exp \{- \beta H\}.
\]
One sometimes uses the notation \(\mathrm{Tr}\) for the sum over all possible spin configurations appearing in (1.3). Hereafter we use this notation for the sum over the values of Ising spins on sites:
\[
Z=\operatorname{Tr} \mathrm{e}^{-\beta H} \tag{1.4}
\]
#+end_definition
The general prescription of statistical mechanics is to calculate the thermal average of a physical quantity using the equilibrium probability distribution. We denote the expectation value for the Gibbs-Boltzmann distribution using angular brackets \(\langle\cdots\rangle\). Spin variables are not necessarily restricted to the Ising type \(\left(S_{i}= \pm 1\right)\). For instance, in the \(X Y\) model, the variable at a site \(i\) has a real value \(\theta_{i}\) with modulo \(2 \pi\), and the interaction energy has the form \(-J \cos \left(\theta_{i}-\theta_{j}\right)\). The energy due to an external field is \(-h \cos \theta_{i}\). The Hamiltonian of the \(X Y\) model is thus written as
\[
H=-J \sum_{(i j) \in B} \cos \left(\theta_{i}-\theta_{j}\right)-h \sum_{i} \cos \theta_{i} \tag{1.5}
\]
The \(X Y\) spin variable \(\theta_{i}\) can be identified with a point on the unit circle. If \(J>0\), the interaction term is ferromagnetic as it favours a parallel spin configuration \(\left(\theta_{i}=\theta_{j}\right)\).
*** Order parameter and phase transition
One of the most important quantities used to characterize the macroscopic properties of the Ising model with ferromagnetic interactions is the magnetization. Magnetization is defined by
\[
m=\frac{1}{N}\left\langle\sum_{i=1}^{N} S_{i}\right\rangle=\frac{1}{N} \operatorname{Tr}\left(\left(\sum_{i} S_{i}\right) P(\boldsymbol{S})\right) \tag{1.6}
\]
and measures the overall ordering in a macroscopic system (i.e.the system in the thermodynamic limit \(N \rightarrow \infty\) ). Magnetization is a typical example of an order parameter which is a measure of whether or not a macroscopic system is in an ordered state in an appropriate sense. The magnetization vanishes if there exist equal numbers of up spins \(S_{i}=1\) and down spins \(S_{i}=-1\), suggesting the absence of a uniformly ordered state.
At low temperatures \(\beta \gg 1\), the Gibbs-Boltzmann distribution (1.2) implies that low-energy states are realized with much higher probability than high-energy
Fig. 1.2. Temperature dependence of magnetization
states. The low-energy states of the ferromagnetic Ising model (1.1) without the external field \(h=0\) have almost all spins in the same direction. Thus at low temperatures the spin states are either up \(S_{i}=1\) at almost all sites or down \(S_{i}=-1\) at almost all sites. The magnetization \(m\) is then very close to either 1 or -1 , respectively.
As the temperature increases, \(\beta\) decreases, and then the states with various energies emerge with similar probabilities. Under such circumstances, \(S_{i}\) would change frequently from 1 to -1 and vice versa, so that the macroscopic state of the system is disordered with the magnetization vanishing. The magnetization \(m\) as a function of the temperature \(T\) therefore has the behaviour depicted in Fig. 1.2. There is a critical temperature \(T_{\mathrm{c}} ; m \neq 0\) for \(T<T_{\mathrm{c}}\) and \(m=0\) for \(T>T_{\mathrm{c}}\).
This type of phenomenon in a macroscopic system is called a phase transition and is characterized by a sharp and singular change of the value of the order parameter between vanishing and non-vanishing values. In magnetic systems the state for \(T<T_{\mathrm{c}}\) with \(m \neq 0\) is called the ferromagnetic phase and the state at \(T>T_{\mathrm{c}}\) with \(m=0\) is called the paramagnetic phase. The temperature \(T_{\mathrm{c}}\) is termed a critical point or a transition point.
*** Mean-field theory
In principle, it is possible to calculate the expectation value of any physical quantity using the Gibbs-Boltzmann distribution (1.2). It is, however, usually very difficult in practice to carry out the sum over \(2^{N}\) terms appearing in the partition function (1.3). One is thus often forced to resort to approximations. Mean-field theory (or the mean-field approximation) is used widely in such situations.
**** Mean-field Hamiltonian
:PROPERTIES:
:CUSTOM_ID: mean-field-hamiltonian
:END:
The essence of mean-field theory is to neglect fluctuations of microscopic variables around their mean values. One splits the spin variable \(S_{i}\) into the mean \(m=\sum_{i}\left\langle S_{i}\right\rangle / N=\left\langle S_{i}\right\rangle\) and the deviation (fluctuation) \(\delta S_{i}=S_{i}-m\) and assumes that the second-order term with respect to the fluctuation \(\delta S_{i}\) is negligibly small in the interaction energy:
\begin{align*}
H & =-J \sum_{(i j) \in B}\left(m+\delta S_{i}\right)\left(m+\delta S_{j}\right)-h \sum_{i} S_{i} \\
& \approx-J m^{2} N_{B}-J m \sum_{(i j) \in B}\left(\delta S_{i}+\delta S_{j}\right)-h \sum_{i} S_{i}
\tag{1.7}
\end{align*}
To simplify this expression, we note that each bond (ij) appears only once in the sum of \(\delta S_{i}+\delta S_{j}\) in the second line. Thus \(\delta S_{i}\) and \(\delta S_{j}\) assigned at both ends of a bond are summed up \(z\) times, where \(z\) is the number of bonds emanating from a given site (the coordination number), in the second sum in the final expression of (1.7):
\begin{align*}
H & =-J m^{2} N_{B}-J m z \sum_{i} \delta S_{i}-h \sum_{i} S_{i} \\
& =N_{B} J m^{2}-(J m z+h) \sum_{i} S_{i}
\tag{1.8}
\end{align*}
A few comments on (1.8) are in order.
1. \(N_{B}\) is the number of elements in the set of bonds \(B, N_{B}=|B|\).
2. We have assumed that the coordination number \(z\) is independent of site \(i\), so that \(N_{B}\) is related to \(z\) by \(z N / 2=N_{B}\). One might imagine that the total number of bonds is \(z N\) since each site has \(z\) bonds emanating from it. However, a bond is counted twice at both its ends and one should divide \(z N\) by two to count the total number of bonds correctly.
3. The expectation value \(\left\langle S_{i}\right\rangle\) has been assumed to be independent of \(i\). This value should be equal to \(m\) according to (1.6). In the conventional ferromagnetic Ising model, the interaction \(J\) is a constant and thus the average order of spins is uniform in space. In spin glasses and other cases to be discussed later this assumption does not hold.
The effects of interactions have now been hidden in the magnetization \(m\) in the mean-field Hamiltonian (1.8). The problem apparently looks like a noninteracting case, which significantly reduces the difficulties in analytical manipulations.
**** Equation of state
:PROPERTIES:
:CUSTOM_ID: equation-of-state
:END:
The mean-field Hamiltonian (1.8) facilitates calculations of various quantities. For example, the partition function is given as
\begin{align*}
Z & =\operatorname{Tr} \exp \left[\beta\left\{-N_{B} J m^{2}+(J m z+h) \sum_{i} S_{i}\right\}\right] \\
& =\mathrm{e}^{-\beta N_{B} J m^{2}}\{2 \cosh \beta(J m z+h)\}^{N}
\tag{1.9}
\end{align*}
A similar procedure with \(S_{i}\) inserted after the trace operation \(\operatorname{Tr}\) in (1.9) yields the magnetization \(m\),

Fig. 1.3. Solution of the mean-field equation of state
\begin{align*}
m=\frac{\operatorname{Tr} S_{i} \mathrm{e}^{-\beta H}}{Z}=\tanh \beta(J m z+h) \tag{1.10}
\end{align*}
This equation (1.10) determines the order parameter \(m\) and is called the equation of state. The magnetization in the absence of the external field \(h=0\), the spontaneous magnetization, is obtained as the solution of (1.10) graphically: as one can see in Fig. 1.3, the existence of a solution with non-vanishing magnetization \(m \neq 0\) is determined by whether the slope of the curve \(\tanh (\beta J m z)\) at \(m=0\) is larger or smaller than unity. The first term of the expansion of the right hand side of (1.10) with \(h=0\) is \(\beta J z m\), so that there exists a solution with \(m \neq 0\) if and only if \(\beta J z>1\). From \(\beta J z=J z / T=1\), the critical temperature is found to be \(T_{\mathrm{c}}=J z\). Figure 1.3 clearly shows that the positive and negative solutions for \(m\) have the same absolute value \(( \pm m)\), corresponding to the change of sign of all spins \(\left(S_{i} \rightarrow-S_{i}, \forall i\right)\). Hereafter we often restrict ourselves to the case of \(m>0\) without loss of generality.
**** Free energy and the Landau theory
:PROPERTIES:
:CUSTOM_ID: free-energy-and-the-landau-theory
:END:
It is possible to calculate the specific heat \(C\), magnetic susceptibility \(\chi\), and other quantities by mean-field theory. We develop an argument starting from the free energy. The general theory of statistical mechanics tells us that the free energy is proportional to the logarithm of the partition function. Using (1.9), we have the mean-field free energy of the Ising model as
\begin{align*}
F=-T \log Z=-N T \log \{2 \cosh \beta(J m z+h)\}+N_{B} J m^{2} \tag{1.11}
\end{align*}
When there is no external field \(h=0\) and the temperature \(T\) is close to the critical point \(T_{\mathrm{c}}\), the magnetization \(m\) is expected to be close to zero. It is then possible to expand the right hand side of (1.11) in powers of \(m\). The expansion to fourth order is
\begin{align*}
F=-N T \log 2+\frac{J z N}{2}(1-\beta J z) m^{2}+\frac{N}{12}(J z m)^{4} \beta^{3} \tag{1.12}
\end{align*}
Fig. 1.4. Free energy as a function of the order parameter
It should be noted that the coefficient of \(m^{2}\) changes sign at \(T_{\mathrm{c}}\). As one can see in Fig. 1.4, the minima of the free energy are located at \(m \neq 0\) when \(T<T_{\mathrm{c}}\) and at \(m=0\) if \(T>T_{\mathrm{c}}\). The statistical-mechanical average of a physical quantity obtained from the Gibbs-Boltzmann distribution (1.2) corresponds to its value at the state that minimizes the free energy (thermal equilibrium state). Thus the magnetization in thermal equilibrium is zero when \(T>T_{\mathrm{c}}\) and is non-vanishing for \(T<T_{\mathrm{c}}\). This conclusion is in agreement with the previous argument using the equation of state. The present theory starting from the Taylor expansion of the free energy by the order parameter is called the Landau theory of phase transitions.
*** Infinite-range model
Mean-field theory is an approximation. However, it gives the exact solution in the case of the infinite-range model where all possible pairs of sites have interactions. The Hamiltonian of the infinite-range model is
\[
H=-\frac{J}{2 N} \sum_{i \neq j} S_{i} S_{j}-h \sum_{i} S_{i} \tag{1.13}
\]
The first sum on the right hand side runs over all pairs of different sites \((i, j)(i=\) \(1, \ldots, N ; j=1, \ldots, N ; i \neq j)\). The factor 2 in the denominator exists so that each pair \((i, j)\) appears only once in the sum, for example \(\left(S_{1} S_{2}+S_{2} S_{1}\right) / 2=S_{1} S_{2}\). The factor \(N\) in the denominator is to make the Hamiltonian (energy) extensive (i.e.\(\mathcal{O}(N)\) ) since the number of terms in the sum is \(N(N-1) / 2\).
The partition function of the infinite-range model can be evaluated as follows. By definition,
\[
Z=\operatorname{Tr} \exp \left(\frac{\beta J}{2 N}\left(\sum_{i} S_{i}\right)^{2}-\frac{\beta J}{2}+\beta h \sum_{i} S_{i}\right) \tag{1.14}
\]
Here the constant term \(-\beta J / 2\) compensates for the contribution \(\sum_{i}\left(S_{i}^{2}\right)\). This term, of \(\mathcal{O}\left(N^{0}=1\right)\), is sufficiently small compared to the other terms, of \(\mathcal{O}(N)\), in the thermodynamic limit \(N \rightarrow \infty\) and will be neglected hereafter. Since we cannot carry out the trace operation with the term \(\left(\sum_{i} S_{i}\right)^{2}\) in the exponent, we decompose this term by the Gaussian integral
\[
\mathrm{e}^{a x^{2} / 2}=\sqrt{\frac{a N}{2 \pi}} \int_{-\infty}^{\infty} \mathrm{d} m \mathrm{e}^{-N a m^{2} / 2+\sqrt{N} a m x} \tag{1.15}
\]
Substituting \(a=\beta J\) and \(x=\sum_{i} S_{i} / \sqrt{N}\) and using (1.9), we find
\begin{align*}
& \operatorname{Tr} \sqrt{\frac{\beta J N}{2 \pi}} \int_{-\infty}^{\infty} \mathrm{d} m \exp \left(-\frac{N \beta J m^{2}}{2}+\beta J m \sum_{i} S_{i}+\beta h \sum_{i} S_{i}\right)  \tag{1.16}\\
& =\sqrt{\frac{\beta J N}{2 \pi}} \int_{-\infty}^{\infty} \mathrm{d} m \exp \left(-\frac{N \beta J m^{2}}{2}+N \log \{2 \cosh \beta(J m+h)\}\right) \tag{1.17}
\end{align*}
The problem has thus been reduced to a simple single integral.
We can evaluate the above integral by steepest descent in the thermodynamic limit \(N \rightarrow \infty\) : the integral (1.17) approaches asymptotically the largest value of its integrand in the thermodynamic limit. The value of the integration variable \(m\) that gives the maximum of the integrand is determined by the saddle-point condition, that is maximization of the exponent:
\[
\frac{\partial}{\partial m}\left(-\frac{\beta J}{2} m^{2}+\log \{2 \cosh \beta(J m+h)\}\right)=0 \tag{1.18}
\]
or
\[
m=\tanh \beta(J m+h) \tag{1.19}
\]
Equation (1.19) agrees with the mean-field equation (1.10) after replacement of \(J\) with \(J / N\) and \(z\) with \(N\). Thus mean-field theory leads to the exact solution for the infinite-range model.
The quantity \(m\) was introduced as an integration variable in the evaluation of the partition function of the infinite-range model. It nevertheless turned out to have a direct physical interpretation, the magnetization, according to the correspondence with mean-field theory through the equation of state (1.19). To understand the significance of this interpretation from a different point of view, we write the saddle-point condition for (1.16) as
\[
m=\frac{1}{N} \sum_{i} S_{i} \tag{1.20}
\]
The sum in (1.20) agrees with the average value \(m\), the magnetization, in the thermodynamic limit \(N \rightarrow \infty\) if the law of large numbers applies. In other words, fluctuations of magnetization vanish in the thermodynamic limit in the infinite-range model and thus mean-field theory gives the exact result.
The infinite-range model may be regarded as a model with nearest neighbour interactions in infinite-dimensional space. To see this, note that the coordination number \(z\) of a site on the \(d\)-dimensional hypercubic lattice is proportional to \(d\). More precisely, \(z=4\) for the two-dimensional square lattice, \(z=6\) for the threedimensional cubic lattice, and \(z=2 d\) in general. Thus a site is connected to very many other sites for large \(d\) so that the relative effects of fluctuations diminish in the limit of large \(d\), leading to the same behaviour as the infinite-range model.
*** Variational approach
:PROPERTIES:
:CUSTOM_ID: variational-approach
:END:
Another point of view is provided for mean-field theory by a variational approach. The source of difficulty in calculations of various physical quantities lies in the non-trivial structure of the probability distribution (1.2) with the Hamiltonian (1.1) where the degrees of freedom \(\boldsymbol{S}\) are coupled with each other. It may thus be useful to employ an approximation to decouple the distribution into simple functions. We therefore introduce a single-site distribution function
\[
P_{i}\left(\sigma_{i}\right)=\operatorname{Tr} P(\boldsymbol{S}) \delta\left(S_{i}, \sigma_{i}\right) \tag{1.21}
\]
and approximate the full distribution by the product of single-site functions:
\[
P(\boldsymbol{S}) \approx \prod_{i} P_{i}\left(S_{i}\right) \tag{1.22}
\]
We determine \(P_{i}\left(S_{i}\right)\) by the general principle of statistical mechanics to minimize the free energy \(F=E-T S\), where the internal energy \(E\) is the expectation value of the Hamiltonian and \(S\) is the entropy (not to be confused with spin). Under the above approximation, one finds
\begin{align*}
& F=\operatorname{Tr}\left\{H(\boldsymbol{S}) \prod_{i} P_{i}\left(S_{i}\right)\right\}+T \operatorname{Tr}\left\{\prod_{i} P_{i}\left(S_{i}\right) \sum_{i} \log P_{i}\left(S_{i}\right)\right\} \\
& =-J \sum_{(i j) \in B} \operatorname{Tr} S_{i} S_{j} P_{i}\left(S_{i}\right) P_{j}\left(S_{j}\right)-h \sum_{i} \operatorname{Tr} S_{i} P_{i}\left(S_{i}\right) \\
& \quad+T \sum_{i} \operatorname{Tr} P_{i}\left(S_{i}\right) \log P_{i}\left(S_{i}\right)
\tag{1.23}
\end{align*}
where we have used the normalization \(\operatorname{Tr} P_{i}\left(S_{i}\right)=1\). Variation of this free energy by \(P_{i}\left(S_{i}\right)\) under the condition of normalization gives
\[
\frac{\delta F}{\delta P_{i}\left(S_{i}\right)}=-J \sum_{j \in I} S_{i} m_{j}-h S_{i}+T \log P_{i}\left(S_{i}\right)+T+\lambda=0 \tag{1.24}
\]
where \(\lambda\) is the Lagrange multiplier for the normalization condition and we have written \(m_{j}\) for \(\operatorname{Tr} S_{j} P_{j}\left(S_{j}\right)\). The set of sites connected to \(i\) has been denoted by \(I\). The minimization condition (1.24) yields the distribution function
\[
P_{i}\left(S_{i}\right)=\frac{\exp \left(\beta J \sum_{j \in I} S_{i} m_{j}+\beta h S_{i}\right)}{Z_{\mathrm{MF}}} \tag{1.25}
\]
where \(Z_{\mathrm{MF}}\) is the normalization factor. In the case of uniform magnetization \(m_{j}(=m)\), this result (1.25) together with the decoupling (1.22) leads to the distribution \(P(\boldsymbol{S}) \propto \mathrm{e}^{-\beta H}\) with \(H\) identical to the mean-field Hamiltonian (1.8) up to a trivial additive constant.
The argument so far has been general in that it did not use the values of the Ising spins \(S_{i}= \pm 1\) and thus applies to any other cases. It is instructive to use the values of the Ising spins explicitly and see its consequence. Since \(S_{i}\) takes only two values \(\pm 1\), the following is the general form of the distribution function:
\[
P_{i}\left(S_{i}\right)=\frac{1+m_{i} S_{i}}{2} \tag{1.26}
\]
which is compatible with the previous notation \(m_{i}=\operatorname{Tr} S_{i} P_{i}\left(S_{i}\right)\). Substitution of (1.26) into (1.23) yields
\begin{align*}
F= & -J \sum_{(i j) \in B} m_{i} m_{j}-h \sum_{i} m_{i} \\
& +T \sum_{i}\left(\frac{1+m_{i}}{2} \log \frac{1+m_{i}}{2}+\frac{1-m_{i}}{2} \log \frac{1-m_{i}}{2}\right)
\tag{1.27}
\end{align*}
Variation of this expression with respect to \(m_{i}\) leads to
\[
m_{i}=\tanh \beta\left(J \sum_{j \in I} m_{j}+h\right) \tag{1.28}
\]
which is identical to (1.10) for uniform magnetization \(\left(m_{i}=m, \forall i\right)\). We have again rederived the previous result of mean-field theory.
* E0 270: Machine Learning
** Fundamentals
*** Probability theory
*** Model selection
*** Curse of dimensionality
*** Decision theory
*** Information theory
** Probability distributions
:LOGBOOK:
CLOCK: [2024-06-12 Wed 12:00]--[2024-06-12 Wed 17:11] =>  5:11
CLOCK: [2024-06-12 Wed 10:07]--[2024-06-12 Wed 11:51] =>  1:44
:END:
*** Binary variables
**** The beta distribution
*** Multinomial variables
**** The Dirichlet distribution
*** The Gaussian distribution
#+NAME: Symmetric matrix
#+begin_definition latex
A square matrix \( A = (a_{ij})_{i=1,\ldots,n}^{j=1,\ldots,n} \) is a symmetric matrix if for every \( (i,j) \) we have \(a_{ij} = a_{ji}  \).
#+end_definition
#+NAME: Transpose of symmetric matrix
#+begin_corollary latex
If \( A \) is is a symmetric matrix, then \( A^{\mathrm{T}} = A \).
#+end_corollary
#+NAME: Multivariate Gaussian distribution
#+begin_defination
The multivariate Gaussian distribution takes the form
\begin{align*}
\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu},\, \boldsymbol{\Sigma})=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\boldsymbol{\Sigma}|^{1 / 2}} \exp \left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}
\end{align*}
where \(\boldsymbol{\mu}\) is a \(D\)-dimensional mean vector, \(\boldsymbol{\Sigma}\) is a \(D \times D\) covariance (symmetric) matrix, and \(|\boldsymbol{\Sigma}|\) denotes the determinant of \(\boldsymbol{\Sigma}\).
#+end_defination
**** Conditional Gaussian distributions
If two sets of variables are jointly Gaussian, then the conditional distribution of one set conditioned on the other is again Gaussian.
#+NAME: Precision matrix
#+begin_definition
Suppose \(\mathbf{x}\) is a \(D\)-dimensional vector with Gaussian distribution \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\). The precision matrix is defined as the inverse of the covariance matrix \(\boldsymbol{\Sigma}\)
\begin{align*}
\Lambda \equiv \mathbf{\Sigma}^{-1}.
\end{align*}
#+end_definition
#+NAME: Partitioned covariance matrix
#+begin_lemma
Let \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu},\, \boldsymbol{\Sigma})\) be a joint Gaussian distribution with mean \(\boldsymbol{\mu}\) and covariance matrix \(\boldsymbol{\Sigma}\) so that
\begin{align*}
\mathbf{x}=\binom{\mathbf{x}_a}{\mathbf{x}_b}, \quad \boldsymbol{\mu} = 
\binom{\boldsymbol{\mu}_a}{\boldsymbol{\mu}_b}, \quad
\boldsymbol{\Sigma} =
\left(\begin{array}{ll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right).
\end{align*}
We have
\[
\(\boldsymbol{\Sigma}^{\mathrm{T}}=\boldsymbol{\Sigma}\) \Longrightarrow \boldsymbol{\Sigma}_{aa} = \boldsymbol{\Sigma}_{bb}.
\]
#+end_lemma
#+NAME: Partitioned covariance matrix
#+begin_lemma
Let \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu},\, \boldsymbol{\Sigma})\) be a joint Gaussian distribution with mean \(\boldsymbol{\mu}\) and covariance matrix \(\boldsymbol{\Sigma}\) so that
\begin{align*}
\mathbf{x}=\binom{\mathbf{x}_a}{\mathbf{x}_b}, \quad \boldsymbol{\mu} = 
\binom{\boldsymbol{\mu}_a}{\boldsymbol{\mu}_b}, \quad
\boldsymbol{\Sigma} =
\left(\begin{array}{ll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right).
\end{align*}
We have
\[
\(\boldsymbol{\Sigma}^{\mathrm{T}}=\boldsymbol{\Sigma}\) \Longrightarrow \boldsymbol{\Sigma}_{ba} = \boldsymbol{\Sigma}_{ab}^{\top}.
\]
#+end_lemma
#+NAME: Partitioned precision matrix
#+begin_lemma
Let \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu},\, \boldsymbol{\Sigma})\) be a joint Gaussian distribution with mean \(\boldsymbol{\mu}\) and covariance matrix \(\boldsymbol{\Sigma}\) so that
\begin{align*}
\mathbf{x}=\binom{\mathbf{x}_a}{\mathbf{x}_b}, \quad \boldsymbol{\mu} = 
\binom{\boldsymbol{\mu}_a}{\boldsymbol{\mu}_b}, \quad
\boldsymbol{\Sigma} =
\left(\begin{array}{ll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right), \quad
\boldsymbol{\Lambda} =
\left(\begin{array}{ll}
\boldsymbol{\Lambda}_{a a} & \boldsymbol{\Lambda}_{a b} \\
\boldsymbol{\Lambda}_{b a} & \boldsymbol{\Lambda}_{b b}
\end{array}\right) = \boldsymbol{\Sigma}^{-1}.
\end{align*}
Using the corollary on the transpose of a symmetric matrix, we have \(\boldsymbol{\Sigma}^{\mathrm{T}}=\boldsymbol{\Sigma} \Longrightarrow \boldsymbol{\Lambda}^{\mathrm{T}}=\boldsymbol{\Lambda}\). In analogy with the lemmas Partitioned precision matrix, we have \(\boldsymbol{\Lambda}_{aa} = \boldsymbol{\Lambda}_{bb}\) and \(\boldsymbol{\Lambda}_{ba} = \boldsymbol{\Lambda}_{ab}^{\top}\).
#+end_lemma
#+NAME: Matrix inversion formula
#+begin_lemma
The inverse of a partitioned matrix
\[
\left(\begin{array}{ll}
\mathbf{A} & \mathbf{B} \\
\mathbf{C} & \mathbf{D}
\end{array}\right)
\]
is given by the matrix inversion formula
\begin{align*}
\left(\begin{array}{ll}
\mathbf{A} & \mathbf{B} \\
\mathbf{C} & \mathbf{D}
\end{array}\right)^{-1}=\left(\begin{array}{cc}
\mathbf{M} & -\mathbf{M B D}^{-1} \\
-\mathbf{D}^{-1} \mathbf{C M} & \mathbf{D}^{-1}+\mathbf{D}^{-1} \mathbf{C M B D}^{-1}
\end{array}\right)
\end{align*}
where \(\mathbf{M} \equiv \left(\mathbf{A}-\mathbf{B D}^{-1} \mathbf{C}\right)^{-1}\). The quantity \(\mathrm{M}^{-1}\) is known as the Schur complement of the matrix
\[
\left(\begin{array}{ll}
\mathbf{A} & \mathbf{B} \\
\mathbf{C} & \mathbf{D}
\end{array}\right)^{-1}
\]
#+end_lemma
#+NAME: Completing the square
#+begin_lemma
Consider the general quadratic form
\[
-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}).
\]
Suppose that it defines the exponent terms in a Gaussian distribution \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\). Completing the square refers to the following rewrite for this quadratic form
\[
-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})=-\frac{1}{2} \mathbf{x}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{x}+\mathbf{x}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}+\text { const }
\]
where \(\text{const}\) denotes terms independent of \(\mathbf{x}\). We have used the fact that the covariance matrix is symmetric: \(\boldsymbol{\Sigma}^{\top} = \boldsymbol{\Sigma}\). and we need to determine the corresponding mean and covariance. From the right hand side of the above rewrite, it is clear that
1) the coefficient of the second order term in \(\mathrm{x}\) is equal to the inverse covariance matrix \(\boldsymbol{\Sigma}^{-1}\), from which we can obtain \(\operatorname{cov} [\mathbf{x}] \equiv \boldsymbol{\Sigma}\) using the matrix inversion formula,
2) the coefficient of the linear term in \(\mathbf{x}\) can be equated to \(\boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}\), from which we can obtain \(\mathbb{E} [\mathbf{x}] \equiv \mu\) using the matrix inversion formula.
#+end_lemma
#+NAME: Completing the square
#+begin_lemma
Let \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu},\, \boldsymbol{\Sigma})\) be a joint Gaussian distribution with mean \(\boldsymbol{\mu}\) and covariance matrix \(\boldsymbol{\Sigma}\) so that
\begin{align*}
\mathbf{x}=\binom{\mathbf{x}_a}{\mathbf{x}_b}, \quad \boldsymbol{\mu} = 
\binom{\boldsymbol{\mu}_a}{\boldsymbol{\mu}_b}, \quad
\boldsymbol{\Sigma} =
\left(\begin{array}{ll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right).
\end{align*}
Symmetry \(\boldsymbol{\Sigma}^{\mathrm{T}}=\boldsymbol{\Sigma}\) of the covariance matrix implies that \(\boldsymbol{\Sigma} a a\) and \(\boldsymbol{\Sigma}_{b b}\) are symmetric, while \(\boldsymbol{\Sigma}_{b a}=\boldsymbol{\Sigma}_{a b}^{\mathrm{T}}\).
#+end_lemma
#+NAME: Conditional distribution of a partitioned Gaussian
#+begin_theorem
Let \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu},\, \boldsymbol{\Sigma})\) be a joint Gaussian distribution with \(\boldsymbol{\Lambda} \equiv \boldsymbol{\Sigma}^{-1}\) so that
\begin{align*}
\mathbf{x}=\binom{\mathbf{x}_a}{\mathbf{x}_b}, \quad \boldsymbol{\mu} = 
\binom{\boldsymbol{\mu}_a}{\boldsymbol{\mu}_b}, \quad
\boldsymbol{\Sigma} =
\left(\begin{array}{ll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right), \quad
\boldsymbol{\Lambda} =
\left(\begin{array}{ll}
\boldsymbol{\Lambda}_{a a} & \boldsymbol{\Lambda}_{a b} \\
\boldsymbol{\Lambda}_{b a} & \boldsymbol{\Lambda}_{b b}
\end{array}\right).
\end{align*}
The conditional distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\) is given by
\begin{align*}
p\left(\mathbf{x}_a \mid \mathbf{x}_b\right) & =\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_{a \mid b},\, \boldsymbol{\Lambda}_{a a}^{-1}), \quad \boldsymbol{\mu}_{a \mid b} = \mu_a-\boldsymbol{\Lambda}_{a a}^{-1} \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right).
\end{align*}
#+end_theorem
#+NAME: Conditional distribution of a partitioned Gaussian
#+begin_proof
Suppose \(\mathbf{x}\) is a \(D\)-dimensional vector with Gaussian distribution \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\) and that we partition \(\mathbf{x}\) into two disjoint subsets \(\mathbf{x}_a\) and \(\mathbf{x}_b\). Without loss of generality, we can take \(\mathbf{x}_a\) to form the first \(M\) components of \(\mathbf{x}\), with \(\mathbf{x}_b\) comprising the remaining \(D-M\) components, so that
\begin{align*}
\mathbf{x}=\binom{\mathbf{x}_a}{\mathbf{x}_b}. \tag{1}
\end{align*}
We also define partitions of the mean vector \(\boldsymbol{\mu}\)
\begin{align*}
\boldsymbol{\mu} = \binom{\boldsymbol{\mu}_a}{\boldsymbol{\mu}_b}, \tag{2}
\end{align*}
partitions of the covariance matrix \(\boldsymbol{\Sigma}\)
\begin{align*}
\boldsymbol{\Sigma}=\left(\begin{array}{ll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right), \tag{3}
\end{align*}
and the partitions of the precision matrix
\begin{align*}
\boldsymbol{\Lambda}=\left(\begin{array}{ll}
\boldsymbol{\Lambda}_{a a} & \boldsymbol{\Lambda}_{a b} \\
\boldsymbol{\Lambda}_{b a} & \boldsymbol{\Lambda}_{b b}
\end{array}\right) = \left(\begin{array}{ll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right)^{\top}. \tag{4}
\end{align*}
It should be stressed at this point that, for instance, \(\boldsymbol{\Lambda}_{a a}\) is not simply given by the inverse of \(\boldsymbol{\Sigma}_{a a}\).
Step 1: the conditional distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\)
Using of the partitioning (1), (2), and (3), we obtain
\begin{align*}
-\frac{1}{2}(\mathbf{x} & -\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})= \\
& -\frac{1}{2}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{a a}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right)-\frac{1}{2}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right) \\
& -\frac{1}{2}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right)-\frac{1}{2}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right). \tag{5}
\end{align*}
With \(\mathbf{x}_b\) fixed, (5) is a quadratic form in \(\mathbf{x}_a\). It follows that the corresponding conditional distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\) is a Gaussian. Because this distribution is completely characterized by its mean and its covariance, our goal will be to identify expressions for the mean and covariance of \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\) by inspection of (5).
Step 2: covariance \(\boldsymbol{\Sigma}_{a \mid b}\) of the conditional distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\)
Let \(\boldsymbol{\Sigma}_{a \mid b}\) denote the covariance of the conditional Gaussian distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\). Consider the second order in \(\mathbf{x}_a\) on the right hand side of (5)
\begin{align*}
-\frac{1}{2} \mathbf{x}_a^{\mathrm{T}} \boldsymbol{\Lambda}_{a a} \mathbf{x}_a
\end{align*}
from which we can immediately conclude that the covariance (inverse precision) of \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\) is given by
\begin{align*}
\boldsymbol{\Sigma}_{a \mid b}=\boldsymbol{\Lambda}_{a a}^{-1}. \tag{6a}
\end{align*}
We have used the method of completing the square for the conditional Gaussian distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\) for which the quadratic form in the exponent is given by (5). Note that the covariance of the conditional distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\), given by (6a), is independent of \(\mathbf{x}_a\).
Step 3: mean \(\boldsymbol{\mu}_{a \mid b}\) of the conditional distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\)
Let \(\boldsymbol{\mu}_{a \mid b}\) denote the mean of the conditional Gaussian distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\). Consider the first order terms in \(\mathbf{x}_{a}\) on the right hand side of (5)
\begin{align*}
\mathbf{x}_a^{\mathrm{T}}\left\{\boldsymbol{\Lambda}_{a a} \boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right)\right\}.
\end{align*}
from which we can immediately conclude that the mean of \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\) is given by
\begin{align*}
\boldsymbol{\mu}_{a \mid b} & =\boldsymbol{\Sigma}_{a \mid b}\left\{\boldsymbol{\Lambda}_{a a} \boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right)\right\} \\
& =\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{a a}^{-1} \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right). \tag{7a}
\end{align*}
We have used the method of completing the square for the conditional Gaussian distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\) for which the quadratic form in the exponent is given by (5). We have also used (6a). Note that the mean of the conditional distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\), given by (7a), is a linear function of \(\mathbf{x}_b\).
#+end_proof
#+NAME: Conditional distribution of a partitioned Gaussian
#+begin_corollary
The results (6a) and (7a) are expressed in terms of the partitioned precision matrix of the original joint distribution \(p\left(\mathbf{x}_a,\,\mathbf{x}_b\right)\). We can obtain analogous results in terms of the partitioned covariance matrix of the original joint distribution \(p\left(\mathbf{x}_a,\,\mathbf{x}_b\right)\). Using the definition of the partitioned precision matrix and the matrix inversion formula we have
\[
\boldsymbol{\Lambda}_{a a}=\left(\boldsymbol{\Sigma}_{a a}-\boldsymbol{\Sigma}_{a b} \boldsymbol{\Sigma}_{b b}^{-1} \boldsymbol{\Sigma}_{b a}\right)^{-1}, \quad 
\boldsymbol{\Lambda}_{a b}=-\left(\boldsymbol{\Sigma}_{a a}-\boldsymbol{\Sigma}_{a b} \boldsymbol{\Sigma}_{b b}^{-1} \boldsymbol{\Sigma}_{b a}\right)^{-1} \boldsymbol{\Sigma}_{a b} \boldsymbol{\Sigma}_{b b}^{-1}. \tag{8}
\]
Using (8), we obtain the following expressions for the mean and covariance of the conditional distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\)
\begin{align*}
\boldsymbol{\Sigma}_{a \mid b}=\boldsymbol{\Sigma}_{a a}-\boldsymbol{\Sigma}_{a b} \boldsymbol{\Sigma}_{b b}^{-1} \boldsymbol{\Sigma}_{b a}, \tag{6b}
\end{align*}
\begin{align*}
\boldsymbol{\mu}_{a \mid b}=\boldsymbol{\mu}_a+\boldsymbol{\Sigma}_{a b} \boldsymbol{\Sigma}_{b b}^{-1}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right). \tag{7b}
\end{align*}
Comparing (6a) and (6b), we see that the conditional distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\) takes a simpler form when expressed in terms of the partitioned precision matrix (6a) as opposed to when it is expressed in terms of the partitioned covariance matrix (6b).
#+end_corollary
**** Marginal Gaussian distributions
If two sets of variables are jointly Gaussian, the marginal distribution of either set is also Gaussian.
#+NAME: Marginal distribution of a partitioned Gaussian
#+begin_theorem
Let \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu},\, \boldsymbol{\Sigma})\) be a joint Gaussian distribution with \(\boldsymbol{\Lambda} \equiv \boldsymbol{\Sigma}^{-1}\) so that
\begin{align*}
\mathbf{x}=\binom{\mathbf{x}_a}{\mathbf{x}_b}, \quad \boldsymbol{\mu} = 
\binom{\boldsymbol{\mu}_a}{\boldsymbol{\mu}_b}, \quad
\boldsymbol{\Sigma} =
\left(\begin{array}{ll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right), \quad
\boldsymbol{\Lambda} =
\left(\begin{array}{ll}
\boldsymbol{\Lambda}_{a a} & \boldsymbol{\Lambda}_{a b} \\
\boldsymbol{\Lambda}_{b a} & \boldsymbol{\Lambda}_{b b}
\end{array}\right).
\end{align*}
The marginal distribution \(p\left(\mathbf{x}_a\right)\) is given by
\begin{align*}
p\left(\mathbf{x}_a\right)=\mathcal{N}\left(\mathbf{x}_a \mid \boldsymbol{\mu}_a,\,\boldsymbol{\Sigma}_{a a}\right).
\end{align*}
#+end_theorem
#+NAME: Marginal distribution of a partitioned Gaussian
#+begin_proof
Suppose \(\mathbf{x}\) is a \(D\)-dimensional vector with Gaussian distribution \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\) and that we partition \(\mathbf{x}\) into two disjoint subsets \(\mathbf{x}_a\) and \(\mathbf{x}_b\). Without loss of generality, we can take \(\mathbf{x}_a\) to form the first \(M\) components of \(\mathbf{x}\), with \(\mathbf{x}_b\) comprising the remaining \(D-M\) components, so that
\begin{align*}
\mathbf{x}=\binom{\mathbf{x}_a}{\mathbf{x}_b}. \tag{1}
\end{align*}
We also define partitions of the mean vector \(\boldsymbol{\mu}\)
\begin{align*}
\boldsymbol{\mu} = \binom{\boldsymbol{\mu}_a}{\boldsymbol{\mu}_b}, \tag{2}
\end{align*}
partitions of the covariance matrix \(\boldsymbol{\Sigma}\)
\begin{align*}
\boldsymbol{\Sigma}=\left(\begin{array}{ll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right), \tag{3}
\end{align*}
and the partitions of the precision matrix
\begin{align*}
\boldsymbol{\Lambda}=\left(\begin{array}{ll}
\boldsymbol{\Lambda}_{a a} & \boldsymbol{\Lambda}_{a b} \\
\boldsymbol{\Lambda}_{b a} & \boldsymbol{\Lambda}_{b b}
\end{array}\right) = \left(\begin{array}{ll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b}
\end{array}\right)^{\top}. \tag{4}
\end{align*}
It should be stressed at this point that, for instance, \(\boldsymbol{\Lambda}_{a a}\) is not simply given by the inverse of \(\boldsymbol{\Sigma}_{a a}\).
Step 1: the marginal distribution \(p(\mathbf{x}_a)\)
By definition, the marginal distribution \(p(\mathbf{x}_{a})\) is
\[
p\left(\mathbf{x}_a\right)=\int p\left(\mathbf{x}_a, \mathbf{x}_b\right) \mathrm{d} \mathbf{x}_b.
\]
Using of the partitioning (1), (2), and (3), we obtain
\begin{align*}
-\frac{1}{2}(\mathbf{x} & -\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})= \\
& -\frac{1}{2}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{a a}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right)-\frac{1}{2}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{a b}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right) \\
& -\frac{1}{2}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right)-\frac{1}{2}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_b-\boldsymbol{\mu}_b\right). \tag{5}
\end{align*}
Step 2: marginalizing over  \(\mathbf{x}_b\)
In order to marginalize \(\mathbf{x}_b\), first consider the terms in (5) that involve  \(\mathbf{x}_b\)
\begin{align*}
-\frac{1}{2}\left(\mathbf{x}_b-\boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_b-\boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\right) = -\frac{1}{2} \mathbf{x}_b^{\mathrm{T}} \boldsymbol{\Lambda}_{b b} \mathbf{x}_b+\mathbf{x}_b^T \mathbf{m} + \text{ const} 
\end{align*}
where
\begin{align*}
\mathbf{m}=\boldsymbol{\Lambda}_{b b} \boldsymbol{\mu}_b-\boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right).
\end{align*}
We have used the method of completing the square with respect to \(\mathbf{x}_b\) to cast the \(\mathbf{x}_b\) dependent terms on the right hand side of (5) into the standard quadratic form of a Gaussian distribution. The integral \(\int p\left(\mathbf{x}_a, \mathbf{x}_b\right) \mathrm{d} \mathbf{x}_b\) over \(\mathbf{x}_b\) will take the form
\begin{align*}
\int \exp \left\{-\frac{1}{2}\left(\mathbf{x}_b-\boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_b-\boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\right)\right\} \mathrm{d} \mathbf{x}_b. \tag{6}
\end{align*}
The integral in (6) is easily performed by noting that it is the integral over an unnormalized Gaussian, and so the result will be the reciprocal of the normalization coefficient.
Step 3: completing the square
Combine
\begin{align*}
-\frac{1}{2}\left(\mathbf{x}_b-\boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\right)^{\mathrm{T}} \boldsymbol{\Lambda}_{b b}\left(\mathbf{x}_b-\boldsymbol{\Lambda}_{b b}^{-1} \mathbf{m}\right) = -\frac{1}{2} \mathbf{x}_b^{\mathrm{T}} \boldsymbol{\Lambda}_{b b} \mathbf{x}_b+\mathbf{x}_b^T \mathbf{m} + \text{ const} 
\end{align*}
with the remaining terms from (5) that depend on \(\mathbf{x}_a\) to obtain
\begin{align*}
\frac{1}{2} &\left[\boldsymbol{\Lambda}_{b b} \boldsymbol{\mu}_b-\boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right)\right]^{\mathrm{T}} \boldsymbol{\Lambda}_{b b}^{-1}\left[\boldsymbol{\Lambda}_{b b} \boldsymbol{\mu}_b-\boldsymbol{\Lambda}_{b a}\left(\mathbf{x}_a-\boldsymbol{\mu}_a\right)\right] \\
&-\frac{1}{2} \mathbf{x}_a^{\mathrm{T}} \boldsymbol{\Lambda}_{a a} \mathbf{x}_a+\mathbf{x}_a^{\mathrm{T}}\left(\boldsymbol{\Lambda}_{a a} \boldsymbol{\mu}_a+\boldsymbol{\Lambda}_{a b} \boldsymbol{\mu}_b\right)+\text { const } \\
&\qquad = -\frac{1}{2} \mathbf{x}_a^{\mathrm{T}}\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right) \mathbf{x}_a + \mathbf{x}_a^{\mathrm{T}}\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right)^{-1} \boldsymbol{\mu}_a+\text { const }
\end{align*}
where \(\text{ const }\) denotes quantities independent of \(\mathbf{x}_\alpha\). Again, we have used the method of completing the square.
Step 4: covariance \(\boldsymbol{\Sigma}_{a}\) of the marginal distribution \(p(\mathbf{x}_a)\)
The covariance of the marginal distribution of \(p\left(\mathbf{x}_a\right)\) is given by
\begin{align*}
\boldsymbol{\Sigma}_a=\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right)^{-1}. \tag{7a}
\end{align*}
Step 5: mean \(\boldsymbol{\mu}_{a}\) of the marginal distribution \(p(\mathbf{x}_a)\)
The mean  \(\boldsymbol{\mu}_{a}\) of the marginal distribution of \(p\left(\mathbf{x}_a\right)\) is given by
\begin{align*}
\boldsymbol{\mu}_a = \boldsymbol{\Sigma}_a\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right) \boldsymbol{\mu}_a. \tag{8a}
\end{align*}
where we have used (7a).
Step 6: mean \(\boldsymbol{\mu}_{a}\) and covariance \(\boldsymbol{\Sigma}_{a}\) of the marginal distribution \(p(\mathbf{x}_a)\) in terms of the partitioned covariance matrix
The results (7a) and (8a) are expressed in terms of the partitioned precision matrix of the original joint distribution \(p\left(\mathbf{x}_a,\,\mathbf{x}_b\right)\). We can obtain analogous results in terms of the partitioned covariance matrix of the original joint distribution \(p\left(\mathbf{x}_a,\,\mathbf{x}_b\right)\). Using the definition of the partitioned precision matrix and the matrix inversion formula we have
\begin{align*}
\left(\boldsymbol{\Lambda}_{a a}-\boldsymbol{\Lambda}_{a b} \boldsymbol{\Lambda}_{b b}^{-1} \boldsymbol{\Lambda}_{b a}\right)^{-1}=\boldsymbol{\Sigma}_{a a}. \tag{9}
\end{align*}
Using (9), we obtain the following expressions for the covariance and mean of the marginal distribution \(p (\mathbf{x}_a)\)
\begin{align*}
\operatorname{cov} [\mathbf{x}_{a}] = \boldsymbol{\Sigma}_{aa}, \tag{7b}
\end{align*}
\begin{align*}
\mathrm{E} [\mathbf{x}_{a}] = \boldsymbol{\mu}_{a}\right). \tag{8b}
\end{align*}
Comparing (7a) and (7b), we see that the marginal distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\) takes a simpler form when expressed in terms of the partitioned covariance matrix (7b) as opposed to when it is expressed in terms of the partitioned precesion matrix (7a). This is in contrast with the conditional distribution \(p\left(\mathbf{x}_a \mid \mathbf{x}_b\right)\) which takes a simpler form when expressed in terms of the partitioned precision matrix.
#+end_proof
**** Bayes's theorem for Gaussian variables
#+NAME: Bayes's theorem for Gaussian variables
#+begin_theorem
Let \(p(\mathbf{x})\) and \(p(\mathbf{y} \mid \mathbf{x})\) denote a Gaussian marginal distribution and a Gaussian conditional distribution respectively. Suppose that the mean of \(p(\mathbf{y} \mid \mathbf{x})\) is a linear function of \(\mathbf{x}\). Further suppose that the covariance of \(p(\mathbf{y} \mid \mathbf{x})\) is independent of \(\mathbf{x}\). Assume the following parametrization of the marginal \(\mathbf{x}\) and the conditional \(p(\mathbf{y} \mid \mathbf{x})\) that is consistent with the two assumptions above:
\begin{align*}
p(\mathbf{x}) &= \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}^{-1}\right) \\
p(\mathbf{y} \mid \mathbf{x}) &= \mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \mathbf{x}+\mathbf{b}, \mathbf{L}^{-1}\right)
\end{align*}
where \(\boldsymbol{\mu}\), \(\mathbf{A}\), and \(\mathbf{b}\) are parameters governing the means, and \(\boldsymbol{\Lambda}\) and \(\mathbf{L}\) are precision matrices. 
The marginal distribution \(p(\mathbf{y})\) and the conditional distribution \(p(\mathbf{x} \mid \mathbf{y})\) are given by
\begin{align*}
p(\mathbf{y}) &= \mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \\
p(\mathbf{x} \mid \mathbf{y}) &= \mathcal{N}\left(\mathbf{x} \mid \mathbf{\Sigma}\left\{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\boldsymbol{\Lambda} \boldsymbol{\mu}\right\}, \mathbf{\Sigma}\right)
\end{align*}
where
\[
\boldsymbol{\Sigma}=\left(\boldsymbol{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{A}\right)^{-1}.
\]
#+end_theorem
#+NAME: Bayes's theorem for Gaussian variables
#+begin_proof
Without loss of generality, suppose \(\mathbf{x}\) has dimensions \(M\) and \(\mathbf{y}\) has dimensions \(D\). Then the matrix \(\mathbf{A}\) has dimensions \(D \times M\). The proof will be presented in three stages:
Step 1: The joint distribution \(p(\mathbf{x},\,\mathbf{y})\)
With \(\mathrm{z}= \begin{pmatrix} \mathrm{x} \\ \mathrm{y} \end{pmatrix}\) we have \(p(\mathrm{z}) \propto p(y \mid x) \, p(x)\) so that
\begin{align*}
\ln p(\mathbf{z})= & \ln p(\mathbf{x})+\ln p(\mathbf{y} \mid \mathbf{x}) \\
= & -\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Lambda}(\mathbf{x}-\boldsymbol{\mu}) \\
& -\frac{1}{2}(\mathbf{y}-\mathbf{A} \mathbf{x}-\mathbf{b})^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{A} \mathbf{x}-\mathbf{b})+\text { const } \tag{1}
\end{align*}
where \(\text{const}\) denotes terms independent of \(\mathbf{z}\). Since \(\ln p(\mathbf{z})\) is a quadratic function of the components of \(\mathbf{z}\), it follows that \(p(\mathbf{z})\) is a Gaussian distribution.
Step 1.1: The covariance \(\operatorname{cov} [\mathbf{z}]\) of the joint distribution \(p(\mathbf{x},\,\mathbf{y})\)
Consider the following rewrite for the second order terms in (1)
\begin{align*}
- \frac{1}{2}(\mathbf{y}-\mathbf{A} \mathbf{x}-\mathbf{b})^{\mathrm{T}} & \mathbf{L}(\mathbf{y}-\mathbf{A} \mathbf{x}-\mathbf{b}) \\
&= - \frac{1}{2} \mathbf{x}^{\mathrm{T}}\left(\boldsymbol{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{A}\right) \mathbf{x}-\frac{1}{2} \mathbf{y}^{\mathrm{T}} \mathbf{L} \mathbf{y}+\frac{1}{2} \mathbf{y}^{\mathrm{T}} \mathbf{L} \mathbf{A} \mathbf{x}+\frac{1}{2} \mathbf{x}^{\mathrm{T}} \mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{y} \\
& =-\frac{1}{2} \binom{\mathbf{x}}{\mathbf{y}}^{\mathrm{T}}\left(\begin{array}{cc}
\boldsymbol{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{A} & -\mathbf{A}^{\mathrm{T}} \mathbf{L} \\
-\mathbf{L} \mathbf{A} & \mathbf{L}
\end{array}\right)\binom{\mathbf{x}}{\mathbf{y}}=-\frac{1}{2} \mathbf{z}^{\mathrm{T}} \mathbf{R z}
\end{align*}
and so the Gaussian distribution over \(\mathbf{z}\) has precision matrix given by
\begin{align*}
\mathbf{R}=\left(\begin{array}{cc}
\boldsymbol{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{A} & -\mathbf{A}^{\mathrm{T}} \mathbf{L} \\
-\mathbf{L} \mathbf{A} & \mathbf{L}
\end{array}\right)
\end{align*}
The covariance matrix is obtained from the precision matrix by using the matrix inversion formula
\begin{align*}
\operatorname{cov}[\mathbf{z}]=\mathbf{R}^{-1}=\left(\begin{array}{cc}
\mathbf{\Lambda}^{-1} & \boldsymbol{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \\
\mathbf{A} \boldsymbol{\Lambda}^{-1} & \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}
\end{array}\right). \tag{2}
\end{align*}
Step 1.2: The mean \(\mathbb{E} [\mathbf{z}]\) of the joint distribution \(p(\mathbf{x},\,\mathbf{y})\)
Consider the following rewrite for the first order terms in (1)
\begin{align*}
-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} &\boldsymbol{\Lambda}(\mathbf{x}-\boldsymbol{\mu}) \\
&= \mathbf{x}^{\mathrm{T}} \boldsymbol{\Lambda} \boldsymbol{\mu}-\mathbf{x}^{\mathrm{T}} \mathbf{A}^{\mathrm{T}} \mathbf{L b}+\mathbf{y}^{\mathrm{T}} \mathbf{L b} \\
&= \binom{\mathbf{x}}{\mathbf{y}}^{\mathrm{T}}\binom{\boldsymbol{\Lambda} \boldsymbol{\mu}-\mathbf{A}^{\mathrm{T}} \mathbf{L b}}{\mathbf{L b}}.
\end{align*}
By completing the square we obtain
\begin{align*}
\mathbb{E}[\mathbf{z}]=\mathbf{R}^{-1}\binom{\Lambda \mu-\mathbf{A}^{\mathrm{T}} \mathbf{L b}}{\mathbf{L b}} .
\end{align*}
Using (2), we then obtain
\begin{align*}
\mathbb{E}[\mathbf{z}]=\binom{\mu}{\mathbf{A} \mu+\mathbf{b}}. \tag{3}
\end{align*}
Step 2: The conditional distribution \(p(\mathbf{x} \mid \mathbf{y})\)
Applying the result (6a) and (7a) from the theorem on the conditional distribution of a partitioned Gaussian to (2) and (3), the mean and covariance of the conditional distribution \(p(\mathbf{x} \mid \mathbf{y})\) are given by
\begin{align*}
\mathbb{E}[\mathbf{x} \mid \mathbf{y}] = \boldsymbol{\Sigma} 
\{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\boldsymbol{\Lambda} \boldsymbol{\mu}\}, \quad
\operatorname{cov}[\mathbf{x} \mid \mathbf{y}] = \boldsymbol{\Sigma}, \quad
\boldsymbol{\Sigma}=\left(\boldsymbol{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{A}\right)^{-1}. \tag{4}
\end{align*}
Step 3: The marginal distribution \(p(\mathbf{y})\)
Applying the result (7b) and (8b) from the theorem on the marginal distribution of a partitioned Gaussian to (2) and (3), the mean and covariance of the marginal distribution \(p(\mathbf{y})\) are given by
\begin{align*}
\mathbb{E}[\mathbf{y}] =\mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \quad
\operatorname{cov}[\mathbf{y}] = \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}. \tag{5}
\end{align*}
#+end_proof
#+NAME: Bayes's theorem for Gaussian variables
#+begin_remark latex
The evaluation of the conditional \(p(\mathbf{x} \mid \mathbf{y})\) can interpreted as an application of Bayes's theorem: We can interpret the distribution \(p(\mathbf{x})\) as a prior distribution over \(\mathbf{x}\). If the variable \(\mathbf{y}\) is observed, then the conditional distribution \(p(\mathbf{x} \mid \mathbf{y})\) represents the corresponding posterior distribution over \(\mathbf{x}\). Having found the marginal and conditional distributions, we effectively expressed the joint distribution \(p(\mathbf{z})=p(\mathbf{x}) p(\mathbf{y} \mid \mathbf{x})\) in the form \(p(\mathbf{x} \mid \mathbf{y}) p(\mathbf{y})\).
#+end_remark
#+NAME: Convolution of two Gaussians
#+begin_corollary latex
A special case of result (5) from the theorem on Bayes's theorem for Gaussian variables concerning the marginal distribution \(p(\mathbf{y})\) is when \(\mathbf{A}=\mathbf{I}\). In this case
\begin{align*}
\mathbb{E}[\mathbf{y}] = \boldsymbol{\mu}+\mathbf{b}, \quad
\operatorname{cov}[\mathbf{y}] = \mathbf{L}^{-1} + \mathbf{\Lambda}^{-1},
\end{align*}
i.e., it reduces to the convolution of two Gaussians, for which we see that the mean of the convolution is the sum of the mean of the two Gaussians, and the covariance of the convolution is the sum of their covariances.
#+end_corollary
**** Maximal likelihood for the Gaussian
**** Sequential estimation
**** Bayesian inference for the Gaussian
**** Student's t-distribution
**** Periodic variables
**** Mixtures of Gaussians
*** The exponential family
#+NAME: Exponential family of distributions
#+begin_definition latex
The exponential family of distributions over \(\mathbf{x}\), given parameters \(\boldsymbol{\eta}\), is defined to be the set of distributions of the form
\begin{align*}
p(\mathbf{x} \mid \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp (\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x}))
\end{align*}
where \(\mathbf{x}\) may be scalar or vector, and may be discrete or continuous. Here \(\boldsymbol{\eta}\) are called the natural parameters of the distribution, and \(\mathbf{u}(\mathbf{x})\) is some function of \(\mathbf{x}\). The function \(g(\boldsymbol{\eta})\) can be interpreted as the coefficient that ensures that the distribution is normalized and therefore satisfies
\begin{align*}
g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp (\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})) \mathrm{d} \mathbf{x}=1
\end{align*}
where the integration is replaced by summation if \(\mathbf{x}\) is a discrete variable.
#+end_definition
**** Examples
***** Bernoulli distribution
#+NAME: Bernoulli distribution 
#+begin_definition latex
The probability density function of the Bernoulli distribution is given by
\begin{align*}
p(x \mid \mu)=\operatorname{Bern}(x \mid \mu)=\mu^x(1-\mu)^{1-x} .
\end{align*}
#+end_definition
#+NAME: Bernoulli distribution belongs to the exponential family
#+begin_theorem latex
The Bernoulli distribution is a member of the exponential family of distribution and has standard form \(p(x \mid \eta)=\sigma(-\eta) \exp (\eta x)\), where \(\sigma(\eta)\) is the logistic sigmoid function.
#+end_theorem
#+NAME: Bernoulli distribution belongs to the exponential family
#+begin_proof latex
By definition
\begin{align*}
p(x \mid \mu)=\operatorname{Bern}(x \mid \mu)=\mu^x(1-\mu)^{1-x} .
\end{align*}
Using \(x = \exp (\ln x)\) we have
\begin{align*}
p(x \mid \mu) & =\exp \{x \ln \mu+(1-x) \ln (1-\mu)\} =(1-\mu) \exp \bigg\{\ln \left(\frac{\mu}{1-\mu}\right) x\bigg\} .
\end{align*}
Comparing with \(h(x) g(\eta) \exp \big (\eta u(x)\big)\), we identify
\[
\eta=\ln \left(\frac{\mu}{1-\mu}\right), \quad u(x) = x, \quad h(x) \cdot g(\eta) = (1 - \mu).
\]
Solving \(\eta=\ln \left(\frac{\mu}{1-\mu}\right)\) for \(\mu\) we obtain
\[
\mu = \frac{1}{1 + \exp (- \eta)} = \sigma (\eta)
\]
where \(\sigma(\eta)\) is the logistic sigmoid function. Using the property  \(1-\sigma(\eta)=\sigma(-\eta)\) of the logistic sigmoid function along with the result \(\mu = \sigma (\eta)\) we obtain \(h \cdot g = \sigma (- \eta)\). Since \(h\) is not a function of \(\eta\) and \(g\) is not a function of \(x\), we must have \(h (x) = 1\) for all \(x\) and \(g(\eta) = \sigma (- \sigma)\). This follows from the fact that \(h (x) \cdot g (\eta) = \sigma (-\eta)\) for all \(x\). Therefore, it is clear that the Bernoulli distribution takes the form \(h(x) g(\eta) \exp \big (\eta u(x)\big)\) with
\[
\eta =\ln \left(\frac{\mu}{1-\mu}\right),\quad \mu = \sigma (\eta), \quad u(x) = x, \quad h(x) = 1, \quad \cdot g(\eta) = \sigma (- \eta),
\]
i.e., it is a member of the exponential family. Substituting \(h(x) = 1\), \(g(\eta) = \sigma (- \eta)\), and \(u(x) = x\) we obtain the standard form \(p(x \mid \eta)=\sigma(-\eta) \exp (\eta x)\), where \(\mu = \sigma(\eta)\) is the logistic sigmoid function.
#+end_proof
***** Multinomial distribution
#+NAME: Multinomial distribution 
#+begin_definition latex
The probability density function of the multinomial distribution, for a single observation \(\mathbf{x}\), is given by
\begin{align*}
p(\mathbf{x} \mid \boldsymbol{\mu})=\prod_{k=1}^M \mu_k^{x_k}=\exp \left\{\sum_{k=1}^M x_k \ln \mu_k\right\}
\end{align*}
where \(\mathbf{x}=\left(x_1, \ldots, x_N\right)^{\mathrm{T}}\).
#+end_definition
#+NAME: Multinomial distribution belongs to the exponential family
#+begin_theorem latex
The Multinomial distribution is a member of the exponential family of distribution and has standard form
\begin{align*}
p(\mathbf{x} \mid \boldsymbol{\eta})=\left(1+\sum_{k=1}^{M-1} \exp \left(\eta_k\right)\right)^{-1} \exp \left(\boldsymbol{\eta}^{\mathrm{T}} \mathbf{x}\right)
\end{align*}
with parameter vector \(\boldsymbol{\eta}=\) \(\left(\eta_1, \ldots, \eta_{M-1}\right)^{\mathrm{T}}\).
#+end_theorem
#+NAME: Multinomial distribution belongs to the exponential family
#+begin_proof latex
By definition
\begin{align*}
p(\mathbf{x} \mid \boldsymbol{\mu})=\prod_{k=1}^M \mu_k^{x_k}=\exp \left\{\sum_{k=1}^M x_k \ln \mu_k\right\}.
\end{align*}
Naive comparison with \(p(\mathbf{x} \mid \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\}\) suggests that we identify
\[
\mathbf{u}(\mathbf{x}) = \mathbf{x}, \quad h (\mathbf{x}) = 1, \quad g(\boldsymbol{\eta}) = 1.
\]
with parameter vector \(\boldsymbol{\eta}=\) \(\left(\eta_1, \ldots, \eta_{M}\right)^{\mathrm{T}}\). However, the parameters \(\eta_k\) are not independent because the parameters \(\mu_k\) are subject to the constraint \(\sum_{k=1}^M \mu_k=1\) so that
\begin{align*}
\exp \bigg( \sum_{k=1}^M x_k \ln \mu_k\bigg) &= \exp \bigg[\sum_{k=1}^{M-1} x_k \ln \mu_k + \bigg(1-\sum_{k=1}^{M-1} x_k\bigg) \ln \bigg(1-\sum_{k=1}^{M-1} \mu_k\bigg)\bigg] \\
&= \exp \bigg[\sum_{k=1}^{M-1} x_k \ln \bigg(\frac{\mu_k}{1-\sum_{j=1}^{M-1} \mu_j}\bigg)+\ln \bigg(1-\sum_{k=1}^{M-1} \mu_k\bigg)\bigg].
\end{align*}
Comparing with \(p(\mathbf{x} \mid \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\}\), we identify
\begin{align*}
\ln \left(\frac{\mu_k}{1-\sum_j \mu_j}\right)=\eta_k
\end{align*}
which we can solve for \(\mu_k\) by first summing both sides over \(k\) and then rearranging and back-substituting to give
\begin{align*}
\mu_k=\frac{\exp \left(\eta_k\right)}{1+\sum_j \exp \left(\eta_j\right)} .
\end{align*}
This is called the softmax function or the normalized exponential. Therefore, it is clear that the multinomial distribution takes the form \(p(\mathbf{x} \mid \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\}\) with
\[
\eta_k = \ln \left(\frac{\mu_k}{1-\sum_j \mu_j}\right),\quad \mu_k=\frac{\exp \left(\eta_k\right)}{1+\sum_j \exp \left(\eta_j\right)}, \quad u(\mathbf{x}) = \mathbf{x}, \quad h(\mathbf{x}) = 1, \quad g(\boldsymbol{\eta}) = \left(1+\sum_{k=1}^{M-1} \exp \left(\eta_k\right)\right)^{-1}
\]
with parameter vector \(\boldsymbol{\eta}=\) \(\left(\eta_1, \ldots, \eta_{M-1}\right)^{\mathrm{T}}\), i.e., it is a member of the exponential family. Substituting \(h(\mathbf{x}) = 1\), \(g(\boldsymbol{\eta}) = 1/ [1+\sum_{k=1}^{M-1} \exp (\eta_k)]\), and \(u (\mathbf{x}) = \mathbf{x}\) we obtain the standard form \(p(\mathbf{x} \mid \boldsymbol{\eta})=\bigg(1+\sum_{k=1}^{M-1} \exp (\eta_k)\bigg)^{-1} \exp ( \boldsymbol{\eta}^{\mathrm{T}} \mathbf{x})\). The Multinomial distribution is a member of the exponential family of distribution and has standard form
\begin{align*}
p(\mathbf{x} \mid \boldsymbol{\eta})=\left(1+\sum_{k=1}^{M-1} \exp \left(\eta_k\right)\right)^{-1} \exp \left(\boldsymbol{\eta}^{\mathrm{T}} \mathbf{x}\right)
\end{align*}
with parameter vector \(\boldsymbol{\eta}=\) \(\left(\eta_1, \ldots, \eta_{M-1}\right)^{\mathrm{T}}\) governed by the constraint \(\sum_{k=1}^M \mu_k=1\).
#+end_proof
***** Gaussian distribution
For the univariate Gaussian, we have
\begin{align*}
p\left(x \mid \mu, \sigma^2\right) & =\frac{1}{\left(2 \pi \sigma^2\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^2}(x-\mu)^2\right\} \\
& =\frac{1}{\left(2 \pi \sigma^2\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^2} x^2+\frac{\mu}{\sigma^2} x-\frac{1}{2 \sigma^2} \mu^2\right\}
\end{align*}
which, after some simple rearrangement, can be cast in the standard exponential family form (2.194) with
\begin{align*}
\boldsymbol{\eta} & =\binom{\mu / \sigma^2}{-1 / 2 \sigma^2} \\
\mathbf{u}(x) & =\binom{x}{x^2} \\
h(\mathbf{x}) & =(2 \pi)^{-1 / 2} \\
g(\boldsymbol{\eta}) & =\left(-2 \eta_2\right)^{1 / 2} \exp \left(\frac{\eta_1^2}{4 \eta_2}\right) .
\end{align*}
**** Maximum likelihood estimator
#+NAME: Log-likelihood of MLE for exponential family
#+begin_theorem
Consider the problem of obtaining a maximum likelihood estimator for the parameter vector \(\boldsymbol{\eta}\) given a general exponential family distribution
\begin{align*}
p(\mathbf{x} \mid \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp (\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x}))
\end{align*}
where \(\mathbf{x}\) may be scalar or vector, and may be discrete or continuous. Here \(\boldsymbol{\eta}\) are called the natural parameters of the distribution, and \(\mathbf{u}(\mathbf{x})\) is some function of \(\mathbf{x}\). The function \(g(\boldsymbol{\eta})\) can be interpreted as the coefficient that ensures that the distribution is normalized and therefore satisfies
\begin{align*}
g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp (\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\big) \mathrm{d} \mathbf{x}=1
\end{align*}
where the integration is replaced by summation if \(\mathbf{x}\) is a discrete variable. The gradient and the Laplacian of the negative log-likelihood with respect to the parameter vector \(\boldsymbol{\eta}\) are equal to the expectation and the covariance of \(\mathbf{u} (\mathbf{x})\):
\[
-\nabla_{\boldsymbol{\eta}} \ln g(\boldsymbol{\eta})=\mathbb{E}[\mathbf{u}(\mathbf{x})], \quad -\nabla^{2}_{\boldsymbol{\eta}} \ln g(\boldsymbol{\eta})=\mathbb{E} [\mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\top}] - \mathbb{E}[\mathbf{u}(\mathbf{x})] \mathbb{E} [\mathbf{u}(\mathbf{x})^{\top}] = \operatorname{cov} [\mathbf{u}(\mathbf{x})].
\]
#+end_theorem
#+NAME: Log-likelihood of MLE for exponential family
#+begin_proof latex
Taking the gradient of both sides of 
\begin{align*}
g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp (\boldsymbol{\eta}^{\top} \mathbf{u}(\mathbf{x})) \mathrm{d} \mathbf{x}=1
\end{align*}
with respect to \(\boldsymbol{\eta}\) yields
\begin{align*}
& \nabla g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right\} \mathrm{d} \mathbf{x} + g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right\} \mathbf{u}(\mathbf{x}) \mathrm{d} \mathbf{x}=0.
\end{align*}
Rearranging, and using \(g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp (\boldsymbol{\eta}^{\top} \mathbf{u}(\mathbf{x})) \mathrm{d} \mathbf{x}=1\) again yields
\begin{align*}
-\frac{1}{g(\boldsymbol{\eta})} \nabla g(\boldsymbol{\eta})=g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right\} \mathbf{u}(\mathbf{x}) \mathrm{d} \mathbf{x}=\mathbb{E}[\mathbf{u}(\mathbf{x})]
\end{align*}
where we have used \(p(\mathbf{x} \mid \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp (\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x}))\). Identifying \(-\frac{1}{g(\boldsymbol{\eta})} \nabla g(\boldsymbol{\eta}) = - \nabla_{\boldsymbol{\eta}} \ln g (\boldsymbol{\eta})\) we obtain
\begin{align*}
-\nabla \ln g(\boldsymbol{\eta})=\mathbb{E}[\mathbf{u}(\mathbf{x})].
\end{align*}
#+end_proof
**** Sufficient statistics
#+NAME: Sufficient statistic
#+begin_definition latex
The exponential family of distributions over \(\mathbf{x}\), given parameters \(\boldsymbol{\eta}\), is defined to be the set of distributions of the form
\begin{align*}
p(\mathbf{x} \mid \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \left\{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right\}
\end{align*}
where \(\mathbf{x}\) may be scalar or vector, and may be discrete or continuous. Here \(\boldsymbol{\eta}\) are called the natural parameters of the distribution, and \(\mathbf{u}(\mathbf{x})\) is some function of \(\mathbf{x}\). The function \(g(\boldsymbol{\eta})\) can be interpreted as the coefficient that ensures that the distribution is normalized and therefore satisfies
\begin{align*}
g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right\} \mathrm{d} \mathbf{x}=1
\end{align*}
where the integration is replaced by summation if \(\mathbf{x}\) is a discrete variable. Let \(\mathbf{X}=\) \(\left\{\mathbf{x}_1, \ldots, \mathbf{x}_n\right\}\) denote a set of independent identically distributed data for which the likelihood function is given by
\begin{align*}
p(\mathbf{X} \mid \boldsymbol{\eta})=\bigg(\prod_{n=1}^N h (\mathbf{x}_n)\bigg) g(\boldsymbol{\eta})^N \exp \bigg(\boldsymbol{\eta}^{\mathrm{T}} \sum_{n=1}^N \mathbf{u} (\mathbf{x}_n)\bigg).
\end{align*}
Setting the gradient of \(\ln p(\mathbf{X} \mid \boldsymbol{\eta})\) with respect to \(\boldsymbol{\eta}\) to zero, we get the following equation whose solution yields the maximum likelihood estimator \(\boldsymbol{\eta}_{\mathrm{ML}}\)
\begin{align*}
-\nabla_{\boldsymbol{\eta}} \ln g (\boldsymbol{\eta}_{\mathrm{ML}}) = \frac{1}{N} \sum_{n=1}^N \mathbf{u} (\mathbf{x}_n).
\end{align*}
\(\sum_n \mathbf{u}\left(\mathbf{x}_n\right)\) is called the sufficient statistic of the distribution \(p(\mathbf{x} \mid \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp (\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x}))\). It is so named because \(\boldsymbol{\eta}_{\mathrm{ML}}\) depends on the data only through \(\sum_n \mathbf{u}\left(\mathbf{x}_n\right)\): we do not need to store the entire dataset \(\mathbf{X}\) but only the value of the sufficient statistic.
#+end_definition
+ Recall that the Bernoulli distribution is a member of the exponential family with \(u(x) = x\). Therefore, the sum of the data points \(\{x_n\}\) is its sufficient statistic.
+ Recall that the Gaussian distribution is a member of the exponential family with  \(\mathbf{u}(x)=\left(x, x^2\right)^{\mathrm{T}}\). Therefore, the sum of \(\left\{x_n\right\}\) and the sum of \(\left\{x_n^2\right\}\) are its sufficient statistics.
+ Comparing \(-\nabla_{\boldsymbol{\eta}} \ln g (\boldsymbol{\eta}_{\mathrm{ML}}) = \frac{1}{N} \sum_{n=1}^N \mathbf{u} (\mathbf{x}_n)\) and \(-\nabla \ln g(\boldsymbol{\eta})=\mathbb{E}[\mathbf{u}(\mathbf{x})]\) it is clear that if \(N \rightarrow \infty\) then \(\boldsymbol{\eta}_{\mathrm{ML}} \to \boldsymbol{\eta}\), i.e., the ML estimator \(\boldsymbol{\eta}_{\mathrm{ML}}\) approaches the true value \(\boldsymbol{\eta}\) of the parameter vector.
**** Conjugate priors
#+NAME: Conjugate priors
#+begin_definition
Let \(p(\mathbf{x} \mid \boldsymbol{\eta})\) be a probability distribution. The conjugate prior is a prior \(p(\boldsymbol{\eta})\) for the parameter vector \(\boldsymbol{\eta}\) that is conjugate to the likelihood function: so that the posterior distribution \(p(\boldsymbol{\eta}  \mid \mathbf{x})\) has the same functional form as the prior.
#+end_definition
#+NAME: Conjugate priors for the exponential family
#+begin_theorem
For any member of the exponential family, there exists a conjugate prior that can be written in the form
\begin{align*}
p(\boldsymbol{\eta} \mid \boldsymbol{\chi}, \nu)=f(\boldsymbol{\chi}, \nu) g(\boldsymbol{\eta})^\nu \exp (\nu \boldsymbol{\eta}^{\mathrm{T}} \boldsymbol{\chi})
\end{align*}
where \(f(\boldsymbol{\chi},\,\nu)\) is a normalization coefficient, and \(g(\boldsymbol{\eta})\) is defined by
\begin{align*}
g(\boldsymbol{\eta})^{\nu} \int f(\boldsymbol{\chi},\,\nu) \exp (\nu \boldsymbol{\eta}^{\mathrm{T}} \boldsymbol{\chi}) \mathrm{d} \boldsymbol{\chi}\, \mathrm{d} \nu = 1.
\end{align*}
#+end_theorem
#+NAME: Conjugate priors for the exponential family
#+begin_proof
Multiply the prior 
\begin{align*}
p(\boldsymbol{\eta} \mid \boldsymbol{\chi},\,\nu)=f(\boldsymbol{\chi}, \nu) g(\boldsymbol{\eta})^\nu \exp (\nu \boldsymbol{\eta}^{\mathrm{T}} \boldsymbol{\chi})
\end{align*}
by the likelihood function
\begin{align*}
p(\mathbf{X} \mid \boldsymbol{\eta})=\bigg(\prod_{n=1}^N h (\mathbf{x}_n)\bigg) g(\boldsymbol{\eta})^N \exp \bigg(\boldsymbol{\eta}^{\mathrm{T}} \sum_{n=1}^N \mathbf{u} (\mathbf{x}_n)\bigg)
\end{align*}
to obtain the posterior distribution, up to a normalization coefficient, in the form
\begin{align*}
p(\boldsymbol{\eta} \mid \mathbf{X},\,\boldsymbol{\chi},\,\nu) \propto g(\boldsymbol{\eta})^{\nu+N} \exp \bigg[\boldsymbol{\eta}^{\top} \bigg(\sum_{n=1}^N \mathbf{u} (\mathbf{x}_n ) + \nu \boldsymbol{\chi} \bigg) \bigg].
\end{align*}
The posterior distribution \(p(\boldsymbol{\eta} \mid \mathbf{X},\,\boldsymbol{\chi},\,\nu)\) has the same functional form as the the prior \(p(\boldsymbol{\eta} \mid \boldsymbol{\chi},\,\nu)\), confirming conjugacy.
#+end_proof
+ The parameter \(\nu\) can be interpreted as a effective number of pseudo-observations in the prior, each of which has a value for the sufficient statistic \(\mathbf{u}(\mathbf{x})\) given by \(\boldsymbol{\chi}\).
+ The conjugate prior of a Bernoulli distribution is a beta distribution.
+ The conjugate prior for the mean of a Gaussian distribution is a Gaussian distribution. The conjugate prior for the precision of a Gaussian distribution is a Wishart distribution.
**** Non-informative priors
*** Non-parametric methods
**** Kernel density estimators
**** Nearest-neighbor methods
** Maximum likelihood estimation (MLE)
:LOGBOOK:
CLOCK: [2024-06-12 Wed 03:16]--[2024-06-12 Wed 04:05] =>  0:49
:END:
*** Definitions
#+NAME: Likelihood function
#+begin_definition latex
Let \(\boldsymbol{v} \in \mathbb{R}^N\) be an \(N\) dimensional observable. \(\mathcal{D}=\left\{\boldsymbol{v}_1, \cdots, \boldsymbol{v}_d\right\}\) is a dataset of \(d\) observations of \(\boldsymbol{v}\). Assume \(\boldsymbol{v}_1, \cdots, \boldsymbol{v}_{\boldsymbol{d}}\) are independent and identically distributed samples from a distribution \(f(\boldsymbol{v} \mid \boldsymbol{\theta})\), where \(\boldsymbol{\theta}\) is a parameter vector from a parameter space \(\boldsymbol{\theta}\) that parametrizes \(f\). Represent this assumption as a hypothesis \(\mathcal{H}\). The likelihood function \(\mathcal{L}(\boldsymbol{\theta} \mid \mathcal{D}, \mathcal{H})\) is defined as (upto an overall multiplication)
\begin{align*}
\mathcal{L}(\boldsymbol{\theta} \mid \mathcal{D}, \mathcal{H}) \equiv f\left(\boldsymbol{v}_1, \cdots, \boldsymbol{v}_d \mid \boldsymbol{\theta}\right)=\prod_{i=1}^d f\left(\boldsymbol{v}_i \mid \boldsymbol{\theta}\right)
\end{align*}
#+end_definition
#+NAME: Log-likelihood function
#+begin_definition latex
The log-likelihood function is defined as the logarithm of the likelihood function
\begin{align*}
\operatorname{In} \mathcal{L}(\boldsymbol{\theta} \mid \boldsymbol{v})=\ln \prod_{i=1}^d f\left(\boldsymbol{v}_{\boldsymbol{i}} \mid \boldsymbol{\theta}\right)=\sum_{i=1}^d \ln f\left(\boldsymbol{v}_{\boldsymbol{i}} \mid \boldsymbol{\theta}\right).
\end{align*}
#+end_definition
#+NAME: Maximum likelihood estimator (MLE)
#+begin_definition latex
A maximum likelihood estimator (MLE) is a function \(\hat{\boldsymbol{\theta}}: \mathbb{R}^d \rightarrow \boldsymbol{\Theta}\) that takes a dataset \(\mathcal{D}\) and returns a parameter vector \(\hat{\boldsymbol{\theta}}\) that maximizes the likelihood function
\begin{align*}
\hat{\boldsymbol{\theta}}=\underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\arg \max } \mathcal{L}(\boldsymbol{\theta} \mid \mathcal{D}, \mathcal{H})
\end{align*}
The necessary conditions for the existence of an MLE are:
  1. \(\operatorname{In} \mathcal{L}(\boldsymbol{\theta} \mid \boldsymbol{v})\) be differentiable function of \(\boldsymbol{\theta}\),
  2. The gradient \(\nabla_{\boldsymbol{\theta}} \mathcal{L}(\hat{\boldsymbol{\theta}} \mid \boldsymbol{v})\) must identically vanish,
  3. The Hessian matrix \(\mathrm{H}(\hat{\boldsymbol{\theta}})\) must be negative semi-definite.
#+end_definition
*** Examples
*** Properties
**** MLE and Kullback-Leibler divergence
** Maximum a posteriori probability (MAP) estimation
In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to the method of maximum likelihood (ML) estimation, but employs an augmented optimization objective which incorporates a prior distribution (that quantifies the additional information available through prior knowledge of a related event) over the quantity one wants to estimate. MAP estimation can therefore be seen as a regularization of maximum likelihood estimation.

Assume that we want to estimate an unobserved population parameter \(\theta\) on the basis of observations \(x\). Let \(f\) be the sampling distribution of \(x\), so that \(f(x \mid \theta)\) is the probability of \(x\) when the underlying population parameter is \(\theta\). Then the function:
\begin{align*}
\theta \mapsto f(x \mid \theta)
\end{align*}
is known as the likelihood function and the estimate:
\begin{align*}
\hat{\theta}_{\text {MLE }}(x)=\underset{\theta}{\arg \max } f(x \mid \theta)
\end{align*}
is the maximum likelihood estimate of \(\theta\).
Now assume that a prior distribution \(g\) over \(\theta\) exists. This allows us to treat \(\theta\) as a random variable as in Bayesian statistics. We can calculate the posterior distribution of \(\theta\) using Bayes' theorem:
\begin{align*}
\theta \mapsto f(\theta \mid x)=\frac{f(x \mid \theta) g(\theta)}{\int_{\Theta} f(x \mid \vartheta) g(\vartheta) d \vartheta}
\end{align*}
where \(g\) is density function of \(\theta, \Theta\) is the domain of \(g\).

The method of maximum a posteriori estimation then estimates \(\theta\) as the mode of the posterior distribution of this random variable:
\begin{align*}
\begin{aligned}
\hat{\theta}_{\mathrm{MAP}}(x) & =\underset{\theta}{\arg \max } f(\theta \mid x) \\
& =\underset{\theta}{\arg \max } \frac{f(x \mid \theta) g(\theta)}{\int_{\Theta} f(x \mid \vartheta) g(\vartheta) d \vartheta} \\
& =\underset{\theta}{\arg \max } f(x \mid \theta) g(\theta) .
\end{aligned}
\end{align*}

The denominator of the posterior distribution (so-called marginal likelihood) is always positive and does not depend on \(\theta\) and therefore plays no role in the optimization. Observe that the MAP estimate of \(\theta\) coincides with the ML estimate when the prior \(g\) is uniform (i.e., \(g\) is a constant function).

When the loss function is of the form
\begin{align*}
L(\theta, a)= \begin{cases}0, & \text { if }|a-\theta|<c \\ 1, & \text { otherwise }\end{cases}
\end{align*}
as \(c\) goes to 0 , the Bayes estimator approaches the MAP estimator, provided that the distribution of \(\theta\) is quasiconcave. But generally a MAP estimator is not a Bayes estimator unless \(\theta\) is discrete.

*** Computation
MAP estimates can be computed in several ways:
1. Analytically, when the mode(s) of the posterior distribution can be given in closed form. This is the case when conjugate priors are used.
2. Via numerical optimization such as the conjugate gradient method or Newton's method. This usually requires first or second derivatives, which have to be evaluated analytically or numerically.
3. Via a modification of an expectation-maximization algorithm. This does not require derivatives of the posterior density.
4. Via a Monte Carlo method using simulated annealing
*** Computation
While only mild conditions are required for MAP estimation to be a limiting case of Bayes estimation (under the 01 loss function), it is not very representative of Bayesian methods in general. This is because MAP estimates are point estimates, whereas Bayesian methods are characterized by the use of distributions to summarize data and draw inferences: thus, Bayesian methods tend to report the posterior mean or median instead, together with credible intervals. This is both because these estimators are optimal under squared-error and linear-error loss respectively-which are more representative of typical loss functions-and for a continuous posterior distribution there is no loss function which suggests the MAP is the optimal point estimator. In addition, the posterior distribution may often not have a simple analytic form: in this case, the distribution can be simulated using Markov chain Monte Carlo techniques, while optimization to find its mode(s) may be difficult or impossible.

In many types of models, such as mixture models, the posterior may be multimodal. In such a case, the usual recommendation is that one should choose the highest mode: this is not always feasible (global optimization is a difficult problem), nor in some cases even possible (such as when identifiability issues arise). Furthermore, the highest mode may be uncharacteristic of the majority of the posterior.

Finally, unlike ML estimators, the MAP estimate is not invariant under reparameterization. Switching from one parameterization to another involves introducing a Jacobian that impacts on the location of the maximum.
*** Example
Suppose that we are given a sequence \(\left(x_1, \ldots, x_n\right)\) of IID \(N\left(\mu, \sigma_v^2\right)\) random variables and a prior distribution of \(\mu\) is given by \(N\left(\mu_0, \sigma_m^2\right)\). We wish to find the MAP estimate of \(\mu\). Note that the normal distribution is its own conjugate prior, so we will be able to find a closed-form solution analytically.

The function to be maximized is then given by
\begin{align*}
f(\mu) f(x \mid \mu)=\pi(\mu) L(\mu)=\frac{1}{\sqrt{2 \pi} \sigma_m} \exp \left(-\frac{1}{2}\left(\frac{\mu-\mu_0}{\sigma_m}\right)^2\right) \prod_{j=1}^n \frac{1}{\sqrt{2 \pi} \sigma_v} \exp \left(-\frac{1}{2}\left(\frac{x_j-\mu}{\sigma_v}\right)^2\right),
\end{align*}
which is equivalent to minimizing the following function of \(\mu\) :
\begin{align*}
\sum_{j=1}^n\left(\frac{x_j-\mu}{\sigma_v}\right)^2+\left(\frac{\mu-\mu_0}{\sigma_m}\right)^2
\end{align*}

Thus, we see that the MAP estimator for \(\mu\) is given by
\begin{align*}
\hat{\mu}_{\mathrm{MAP}}=\frac{\sigma_m^2 n}{\sigma_m^2 n+\sigma_v^2}\left(\frac{1}{n} \sum_{j=1}^n x_j\right)+\frac{\sigma_v^2}{\sigma_m^2 n+\sigma_v^2} \mu_0=\frac{\sigma_m^2\left(\sum_{j=1}^n x_j\right)+\sigma_v^2 \mu_0}{\sigma_m^2 n+\sigma_v^2} .
\end{align*}
which turns out to be a linear interpolation between the prior mean and the sample mean weighted by their respective covariances.

The case of \(\sigma_m \rightarrow \infty\) is called a non-informative prior and leads to an improper probability distribution; in this case \(\hat{\mu}_{\mathrm{MAP}} \rightarrow \hat{\mu}_{\mathrm{MLE}}\).

As an example of the difference between Bayes estimators mentioned above (mean and median estimators) and using a MAP estimate, consider the case where there is a need to classify inputs \(x\) as either positive or negative (for example, loans as risky or safe). Suppose there are just three possible hypotheses about the correct method of classification \(h_1, h_2\) and \(h_3\) with posteriors \(0.4,0.3\) and 0.3 respectively. Suppose given a new instance, \(x, h_1\) classifies it as positive, whereas the other two classify it as negative. Using the MAP estimate for the correct classifier \(h_1, x\) is classified as positive, whereas the Bayes estimators would average over all hypotheses and classify \(x\) as negative.
** Gradient descent (GD)
:LOGBOOK:
CLOCK: [2024-06-12 Wed 08:59]--[2024-06-12 Wed 10:07] =>  1:08
:END:
*** Stochastic gradient descent (SGD)
+ Stochastic gradient descent (SGD) is a /sequential algorithm/: the \(N\) observations in the dataset \((\mathbf{X},\,\mathbf{T}) \equiv (\mathbf{x}_{n},\,\mathbf{t}_{n})_{n=1,\,\ldots,\,N}\) are considered one at a time and the parameter vector \(\mathbf{w}\) is updated after the the presentation of each \((\mathbf{x}_{n},\,\mathbf{t}_{n})\).
+ SGD is applicable whenever:  1) the log-likelihood function is proportional to an error function
  \[
  \ln p(\mathbf{T} \mid \mathbf{X},\, \mathbf{w}) \propto E (\mathbf{T},\,\mathbf{X},\,\mathbf{w}),
  \] 2) the error function \(E (\mathbf{T},\,\mathbf{X},\,\mathbf{w})\) admits the decomposition \(E (\mathbf{T},\,\mathbf{X},\,\mathbf{w}) = \sum_{n=1}^{N} E_{n} (\mathbf{t}_{n},\,\mathbf{x}_{n},\,\mathbf{w})\) where \(E_{n} (\mathbf{t}_{n},\,\mathbf{x}_{n},\,\mathbf{w})\) is the error for the observation \((\mathbf{x}_{n},\,\mathbf{t}_{n})\).
+ The /SGD algorithm/ is:
  \begin{algorithm}
  \caption{Stochastic gradient descent (SGD) algorithm}
  \begin{algorithmic}[1] % The [1] here specifies that lines should be numbered
  \State Initialize the learning rate \(\eta\)
  \State Initialize the parameter vector \(\mathbf{w}^{(0)}\)
  \For{iteration number \(\tau\) presenting observation \((\mathbf{x}_{n},\,\mathbf{t}_{n})\)}
      \State Update the gradient of the error \(\nabla_{\mathbf{w}} E_{n} \leftarrow \nabla_{\mathbf{w}} E_{n} (\mathbf{t}_{n},\,\mathbf{x}_{n},\,\mathbf{w}^{\tau})\)
      \State Update \(\mathbf{w}\) using the SGD update rule:
      \[
      \mathbf{w}^{(\tau+1)} \leftarrow \mathbf{w}^{(\tau)} - \eta \nabla_{\mathbf{w}} E_{n}
      \]
  \EndFor
  \end{algorithmic}
  \end{algorithm}
  The value of \(\eta\) needs to be chosen with care to ensure that the algorithm converges.
** Linear regression
:LOGBOOK:
CLOCK: [2024-06-12 Wed 05:53]--[2024-06-12 Wed 08:30] =>  2:37
:END:
*** Definition
#+NAME: Linear regression
#+begin_definition
For a dataset \(\mathcal{D} = \{(\mathbf{x}_{n}, t_{n})\}_{n=1,\,\ldots,\,N}\), where \(t_{n} \in \mathbb{R}\) and \(\mathbf{x}_{n} \in \mathbb{R}^{D}\) containing \(N\) observations of a \(D\)-dimensional observable \(\mathbb{x}\), a *linear regressor* is a function of the form
\[
y(\mathbf{x}, \mathbf{w})=\sum_{j=0}^{M-1} w_{j} \cdot \phi_{j}(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \phi(\mathbf{x})
\]
where \(\mathbf{w}=\left(w_{0}, \ldots, w_{M-1}\right)^{\mathrm{T}}\) and \(\boldsymbol{\phi}=\left(\phi_{0}, \ldots, \phi_{M-1}\right)^{\mathrm{T}}\). \(\phi_{j}(\mathbf{x})\) are known as *basis functions*. \( \phi_{0} \equiv 1  \) is a spectator basis functions, allowing \(w_{0}\) to act as a fixed offset. The linear regressor predicts the value of continuous target variables \( t \) given a \( D \)-dimensional input vector \( \mathbf{x} \).
#+end_definition
+ The key property of this model is that it is a linear function of the parameters \(w_{0}, \ldots, w_{D}\) - it is immaterial that the basis functions can be arbitrary non-linear functions of the input variables.
+ As an example, these basis functions may be the outcome of /feature extraction/. We give some examples for the set of basis functions \(\{\phi_{j}(\mathbf{x})\}_{j=0,\,\ldots,\,M-1}\) next. Note that \(\phi_{0} (\mathbf{x}) \equiv 1\) for all \(\mathbf{x}\).
**** The identity basis
When the basis function are \( \phi_{j} (x) = x_j \) for all \( j = 1,\,\ldots,\, M -1 \), the linear regressor reduces to
\begin{align*}
y(\mathbf{x}, \mathbf{w})=w_{0} \cdot 1 + w_{1} x_{1}+\ldots+w_{D} x_{D}.
\end{align*}
This is traditional form of the linear regression and corresponds to not using any kind of feature extraction. This regressor is linear in both \(\mathbf{w}\) and \(\mathbf{x}\).
**** The polynomial basis
When the basis function are \( \phi_{j} (x) = x^{j} \) for all \(j = 1,\,\ldots,\,M-1\), the linear regressor reduces to
\begin{align*}
y(\mathbf{x}, \mathbf{w})=w_{0} \cdot 1 + w_{1} \cdot x_{1} + \ldots + w_{D} \cdot x_{D}^{D}.
\end{align*}
This is called /polynomial regression/.
#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-3-1a.png]]
**** The Gaussian basis
When the basis functions are \(\phi_{j}(x)=\exp ((x-\mu_{j})^{2}/2 s^2)\) for all \(j = 1,\,\ldots,\,M-1\), the linear regressor reduces to
\begin{align*}
y(\mathbf{x}, \mathbf{w})=w_{0} + \sum_{j=1}^{M-1} w_{j} \cdot \exp \bigg(-\frac{(x-\mu_{j})^{2}}{2s^{2}}\bigg).
\end{align*}
This is called /Gaussian regression/.
#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-3-1b.png]]
**** The hyperbolic basis
When the basis functions are \(\phi_{j}(x)=\sigma ((x-\mu_{j})/s)\) for all \(j = 1,\,\ldots,\,M-1\), the linear regressor reduces to
\begin{align*}
y(\mathbf{x}, \mathbf{w})=w_{0}+\sum_{j=1}^{M-1} w_{j} \cdot \sigma\left(\frac{x-\mu_{j}}{s}\right).
\end{align*}
Here \(\sigma(a)\) is the logistic sigmoid function defined by \( \sigma(a)=1/(1+\exp (-a)) \). The sigmoid function is related to the \( \tanh \) function by \(\tanh (a)=2 \sigma(a)-1\).
#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-3-1c.png]]
*** Extensions
**** Regularized linear regression
The idea of *regularization* is to add a penalty function \(E_{W} (\mathbf{w})\) to the error function \(E (\mathbf{T},\,\mathbf{X},\,\mathbf{w})\) in order to control over-fitting and improve generalization. The /total error function/ is then given by a linear combination of the error function and the penalty function:
\[
E (\mathbf{T},\,\mathbf{X},\,\mathbf{w}) = E_{D} (\mathbf{T},\,\mathbf{X},\,\mathbf{w}) + \lambda \, E_{W} (\mathbf{w})
\]
where \(\lambda\) is the *regularization coefficient* that controls the relative importance of the /data-dependent/ error \(E_{D} (\mathbf{T},\,\mathbf{X},\,\mathbf{w})\) and the /data-independent/ penalty \(E_{W}(\mathbf{w})\).
***** Weight decay
#+NAME: Weight decay
#+begin_definition
Weight decay refers to regularization by use of the penalty function
\[
E_{W}(\mathbf{w})=\frac{1}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}.
\]
#+end_definition
Using the sum-of-squares error function
\[
E_{D}(\mathbf{t},\,\mathbf{x},\,\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N}\{t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}_{n})\}^{2}
\]
the total error function becomes
\[
E (\mathbf{t},\,\mathbf{x},\,\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right\}^{2}+\frac{\lambda}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}
\]
+ *Weight decay* is named so (mainly in the machine learning literature) because in /sequential learning algorithms/, it encourages weight vector values to decay towards zero, unless supported by the data.
+ In statistics, weight decay is an example of a *parameter shrinkage method* because it shrinks parameter values towards zero.
+ It has the advantage that the error function remains a quadratic function of \(\mathbf{w}\), and so its exact minimizer can be found in closed form. Specifically, setting the gradient of (4) with respect to \(\mathbf{w}\) to zero, and solving for \(\mathrm{w}\) as before, we obtain
\begin{align*}
\boxed{
\mathbf{w}_{\text{ML}} =\left(\lambda \mathbf{I}+\boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{\mathrm{T}} \mathbf{t} \tag{5}
}
\end{align*}
where \(\boldsymbol{\Phi}\), called the design matrix, is a \( N \times M \) matrix of the form
\[
\boldsymbol{\Phi} = \begin{pmatrix}
\phi_0(\mathbf{x}_1) & \phi_1(\mathbf{x}_1) & \cdots & \phi_{M-1}(\mathbf{x}_1) \\
\phi_0(\mathbf{x}_2) & \phi_1(\mathbf{x}_2) & \cdots & \phi_{M-1}(\mathbf{x}_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(\mathbf{x}_N) & \phi_1(\mathbf{x}_N) & \cdots & \phi_{M-1}(\mathbf{x}_N)
\end{pmatrix}
\]
This represents a simple extension of the least-squares solution.
***** Generalized weight decay
A more general regularizer is sometimes used, for which the regularized error takes the form
\begin{align*}
\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right\}^{2}+\frac{\lambda}{2} \sum_{j=1}^{M}\left|w_{j}\right|^{q} \tag{6}
\end{align*}
where \(q=2\) corresponds to the quadratic regularizer (4). The figure below shows contours of the regularization function for different values of \(q\) (in [[id:03678afe-4c0e-495c-9c96-ad5a45910fcb][Polar coordinate system]]).
#+ATTR_HTML: :width 600px
[[file:~/.local/images/prml-3-3.png]]
***** Lasso regression
The case of \(q=1\) is know as the *lasso* in the statistics literature. It has the property that if \(\lambda\) is sufficiently large, some of the coefficients \(w_{j}\) are driven to zero, leading to a sparse model in which the corresponding basis functions play no role.
To see this, we first note that minimizing (6) is equivalent to minimizing the unregularized sum-of-squares error subject to the constraint
\begin{align*}
\sum_{j=1}^{M}\left|w_{j}\right|^{q} \leqslant \eta \tag{7}
\end{align*}
for an appropriate value of the parameter \(\eta\), where the two approaches can be related using Lagrange multipliers. The origin of the sparsity can be seen from the figure below which shows that the minimum of the error function, subject to the constraint (7). As \(\lambda\) is increased, so an increasing number of parameters are driven to zero.
#+ATTR_HTML: :width 200px
[[file:~/.local/images/prml-3-4a.png]]
#+ATTR_HTML: :width 200px
[[file:~/.local/images/prml-3-4b.png]]
Regularization allows complex models to be trained on data sets of limited size without severe over-fitting, essentially by limiting the effective model complexity. However, the problem of determining the optimal model complexity is then shifted from one of finding the appropriate number of basis functions to one of determining a suitable value of the regularization coefficient \(\lambda\).
**** Multivariate linear regression
**** Bayesian linear regression
*** Estimation
**** ML estimation
***** Ordinary least squares (OLS)
#+NAME: Ordinary least squares (OLS)
#+begin_theorem
For the linear regression problem defined above, assume that the target variable \( t = y(\mathbf{x},\,\mathbf{w}) \) is given by
\[
t = y(\mathbf{x}, \mathbf{w}) + \epsilon
\]
where \(\epsilon\) is zero mean Gaussian noise with precision \(\beta\), the likelihood function is
  \begin{equation*}
  \boxed{
  p(t \mid \mathbf{x}, \mathbf{w}, \beta) = \mathcal{N}(t \mid y(\mathbf{x}, \mathbf{w}),\, \beta^{-1})
  }
  \end{equation*}
where
\[
\mathcal{N}(x \mid \mu, \sigma^2)=\frac{1}{(2 \pi \sigma^2)^{1 / 2}} \exp \bigg(-\frac{(x-\mu)^2}{2 \sigma^2}\bigg).
\]
The log-likelihood function is
\begin{equation*}
\boxed{
\ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = \frac{N}{2} \ln \beta - \frac{N}{2} \ln (2 \pi) + \beta E_{D}
}
\end{equation*}
where \(E_{D}\) is the sum-of-squares error function defined by
\begin{equation*}
\boxed{
E_D(\mathbf{t},\,\mathbf{X},\,\mathbf{w}) = - \frac{1}{2} \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\}^2.
}
\end{equation*}
The maximum likelihood estimator of \(\mathbf{w}\) and \(\beta\) are
\begin{equation*}
\boxed{
\mathbf{w}_{\mathrm{ML}} = \boldsymbol{\Phi}^\mathrm{T} \mathbf{t} \, (\boldsymbol{\Phi}^\mathrm{T} \boldsymbol{\Phi})^{-1}, \qquad
\beta_{\mathrm{ML}}^{-1} = N^{-1} \sum_{n=1} \{t_n - \mathbf{w}_{\mathrm{ML}}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\}^2,
}
\end{equation*}
where \(\boldsymbol{\Phi}\), called the design matrix, is a \( N \times M \) matrix of the form
\[
\boldsymbol{\Phi} = \begin{pmatrix}
\phi_0(\mathbf{x}_1) & \phi_1(\mathbf{x}_1) & \cdots & \phi_{M-1}(\mathbf{x}_1) \\
\phi_0(\mathbf{x}_2) & \phi_1(\mathbf{x}_2) & \cdots & \phi_{M-1}(\mathbf{x}_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(\mathbf{x}_N) & \phi_1(\mathbf{x}_N) & \cdots & \phi_{M-1}(\mathbf{x}_N)
\end{pmatrix}.
\]
#+end_theorem
#+NAME: Ordinary least squares (OLS)
#+begin_proof
The log-likelihood function is
\begin{equation*}
\ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = \frac{N}{2} \ln \beta - \frac{N}{2} \ln (2 \pi) + \beta E_{D}
\end{equation*}
where \(E_{D}\) is the sum-of-squared-error error function defined by
\begin{equation*}
E_D(\mathbf{t},\,\mathbf{X},\,\mathbf{w}) = - \frac{1}{2} \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\}^2.
\end{equation*}
1) \(\mathbf{w}_{\text{ML}}\) is obtained by solving \(\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = 0\) for \( \mathbf{w} \).
\[
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\} \boldsymbol{\phi}(\mathbf{x}_n)^\mathrm{T} = 0
\]
yields
\[
\sum_{n=1}^{N} t_n \boldsymbol{\phi}(\mathbf{x}_n)^\mathrm{T} = \mathbf{w}^\mathrm{T} \left( \sum_{n=1}^{N} \boldsymbol{\phi}(\mathbf{x}_n) \boldsymbol{\phi}(\mathbf{x}_n)^\mathrm{T} \right).
\]
Solving for \(\mathbf{w}\) we obtain
\[
\mathbf{w}_{\mathrm{ML}} = \boldsymbol{\Phi}^\mathrm{T} \mathbf{t} \, (\boldsymbol{\Phi}^\mathrm{T} \boldsymbol{\Phi})^{-1}
\]
where \(\boldsymbol{\Phi}\) is a \( N \times M \) matrix called the design matrix
\[
\boldsymbol{\Phi} = \begin{pmatrix}
\phi_0(\mathbf{x}_1) & \phi_1(\mathbf{x}_1) & \cdots & \phi_{M-1}(\mathbf{x}_1) \\
\phi_0(\mathbf{x}_2) & \phi_1(\mathbf{x}_2) & \cdots & \phi_{M-1}(\mathbf{x}_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(\mathbf{x}_N) & \phi_1(\mathbf{x}_N) & \cdots & \phi_{M-1}(\mathbf{x}_N)
\end{pmatrix}
\]
whose Moore-Penrose pseudo-inverse is
\[
\boldsymbol{\Phi}^\dagger = (\boldsymbol{\Phi}^\mathrm{T} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^\mathrm{T}.
\]
2) \(\beta_{\text{ML}}\) is obtained by solving \(\mathrm{D}_{\beta} \ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w}_{\text{ML}},\, \beta) = 0\) for \( \beta \).
\[
\mathrm{D}_{\beta} \ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w}_{\text{ML}},\, \beta) = \frac{N}{2 \beta} - \frac{1}{2} \sum_{n=1}^{N} \{t_{n} - \mathbf{w}_{\text{ML}} \, \boldsymbol{\phi} (\mathbf{x}_{n})\}^{2} = 0
\]
yields
\[
\frac{N}{2 \beta} = \frac{1}{2} \sum_{n=1}^{N} \{t_{n} - \mathbf{w}_{\text{ML}} \, \boldsymbol{\phi} (\mathbf{x}_{n})\}^{2}
\]
Solving for \(\beta\) we obtain
\[
\beta_{\mathrm{ML}}^{-1} = \frac{1}{N} \sum_{n=1}^{N} \{t_n - \mathbf{w}_{\mathrm{ML}}^\mathrm{T} \, \boldsymbol{\phi}(\mathbf{x}_n)\}^2.
\]
\(\beta^{-1}\) called the *inverse noise precision*.
#+end_proof
 + The maximum likelihood estimator \(\beta_{\text{ML}}^{-1}\) of the inverse noise precision is the residual variance of the target values around the regression function.
***** Weighted least squares
***** Generalized least squares (GLS)
***** Least absolute deviation (LAD)
**** MAP estimation
***** Simple ridge estimator
#+NAME: Simple ridge estimator
#+begin_theorem
For the linear regression problem defined above, assume that the target variable \( t = y(\mathbf{x},\,\mathbf{w}) \) is given by
\[
t = y(\mathbf{x}, \mathbf{w}) + \epsilon
\]
where \(\epsilon\) is zero mean Gaussian noise with precision \(\beta\), the likelihood function is
  \begin{equation*}
  \boxed{
  p(t \mid \mathbf{x}, \mathbf{w}, \beta) = \mathcal{N}(t \mid y(\mathbf{x}, \mathbf{w}),\, \beta^{-1})
  }
  \end{equation*}
where
\[
\mathcal{N}(x \mid \mu, \sigma^2)=\frac{1}{(2 \pi \sigma^2)^{1 / 2}} \exp \bigg(-\frac{(x-\mu)^2}{2 \sigma^2}\bigg).
\]
  \begin{equation*}
  \boxed{
  p(t \mid \mathbf{x}, \mathbf{w}, \beta) = \mathcal{N}(t \mid y(\mathbf{x}, \mathbf{w}), \beta^{-1})
  }
  \end{equation*}
where
\[
\mathcal{N}(x \mid \mu, \sigma^2)=\frac{1}{(2 \pi \sigma^2)^{1 / 2}} \exp \bigg(-\frac{(x-\mu)^2}{2 \sigma^2}\bigg).
\]
\begin{equation*}
\boxed{
p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = \exp \bigg(\frac{N}{2} \ln \beta - \frac{N}{2} \ln (2 \pi) + \beta \bigg[ - \frac{1}{2} \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{\top} \cdot \boldsymbol{\phi}(\mathbf{x}_n)\}^2 + \frac{\lambda}{2} \mathbf{w}^\mathrm{\top} \cdot \mathbf{w} \bigg] \bigg)
}
\end{equation*}
The log-likelihood function is
\begin{equation*}
\boxed{
\ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = \frac{N}{2} \ln \beta - \frac{N}{2} \ln (2 \pi) + \beta \bigg[ - \frac{1}{2} \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{\top} \cdot \boldsymbol{\phi}(\mathbf{x}_n)\}^2 + \frac{\lambda}{2} \mathbf{w}^\mathrm{\top} \cdot \mathbf{w} \bigg]
}
\end{equation*}
where \(E_{D}\) is the sum-of-squares error function defined by
\begin{equation*}
\boxed{
E (\mathbf{t},\,\mathbf{X},\,\mathbf{w}) = - \frac{1}{2} \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{\top} \cdot \boldsymbol{\phi}(\mathbf{x}_n)\}^2 + \frac{1}{2} \mathbf{w}^\mathrm{\top} \cdot \mathbf{w}
}
\end{equation*}
The maximum likelihood estimator of \(\mathbf{w}\) and \(\beta\) are
\begin{equation*}
\boxed{
\mathbf{w}_{\mathrm{ML}} = \boldsymbol{\Phi}^\mathrm{T} \mathbf{t} \, (\boldsymbol{\Phi}^\mathrm{T} \boldsymbol{\Phi})^{-1}, \qquad
\beta_{\mathrm{ML}}^{-1} = N^{-1} \sum_{n=1} \{t_n - \mathbf{w}_{\mathrm{ML}}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\}^2,
}
\end{equation*}
where \(\boldsymbol{\Phi}\), called the design matrix, is a \( N \times M \) matrix of the form
\[
\boldsymbol{\Phi} = \begin{pmatrix}
\phi_0(\mathbf{x}_1) & \phi_1(\mathbf{x}_1) & \cdots & \phi_{M-1}(\mathbf{x}_1) \\
\phi_0(\mathbf{x}_2) & \phi_1(\mathbf{x}_2) & \cdots & \phi_{M-1}(\mathbf{x}_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(\mathbf{x}_N) & \phi_1(\mathbf{x}_N) & \cdots & \phi_{M-1}(\mathbf{x}_N)
\end{pmatrix}.
\]
#+end_theorem
#+NAME: Ordinary least squares (OLS)
#+begin_proof
The log-likelihood function is
\begin{equation*}
\ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = \frac{N}{2} \ln \beta - \frac{N}{2} \ln (2 \pi) + \beta E_{D}
\end{equation*}
where \(E_{D}\) is the sum-of-squared-error error function defined by
\begin{equation*}
E_D(\mathbf{t},\,\mathbf{X},\,\mathbf{w}) = - \frac{1}{2} \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\}^2.
\end{equation*}
1) \(\mathbf{w}_{\text{ML}}\) is obtained by solving \(\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = 0\) for \( \mathbf{w} \).
\[
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w},\, \beta) = \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\} \boldsymbol{\phi}(\mathbf{x}_n)^\mathrm{T} = 0
\]
yields
\[
\sum_{n=1}^{N} t_n \boldsymbol{\phi}(\mathbf{x}_n)^\mathrm{T} = \mathbf{w}^\mathrm{T} \left( \sum_{n=1}^{N} \boldsymbol{\phi}(\mathbf{x}_n) \boldsymbol{\phi}(\mathbf{x}_n)^\mathrm{T} \right).
\]
Solving for \(\mathbf{w}\) we obtain
\[
\mathbf{w}_{\mathrm{ML}} = \boldsymbol{\Phi}^\mathrm{T} \mathbf{t} \, (\boldsymbol{\Phi}^\mathrm{T} \boldsymbol{\Phi})^{-1}
\]
where \(\boldsymbol{\Phi}\) is a \( N \times M \) matrix called the design matrix
\[
\boldsymbol{\Phi} = \begin{pmatrix}
\phi_0(\mathbf{x}_1) & \phi_1(\mathbf{x}_1) & \cdots & \phi_{M-1}(\mathbf{x}_1) \\
\phi_0(\mathbf{x}_2) & \phi_1(\mathbf{x}_2) & \cdots & \phi_{M-1}(\mathbf{x}_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(\mathbf{x}_N) & \phi_1(\mathbf{x}_N) & \cdots & \phi_{M-1}(\mathbf{x}_N)
\end{pmatrix}
\]
whose Moore-Penrose pseudo-inverse is
\[
\boldsymbol{\Phi}^\dagger = (\boldsymbol{\Phi}^\mathrm{T} \boldsymbol{\Phi})^{-1} \boldsymbol{\Phi}^\mathrm{T}.
\]
2) \(\beta_{\text{ML}}\) is obtained by solving \(\mathrm{D}_{\beta} \ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w}_{\text{ML}},\, \beta) = 0\) for \( \beta \).
\[
\mathrm{D}_{\beta} \ln p(\mathbf{t} \mid \mathbf{X},\, \mathbf{w}_{\text{ML}},\, \beta) = \frac{N}{2 \beta} - \frac{1}{2} \sum_{n=1}^{N} \{t_{n} - \mathbf{w}_{\text{ML}} \, \boldsymbol{\phi} (\mathbf{x}_{n})\}^{2} = 0
\]
yields
\[
\frac{N}{2 \beta} = \frac{1}{2} \sum_{n=1}^{N} \{t_{n} - \mathbf{w}_{\text{ML}} \, \boldsymbol{\phi} (\mathbf{x}_{n})\}^{2}
\]
Solving for \(\beta\) we obtain
\[
\beta_{\mathrm{ML}}^{-1} = \frac{1}{N} \sum_{n=1}^{N} \{t_n - \mathbf{w}_{\mathrm{ML}}^\mathrm{T} \, \boldsymbol{\phi}(\mathbf{x}_n)\}^2.
\]
\(\beta^{-1}\) called the *inverse noise precision*.
#+end_proof
*** Interpretation
+ Consider an \(N\)-dimensional space whose axes are given by the \(t_{n}\), so that \(\mathbf{t}=\left(t_{1}, \ldots, t_{N}\right)^{\mathrm{T}}\) is a vector in this space. Each basis function \(\phi_{j}\left(\mathbf{x}_{n}\right)\), evaluated at the \(N\) data points, can also be represented as a vector in the same space, denoted by \(\varphi_{j}\), as illustrated in the figure below
  #+ATTR_HTML: :width 400px
  [[file:~/.local/images/prml-3-2.png]]
  Note that \(\varphi_{j}\) corresponds to the \(j^{\text {th }}\) column of \(\boldsymbol{\Phi}\), whereas \(\phi\left(\mathbf{x}_{n}\right)\) corresponds to the \(n^{\text {th }}\) row of \(\boldsymbol{\Phi}\).
+ If the number \(M\) of basis functions is smaller than the number \(N\) of data points, then the \(M\) vectors \(\phi_{j}\left(\mathbf{x}_{n}\right)\) will span a linear subspace \(\mathcal{S}\) of dimension \(M\).
+ Let \(\mathbf{y}\) be an \(N\)-dimensional vector whose element \(n\) element is given by \(y\left(\mathbf{x}_{n}, \mathbf{w}\right)\), where \(n=1, \ldots, N\). Because \(\mathbf{y}\) is an arbitrary linear combination of the vectors \(\varphi_{j}\), it can live anywhere in the \(M\)-dimensional subspace. The sum-of-squares error \(E_{D}\) is then equal (up to a factor of \(1 / 2\)) to the squared Euclidean distance between \(\mathbf{y}\) and \(t\). 
+ Thus the least-squares solution for \(\mathbf{w}\) corresponds to that choice of \(\mathbf{y}\) that lies in subspace \(\mathcal{S}\) and that is closest to \(\mathbf{t}\).
+ Intuitively, from the figure above, we anticipate that this solution corresponds to the orthogonal projection of \(\mathbf{t}\) onto the subspace \(\mathcal{S}\). This is indeed the case, as can easily be verified by noting that the solution for \(\mathbf{y}\) is given by \(\boldsymbol{\Phi} \mathbf{w}_{\mathrm{ML}}\), and then confirming that this takes the form of an orthogonal projection.
+ In practice, a direct solution of the normal equations can lead to numerical difficulties when \(\boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\) is close to singular: When two or more of the basis vectors \(\varphi_{j}\) are co-linear, or nearly so, the resulting parameter values can have large magnitudes. Such near degeneracy will not be uncommon when dealing with real datasets. The resulting numerical difficulties can be addressed using the technique of singular value decomposition (SVD).
+ Note that the addition of /regularization/ ensures that the matrix is non-singular, even in the presence of degeneracy.
*** Algorithms
+ Many estimators for the problem of linear regression have closed form expressions so that a /learning algorithm/ is not necessary per se.
+ For estimators that do not have closed form expressions, we can use a learning algorithm that suitably approximates the estimator.
+ Learning algorithms can be based on /batch processing/ or /sequential processing/.
+ In a /batch algorithms/, the entire dataset \((\mathbf{X},\,\mathbf{T}) \equiv (\mathbf{x}_{n},\,\mathbf{t}_{n})_{n=1,\,\ldots,\,N}\) is processed in one go. In other words, the parameter vector \(\mathbf{w}\) is updated once for every presentation of the entire dataset \((\mathbf{X}_{n},\,\mathbf{T}_{n})\).
+ In a /sequential algorithm/, the \(N\) observations in the dataset \((\mathbf{X},\,\mathbf{T}) \equiv (\mathbf{x}_{n},\,\mathbf{t}_{n})_{n=1,\,\ldots,\,N}\) are considered one at a time and the parameter vector \(\mathbf{w}\) is updated after the the presentation of each \((\mathbf{x}_{n},\,\mathbf{t}_{n})\).
+ Batch algorithms can be computationally expensive for large datasets. If the dataset is sufficiently large, it may be worthwhile to use sequential algorithms instead.
**** Least-mean-squares (LMS) algorithm
+ The *least-mean-squares (LMS) algorithm* is a sequential learning algorithm for approximating the OLE estimator of the parameter vector \(\mathbf{w}\) of a linear regressor \(y(\mathbf{x}, \mathbf{w})\).
  \begin{algorithm}
  \caption{Least-mean-squares (LMS) algorithm}
  \begin{algorithmic}[1]
  \State Initialize the learning rate \(\eta\)
  \State Initialize the parameter vector \(\mathbf{w}^{(0)}\)
  \For{iteration number \(\tau\) presenting observation \((\mathbf{x}_{n},\,t_{n})\)}
      \State Update \(\mathbf{w}\) using the SGD update rule:
      \[
      \mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}+\eta \left(t_{n}-\mathbf{w}^{(\tau) \mathrm{T}} \boldsymbol{\phi} (\mathbf{x}_{n})\right) \boldsymbol{\phi} (\mathbf{x}_{n})
      \]
  \EndFor
  \end{algorithmic}
  \end{algorithm}
** Logistic regression
:LOGBOOK:
CLOCK: [2024-06-14 Fri 23:55]--[2024-06-15 Sat 00:12] =>  0:17
:END:
*** Preliminaries
**** Logistic sigmoid function
#+NAME: Logistic sigmoid function
#+begin_definition latex
The logistic sigmoid function \( \sigma(x) \) is by definition
\begin{align*}
\sigma(x) \equiv \frac{1}{1+\exp (-x)}
\end{align*}
#+end_definition
A plot of the logistic sigmoid function \(\sigma(x)\) is shown in red, together with the scaled probit function \(\Phi(\lambda a)\), for \(\lambda^{2}=\pi / 8\), shown in dashed blue, where \(\Phi(a)\) is defined by (4.114). The scaling factor \(\pi / 8\) is chosen so that the derivatives of the two curves are equal for \(a=0\).
#+ATTR_HTML: :width 500px
[[file:~/.local/images/prml-4-9.png]]
The term 'sigmoid' means S-shaped. This type of function is sometimes also called a 'squashing function' because it maps the whole real axis into a finite interval.
***** Properties
+ It satisfies the following symmetry property
\begin{align*}
\sigma(-x)=1-\sigma(x).
\end{align*}
+ The derivative of the logistic sigmoid function can be expressed in terms of the sigmoid function itself
\begin{align*}
\mathrm{D}_x \sigma = \sigma (1-\sigma)
\end{align*}
**** Logit function
#+NAME: Logit function
#+begin_definition latex
The logit function is the inverse of the logistic sigmoid functions and is given by
\begin{align*}
x=\ln \left(\frac{\sigma}{1-\sigma}\right).
\end{align*}
#+end_definition
**** Softmax function
#+NAME: Softmax function
#+begin_definition latex
The softmax or normalized exponential function \(\sigma: \mathbb{R}^K \rightarrow(0,1)^K\), where \(K \geq 1\), takes a vector \(\mathbf{x}=\left(x_1, \ldots, x_K\right) \in \mathbb{R}^K\) and computes each component of vector \(\sigma(\mathbf{x}) \in(0,1)^K\) with
\begin{align*}
\sigma(\mathbf{x})_k=\frac{\exp(x_k)}{\sum_{j=1}^K \exp(x_j)}
\end{align*}
#+end_definition
+ The softmax function is a multiclass generalization of the logistic sigmoid.
**** Probabilistic generative models
Probabilistic generative models model the class-conditional densities \(p\left(\mathbf{x} \mid \mathcal{C}_{k}\right)\), as well as the class priors \(p\left(\mathcal{C}_{k}\right)\), and then use these to compute posterior probabilities \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)\) using Bayes's theorem.
***** K = 2 classes
Consider the classification problem with \( K = 2 \) classes. The posterior probability for class \(\mathcal{C}_{1}\) can be written as
\begin{align*}
p\left(\mathcal{C}_{1} \mid \mathbf{x}\right) & =\frac{p\left(\mathbf{x} \mid \mathcal{C}_{1}\right) p\left(\mathcal{C}_{1}\right)}{p\left(\mathbf{x} \mid \mathcal{C}_{1}\right) p\left(\mathcal{C}_{1}\right)+p\left(\mathbf{x} \mid \mathcal{C}_{2}\right) p\left(\mathcal{C}_{2}\right)} =\frac{1}{1+\exp (-a)}=\sigma(a),  \tag{1}
\end{align*}
where we have defined
\begin{align*}
a=\ln \frac{p\left(\mathbf{x} \mid \mathcal{C}_{1}\right) p\left(\mathcal{C}_{1}\right)}{p\left(\mathbf{x} \mid \mathcal{C}_{2}\right) p\left(\mathcal{C}_{2}\right)} \tag{2}
\end{align*}
and \(\sigma(a)\) is the logistic sigmoid function. The inverse of the logistic sigmoid is given by
\begin{align*}
a=\ln \left(\frac{\sigma}{1-\sigma}\right) \tag{3}
\end{align*}
and is known as the logit function. It represents the log of the ratio of probabilities \(\ln \left[p\left(\mathcal{C}_{1} \mid \mathbf{x}\right) / p\left(\mathcal{C}_{2} \mid \mathbf{x}\right)\right]\) for the two classes, also known as the /log odds/.
***** K > 2 classes
Consider the classification problem with \( K > 2 \) classes. The posterior probability for class \(\mathcal{C}_{1}\) can be written as
\begin{align*}
p\left(\mathcal{C}_{k} \mid \mathbf{x}\right) & =\frac{p\left(\mathbf{x} \mid \mathcal{C}_{k}\right) p\left(\mathcal{C}_{k}\right)}{\sum_{j} p\left(\mathbf{x} \mid \mathcal{C}_{j}\right) p\left(\mathcal{C}_{j}\right)} =\frac{\exp \left(a_{k}\right)}{\sum_{j} \exp \left(a_{j}\right)},
\tag{1}
\end{align*}
where we have defined
\begin{align*}
a_{k}=\ln p\left(\mathbf{x} \mid \mathcal{C}_{k}\right) p\left(\mathcal{C}_{k}\right) \tag{2}
\end{align*}
and \( \exp(a_k) / \sum_j \exp(a_j) \) is called the softmax or normalized exponential. The softmax function is a multiclass generalization of the logistic sigmoid. It is called "softmax" because it represents a smoothed version of the "max" function: if \(a_{k} \gg a_{j}\) for all \(j \neq k\), then \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right) \simeq 1\), and \(p\left(\mathcal{C}_{j} \mid \mathbf{x}\right) \simeq 0\).
*** Definition
Under rather general assumptions, the posterior probability of class \(\mathcal{C}_{1}\) can be written as a logistic sigmoid function acting on a linear function of the feature vector \(\phi\) so that
\begin{align*}
p\left(\mathcal{C}_{1} \mid \boldsymbol{\phi}\right)=y(\boldsymbol{\phi})=\sigma (\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}) \tag{1}
\end{align*}
with \(p\left(\mathcal{C}_{2} \mid \phi\right)=1-p\left(\mathcal{C}_{1} \mid \phi\right)\). Here \(\sigma(\cdot)\) is the logistic sigmoid function.
#+BEGIN_COMMENT
In the terminology of statistics, this model is known as logistic regression, although it should be emphasized that this is a model for classification rather than regression.
#+END_COMMENT
For an \(M\)-dimensional feature space \(\phi\), this model has \(M\) adjustable parameters. By contrast, if we had fitted Gaussian class conditional densities using maximum likelihood, we would have used \(2 M\) parameters for the means and \(M(M+1) / 2\) parameters for the (shared) covariance matrix. Together with the class prior \(p\left(\mathcal{C}_{1}\right)\), this gives a total of \(M(M+5) / 2+1\) parameters, which is quadratic in \(M\), in contrast to the linear dependence on \(M\) of the number of parameters in logistic regression. *For large values of* \(M\), *there is a clear advantage in working with the logistic regression model directly.*
The logistic regression logistic regression model is defined as
\begin{align*}
p\left(\mathcal{C}_{1} \mid \boldsymbol{\phi}\right)=y(\boldsymbol{\phi})=\sigma (\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}) \tag{1}
\end{align*}
with \(p\left(\mathcal{C}_{2} \mid \phi\right)=1-p\left(\mathcal{C}_{1} \mid \phi\right)\). Here \(\sigma(\cdot)\) is the logistic sigmoid function. For an \(M\)-dimensional feature space \(\phi\), this model has \(M\) adjustable parameters.
*** Extensions
**** Multiclass logistic regression
**** Probit regression
**** Bayesian logistic regression
*** Estimation
**** ML estimation
For a data set \(\left\{\phi_{n}, t_{n}\right\}\), where \(t_{n} \in\{0,1\}\) and \(\phi_{n}=\boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\), with \(n=\) \(1, \ldots, N\), the likelihood function can be written as
\begin{align*}
p(\mathbf{t} \mid \mathbf{w})=\prod_{n=1}^{N} y_{n}^{t_{n}}\left\{1-y_{n}\right\}^{1-t_{n}} \tag{2}
\end{align*}
where \(\mathbf{t}=\left(t_{1}, \ldots, t_{N}\right)^{\mathrm{T}}\) and \(y_{n}=p\left(\mathcal{C}_{1} \mid \boldsymbol{\phi}_{n}\right)\). 
As usual, we can define an error function by taking the negative logarithm of the likelihood, which gives the *cross-entropy error function* given by
\begin{align*}
E(\mathbf{w})=-\ln p(\mathbf{t} \mid \mathbf{w})=-\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\} \tag{3}
\end{align*}
where \(y_{n}=\sigma\left(a_{n}\right)\) and \(a_{n}=\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}_{n}\). Taking the gradient of the error function with respect to \(\mathbf{w}\), we obtain
\begin{align*}
\nabla E(\mathbf{w})=\sum_{n=1}^{N}\left(y_{n}-t_{n}\right) \phi_{n} \tag{5}
\end{align*}
where we have made use of (2). The contribution to the gradient from data point \(n\) is given by the 'error' \(y_{n}-t_{n}\) between the target value and the prediction of the model, times the basis function vector \(\phi_{n}\). Furthermore, comparison with OLS for linear regression shows that this takes precisely the same form as the gradient of the sum-of-squares error function
\[
E_D(\mathbf{w}) = - \frac{1}{2} \sum_{n=1}^{N} \{t_n - \mathbf{w}^\mathrm{T} \boldsymbol{\phi}(\mathbf{x}_n)\}^2. \tag{6}
\]
for the linear regression model.
If desired, we could make use of the result (5) to give a SGD based sequential algorithm in which patterns are presented one at a time and the weight vectors is updated using
\begin{align*}
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta \nabla E_{n} \tag{7}
\end{align*}
similar to the LMS algorithm used for linear regression in which \(\nabla E_{n}\) is the \(n^{\text {th }}\) term in (5).
Maximum likelihood estimators for logistic regression can exhibit severe over-fitting for data sets that are linearly separable. This arises because the maximum likelihood solution occurs when the hyperplane corresponding to \(\sigma=0.5\), equivalent to \(\mathbf{w}^{\mathrm{T}} \phi=\) 0, separates the two classes and the magnitude of \(\mathbf{w}\) goes to infinity. In this case, the logistic sigmoid function becomes infinitely steep in feature space, corresponding to a step function, so that every training point from each class \(k\) is assigned a posterior probability \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)=1\). The problem will arise even if the number of data points is large compared with the number of parameters in the model, so long as the training data set is linearly separable. The singularity can be avoided by inclusion of a prior and finding a MAP solution for \(\mathbf{w}\), or equivalently by adding a regularization term to the cross entropy error function.
**** MAP estimation
*** Algorithms
** Perceptrons
:LOGBOOK:
CLOCK: [2024-06-14 Fri 15:58]--[2024-06-14 Fri 16:33] =>  0:35
:END:
*** Definitions
The perceptron is a linear discriminant functions which occupies an important place in the history of pattern recognition algorithms. It corresponds to a two-class model in which the input vector \(\mathrm{x}\) is first transformed using a fixed nonlinear transformation to give a feature vector \(\phi(\mathrm{x})\), and this is then used to construct a generalized linear model of the form
\begin{align*}
y(\mathbf{x})=f\big(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})\big) \tag{1}
\end{align*}
where the nonlinear activation function \(f(\cdot)\) is given by a step function of the form
\begin{align*}
f(a)= 
\begin{cases}
+1, & a \geqslant 0 \\
 -1, & a<0
\end{cases}
\tag{2}
\end{align*}
The vector \(\phi(\mathbf{x})\) will typically include a bias component \(\phi_{0}(\mathbf{x})=1\).
#+BEGIN_COMMENT
As a linear classifier, the single-layer perceptron is the simplest feedforward neural network.
#+END_COMMENT
**** Perceptron criterion
#+BEGIN_COMMENT
The algorithm used to determine the parameters \(\mathbf{w}\) of the perceptron can most easily be motivated by error function minimization. A natural choice of error function would be the total number of misclassified patterns. However, this does not lead to a simple learning algorithm because the error is a piecewise constant function of \(\mathbf{w}\), with discontinuities wherever a change in \(\mathbf{w}\) causes the decision boundary to move across one of the data points. Methods based on changing w using the gradient of the error function cannot then be applied, because the gradient is zero almost everywhere.
#+END_COMMENT
We consider an error function known as the *perceptron criterion* given by
\begin{align*}
E_{\mathrm{P}}(\mathbf{w})=-\sum_{n \in \mathcal{M}} \langle \mathbf{w},\, \boldsymbol{\phi}_{n} t_{n} \rangle. \tag{3}
\end{align*}
where \(\mathcal{M}\) denotes the set of all misclassified patterns. The perceptron criterion is motivated by the simple fact that we are seeking a weight vector \(\mathbf{w}\) such that patterns \(\mathbf{x}_{n}\) in class \(\mathcal{C}_{1}\) will have \(\langle \mathbf{w},\, \boldsymbol{\phi}_{n} \rangle>0\), whereas patterns \(\mathbf{x}_{n}\) in class \(\mathcal{C}_{2}\) have \(\langle \mathbf{w},\, \boldsymbol{\phi}_{n} \rangle<0\). Using the target values \(t=+1\) for class \(\mathcal{C}_{1}\) and \(t=-1\) for class \(\mathcal{C}_{2}\) we would like all patterns to satisfy \(\langle \mathbf{w},\, \boldsymbol{\phi}_{n} (\mathbf{x}_n) t_n \rangle>0\). The perceptron criterion associates zero error with any pattern that is correctly classified, whereas for a misclassified pattern \(\mathbf{x}_{n}\) it tries to minimize the quantity \(-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right) t_{n}\).
**** Perceptron learning algorithm
We now apply the SGD algorithm to the error function in (3). The change in the weight vector \(\mathbf{w}\) is then given by
\begin{align*}
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta \nabla E_{\mathrm{P}}(\mathbf{w})=\mathbf{w}^{(\tau)}+\eta \phi_{n} t_{n} \tag{4}
\end{align*}
where \(\eta\) is the learning rate parameter and \(\tau\) is an integer that indexes the steps of the algorithm. Because the perceptron function \(y(\mathbf{x}, \mathbf{w})\) is unchanged if we multiply \(\mathbf{w}\) by a constant, we can set the learning rate parameter \(\eta\) equal to 1 without loss of generality.
\begin{align*}
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}- \nabla E_{\mathrm{P}}(\mathbf{w})=\mathbf{w}^{(\tau)}+ \phi_{n} t_{n} \tag{5}
\end{align*}
The perceptron learning algorithm has a simple interpretation, as follows. We cycle through the training patterns in turn, and for each pattern \(\mathbf{x}_{n}\) we evaluate the *perceptron function* \(y(\mathbf{x}, \mathbf{w}) = f\big(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})\big)\). If the pattern is correctly classified, then the weight vector remains unchanged, whereas if it is incorrectly classified, then for class \(\mathcal{C}_{1}\) we add the vector \(\phi\left(\mathbf{x}_{n}\right)\) onto the current estimate of weight vector \(\mathbf{w}\) while for class \(\mathcal{C}_{2}\) we subtract the vector \(\phi\left(\mathbf{x}_{n}\right)\) from \(\mathbf{w}\).
The figures below give an illustration of the convergence of the perceptron learning algorithm, showing data points from two classes (red and blue) in a two-dimensional feature space \(\left(\phi_{1}, \phi_{2}\right)\). 
#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-4-7a.png]]
Shown above are the initial parameter vector w shown as a black arrow together with the corresponding decision boundary (black line), in which the arrow points towards the decision region which classified as belonging to the red class. The data point circled in green is misclassified and so its feature vector is added to the current weight vector, giving the new decision boundary shown below.
#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-4-7b.png]]
#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-4-7c.png]]
Shown above is the next misclassified point to be considered, indicated by the green circle, and its feature vector is again added to the weight vector giving the decision boundary shown below for which all data points are correctly classified.
#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-4-7d.png]]
**** Perceptron convergence theorem
#+NAME: Single perceptron update
#+begin_lemma latex
If we consider the effect of a single update in the perceptron learning algorithm, we see that the contribution to the error from a misclassified pattern will be reduced.
#+end_lemma
#+NAME: Single perceptron update
#+begin_proof latex
To see this, take the inner product of both sides of \( - \mathbf{w}^{(\tau+1)} = - \mathbf{w}^{(\tau)} - \phi_{n} t_{n} \) and \( \boldsymbol{\phi_n} t_n \):
\begin{align*}
- \langle \mathbf{w}^{(\tau+1)},\, \boldsymbol{\phi}_{n} t_{n} \rangle = - \langle \mathbf{w}^{(\tau)},\, \boldsymbol{\phi}_{n} t_{n} \rangle - \langle \boldsymbol{\phi}_{n} t_{n},\, \boldsymbol{\phi}_{n} t_{n} \rangle < - \langle \mathbf{w}^{(\tau)},\, \boldsymbol{\phi}_{n} t_{n} \rangle. \tag{6}
\end{align*}
In the final inequality, we have made use of \(\langle \boldsymbol{\phi}_{n} t_{n},\, \boldsymbol{\phi}_{n} t_{n} \rangle = \left\|\phi_{n} t_{n}\right\|^{2}>0\).
#+end_proof
The perceptron convergence theorem
#+NAME: Linear separability
#+begin_definition latex
Let \(X = (\mathbf{x}_i)_{i=1,\ldots,k}\) and \(Y = (\mathbf{y}_j)_{j=1,\ldots,l}\) be two sets of points in a vector space \( \mathbb{R}^{n} \). Then \(X\) and \(Y\) are linearly separable if there exist at least one vector \( \mathbf{w} \in \mathbb{R}^n \), and an associated scalar \( c \) such that:
1)\( \langle \mathbf{x}_i,\, \mathbf{w} \rangle > c \) for every \( \mathbf{x}_i \in X \),
1)\( \langle \mathbf{y}_i,\, \mathbf{w} \rangle < c \) for every \( \mathbf{y}_i \in Y \).
#+end_definition
The perceptron learning rule is *not* guaranteed to reduce the total error function at each stage. This is because the change in weight vector may have caused some previously correctly classified patterns to become misclassified. Nevertheless, the *perceptron convergence theorem* states that if there exists at least one exact solution (meaning the data is linearly separable), then the perceptron learning algorithm is guaranteed to find an exact solution in a finite number of steps. Familiarity with /Cauchy-Schwarz inequality/ and the notion of /linear separability/ are prerequisites for the study of the perceptron convergence theorem.
#+NAME: Cauchy-Schwarz inequality
#+begin_theorem latex
Let \(\mathbf{v}\) and \mathbf{w} be elements of a vector space with norms \( | \mathbf{v} | \) and \( | \mathbf{w} | \) respectively. We then have
\begin{align*}
|\langle \mathbf{v}, \, \mathbf{w} \rangle| \leqslant | \mathbf{v} | | \mathbf{w} |.
\end{align*}
where \( \langle \cdot, \, \cdot \rangle \) denotes the inner product operation.
#+end_theorem
#+NAME: Perceptron convergence theorem
#+begin_theorem latex
Let \(D\) be a linearly separable data set such that \(\max_{(\mathbf{x},\,t) \in D} |\boldsymbol{\phi}(\mathbf{x})|^2 = R\). Suppose that the unit vector \( \mathbf{w}^{\ast} \) separates \( \mathbf{x} \in D \) with \( t = + 1 \) from \( \mathbf{x} \in D \) with \( t = -1 \) in the feature space \( \{\boldsymbol{\phi} (\mathbf{x}) \} \) with a margin \(\gamma\):
\begin{align*}
\gamma \equiv \min_{(\mathbf{x},\,t) \in D} \langle \mathbf{w}^{\ast},\, \boldsymbol{\phi} t \rangle 
\end{align*}
Then the perceptron learning algorithm converges after making at most \((R / \gamma)^2\) mistakes, for any learning rate, and any method of sampling from the data set.
#+end_theorem
#+NAME: Perceptron convergence theorem
#+begin_proof latex
To prove the perceptron convergence theorem, we start by considering the perceptron update rule:
\begin{align*}
\mathbf{w}^{(\tau+1)} = \mathbf{w}^{(\tau)} + \boldsymbol{\phi}(\mathbf{x}) t
\end{align*}
where \( \mathbf{w}^{(\tau)} \) is the weight vector at iteration \( \tau \), \( t \) is the true label of the data point \( \mathbf{x} \), and \( \boldsymbol{\phi}(\mathbf{x}) \) is the feature-mapped input vector.
Step 1: Projection Increase
From Step 1, each time a mistake occurs and the perceptron updates, we have:
\[
\langle \mathbf{w}^{(\tau+1)}, \mathbf{w}^* \rangle = \langle \mathbf{w}^{(\tau)}, \mathbf{w}^* \rangle + t \langle \boldsymbol{\phi}(\mathbf{x}), \mathbf{w}^* \rangle
\]
Given \( t \langle \boldsymbol{\phi}(\mathbf{x}), \mathbf{w}^* \rangle \geq \gamma \) (because \( t \) is the correct label and \( \boldsymbol{\phi}(\mathbf{x}) \) is on the correct side of the margin defined by \( \mathbf{w}^* \)), each update step due to a mistake increases the projection on \( \mathbf{w}^* \) by at least \( \gamma \):
\[
\langle \mathbf{w}^{(\tau+1)}, \mathbf{w}^* \rangle \geq \langle \mathbf{w}^{(\tau)}, \mathbf{w}^* \rangle + \gamma
\]
Now, if we sum this inequality over \( k \) mistakes, where \( k \) indices are from 1 to \( k \), each corresponding to a mistake, we accumulate the increases:
\[
\langle \mathbf{w}^{(1)}, \mathbf{w}^* \rangle \geq \langle \mathbf{w}^{(0)}, \mathbf{w}^* \rangle + \gamma
\]
\[
\langle \mathbf{w}^{(2)}, \mathbf{w}^* \rangle \geq \langle \mathbf{w}^{(1)}, \mathbf{w}^* \rangle + \gamma
\]
\[
\vdots
\]
\[
\langle \mathbf{w}^{(k)}, \mathbf{w}^* \rangle \geq \langle \mathbf{w}^{(k-1)}, \mathbf{w}^* \rangle + \gamma
\]
Adding these inequalities together (telescoping sum), we get:
\[
\langle \mathbf{w}^{(k)}, \mathbf{w}^* \rangle \geq \langle \mathbf{w}^{(0)}, \mathbf{w}^* \rangle + k\gamma
\]
Since \( \mathbf{w}^{(0)} \) is typically initialized to zero in the perceptron algorithm, we have \( \langle \mathbf{w}^{(0)}, \mathbf{w}^* \rangle = 0 \). Therefore:
\[
\langle \mathbf{w}^{(k)}, \mathbf{w}^* \rangle \geq k\gamma
\]
This relation shows that after \( k \) mistakes, the inner product between the current weights and the optimal weight vector has increased by at least \( k\gamma \), indicating the progress made towards achieving a correct classification. This is an accumulation effect of the corrections made at each mistake.
Step 2: Norm Increase
The norm of the weight vector after the update is:
\begin{align*}
|\mathbf{w}^{(\tau+1)}|^2 &= |\mathbf{w}^{(\tau)} + \boldsymbol{\phi}(\mathbf{x}) t|^2 \\
&= |\mathbf{w}^{(\tau)}|^2 + 2 t \langle \mathbf{w}^{(\tau)}, \boldsymbol{\phi}(\mathbf{x}) \rangle + |\boldsymbol{\phi}(\mathbf{x})|^2
\end{align*}
Since \( \langle \mathbf{w}^{(\tau)}, \boldsymbol{\phi}(\mathbf{x}) \rangle \leq 0 \) when a mistake is made, and \( |\boldsymbol{\phi}(\mathbf{x})|^2 \leq R \):
\begin{align*}
|\mathbf{w}^{(\tau+1)}|^2 \leq |\mathbf{w}^{(\tau)}|^2 + R
\end{align*}
Step 3: Cauchy-Schwarz inequality
Combining the results from steps 1 and 2, we observe that after \( k \) mistakes:
\begin{align*}
\langle \mathbf{w}_k, \mathbf{w}^* \rangle \geq k \gamma \quad \text{and} \quad |\mathbf{w}_k|^2 \leq k R
\end{align*}
Using the Cauchy-Schwarz inequality, \( |\langle \mathbf{w}_k, \mathbf{w}^* \rangle | \leq |\mathbf{w}_k| |\mathbf{w}^*| \). Substituting and rearranging gives:
\begin{align*}
k \gamma \leq \sqrt{k} R |\mathbf{w}^*|
\end{align*}
Squaring both sides and simplifying gives the maximum number of mistakes \( k \) before convergence:
\begin{align*}
k \leq \left( \frac{R |\mathbf{w}^*|}{\gamma} \right)^2
\end{align*}
Substituting \( |\mathbf{w}^*| = 1 \), since \( \mathbf{w}^* \) is a unit vector, yields
\begin{align*}
k \leq \left( R / \gamma \right)^2
\end{align*}
Thus the perceptron learning algorithm converges after making at most \((R / \gamma)^2\) mistakes, for any learning rate, and any method of sampling from the data set.
#+end_proof
*** Limitations
1) For data sets that are *not* linearly separable, the perceptron learning algorithm will never converge.
2) The perceptron does not generalize readily to \(K>2\) classes.
3) Even when the data set is linearly separable, there may be multiple solutions. The solution found by the perceptron learning algorithm will depend on i) the initialization of the parameters, and ii) the order of presentation of the data points.
4) The [[id:edbb570a-e729-478e-b67c-5b50f3dbbc65][perceptron convergence theorem]] notwithstanding, the number of steps required to achieve convergence is indeterminate. We cannot distinguish a problem that is *not* [[id:46d7eeb7-d01e-46a8-a4ea-6b27285b265f][linearly separabile]] from one that is [[id:46d7eeb7-d01e-46a8-a4ea-6b27285b265f][linearly separable]] but is simply slow to converge.
5) Needless to say, the convergence of the perceptron learning algorithm is a bad diagnostic criterion for [[id:46d7eeb7-d01e-46a8-a4ea-6b27285b265f][linear separability]]: 
6) Being a [[id:43aa39fd-b16a-429b-b59a-408240ae3523][discriminant function]], the perceptron does not provide probabilistic outputs (see [[id:588541e6-4692-4208-b6e9-7847bf42dc5a][the many merits of posterior-class probabilities in classification problems]]).
** Artificial neural networks (ANN)
:LOGBOOK:
CLOCK: [2024-06-19 Wed 12:11]--[2024-06-19 Wed 15:17] =>  3:06
:END:
*** Introduction
Linear models for regression are of the form
\[
y(\mathbf{x}, \, \mathbf{w})=\sum_{j=0}^{M-1} w_{j} \cdot \phi_{j}(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \phi(\mathbf{x}),
\]
where \( (\phi_{i}(\mathbf{x}))_{i=0\ldots M-1} \) is a set of a fixed number \( M \) of basis functions, \( \mathbf{w} \) is the set of fixed number \( M \) of model parameters. Recall that \( y(\mathbf{x}, \, \mathbf{w}) \) is a non-linear function of \( \mathbf{x} \) because of the basis functions \( (\phi_{i}(\mathbf{x}))_{i=0\ldots M-1} \), but a linear function of \( \mathbf{w} \).
Linear models for classification are of the form 
\begin{align*}
y(\mathbf{x}, \, \mathbf{w}) = f \bigg(\sum_{j=0}^{M-1} w_{j} \cdot \phi_{j}(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \phi(\mathbf{x})\bigg)
\end{align*}
where \( (\phi_{i}(\mathbf{x}))_{i=0\ldots M-1} \) is a set of a fixed number \( M \) of basis functions, \( \mathbf{w} \) is the set of fixed number \( M \) of model parameters, and \( f \) is an appropriately chosen activation function (Heaviside step function for the perception, logistic sigmoid function for 2-class logistic regression, softmax function for K-class logistic regression). Recall that \( y(\mathbf{x}, \, \mathbf{w}) \) is a non-linear function of \( \mathbf{x} \) because of the basis functions \( (\phi_{i}(\mathbf{x}))_{i=0\ldots M-1} \), and a /non-linear/ function of \( \mathbf{w} \) as well because of the activation function \( f \). Even still, such models are /linear/ in the sense that the /decision surfaces/, defined by \( y(\mathbf{x},\, \mathbf{w}) = \text{ const. }\), or equivalently \( \mathbf{w}^{\mathrm{T}} \phi(\mathbf{x}) = \text{ const. } \), are /linear/ functions of \( \mathbf{w} \).
Linear models for regression and classification both consist of a fixed number \( M \) of basis functions. These models are analytically and computationally tractable but are limited by the curse of dimensionality. In order to apply such models to large scale problems, it is necessary to adapt the basis functions to the data.
Two typical approaches for adapting the basis functions are:
  1) *Define a fixed number of basis functions with fixed functional forms and select a certain subset of these during training*. One example of a model that uses this approach are /support vector machines/ (SVMs) which can be used as /non-linear regressors/ or as /discriminant function classifiers/. Training SVMs involves /convex optimization/. Another example is the /relevance vector machines/ (RVMs) which can be used as /non-linear regressors/ or as /probabilistic discriminative classifiers/. Unlike SVMs, training RVMs involves /non-convex 
optimization/.
  2) *Define a fixed number of basis functions but adaptive functional forms, by using parametric forms, and adapt the basis functions to the data by optimizing the parameters that define them.* The most veritable example of a model that uses this approach is the /feed-forward neural network/. Training feed-forward neural networks involves /non-convex optimization/, so such models are harder to train in comparison to SVMs and, in general, not any easier to train in comparison to RVMs. However, once trained, the resulting model is significantly more compact than a SVM or an RVM having the same generalization performance. This means that while the statistical inference (training) of such models may often be as hard or harder than the inference of an RVM that achieves the same generalization, making prediction using these models on new data is relatively fast.
We restrict our focus on models the use the later approach, in particular the feed-forward neural network. The term 'neural network' has its origins in attempts to find mathematical representations of information processing in biological systems. Our focus however will be on the interpretation of neural networks as statistical models. In particular, we will be concerned with a specific class of neural networks called /the multilayer perceptron/.
*** Definitions
The linear models for regression and classification discussed in Chapters 3 and 4, respectively, are based on linear combinations of fixed nonlinear basis functions \(\phi_{j}(\mathbf{x})\) and take the form
\[
y(\mathbf{x}, \mathbf{w})=f\left(\sum_{j=1}^{M} w_{j} \phi_{j}(\mathbf{x})\right)
\]
where \(f(\cdot)\) is a nonlinear activation function in the case of classification and is the identity in the case of regression. Our goal is to extend this model by making the basis functions \(\phi_{j}(\mathbf{x})\) depend on parameters and then to allow these parameters to be adjusted, along with the coefficients \(\left\{w_{j}\right\}\), during training. There are, of course, many ways to construct parametric nonlinear basis functions. Neural networks use basis functions that follow the same form as (5.1), so that each basis function is itself a nonlinear function of a linear combination of the inputs, where the coefficients in the linear combination are adaptive parameters.
This leads to the basic neural network model, which can be described a series of functional transformations. First we construct \(M\) linear combinations of the input variables \(x_{1}, \ldots, x_{D}\) in the form
\[
a_{j}=\sum_{i=1}^{D} w_{j i}^{(1)} x_{i}+w_{j 0}^{(1)}
\]
where \(j=1, \ldots, M\), and the superscript (1) indicates that the corresponding parameters are in the first 'layer' of the network. We shall refer to the parameters \(w_{j i}^{(1)}\) as weights and the parameters \(w_{j 0}^{(1)}\) as biases, following the nomenclature of Chapter 3. The quantities \(a_{j}\) are known as activations. Each of them is then transformed using a differentiable, nonlinear activation function \(h(\cdot)\) to give
\[
z_{j}=h\left(a_{j}\right)
\]
These quantities correspond to the outputs of the basis functions in (5.1) that, in the context of neural networks, are called hidden units. The nonlinear functions \(h(\cdot)\) are generally chosen to be sigmoidal functions such as the logistic sigmoid or the 'tanh' function. Following (5.1), these values are again linearly combined to give output unit activations
\[
a_{k}=\sum_{j=1}^{M} w_{k j}^{(2)} z_{j}+w_{k 0}^{(2)}
\]
where \(k=1, \ldots, K\), and \(K\) is the total number of outputs. This transformation corresponds to the second layer of the network, and again the \(w_{k 0}^{(2)}\) are bias parameters. Finally, the output unit activations are transformed using an appropriate activation function to give a set of network outputs \(y_{k}\). The choice of activation function is determined by the nature of the data and the assumed distribution of target variables
#+HTML_ATTR: :width 500px
[[file:~/.local/images/prml-5-1.png]]
#+CAPTION: Network diagram for the twolayer neural network. The input, hidden, and output variables are represented by nodes, and the weight parameters are represented by links between the nodes, in which the bias parameters are denoted by links coming from additional input and hidden variables x0 and z0. Arrows denote the direction of information flow through the network during forward propagation.
and follows the same considerations as for linear models discussed in Chapters 3 and 4. Thus for standard regression problems, the activation function is the identity so that \(y_{k}=a_{k}\). Similarly, for multiple binary classification problems, each output unit activation is transformed using a logistic sigmoid function so that
\[
y_{k}=\sigma\left(a_{k}\right)
\]
where
\[
\sigma(a)=\frac{1}{1+\exp (-a)}
\]
Finally, for multiclass problems, a softmax activation function of the form (4.62) is used. The choice of output unit activation function is discussed in detail in Section 5.2.
We can combine these various stages to give the overall network function that, for sigmoidal output unit activation functions, takes the form
\[
y_{k}(\mathbf{x}, \mathbf{w})=\sigma\left(\sum_{j=1}^{M} w_{k j}^{(2)} h\left(\sum_{i=1}^{D} w_{j i}^{(1)} x_{i}+w_{j 0}^{(1)}\right)+w_{k 0}^{(2)}\right)
\]
where the set of all weight and bias parameters have been grouped together into a vector \(\mathbf{w}\). Thus the neural network model is simply a nonlinear function from a set of input variables \(\left\{x_{i}\right\}\) to a set of output variables \(\left\{y_{k}\right\}\) controlled by a vector \(\mathbf{w}\) of adjustable parameters.
This function can be represented in the form of a network diagram as shown in Figure 5.1. The process of evaluating (5.7) can then be interpreted as a forward propagation of information through the network. It should be emphasized that these diagrams do not represent probabilistic graphical models of the kind to be considered in Chapter 8 because the internal nodes represent deterministic variables rather than stochastic ones. For this reason, we have adopted a slightly different graphical notation for the two kinds of model. We shall see later how to give a probabilistic interpretation to a neural network.
As discussed in Section 3.1, the bias parameters in (5.2) can be absorbed into the set of weight parameters by defining an additional input variable \(x_{0}\) whose value is clamped at \(x_{0}=1\), so that (5.2) takes the form
\[
a_{j}=\sum_{i=0}^{D} w_{j i}^{(1)} x_{i}
\]
We can similarly absorb the second-layer biases into the second-layer weights, so that the overall network function becomes
\[
y_{k}(\mathbf{x}, \mathbf{w})=\sigma\left(\sum_{j=0}^{M} w_{k j}^{(2)} h\left(\sum_{i=0}^{D} w_{j i}^{(1)} x_{i}\right)\right)
\]
As can be seen from Figure 5.1, the neural network model comprises two stages of processing, each of which resembles the perceptron model of Section 4.1.7, and for this reason the neural network is also known as the multilayer perceptron, or MLP. A key difference compared to the perceptron, however, is that the neural network uses continuous sigmoidal nonlinearities in the hidden units, whereas the perceptron uses step-function nonlinearities. This means that the neural network function is differentiable with respect to the network parameters, and this property will play a central role in network training.
If the activation functions of all the hidden units in a network are taken to be linear, then for any such network we can always find an equivalent network without hidden units. This follows from the fact that the composition of successive linear transformations is itself a linear transformation. However, if the number of hidden units is smaller than either the number of input or output units, then the transformations that the network can generate are not the most general possible linear transformations from inputs to outputs because information is lost in the dimensionality reduction at the hidden units. In Section 12.4.2, we show that networks of linear units give rise to principal component analysis. In general, however, there is little interest in multilayer networks of linear units.
The network architecture shown in Figure 5.1 is the most commonly used one in practice. However, it is easily generalized, for instance by considering additional layers of processing each consisting of a weighted linear combination of the form (5.4) followed by an element-wise transformation using a nonlinear activation function. Note that there is some confusion in the literature regarding the terminology for counting the number of layers in such networks. Thus the network in Figure 5.1 may be described as a 3-layer network (which counts the number of layers of units, and treats the inputs as units) or sometimes as a single-hidden-layer network (which counts the number of layers of hidden units). We recommend a terminology in which Figure 5.1 is called a two-layer network, because it is the number of layers of adaptive weights that is important for determining the network properties.
Another generalization of the network architecture is to include skip-layer connections, each of which is associated with a corresponding adaptive parameter. For instance, in a two-layer network these would go directly from inputs to outputs. In principle, a network with sigmoidal hidden units can always mimic skip layer connections (for bounded input values) by using a sufficiently small first-layer weight that, over its operating range, the hidden unit is effectively linear, and then compensating with a large weight value from the hidden unit to the output. In practice, however, it may be advantageous to include skip-layer connections explicitly.
#+HTML_ATTR: :width 500px
[[file:~/.local/images/prml-5-2.png]]
#+CAPTION: Example of a neural network having a general feed-forward topology. Note that each hidden and output unit has an associated bias parameter (omitted for clarity).
Furthermore, the network can be sparse, with not all possible connections within a layer being present. We shall see an example of a sparse network architecture when we consider convolutional neural networks in Section 5.5.6.
Because there is a direct correspondence between a network diagram and its mathematical function, we can develop more general network mappings by considering more complex network diagrams. However, these must be restricted to a feed-forward architecture, in other words to one having no closed directed cycles, to ensure that the outputs are deterministic functions of the inputs. This is illustrated with a simple example in Figure 5.2. Each (hidden or output) unit in such a network computes a function given by
\[
z_{k}=h\left(\sum_{j} w_{k j} z_{j}\right)
\]
where the sum runs over all units that send connections to unit \(k\) (and a bias parameter is included in the summation). For a given set of values applied to the inputs of the network, successive application of (5.10) allows the activations of all units in the network to be evaluated including those of the output units.
The approximation properties of feed-forward networks have been widely studied (Funahashi, 1989; Cybenko, 1989; Hornik et al., 1989; Stinchecombe and White, 1989; Cotter, 1990; Ito, 1991; Hornik, 1991; Kreinovich, 1991; Ripley, 1996) and found to be very general. Neural networks are therefore said to be universal approximators. For example, a two-layer network with linear outputs can uniformly approximate any continuous function on a compact input domain to arbitrary accuracy provided the network has a sufficiently large number of hidden units. This result holds for a wide range of hidden unit activation functions, but excluding polynomials. Although such theorems are reassuring, the key problem is how to find suitable parameter values given a set of training data, and in later sections of this chapter we will show that there exist effective solutions to this problem based on both maximum likelihood and Bayesian approaches.
The capability of a two-layer network to model a broad range of functions is illustrated in a nearby figure. The same figure also shows how individual hidden units work collaboratively to approximate the final function.
#+HTML_ATTR: :width 500px
[[file:~/.local/images/prml-5-3.png]]
#+CAPTION: Illustration of the capability of a multilayer perceptron to approximate four different functions comprising (a) f(x) = x, (b) f(x) = sin(x), (c) f(x) = |x|, and (d) f(x) = H(x) where H(x) is the Heaviside step function. In each case, N = 50 data points, shown as blue dots, have been sampled uniformly in x over the interval (-1, 1) and the corresponding values of f(x) evaluated. These data points are then used to train a two-layer network having 3 hidden units with 'tanh' activation functions and linear output units. The resulting network functions are shown by the red curves, and the outputs of the three hidden units are shown by the three dashed curves.
The role of hidden units in a simple classification problem is illustrated in another nearby figure using a synthetic classification data set.
#+HTML_ATTR: :width 500px
[[file:~/.local/images/prml-5-4.png]]
#+CAPTION: Example of the solution of a simple twoclass classification problem involving synthetic data using a neural network having two inputs, two hidden units with 'tanh' activation functions, and a single output having a logistic sigmoid activation function. The dashed blue lines show the z=0.5 contours for each of the hidden units, and the red line shows the y=0.5 decision surface for the network. For comparison, the green line denotes the optimal decision boundary computed from the distributions used to generate the data.
*** Estimation
**** Network training
So far, we have viewed neural networks as a general class of parametric nonlinear functions from a vector \(\mathbf{x}\) of input variables to a vector \(\mathbf{y}\) of output variables. A simple approach to the problem of determining the network parameters is to make an analogy with the discussion of polynomial curve fitting in Section 1.1, and therefore to minimize a sum-of-squares error function. Given a training set comprising a set of input vectors \(\left\{\mathbf{x}_{n}\right\}\), where \(n=1, \ldots, N\), together with a corresponding set of target vectors \(\left\{\mathbf{t}_{n}\right\}\), we minimize the error function
\[
E(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\|\mathbf{y}\left(\mathbf{x}_{n}, \mathbf{w}\right)-\mathbf{t}_{n}\right\|^{2}
\]
However, we can provide a much more general view of network training by first giving a probabilistic interpretation to the network outputs. We have already seen many advantages of using probabilistic predictions in Section 1.5.4. Here it will also provide us with a clearer motivation both for the choice of output unit nonlinearity and the choice of error function.
We start by discussing regression problems, and for the moment we consider a single target variable \(t\) that can take any real value. Following the discussions in Section 1.2.5 and 3.1, we assume that \(t\) has a Gaussian distribution with an \( \mathbf{x} \) dependent mean, which is given by the output of the neural network, so that
\[
p(t \mid \mathbf{x}, \mathbf{w})=\mathcal{N}\left(t \mid y(\mathbf{x}, \mathbf{w}), \beta^{-1}\right) \tag{5.12}
\]
where \(\beta\) is the precision (inverse variance) of the Gaussian noise. Of course this is a somewhat restrictive assumption, and in Section 5.6 we shall see how to extend this approach to allow for more general conditional distributions. For the conditional distribution given by (5.12), it is sufficient to take the output unit activation function to be the identity, because such a network can approximate any continuous function from \(\mathbf{x}\) to \(y\). Given a data set of \(N\) independent, identically distributed observations \(\mathbf{X}=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\right\}\), along with corresponding target values \(\mathbf{t}=\left\{t_{1}, \ldots, t_{N}\right\}\), we can construct the corresponding likelihood function
\[
p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta)=\prod_{n=1}^{N} p\left(t_{n} \mid \mathbf{x}_{n}, \mathbf{w}, \beta\right)
\]
Taking the negative logarithm, we obtain the error function
\[
\frac{\beta}{2} \sum_{n=1}^{N}\left\{y\left(\mathbf{x}_{n}, \mathbf{w}\right)-t_{n}\right\}^{2}-\frac{N}{2} \ln \beta+\frac{N}{2} \ln (2 \pi) \tag{5.13}
\]
which can be used to learn the parameters \(\mathbf{w}\) and \(\beta\). In Section 5.7, we shall discuss the Bayesian treatment of neural networks, while here we consider a maximum likelihood approach. Note that in the neural networks literature, it is usual to consider the minimization of an error function rather than the maximization of the (log) likelihood, and so here we shall follow this convention. Consider first the determination of \(\mathbf{w}\). Maximizing the likelihood function is equivalent to minimizing the sum-of-squares error function given by
\[
E(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{y\left(\mathbf{x}_{n}, \mathbf{w}\right)-t_{n}\right\}^{2}
\] where we have discarded additive and multiplicative constants. The value of \(\mathbf{w}\) found by minimizing \(E(\mathbf{w})\) will be denoted \(\mathbf{w}_{\mathrm{ML}}\) because it corresponds to the maximum likelihood solution. In practice, the nonlinearity of the network function \(y\left(\mathbf{x}_{n}, \mathbf{w}\right)\) causes the error \(E(\mathbf{w})\) to be nonconvex, and so in practice local maxima of the likelihood may be found, corresponding to local minima of the error function, as discussed in Section 5.2.1.
Having found \(\mathbf{w}_{\mathrm{ML}}\), the value of \(\beta\) can be found by minimizing the negative \(\log\) likelihood to give
\[
\frac{1}{\beta_{\mathrm{ML}}}=\frac{1}{N} \sum_{n=1}^{N}\left\{y\left(\mathbf{x}_{n}, \mathbf{w}_{\mathrm{ML}}\right)-t_{n}\right\}^{2} \tag{5.15}
\]
Note that this can be evaluated once the iterative optimization required to find \(\mathbf{w}_{\mathrm{ML}}\) is completed. If we have multiple target variables, and we assume that they are independent conditional on \(\mathbf{x}\) and \(\mathbf{w}\) with shared noise precision \(\beta\), then the conditional distribution of the target values is given by
\[
p(\mathbf{t} \mid \mathbf{x}, \mathbf{w})=\mathcal{N}\left(\mathbf{t} \mid \mathbf{y}(\mathbf{x}, \mathbf{w}), \beta^{-1} \mathbf{I}\right)
\]
Following the same argument as for a single target variable, we see that the maximum likelihood weights are determined by minimizing the sum-of-squares error function (5.11). The noise precision is then given by
\[
\frac{1}{\beta_{\mathrm{ML}}}=\frac{1}{N K} \sum_{n=1}^{N}\left\|\mathbf{y}\left(\mathbf{x}_{n}, \mathbf{w}_{\mathrm{ML}}\right)-\mathbf{t}_{n}\right\|^{2}
\]
where \(K\) is the number of target variables. The assumption of independence can be dropped at the expense of a slightly more complex optimization problem.
Recall from Section 4.3.6 that there is a natural pairing of the error function (given by the negative log likelihood) and the output unit activation function. In the regression case, we can view the network as having an output activation function that is the identity, so that \(y_{k}=a_{k}\). The corresponding sum-of-squares error function has the property
\[
\frac{\partial E}{\partial a_{k}}=y_{k}-t_{k}
\]
which we shall make use of when discussing error backpropagation in Section 5.3.
Now consider the case of binary classification in which we have a single target variable \(t\) such that \(t=1\) denotes class \(\mathcal{C}_{1}\) and \(t=0\) denotes class \(\mathcal{C}_{2}\). Following the discussion of canonical link functions in Section 4.3.6, we consider a network having a single output whose activation function is a logistic sigmoid
\[
y=\sigma(a) \equiv \frac{1}{1+\exp (-a)} \tag{5.19}
\]
so that \(0 \leqslant y(\mathbf{x}, \mathbf{w}) \leqslant 1\). We can interpret \(y(\mathbf{x}, \mathbf{w})\) as the conditional probability \(p\left(\mathcal{C}_{1} \mid \mathbf{x}\right)\), with \(p\left(\mathcal{C}_{2} \mid \mathbf{x}\right)\) given by \(1-y(\mathbf{x}, \mathbf{w})\). The conditional distribution of targets given inputs is then a Bernoulli distribution of the form
\[
p(t \mid \mathbf{x}, \mathbf{w})=y(\mathbf{x}, \mathbf{w})^{t}\{1-y(\mathbf{x}, \mathbf{w})\}^{1-t} \tag{5.20}
\]
If we consider a training set of independent observations, then the error function, which is given by the negative log likelihood, is then a cross-entropy error function of the form
\[
E(\mathbf{w})=-\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\} \tag{5.21}
\]
where \(y_{n}\) denotes \(y\left(\mathbf{x}_{n}, \mathbf{w}\right)\). Note that there is no analogue of the noise precision \(\beta\) because the target values are assumed to be correctly labelled. However, the model is easily extended to allow for labelling errors. Simard et al.(2003) found that using the cross-entropy error function instead of the sum-of-squares for a classification problem leads to faster training as well as improved generalization.
If we have \(K\) separate binary classifications to perform, then we can use a network having \(K\) outputs each of which has a logistic sigmoid activation function. Associated with each output is a binary class label \(t_{k} \in\{0,1\}\), where \(k=1, \ldots, K\). If we assume that the class labels are independent, given the input vector, then the conditional distribution of the targets is
\[
p(\mathbf{t} \mid \mathbf{x}, \mathbf{w})=\prod_{k=1}^{K} y_{k}(\mathbf{x}, \mathbf{w})^{t_{k}}\left[1-y_{k}(\mathbf{x}, \mathbf{w})\right]^{1-t_{k}}
\]
Taking the negative logarithm of the corresponding likelihood function then gives the following error function
\[
E(\mathbf{w})=-\sum_{n=1}^{N} \sum_{k=1}^{K}\left\{t_{n k} \ln y_{n k}+\left(1-t_{n k}\right) \ln \left(1-y_{n k}\right)\right\}
\]
where \(y_{n k}\) denotes \(y_{k}\left(\mathbf{x}_{n}, \mathbf{w}\right)\). Again, the derivative of the error function with respect to the activation for a particular output unit takes the form (5.18) just as in the regression case.
It is interesting to contrast the neural network solution to this problem with the corresponding approach based on a linear classification model of the kind discussed in Chapter 4. Suppose that we are using a standard two-layer network of the kind shown in Figure 5.1. We see that the weight parameters in the first layer of the network are shared between the various outputs, whereas in the linear model each classification problem is solved independently. The first layer of the network can be viewed as performing a nonlinear feature extraction, and the sharing of features between the different outputs can save on computation and can also lead to improved generalization.
Finally, we consider the standard multiclass classification problem in which each input is assigned to one of \(K\) mutually exclusive classes. The binary target variables \(t_{k} \in\{0,1\}\) have a 1-of- \(K\) coding scheme indicating the class, and the network outputs are interpreted as \(y_{k}(\mathbf{x}, \mathbf{w})=p\left(t_{k}=1 \mid \mathbf{x}\right)\), leading to the following error function
\[
E(\mathbf{w})=-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{k n} \ln y_{k}\left(\mathbf{x}_{n}, \mathbf{w}\right)
\]
#+HTML_ATTR: :width 500px
[[file:~/.local/images/prml-5-5.png]]
#+CAPTION: Geometrical view of the error function E(w) as a surface sitting over weight space. Point w_A is a local minimum and w_B is the global minimum. At any point w_C, the local gradient of the error surface is given by the vector E.
Following the discussion of Section 4.3.4, we see that the output unit activation function, which corresponds to the canonical link, is given by the softmax function
\[
y_{k}(\mathbf{x}, \mathbf{w})=\frac{\exp \left(a_{k}(\mathbf{x}, \mathbf{w})\right)}{\sum_{j} \exp \left(a_{j}(\mathbf{x}, \mathbf{w})\right)}
\]
which satisfies \(0 \leqslant y_{k} \leqslant 1\) and \(\sum_{k} y_{k}=1\). Note that the \(y_{k}(\mathbf{x}, \mathbf{w})\) are unchanged if a constant is added to all of the \(a_{k}(\mathbf{x}, \mathbf{w})\), causing the error function to be constant for some directions in weight space. This degeneracy is removed if an appropriate regularization term (Section 5.5) is added to the error function.
Once again, the derivative of the error function with respect to the activation for a particular output unit takes the familiar form (5.18).
In summary, there is a natural choice of both output unit activation function and matching error function, according to the type of problem being solved. For regression we use linear outputs and a sum-of-squares error, for (multiple independent) binary classifications we use logistic sigmoid outputs and a cross-entropy error function, and for multiclass classification we use softmax outputs with the corresponding multiclass cross-entropy error function. For classification problems involving two classes, we can use a single logistic sigmoid output, or alternatively we can use a network with two outputs having a softmax output activation function.
***** Parameter optimization
We turn next to the task of finding a weight vector \(\mathbf{w}\) which minimizes the chosen function \(E(\mathbf{w})\). At this point, it is useful to have a geometrical picture of the error function, which we can view as a surface sitting over weight space as shown in Figure 5.5. First note that if we make a small step in weight space from \(\mathbf{w}\) to \(\mathbf{w}+\delta \mathbf{w}\) then the change in the error function is \(\delta E \simeq \delta \mathbf{w}^{\mathrm{T}} \nabla E(\mathbf{w})\), where the vector \(\nabla E(\mathbf{w})\) points in the direction of greatest rate of increase of the error function. Because the error \(E(\mathbf{w})\) is a smooth continuous function of \(\mathbf{w}\), its smallest value will occur at a point in weight space such that the gradient of the error function vanishes, so that
\[
\nabla E(\mathbf{w})=0
\]
as otherwise we could make a small step in the direction of \(-\nabla E(\mathbf{w})\) and thereby further reduce the error. Points at which the gradient vanishes are called stationary points, and may be further classified into minima, maxima, and saddle points.
Our goal is to find a vector \(\mathbf{w}\) such that \(E(\mathbf{w})\) takes its smallest value. However, the error function typically has a highly nonlinear dependence on the weights and bias parameters, and so there will be many points in weight space at which the gradient vanishes (or is numerically very small). Indeed, from the discussion in Section 5.1.1 we see that for any point \(\mathbf{w}\) that is a local minimum, there will be other points in weight space that are equivalent minima. For instance, in a two-layer network of the kind shown in Figure 5.1, with \(M\) hidden units, each point in weight space is a member of a family of \(M!2^{M}\) equivalent points.
Furthermore, there will typically be multiple inequivalent stationary points and in particular multiple inequivalent minima. A minimum that corresponds to the smallest value of the error function for any weight vector is said to be a global minimum. Any other minima corresponding to higher values of the error function are said to be local minima. For a successful application of neural networks, it may not be necessary to find the global minimum (and in general it will not be known whether the global minimum has been found) but it may be necessary to compare several local minima in order to find a sufficiently good solution.
Because there is clearly no hope of finding an analytical solution to the equation \(\nabla E(\mathbf{w})=0\) we resort to iterative numerical procedures. The optimization of continuous nonlinear functions is a widely studied problem and there exists an extensive literature on how to solve it efficiently. Most techniques involve choosing some initial value \(\mathbf{w}^{(0)}\) for the weight vector and then moving through weight space in a succession of steps of the form
\[
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}+\Delta \mathbf{w}^{(\tau)}
\]
where \(\tau\) labels the iteration step. Different algorithms involve different choices for the weight vector update \(\Delta \mathbf{w}^{(\tau)}\). Many algorithms make use of gradient information and therefore require that, after each update, the value of \(\nabla E(\mathbf{w})\) is evaluated at the new weight vector \(\mathbf{w}^{(\tau+1)}\). In order to understand the importance of gradient information, it is useful to consider a local approximation to the error function based on a Taylor expansion.
***** Local quadratic approximation
Insight into the optimization problem, and into the various techniques for solving it, can be obtained by considering a local quadratic approximation to the error function.
Consider the Taylor expansion of \(E(\mathbf{w})\) around some point \(\widehat{\mathbf{w}}\) in weight space
\[
E(\mathbf{w}) \simeq E(\widehat{\mathbf{w}})+(\mathbf{w}-\widehat{\mathbf{w}})^{\mathrm{T}} \mathbf{b}+\frac{1}{2}(\mathbf{w}-\widehat{\mathbf{w}})^{\mathrm{T}} \mathbf{H}(\mathbf{w}-\widehat{\mathbf{w}})
\]
where cubic and higher terms have been omitted. Here \(\mathbf{b}\) is defined to be the gradient of \(E\) evaluated at \(\widehat{\mathbf{w}}\)
\[
\mathbf{b} \equiv \nabla E (\widehat{\mathbf{w}})
\]
and the Hessian matrix \(\mathbf{H}=\nabla \nabla E\) has elements
\[
(\mathbf{H})_{ij} \equiv (\partial_{w_{i}} \partial_{w_{j}} E)_{\mathbf{w}=\widehat{\mathbf{w}}}
\]
From (5.28), the corresponding local approximation to the gradient is given by
\[
\nabla E \simeq \mathbf{b}+\mathbf{H}(\mathbf{w}-\widehat{\mathbf{w}})
\]
For points \(\mathbf{w}\) that are sufficiently close to \(\widehat{\mathbf{w}}\), these expressions will give reasonable approximations for the error and its gradient.
Consider the particular case of a local quadratic approximation around a point \(\mathbf{w}^{\star}\) that is a minimum of the error function. In this case there is no linear term, because \(\nabla E=0\) at \(\mathbf{w}^{\star}\), and (5.28) becomes
\[
E(\mathbf{w})=E (\mathbf{w}^{\star}) + \frac{1}{2} (\mathbf{w}-\mathbf{w}^{\star})^{\mathrm{T}} \mathbf{H} (\mathbf{w}-\mathbf{w}^{\star})
\]
where the Hessian \(\mathbf{H}\) is evaluated at \(\mathbf{w}^{\star}\). In order to interpret this geometrically, consider the eigenvalue equation for the Hessian matrix
\[
\mathbf{H} \mathbf{u}_{i}=\lambda_{i} \mathbf{u}_{i}
\]
where the eigenvectors \(\mathbf{u}_{i}\) form a complete orthonormal set (Appendix \(\mathrm{C}\) ) so that
\[
\mathbf{u}_{i}^{\mathrm{T}} \mathbf{u}_{j}=\delta_{i j}
\]
We now expand \(\left(\mathbf{w}-\mathbf{w}^{\star}\right)\) as a linear combination of the eigenvectors in the form
\[
\mathbf{w}-\mathbf{w}^{\star}=\sum_{i} \alpha_{i} \mathbf{u}_{i}
\]
This can be regarded as a transformation of the coordinate system in which the origin is translated to the point \(\mathbf{w}^{\star}\), and the axes are rotated to align with the eigenvectors (through the orthogonal matrix whose columns are the \(\mathbf{u}_{i}\) ), and is discussed in more detail in Appendix C. Substituting (5.35) into (5.32), and using (5.33) and (5.34), allows the error function to be written in the form
\[
E(\mathbf{w})=E\left(\mathbf{w}^{\star}\right)+\frac{1}{2} \sum_{i} \lambda_{i} \alpha_{i}^{2}
\]
A matrix \(\mathbf{H}\) is said to be positive definite if, and only if,
\[
\mathbf{v}^{\mathrm{T}} \mathbf{H} \mathbf{v}>0 \quad \text { for all } \mathbf{v}
\]
#+HTML_ATTR: :width 500px
[[file:~/.local/images/prml-5-6.png]]
#+CAPTION: In the neighbourhood of a minimum w*, the error function can be approximated by a quadratic. Contours of constant error are then ellipses whose axes are aligned with the eigenvectors u_i of the Hessian matrix, with lengths that are inversely proportional to the square roots of the corresponding eigenvectors _i.
Because the eigenvectors \(\left\{\mathbf{u}_{i}\right\}\) form a complete set, an arbitrary vector \(\mathbf{v}\) can be written in the form
\[
\mathbf{v}=\sum_{i} c_{i} \mathbf{u}_{i}
\]
From (5.33) and (5.34), we then have
\[
\mathbf{v}^{\mathrm{T}} \mathbf{H} \mathbf{v}=\sum_{i} c_{i}^{2} \lambda_{i}
\]
and so \(\mathbf{H}\) will be positive definite if, and only if, all of its eigenvalues are positive. In the new coordinate system, whose basis vectors are given by the eigenvectors \(\left\{\mathbf{u}_{i}\right\}\), the contours of constant \(E\) are ellipses centred on the origin, as illustrated in a nearby figure. For a one-dimensional weight space, a stationary point \(w^{\star}\) will be a minimum if
\[
\left.\frac{\partial^{2} E}{\partial w^{2}}\right|_{w^{\star}}>0 \tag{5.40}
\]
The corresponding result in \(D\)-dimensions is that the Hessian matrix, evaluated at \(\mathbf{w}^{\star}\), should be positive definite.
***** Use of gradient information
As we shall see in Section 5.3, it is possible to evaluate the gradient of an error function efficiently by means of the backpropagation procedure. The use of this gradient information can lead to significant improvements in the speed with which the minima of the error function can be located. We can see why this is so, as follows.
In the quadratic approximation to the error function, given in (5.28), the error surface is specified by the quantities \(\mathbf{b}\) and \(\mathbf{H}\), which contain a total of \(W(W+\) 3) \(/ 2\) independent elements (because the matrix \(\mathbf{H}\) is symmetric), where \(W\) is the dimensionality of \(\mathbf{w}\) (i.e., the total number of adaptive parameters in the network). The location of the minimum of this quadratic approximation therefore depends on \(O\left(W^{2}\right)\) parameters, and we should not expect to be able to locate the minimum until we have gathered \(O\left(W^{2}\right)\) independent pieces of information. If we do not make use of gradient information, we would expect to have to perform \(O\left(W^{2}\right)\) function evaluations, each of which would require \(O(W)\) steps. Thus, the computational effort needed to find the minimum using such an approach would be \(O\left(W^{3}\right)\).
Now compare this with an algorithm that makes use of the gradient information. Because each evaluation of \(\nabla E\) brings \(W\) items of information, we might hope to find the minimum of the function in \(O(W)\) gradient evaluations. As we shall see, by using error backpropagation, each such evaluation takes only \(O(W)\) steps and so the minimum can now be found in \(O\left(W^{2}\right)\) steps. For this reason, the use of gradient information forms the basis of practical algorithms for training neural networks.
***** Gradient descent optimization
The simplest approach to using gradient information is to choose the weight update in (5.27) to comprise a small step in the direction of the negative gradient, so that
\[
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta \nabla E\left(\mathbf{w}^{(\tau)}\right)
\]
where the parameter \(\eta>0\) is known as the learning rate. After each such update, the gradient is re-evaluated for the new weight vector and the process repeated. Note that the error function is defined with respect to a training set, and so each step requires that the entire training set be processed in order to evaluate \(\nabla E\). Techniques that use the whole data set at once are called batch methods. At each step the weight vector is moved in the direction of the greatest rate of decrease of the error function, and so this approach is known as gradient descent or steepest descent. Although such an approach might intuitively seem reasonable, in fact it turns out to be a poor algorithm, for reasons discussed in Bishop and Nabney (2008).
For batch optimization, there are more efficient methods, such as conjugate gradients and quasi-Newton methods, which are much more robust and much faster than simple gradient descent (Gill et al., 1981; Fletcher, 1987; Nocedal and Wright, 1999). Unlike gradient descent, these algorithms have the property that the error function always decreases at each iteration unless the weight vector has arrived at a local or global minimum.
In order to find a sufficiently good minimum, it may be necessary to run a gradient-based algorithm multiple times, each time using a different randomly chosen starting point, and comparing the resulting performance on an independent validation set.
There is, however, an on-line version of gradient descent that has proved useful in practice for training neural networks on large data sets (Le Cun et al., 1989). Error functions based on maximum likelihood for a set of independent observations comprise a sum of terms, one for each data point
\[
E(\mathbf{w})=\sum_{n=1}^{N} E_{n}(\mathbf{w})
\]
On-line gradient descent, also known as sequential gradient descent or stochastic gradient descent, makes an update to the weight vector based on one data point at a time, so that
\[
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta \nabla E_{n}\left(\mathbf{w}^{(\tau)}\right)
\]
This update is repeated by cycling through the data either in sequence or by selecting points at random with replacement. There are of course intermediate scenarios in which the updates are based on batches of data points.
One advantage of on-line methods compared to batch methods is that the former handle redundancy in the data much more efficiently. To see, this consider an extreme example in which we take a data set and double its size by duplicating every data point. Note that this simply multiplies the error function by a factor of 2 and so is equivalent to using the original error function. Batch methods will require double the computational effort to evaluate the batch error function gradient, whereas online methods will be unaffected. Another property of on-line gradient descent is the possibility of escaping from local minima, since a stationary point with respect to the error function for the whole data set will generally not be a stationary point for each data point individually.
Nonlinear optimization algorithms, and their practical application to neural network training, are discussed in detail in Bishop and Nabney (2008).
**** Backpropagation algorithm
Our goal in this section is to find an efficient technique for evaluating the gradient of an error function \(E(\mathbf{w})\) for a feed-forward neural network. We shall see that this can be achieved using a local message passing scheme in which information is sent alternately forwards and backwards through the network and is known as error backpropagation, or sometimes simply as backprop.
It should be noted that the term backpropagation is used in the neural computing literature to mean a variety of different things. For instance, the multilayer perceptron architecture is sometimes called a backpropagation network. The term backpropagation is also used to describe the training of a multilayer perceptron using gradient descent applied to a sum-of-squares error function. In order to clarify the terminology, it is useful to consider the nature of the training process more carefully. Most training algorithms involve an iterative procedure for minimization of an error function, with adjustments to the weights being made in a sequence of steps. At each such step, we can distinguish between two distinct stages. In the first stage, the derivatives of the error function with respect to the weights must be evaluated. As we shall see, the important contribution of the backpropagation technique is in providing a computationally efficient method for evaluating such derivatives. Because it is at this stage that errors are propagated backwards through the network, we shall use the term backpropagation specifically to describe the evaluation of derivatives. In the second stage, the derivatives are then used to compute the adjustments to be made to the weights. The simplest such technique, and the one originally considered by Rumelhart et al.(1986), involves gradient descent. It is important to recognize that the two stages are distinct. Thus, the first stage, namely the propagation of errors backwards through the network in order to evaluate derivatives, can be applied to many other kinds of network and not just the multilayer perceptron. It can also be applied to error functions other that just the simple sum-of-squares, and to the eval- uation of other derivatives such as the Jacobian and Hessian matrices, as we shall see later in this chapter. Similarly, the second stage of weight adjustment using the calculated derivatives can be tackled using a variety of optimization schemes, many of which are substantially more powerful than simple gradient descent.
***** Evaluation of error-function derivatives
We now derive the backpropagation algorithm for a general network having arbitrary feed-forward topology, arbitrary differentiable nonlinear activation functions, and a broad class of error function. The resulting formulae will then be illustrated using a simple layered network structure having a single layer of sigmoidal hidden units together with a sum-of-squares error.
Many error functions of practical interest, for instance those defined by maximum likelihood for a set of i.i.d. data, comprise a sum of terms, one for each data point in the training set, so that
\[
E(\mathbf{w})=\sum_{n=1}^{N} E_{n}(\mathbf{w})
\]
Here we shall consider the problem of evaluating \(\nabla E_{n}(\mathbf{w})\) for one such term in the error function. This may be used directly for sequential optimization, or the results can be accumulated over the training set in the case of batch methods.
Consider first a simple linear model in which the outputs \(y_{k}\) are linear combinations of the input variables \(x_{i}\) so that
\[
y_{k}=\sum_{i} w_{k i} x_{i}
\]
together with an error function that, for a particular input pattern \(n\), takes the form
\[
E_{n}=\frac{1}{2} \sum_{k}\left(y_{n k}-t_{n k}\right)^{2}
\]
where \(y_{n k}=y_{k}\left(\mathbf{x}_{n}, \mathbf{w}\right)\). The gradient of this error function with respect to a weight \(w_{j i}\) is given by
\[
\frac{\partial E_{n}}{\partial w_{j i}}=\left(y_{n j}-t_{n j}\right) x_{n i}
\]
which can be interpreted as a 'local' computation involving the product of an 'error signal' \(y_{n j}-t_{n j}\) associated with the output end of the link \(w_{j i}\) and the variable \(x_{n i}\) associated with the input end of the link. In Section 4.3.2, we saw how a similar formula arises with the logistic sigmoid activation function together with the cross entropy error function, and similarly for the softmax activation function together with its matching cross-entropy error function. We shall now see how this simple result extends to the more complex setting of multilayer feed-forward networks.
In a general feed-forward network, each unit computes a weighted sum of its inputs of the form
\[
a_{j}=\sum_{i} w_{j i} z_{i}
\] where \(z_{i}\) is the activation of a unit, or input, that sends a connection to unit \(j\), and \(w_{j i}\) is the weight associated with that connection. In Section 5.1, we saw that biases can be included in this sum by introducing an extra unit, or input, with activation fixed at +1 . We therefore do not need to deal with biases explicitly. The sum in (5.48) is transformed by a nonlinear activation function \(h(\cdot)\) to give the activation \(z_{j}\) of unit \(j\) in the form
\[
z_{j}=h (a_{j})
\]
Note that one or more of the variables \(z_{i}\) in the sum in (5.48) could be an input, and similarly, the unit \(j\) in (5.49) could be an output.
For each pattern in the training set, we shall suppose that we have supplied the corresponding input vector to the network and calculated the activations of all of the hidden and output units in the network by successive application of (5.48) and (5.49). This process is often called forward propagation because it can be regarded as a forward flow of information through the network.
Now consider the evaluation of the derivative of \(E_{n}\) with respect to a weight \(w_{j i}\). The outputs of the various units will depend on the particular input pattern \(n\). However, in order to keep the notation uncluttered, we shall omit the subscript \(n\) from the network variables. First we note that \(E_{n}\) depends on the weight \(w_{j i}\) only via the summed input \(a_{j}\) to unit \(j\). We can therefore apply the chain rule for partial derivatives to give
\[
\frac{\partial E_{n}}{\partial w_{j i}}=\frac{\partial E_{n}}{\partial a_{j}} \frac{\partial a_{j}}{\partial w_{j i}}
\]
We now introduce a useful notation
\[
\delta_{j} \equiv \frac{\partial E_{n}}{\partial a_{j}} \tag{5.51}
\]
where the \(\delta\) 's are often referred to as errors for reasons we shall see shortly. Using (5.48), we can write
\[
\frac{\partial a_{j}}{\partial w_{j i}}=z_{i}
\]
Substituting (5.51) and (5.52) into (5.50), we then obtain
\[
\frac{\partial E_{n}}{\partial w_{j i}}=\delta_{j} z_{i}
\]
Equation (5.53) tells us that the required derivative is obtained simply by multiplying the value of \(\delta\) for the unit at the output end of the weight by the value of \(z\) for the unit at the input end of the weight (where \(z=1\) in the case of a bias). Note that this takes the same form as for the simple linear model considered at the start of this section. Thus, in order to evaluate the derivatives, we need only to calculate the value of \(\delta_{j}\) for each hidden and output unit in the network, and then apply (5.53).
As we have seen already, for the output units, we have
\[
\delta_{k}=y_{k}-t_{k} \tag{5.54}
\]
provided we are using the canonical link as the output-unit activation function. To evaluate the \(\delta\) 's for hidden units, we again make use of the chain rule for partial derivatives,
\[
\delta_{j} \equiv \frac{\partial E_{n}}{\partial a_{j}}=\sum_{k} \frac{\partial E_{n}}{\partial a_{k}} \frac{\partial a_{k}}{\partial a_{j}}
\]
where the sum runs over all units \(k\) to which unit \(j\) sends connections. The arrangement of units and weights is illustrated in a nearby figure. Note that the units labelled \(k\) could include other hidden units and/or output units. In writing down (5.55), we are making use of the fact that variations in \(a_{j}\) give rise to variations in the error function only through variations in the variables \(a_{k}\). If we now substitute the definition of \(\delta\) given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the following backpropagation formula
\[
\delta_{j}=h^{\prime}\left(a_{j}\right) \sum_{k} w_{k j} \delta_{k}
\]
which tells us that the value of \(\delta\) for a particular hidden unit can be obtained by propagating the \(\delta\) 's backwards from units higher up in the network, as illustrated in Figure 5.7. Note that the summation in (5.56) is taken over the first index on \(w_{k j}\) (corresponding to backward propagation of information through the network), whereas in the forward propagation equation (5.10) it is taken over the second index. Because we already know the values of the \(\delta\) 's for the output units, it follows that by recursively applying (5.56) we can evaluate the \(\delta\) 's for all of the hidden units in a feed-forward network, regardless of its topology.
#+HTML_ATTR: :width 500px
[[file:~/.local/images/prml-5-7.png]]
#+CAPTION: Illustration of the calculation of _j for hidden unit j by backpropagation of the 's from those units k to which unit j sends connections. The blue arrow denotes the direction of information flow during forward propagation, and the red arrows indicate the backward propagation of error information.
The backpropagation procedure can therefore be summarized as follows.
1. Apply an input vector \(\mathbf{x}_{n}\) to the network and forward propagate through the network using (5.48) and (5.49) to find the activations of all the hidden and output units.
2. Evaluate the \(\delta_{k}\) for all the output units using (5.54).
3. Backpropagate the \(\delta\) 's using (5.56) to obtain \(\delta_{j}\) for each hidden unit in the network.
4. Use (5.53) to evaluate the required derivatives.
For batch methods, the derivative of the total error \(E\) can then be obtained by repeating the above steps for each pattern in the training set and then summing over all patterns:
\[
\frac{\partial E}{\partial w_{j i}}=\sum_{n} \frac{\partial E_{n}}{\partial w_{j i}}
\]
In the above derivation we have implicitly assumed that each hidden or output unit in the network has the same activation function \(h(\cdot)\). The derivation is easily generalized, however, to allow different units to have individual activation functions, simply by keeping track of which form of \(h(\cdot)\) goes with which unit.
***** A simple example
The above derivation of the backpropagation procedure allowed for general forms for the error function, the activation functions, and the network topology. In order to illustrate the application of this algorithm, we shall consider a particular example. This is chosen both for its simplicity and for its practical importance, because many applications of neural networks reported in the literature make use of this type of network. Specifically, we shall consider a two-layer network of the form illustrated in Figure 5.1, together with a sum-of-squares error, in which the output units have linear activation functions, so that \(y_{k}=a_{k}\), while the hidden units have logistic sigmoid activation functions given by
\[
h(a) \equiv \tanh (a)
\]
where
\[
\tanh (a)=\frac{e^{a}-e^{-a}}{e^{a}+e^{-a}}
\]
A useful feature of this function is that its derivative can be expressed in a particularly simple form:
\[
h^{\prime}(a)=1-h(a)^{2}
\]
We also consider a standard sum-of-squares error function, so that for pattern \(n\) the error is given by
\[
E_{n}=\frac{1}{2} \sum_{k=1}^{K}\left(y_{k}-t_{k}\right)^{2}
\]
where \(y_{k}\) is the activation of output unit \(k\), and \(t_{k}\) is the corresponding target, for a particular input pattern \(\mathbf{x}_{n}\).
For each pattern in the training set in turn, we first perform a forward propagation using
\begin{align*}
a_{j} & =\sum_{i=0}^{D} w_{j i}^{(1)} x_{i} \\
z_{j} & =\tanh \left(a_{j}\right) \\
y_{k} & =\sum_{j=0}^{M} w_{k j}^{(2)} z_{j}
\end{align*}
Next we compute the \(\delta\) 's for each output unit using
\[
\delta_{k}=y_{k}-t_{k}
\]
Then we backpropagate these to obtain \(\delta\) s for the hidden units using
\[
\delta_{j}=\left(1-z_{j}^{2}\right) \sum_{k=1}^{K} w_{k j} \delta_{k}
\]
Finally, the derivatives with respect to the first-layer and second-layer weights are given by
\[
\frac{\partial E_{n}}{\partial w_{j i}^{(1)}}=\delta_{j} x_{i}, \quad \frac{\partial E_{n}}{\partial w_{k j}^{(2)}}=\delta_{k} z_{j}
\]
***** Efficiency of backpropagation
One of the most important aspects of backpropagation is its computational efficiency. To understand this, let us examine how the number of computer operations required to evaluate the derivatives of the error function scales with the total number \(W\) of weights and biases in the network. A single evaluation of the error function (for a given input pattern) would require \(\mathcal{O}(W)\) operations, for sufficiently large \(W\). This follows from the fact that, except for a network with very sparse connections, the number of weights is typically much greater than the number of units, and so the bulk of the computational effort in forward propagation is concerned with evaluating the sums in (5.48), with the evaluation of the activation functions representing a small overhead. Each term in the sum in (5.48) requires one multiplication and one addition, leading to an overall computational cost that is \(\mathcal{O}(W)\).
An alternative approach to backpropagation for computing the derivatives of the error function is to use finite differences. This can be done by perturbing each weight in turn, and approximating the derivatives by the expression
\[
\frac{\partial E_{n}}{\partial w_{j i}}=\frac{E_{n}\left(w_{j i}+\epsilon\right)-E_{n}\left(w_{j i}\right)}{\epsilon}+ \mathcal{O}(\epsilon)
\]
where \(\epsilon \ll 1\). In a software simulation, the accuracy of the approximation to the derivatives can be improved by making \(\epsilon\) smaller, until numerical roundoff problems arise. The accuracy of the finite differences method can be improved significantly by using symmetrical central differences of the form
\[
\frac{\partial E_{n}}{\partial w_{j i}}=\frac{E_{n}\left(w_{j i}+\epsilon\right)-E_{n}\left(w_{j i}-\epsilon\right)}{2 \epsilon} + \mathcal{O} \left(\epsilon^{2}\right)
\]
In this case, the \(O(\epsilon)\) corrections cancel, as can be verified by Taylor expansion on the right-hand side of (5.69), and so the residual corrections are \(\mathcal{O}\left(\epsilon^{2}\right)\). The number of computational steps is, however, roughly doubled compared with (5.68).
The main problem with numerical differentiation is that the highly desirable \(O(W)\) scaling has been lost. Each forward propagation requires \(\mathcal{O}(W)\) steps, and there are \(W\) weights in the network each of which must be perturbed individually, so that the overall scaling is \(\mathcal{O}\left(W^{2}\right)\).
However, numerical differentiation plays an important role in practice, because a comparison of the derivatives calculated by backpropagation with those obtained using central differences provides a powerful check on the correctness of any software implementation of the backpropagation algorithm. When training networks in practice, derivatives should be evaluated using backpropagation, because this gives the greatest accuracy and numerical efficiency. However, the results should be compared with numerical differentiation using (5.69) for some test cases in order to check the correctness of the implementation.
*** Extensions
**** Backpropagating the Jacobian
We have seen how the derivatives of an error function with respect to the weights can be obtained by the propagation of errors backwards through the network. The technique of backpropagation can also be applied to the calculation of other derivatives. Here we consider the evaluation of the Jacobian matrix, whose elements are given by the derivatives of the network outputs with respect to the inputs
\[
J_{k i} \equiv \frac{\partial y_{k}}{\partial x_{i}}
\]
where each such derivative is evaluated with all other inputs held fixed. Jacobian matrices play a useful role in systems built from a number of distinct modules, as illustrated in a nearby figure. Each module can comprise a fixed or adaptive function, which can be linear or nonlinear, so long as it is differentiable.
#+HTML_ATTR: :width 500px
[[file:~/.local/images/prml-5-8.png]]
#+CAPTION: Illustration of a modular pattern recognition system in which the Jacobian matrix can be used to backpropagate error signals from the outputs through to earlier modules in the system.
Suppose we wish to minimize an error function \(E\) with respect to the parameter \(w\) from the nearby figure. The derivative of the error function is given by
\[
\frac{\partial E}{\partial w}=\sum_{k, j} \frac{\partial E}{\partial y_{k}} \frac{\partial y_{k}}{\partial z_{j}} \frac{\partial z_{j}}{\partial w}
\]
in which the Jacobian matrix for the red module in the nearby figure appears in the middle term.
Because the Jacobian matrix provides a measure of the local sensitivity of the outputs to changes in each of the input variables, it also allows any known errors \(\Delta x_{i}\) associated with the inputs to be propagated through the trained network in order to estimate their contribution \(\Delta y_{k}\) to the errors at the outputs, through the relation
\[
\Delta y_{k} \simeq \sum_{i} \frac{\partial y_{k}}{\partial x_{i}} \Delta x_{i}
\]
which is valid provided the \(\left|\Delta x_{i}\right|\) are small. In general, the network mapping represented by a trained neural network will be nonlinear, and so the elements of the Jacobian matrix will not be constants but will depend on the particular input vector used. Thus (5.72) is valid only for small perturbations of the inputs, and the Jacobian itself must be re-evaluated for each new input vector.
The Jacobian matrix can be evaluated using a backpropagation procedure that is similar to the one derived earlier for evaluating the derivatives of an error function with respect to the weights. We start by writing the element \(J_{k i}\) in the form
\begin{align*}
J_{k i}=\frac{\partial y_{k}}{\partial x_{i}} & =\sum_{j} \frac{\partial y_{k}}{\partial a_{j}} \frac{\partial a_{j}}{\partial x_{i}} \\
& =\sum_{j} w_{j i} \frac{\partial y_{k}}{\partial a_{j}}
\end{align*}
where we have made use of (5.48). The sum in (5.73) runs over all units \(j\) to which the input unit \(i\) sends connections (for example, over all units in the first hidden layer in the layered topology considered earlier). We now write down a recursive backpropagation formula to determine the derivatives \(\partial y_{k} / \partial a_{j}\)
\begin{align*}
\frac{\partial y_{k}}{\partial a_{j}} & =\sum_{l} \frac{\partial y_{k}}{\partial a_{l}} \frac{\partial a_{l}}{\partial a_{j}} \\
& =h^{\prime}\left(a_{j}\right) \sum_{l} w_{l j} \frac{\partial y_{k}}{\partial a_{l}}
\end{align*}
where the sum runs over all units \(l\) to which unit \(j\) sends connections (corresponding to the first index of \(w_{l j}\) ). Again, we have made use of (5.48) and (5.49). This backpropagation starts at the output units for which the required derivatives can be found directly from the functional form of the output-unit activation function. For instance, if we have individual sigmoidal activation functions at each output unit, then
\[
\frac{\partial y_{k}}{\partial a_{j}}=\delta_{k j} \sigma^{\prime}\left(a_{j}\right)
\]
whereas for softmax outputs we have
\[
\frac{\partial y_{k}}{\partial a_{j}}=\delta_{k j} y_{k}-y_{k} y_{j}
\]
We can summarize the procedure for evaluating the Jacobian matrix as follows. Apply the input vector corresponding to the point in input space at which the Jacobian matrix is to be found, and forward propagate in the usual way to obtain the activations of all of the hidden and output units in the network. Next, for each row \(k\) of the Jacobian matrix, corresponding to the output unit \(k\), backpropagate using the recursive relation (5.74), starting with (5.75) or (5.76), for all of the hidden units in the network. Finally, use (5.73) to do the backpropagation to the inputs. The Jacobian can also be evaluated using an alternative forward propagation formalism, which can be derived in an analogous way to the backpropagation approach given here.
Again, the implementation of such algorithms can be checked by using numerical differentiation in the form
\[
\frac{\partial y_{k}}{\partial x_{i}}=\frac{y_{k}\left(x_{i}+\epsilon\right)-y_{k}\left(x_{i}-\epsilon\right)}{2 \epsilon} + \mathcal{O} \left(\epsilon^{2}\right)
\]
which involves \(2 D\) forward propagations for a network having \(D\) inputs.
**** Backpropagating the Hessian
We have shown how the technique of backpropagation can be used to obtain the first derivatives of an error function with respect to the weights in the network. Backpropagation can also be used to evaluate the second derivatives of the error, given by
\[
\begin{align*}
\frac{\partial^{2} E}{\partial w_{j i} \partial w_{l k}} \tag{5.78}
\end{align*}
\]
Note that it is sometimes convenient to consider all of the weight and bias parameters as elements \(w_{i}\) of a single vector, denoted \(\mathbf{w}\), in which case the second derivatives form the elements \(H_{i j}\) of the Hessian matrix \(\mathbf{H}\), where \(i, j \in\{1, \ldots, W\}\) and \(W\) is the total number of weights and biases. The Hessian plays an important role in many aspects of neural computing, including the following:
1. Several nonlinear optimization algorithms used for training neural networks are based on considerations of the second-order properties of the error surface, which are controlled by the Hessian matrix (Bishop and Nabney, 2008).
2. The Hessian forms the basis of a fast procedure for re-training a feed-forward network following a small change in the training data (Bishop, 1991).
3. The inverse of the Hessian has been used to identify the least significant weights in a network as part of network 'pruning' algorithms (Le Cun et al., 1990).
4. The Hessian plays a central role in the Laplace approximation for a Bayesian neural network (see Section 5.7). Its inverse is used to determine the predictive distribution for a trained network, its eigenvalues determine the values of hyperparameters, and its determinant is used to evaluate the model evidence.
Various approximation schemes have been used to evaluate the Hessian matrix for a neural network. However, the Hessian can also be calculated exactly using an extension of the backpropagation technique.
An important consideration for many applications of the Hessian is the efficiency with which it can be evaluated. If there are \(W\) parameters (weights and biases) in the network, then the Hessian matrix has dimensions \(W \times W\) and so the computational effort needed to evaluate the Hessian will scale like \(O\left(W^{2}\right)\) for each pattern in the data set. As we shall see, there are efficient methods for evaluating the Hessian whose scaling is indeed \(O\left(W^{2}\right)\).
***** Diagonal approximation
:PROPERTIES:
:CUSTOM_ID: diagonal-approximation
:END:
Some of the applications for the Hessian matrix discussed above require the inverse of the Hessian, rather than the Hessian itself. For this reason, there has been some interest in using a diagonal approximation to the Hessian, in other words one that simply replaces the off-diagonal elements with zeros, because its inverse is trivial to evaluate. Again, we shall consider an error function that consists of a sum of terms, one for each pattern in the data set, so that \(E=\sum_{n} E_{n}\). The Hessian can then be obtained by considering one pattern at a time, and then summing the results over all patterns. From (5.48), the diagonal elements of the Hessian, for pattern \(n\), can be written
\[
\begin{align*}
\frac{\partial^{2} E_{n}}{\partial w_{j i}^{2}}=\frac{\partial^{2} E_{n}}{\partial a_{j}^{2}} z_{i}^{2} \tag{5.79}
\end{align*}
\]
Using (5.48) and (5.49), the second derivatives on the right-hand side of (5.79) can be found recursively using the chain rule of differential calculus to give a backpropagation equation of the form
\[
\begin{align*}
\frac{\partial^{2} E_{n}}{\partial a_{j}^{2}}=h^{\prime}\left(a_{j}\right)^{2} \sum_{k} \sum_{k^{\prime}} w_{k j} w_{k^{\prime} j} \frac{\partial^{2} E_{n}}{\partial a_{k} \partial a_{k^{\prime}}}+h^{\prime \prime}\left(a_{j}\right) \sum_{k} w_{k j} \frac{\partial E^{n}}{\partial a_{k}} \tag{5.80}
\end{align*}
\]
If we now neglect off-diagonal elements in the second-derivative terms, we obtain (Becker and Le Cun, 1989; Le Cun et al., 1990)
\[
\begin{align*}
\frac{\partial^{2} E_{n}}{\partial a_{j}^{2}}=h^{\prime}\left(a_{j}\right)^{2} \sum_{k} w_{k j}^{2} \frac{\partial^{2} E_{n}}{\partial a_{k}^{2}}+h^{\prime \prime}\left(a_{j}\right) \sum_{k} w_{k j} \frac{\partial E_{n}}{\partial a_{k}} \tag{5.81}
\end{align*}
\]
Note that the number of computational steps required to evaluate this approximation is \(O(W)\), where \(W\) is the total number of weight and bias parameters in the network, compared with \(O\left(W^{2}\right)\) for the full Hessian.
Ricotti et al.(1988) also used the diagonal approximation to the Hessian, but they retained all terms in the evaluation of \(\partial^{2} E_{n} / \partial a_{j}^{2}\) and so obtained exact expressions for the diagonal terms. Note that this no longer has \(O(W)\) scaling. The major problem with diagonal approximations, however, is that in practice the Hessian is typically found to be strongly nondiagonal, and so these approximations, which are driven mainly be computational convenience, must be treated with care.
***** Outer product approximation
:PROPERTIES:
:CUSTOM_ID: outer-product-approximation
:END:
When neural networks are applied to regression problems, it is common to use a sum-of-squares error function of the form
\[
\begin{align*}
E=\frac{1}{2} \sum_{n=1}^{N}\left(y_{n}-t_{n}\right)^{2} \tag{5.82}
\end{align*}
\]
Exercise 5.16
Exercise 5.17
Exercise 5.19
where we have considered the case of a single output in order to keep the notation simple (the extension to several outputs is straightforward). We can then write the Hessian matrix in the form
\[
\begin{align*}
\mathbf{H}=\nabla \nabla E=\sum_{n=1}^{N} \nabla y_{n} \nabla y_{n}+\sum_{n=1}^{N}\left(y_{n}-t_{n}\right) \nabla \nabla y_{n} \tag{5.83}
\end{align*}
\]
If the network has been trained on the data set, and its outputs \(y_{n}\) happen to be very close to the target values \(t_{n}\), then the second term in (5.83) will be small and can be neglected. More generally, however, it may be appropriate to neglect this term by the following argument. Recall from Section 1.5.5 that the optimal function that minimizes a sum-of-squares loss is the conditional average of the target data. The quantity \(\left(y_{n}-t_{n}\right)\) is then a random variable with zero mean. If we assume that its value is uncorrelated with the value of the second derivative term on the right-hand side of (5.83), then the whole term will average to zero in the summation over \(n\).
By neglecting the second term in (5.83), we arrive at the Levenberg-Marquardt approximation or outer product approximation (because the Hessian matrix is built up from a sum of outer products of vectors), given by
\[
\begin{align*}
\mathbf{H} \simeq \sum_{n=1}^{N} \mathbf{b}_{n} \mathbf{b}_{n}^{\mathrm{T}} \tag{5.84}
\end{align*}
\]
where \(\mathbf{b}_{n}=\nabla y_{n}=\nabla a_{n}\) because the activation function for the output units is simply the identity. Evaluation of the outer product approximation for the Hessian is straightforward as it only involves first derivatives of the error function, which can be evaluated efficiently in \(O(W)\) steps using standard backpropagation. The elements of the matrix can then be found in \(O\left(W^{2}\right)\) steps by simple multiplication. It is important to emphasize that this approximation is only likely to be valid for a network that has been trained appropriately, and that for a general network mapping the second derivative terms on the right-hand side of (5.83) will typically not be negligible.
In the case of the cross-entropy error function for a network with logistic sigmoid output-unit activation functions, the corresponding approximation is given by
\[
\begin{align*}
\mathbf{H} \simeq \sum_{n=1}^{N} y_{n}\left(1-y_{n}\right) \mathbf{b}_{n} \mathbf{b}_{n}^{\mathrm{T}} \tag{5.85}
\end{align*}
\]
An analogous result can be obtained for multiclass networks having softmax outputunit activation functions.
***** Inverse Hessian
:PROPERTIES:
:CUSTOM_ID: inverse-hessian
:END:
We can use the outer-product approximation to develop a computationally efficient procedure for approximating the inverse of the Hessian (Hassibi and Stork, 1993). First we write the outer-product approximation in matrix notation as
\[
\begin{align*}
\mathbf{H}_{N}=\sum_{n=1}^{N} \mathbf{b}_{n} \mathbf{b}_{n}^{\mathrm{T}} \tag{5.86}
\end{align*}
\]
where \(\mathbf{b}_{n} \equiv \nabla_{\mathbf{w}} a_{n}\) is the contribution to the gradient of the output unit activation arising from data point \(n\). We now derive a sequential procedure for building up the Hessian by including data points one at a time. Suppose we have already obtained the inverse Hessian using the first \(L\) data points. By separating off the contribution from data point \(L+1\), we obtain
\[
\begin{align*}
\mathbf{H}_{L+1}=\mathbf{H}_{L}+\mathbf{b}_{L+1} \mathbf{b}_{L+1}^{\mathrm{T}} \tag{5.87}
\end{align*}
\]
In order to evaluate the inverse of the Hessian, we now consider the matrix identity
\[
\begin{align*}
\left(\mathbf{M}+\mathbf{v}^{\mathrm{T}}\right)^{-1}=\mathbf{M}^{-1}-\frac{\left(\mathbf{M}^{-1} \mathbf{v}\right)\left(\mathbf{v}^{\mathrm{T}} \mathbf{M}^{-1}\right)}{1+\mathbf{v}^{\mathrm{T}} \mathbf{M}^{-1} \mathbf{v}} \tag{5.88}
\end{align*}
\]
where \(\mathbf{I}\) is the unit matrix, which is simply a special case of the Woodbury identity (C.7). If we now identify \(\mathbf{H}_{L}\) with \(\mathbf{M}\) and \(\mathbf{b}_{L+1}\) with \(\mathbf{v}\), we obtain
\[
\begin{align*}
\mathbf{H}_{L+1}^{-1}=\mathbf{H}_{L}^{-1}-\frac{\mathbf{H}_{L}^{-1} \mathbf{b}_{L+1} \mathbf{b}_{L+1}^{\mathrm{T}} \mathbf{H}_{L}^{-1}}{1+\mathbf{b}_{L+1}^{\mathrm{T}} \mathbf{H}_{L}^{-1} \mathbf{b}_{L+1}} \tag{5.89}
\end{align*}
\]
In this way, data points are sequentially absorbed until \(L+1=N\) and the whole data set has been processed. This result therefore represents a procedure for evaluating the inverse of the Hessian using a single pass through the data set. The initial matrix \(\mathbf{H}_{0}\) is chosen to be \(\alpha \mathbf{I}\), where \(\alpha\) is a small quantity, so that the algorithm actually finds the inverse of \(\mathbf{H}+\alpha \mathbf{I}\). The results are not particularly sensitive to the precise value of \(\alpha\). Extension of this algorithm to networks having more than one output is straightforward.
We note here that the Hessian matrix can sometimes be calculated indirectly as part of the network training algorithm. In particular, quasi-Newton nonlinear optimization algorithms gradually build up an approximation to the inverse of the Hessian during training. Such algorithms are discussed in detail in Bishop and Nabney (2008).
***** Finite differences
:PROPERTIES:
:CUSTOM_ID: finite-differences
:END:
As in the case of the first derivatives of the error function, we can find the second derivatives by using finite differences, with accuracy limited by numerical precision. If we perturb each possible pair of weights in turn, we obtain
\[
\begin{align*}
\begin{gather*}
\frac{\partial^{2} E}{\partial w_{j i} \partial w_{l k}}=\frac{1}{4 \epsilon^{2}}\left\{E\left(w_{j i}+\epsilon, w_{l k}+\epsilon\right)-E\left(w_{j i}+\epsilon, w_{l k}-\epsilon\right)\right. \\
\left.-E\left(w_{j i}-\epsilon, w_{l k}+\epsilon\right)+E\left(w_{j i}-\epsilon, w_{l k}-\epsilon\right)\right\}+O\left(\epsilon^{2}\right)
\end{gather*} \tag{5.90}
\end{align*}
\]
Again, by using a symmetrical central differences formulation, we ensure that the residual errors are \(O\left(\epsilon^{2}\right)\) rather than \(O(\epsilon)\). Because there are \(W^{2}\) elements in the Hessian matrix, and because the evaluation of each element requires four forward propagations each needing \(O(W)\) operations (per pattern), we see that this approach will require \(O\left(W^{3}\right)\) operations to evaluate the complete Hessian. It therefore has poor scaling properties, although in practice it is very useful as a check on the software implementation of backpropagation methods.
A more efficient version of numerical differentiation can be found by applying central differences to the first derivatives of the error function, which are themselves calculated using backpropagation. This gives
\[
\begin{align*}
\frac{\partial^{2} E}{\partial w_{j i} \partial w_{l k}}=\frac{1}{2 \epsilon}\left\{\frac{\partial E}{\partial w_{j i}}\left(w_{l k}+\epsilon\right)-\frac{\partial E}{\partial w_{j i}}\left(w_{l k}-\epsilon\right)\right\}+O\left(\epsilon^{2}\right) \tag{5.91}
\end{align*}
\]
Because there are now only \(W\) weights to be perturbed, and because the gradients can be evaluated in \(O(W)\) steps, we see that this method gives the Hessian in \(O\left(W^{2}\right)\) operations.
***** Exact evaluation of the Hessian
:PROPERTIES:
:CUSTOM_ID: exact-evaluation-of-the-hessian
:END:
So far, we have considered various approximation schemes for evaluating the Hessian matrix or its inverse. The Hessian can also be evaluated exactly, for a network of arbitrary feed-forward topology, using extension of the technique of backpropagation used to evaluate first derivatives, which shares many of its desirable features including computational efficiency (Bishop, 1991; Bishop, 1992). It can be applied to any differentiable error function that can be expressed as a function of the network outputs and to networks having arbitrary differentiable activation functions. The number of computational steps needed to evaluate the Hessian scales like \(O\left(W^{2}\right)\). Similar algorithms have also been considered by Buntine and Weigend (1993).
Here we consider the specific case of a network having two layers of weights, for which the required equations are easily derived. We shall use indices \(i\) and \(i^{\prime}\) to denote inputs, indices \(j\) and \(j^{\prime}\) to denoted hidden units, and indices \(k\) and \(k^{\prime}\) to denote outputs. We first define
\[
\begin{align*}
\delta_{k}=\frac{\partial E_{n}}{\partial a_{k}}, \quad M_{k k^{\prime}} \equiv \frac{\partial^{2} E_{n}}{\partial a_{k} \partial a_{k^{\prime}}} \tag{5.92}
\end{align*}
\]
where \(E_{n}\) is the contribution to the error from data point \(n\). The Hessian matrix for this network can then be considered in three separate blocks as follows.
1. Both weights in the second layer:
\[
\begin{align*}
\frac{\partial^{2} E_{n}}{\partial w_{k j}^{(2)} \partial w_{k^{\prime} j^{\prime}}^{(2)}}=z_{j} z_{j^{\prime}} M_{k k^{\prime}} \tag{5.93}
\end{align*}
\]
2. [@2] Both weights in the first layer:
\[
\begin{align*}
\begin{align*}
& \frac{\partial^{2} E_{n}}{\partial w_{j i}^{(1)} \partial w_{j^{\prime} i^{\prime}}^{(1)}}=x_{i} x_{i^{\prime}} h^{\prime \prime}\left(a_{j^{\prime}}\right) I_{j j^{\prime}} \sum_{k} w_{k j^{\prime}}^{(2)} \delta_{k} \\
& \quad+x_{i} x_{i^{\prime}} h^{\prime}\left(a_{j^{\prime}}\right) h^{\prime}\left(a_{j}\right) \sum_{k} \sum_{k^{\prime}} w_{k^{\prime} j^{\prime}}^{(2)} w_{k j}^{(2)} M_{k k^{\prime}}
\end{align*} \tag{5.94}
\end{align*}
\]
3. [@3] One weight in each layer:
\[
\begin{align*}
\frac{\partial^{2} E_{n}}{\partial w_{j i}^{(1)} \partial w_{k j^{\prime}}^{(2)}}=x_{i} h^{\prime}\left(a_{j^{\prime}}\right)\left\{\delta_{k} I_{j j^{\prime}}+z_{j} \sum_{k^{\prime}} w_{k^{\prime} j^{\prime}}^{(2)} H_{k k^{\prime}}\right\} \tag{5.95}
\end{align*}
\]
Here \(I_{j j^{\prime}}\) is the \(j, j^{\prime}\) element of the identity matrix. If one or both of the weights is a bias term, then the corresponding expressions are obtained simply by setting the appropriate activation(s) to 1. Inclusion of skip-layer connections is straightforward.
***** Fast multiplication by the Hessian
:PROPERTIES:
:CUSTOM_ID: fast-multiplication-by-the-hessian
:END:
For many applications of the Hessian, the quantity of interest is not the Hessian matrix \(\mathbf{H}\) itself but the product of \(\mathbf{H}\) with some vector \(\mathbf{v}\). We have seen that the evaluation of the Hessian takes \(O\left(W^{2}\right)\) operations, and it also requires storage that is \(O\left(W^{2}\right)\). The vector \(\mathbf{v}^{\mathrm{T}} \mathbf{H}\) that we wish to calculate, however, has only \(W\) elements, so instead of computing the Hessian as an intermediate step, we can instead try to find an efficient approach to evaluating \(\mathbf{v}^{\mathrm{T}} \mathbf{H}\) directly in a way that requires only \(O(W)\) operations.
To do this, we first note that
\[
\begin{align*}
\mathbf{v}^{\mathrm{T}} \mathbf{H}=\mathbf{v}^{\mathrm{T}} \nabla(\nabla E) \tag{5.96}
\end{align*}
\]
where \(\nabla\) denotes the gradient operator in weight space. We can then write down the standard forward-propagation and backpropagation equations for the evaluation of \(\nabla E\) and apply (5.96) to these equations to give a set of forward-propagation and backpropagation equations for the evaluation of \(\mathbf{v}^{\mathrm{T}} \mathbf{H}\) (Mller, 1993; Pearlmutter, 1994). This corresponds to acting on the original forward-propagation and backpropagation equations with a differential operator \(\mathbf{v}^{\mathrm{T}} \nabla\). Pearlmutter (1994) used the notation \(\mathcal{R}\{\cdot\}\) to denote the operator \(\mathbf{v}^{\mathrm{T}} \nabla\), and we shall follow this convention. The analysis is straightforward and makes use of the usual rules of differential calculus, together with the result
\[
\begin{align*}
\mathcal{R}\{\mathbf{w}\}=\mathbf{v} \tag{5.97}
\end{align*}
\]
The technique is best illustrated with a simple example, and again we choose a two-layer network of the form shown in Figure 5.1, with linear output units and a sum-of-squares error function. As before, we consider the contribution to the error function from one pattern in the data set. The required vector is then obtained as usual by summing over the contributions from each of the patterns separately. For the two-layer network, the forward-propagation equations are given by
\[
\begin{align*}
\begin{align*}
a_{j} & =\sum_{i} w_{j i} x_{i}  \tag{5.98}\\
z_{j} & =h\left(a_{j}\right)  \tag{5.99}\\
y_{k} & =\sum_{j} w_{k j} z_{j}
\end{align*} \tag{5.100}
\end{align*}
\]
We now act on these equations using the \(\mathcal{R}\{\cdot\}\) operator to obtain a set of forward propagation equations in the form
\[
\begin{align*}
\begin{align*}
\mathcal{R}\left\{a_{j}\right\} & =\sum_{i} v_{j i} x_{i}  \tag{5.101}\\
\mathcal{R}\left\{z_{j}\right\} & =h^{\prime}\left(a_{j}\right) \mathcal{R}\left\{a_{j}\right\}  \tag{5.102}\\
\mathcal{R}\left\{y_{k}\right\} & =\sum_{j} w_{k j} \mathcal{R}\left\{z_{j}\right\}+\sum_{j} v_{k j} z_{j}
\end{align*} \tag{5.103}
\end{align*}
\]
where \(v_{j i}\) is the element of the vector \(\mathbf{v}\) that corresponds to the weight \(w_{j i}\). Quantities of the form \(\mathcal{R}\left\{z_{j}\right\}, \mathcal{R}\left\{a_{j}\right\}\) and \(\mathcal{R}\left\{y_{k}\right\}\) are to be regarded as new variables whose values are found using the above equations.
Because we are considering a sum-of-squares error function, we have the following standard backpropagation expressions:
\[
\begin{align*}
\begin{align*}
\delta_{k} & =y_{k}-t_{k}  \tag{5.104}\\
\delta_{j} & =h^{\prime}\left(a_{j}\right) \sum_{k} w_{k j} \delta_{k}
\end{align*} \tag{5.105}
\end{align*}
\]
Again, we act on these equations with the \(\mathcal{R}\{\cdot\}\) operator to obtain a set of backpropagation equations in the form
\[
\begin{align*}
\begin{align*}
\mathcal{R}\left\{\delta_{k}\right\}= & \mathcal{R}\left\{y_{k}\right\}  \tag{5.106}\\
\mathcal{R}\left\{\delta_{j}\right\}= & h^{\prime \prime}\left(a_{j}\right) \mathcal{R}\left\{a_{j}\right\} \sum_{k} w_{k j} \delta_{k} \\
& +h^{\prime}\left(a_{j}\right) \sum_{k} v_{k j} \delta_{k}+h^{\prime}\left(a_{j}\right) \sum_{k} w_{k j} \mathcal{R}\left\{\delta_{k}\right\}
\end{align*} \tag{5.107}
\end{align*}
\]
Finally, we have the usual equations for the first derivatives of the error
\[
\begin{align*}
\begin{align*}
\frac{\partial E}{\partial w_{k j}} & =\delta_{k} z_{j}  \tag{5.108}\\
\frac{\partial E}{\partial w_{j i}} & =\delta_{j} x_{i}
\end{align*} \tag{5.109}
\end{align*}
\] and acting on these with the \(\mathcal{R}\{\cdot\}\) operator, we obtain expressions for the elements of the vector \(\mathbf{v}^{\mathrm{T}} \mathbf{H}\)
\[
\begin{align*}
\begin{align*}
\mathcal{R}\left\{\frac{\partial E}{\partial w_{k j}}\right\} & =\mathcal{R}\left\{\delta_{k}\right\} z_{j}+\delta_{k} \mathcal{R}\left\{z_{j}\right\}  \tag{5.110}\\
\mathcal{R}\left\{\frac{\partial E}{\partial w_{j i}}\right\} & =x_{i} \mathcal{R}\left\{\delta_{j}\right\}
\end{align*} \tag{5.111}
\end{align*}
\]
The implementation of this algorithm involves the introduction of additional variables \(\mathcal{R}\left\{a_{j}\right\}, \mathcal{R}\left\{z_{j}\right\}\) and \(\mathcal{R}\left\{\delta_{j}\right\}\) for the hidden units and \(\mathcal{R}\left\{\delta_{k}\right\}\) and \(\mathcal{R}\left\{y_{k}\right\}\) for the output units. For each input pattern, the values of these quantities can be found using the above results, and the elements of \(\mathbf{v}^{\mathrm{T}} \mathbf{H}\) are then given by (5.110) and (5.111). An elegant aspect of this technique is that the equations for evaluating \(\mathbf{v}^{\mathrm{T}} \mathbf{H}\) mirror closely those for standard forward and backward propagation, and so the extension of existing software to compute this product is typically straightforward.
If desired, the technique can be used to evaluate the full Hessian matrix by choosing the vector \(\mathbf{v}\) to be given successively by a series of unit vectors of the form \((0,0, \ldots, 1, \ldots, 0)\) each of which picks out one column of the Hessian. This leads to a formalism that is analytically equivalent to the backpropagation procedure of Bishop (1992), as described in Section 5.4.5, though with some loss of efficiency due to redundant calculations.
**** Regularized neural networks
The number of input and outputs units in a neural network is generally determined by the dimensionality of the data set, whereas the number \(M\) of hidden units is a free parameter that can be adjusted to give the best predictive performance. Note that \(M\) controls the number of parameters (weights and biases) in the network, and so we might expect that in a maximum likelihood setting there will be an optimum value of \(M\) that gives the best generalization performance, corresponding to the optimum balance between under-fitting and over-fitting. Figure 5.9 shows an example of the effect of different values of \(M\) for the sinusoidal regression problem.
The generalization error, however, is not a simple function of \(M\) due to the presence of local minima in the error function, as illustrated in Figure 5.10. Here we see the effect of choosing multiple random initializations for the weight vector for a range of values of \(M\). The overall best validation set performance in this case occurred for a particular solution having \(M=8\). In practice, one approach to choosing \(M\) is in fact to plot a graph of the kind shown in Figure 5.10 and then to choose the specific solution having the smallest validation set error.
There are, however, other ways to control the complexity of a neural network model in order to avoid over-fitting. From our discussion of polynomial curve fitting in Chapter 1, we see that an alternative approach is to choose a relatively large value for \(M\) and then to control complexity by the addition of a regularization term to the error function. The simplest regularizer is the quadratic, giving a regularized error
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-277.jpg?height=370&width=1539&top_left_y=233&top_left_x=107]]
Figure 5.9 Examples of two-layer networks trained on 10 data points drawn from the sinusoidal data set. The graphs show the result of fitting networks having \(M=1,3\) and 10 hidden units, respectively, by minimizing a sum-of-squares error function using a scaled conjugate-gradient algorithm.
of the form
\[
\begin{align*}
\widetilde{E}(\mathbf{w})=E(\mathbf{w})+\frac{\lambda}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \tag{5.112}
\end{align*}
\]
This regularizer is also known as weight decay and has been discussed at length in Chapter 3. The effective model complexity is then determined by the choice of the regularization coefficient \(\lambda\). As we have seen previously, this regularizer can be interpreted as the negative logarithm of a zero-mean Gaussian prior distribution over the weight vector \(\mathbf{w}\).
***** Consistent Gaussian priors
:PROPERTIES:
:CUSTOM_ID: consistent-gaussian-priors
:END:
One of the limitations of simple weight decay in the form (5.112) is that is inconsistent with certain scaling properties of network mappings. To illustrate this, consider a multilayer perceptron network having two layers of weights and linear output units, which performs a mapping from a set of input variables \(\left\{x_{i}\right\}\) to a set of output variables \(\left\{y_{k}\right\}\). The activations of the hidden units in the first hidden layer
Figure 5.10 Plot of the sum-of-squares test-set error for the polynomial data set versus the number of hidden units in the network, with 30 random starts for each network size, showing the effect of local minima. For each new start, the weight vector was initialized by sampling from an isotropic Gaussian distribution having a mean of zero and a variance of 10 .
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-277.jpg?height=533&width=705&top_left_y=1498&top_left_x=937]] take the form
\[
\begin{align*}
z_{j}=h\left(\sum_{i} w_{j i} x_{i}+w_{j 0}\right) \tag{5.113}
\end{align*}
\]
while the activations of the output units are given by
\[
\begin{align*}
y_{k}=\sum_{j} w_{k j} z_{j}+w_{k 0} \tag{5.114}
\end{align*}
\]
Suppose we perform a linear transformation of the input data of the form
\[
\begin{align*}
x_{i} \rightarrow \widetilde{x}_{i}=a x_{i}+b \tag{5.115}
\end{align*}
\]
Then we can arrange for the mapping performed by the network to be unchanged by making a corresponding linear transformation of the weights and biases from the inputs to the units in the hidden layer of the form
\[
\begin{align*}
\begin{align*}
& w_{j i} \rightarrow \widetilde{w}_{j i}=\frac{1}{a} w_{j i}  \tag{5.116}\\
& w_{j 0} \rightarrow \widetilde{w}_{j 0}=w_{j 0}-\frac{b}{a} \sum_{i} w_{j i}
\end{align*} \tag{5.117}
\end{align*}
\]
Similarly, a linear transformation of the output variables of the network of the form
\[
\begin{align*}
y_{k} \rightarrow \widetilde{y}_{k}=c y_{k}+d \tag{5.118}
\end{align*}
\]
can be achieved by making a transformation of the second-layer weights and biases using
\[
\begin{align*}
\begin{align*}
w_{k j} \rightarrow \widetilde{w}_{k j} & =c w_{k j}  \tag{5.119}\\
w_{k 0} \rightarrow \widetilde{w}_{k 0} & =c w_{k 0}+d
\end{align*} \tag{5.120}
\end{align*}
\]
If we train one network using the original data and one network using data for which the input and/or target variables are transformed by one of the above linear transformations, then consistency requires that we should obtain equivalent networks that differ only by the linear transformation of the weights as given. Any regularizer should be consistent with this property, otherwise it arbitrarily favours one solution over another, equivalent one. Clearly, simple weight decay (5.112), that treats all weights and biases on an equal footing, does not satisfy this property.
We therefore look for a regularizer which is invariant under the linear transformations (5.116), (5.117), (5.119) and (5.120). These require that the regularizer should be invariant to re-scaling of the weights and to shifts of the biases. Such a regularizer is given by
\[
\begin{align*}
\frac{\lambda_{1}}{2} \sum_{w \in \mathcal{W}_{1}} w^{2}+\frac{\lambda_{2}}{2} \sum_{w \in \mathcal{W}_{2}} w^{2} \tag{5.121}
\end{align*}
\]
where \(\mathcal{W}_{1}\) denotes the set of weights in the first layer, \(\mathcal{W}_{2}\) denotes the set of weights in the second layer, and biases are excluded from the summations. This regularizer will remain unchanged under the weight transformations provided the regularization parameters are re-scaled using \(\lambda_{1} \rightarrow a^{1 / 2} \lambda_{1}\) and \(\lambda_{2} \rightarrow c^{-1 / 2} \lambda_{2}\).
The regularizer (5.121) corresponds to a prior of the form
\[
\begin{align*}
p\left(\mathbf{w} \mid \alpha_{1}, \alpha_{2}\right) \propto \exp \left(-\frac{\alpha_{1}}{2} \sum_{w \in \mathcal{W}_{1}} w^{2}-\frac{\alpha_{2}}{2} \sum_{w \in \mathcal{W}_{2}} w^{2}\right) \tag{5.122}
\end{align*}
\]
Note that priors of this form are improper (they cannot be normalized) because the bias parameters are unconstrained. The use of improper priors can lead to difficulties in selecting regularization coefficients and in model comparison within the Bayesian framework, because the corresponding evidence is zero. It is therefore common to include separate priors for the biases (which then break shift invariance) having their own hyperparameters. We can illustrate the effect of the resulting four hyperparameters by drawing samples from the prior and plotting the corresponding network functions, as shown in Figure 5.11.
More generally, we can consider priors in which the weights are divided into any number of groups \(\mathcal{W}_{k}\) so that
\[
\begin{align*}
p(\mathbf{w}) \propto \exp \left(-\frac{1}{2} \sum_{k} \alpha_{k}\|\mathbf{w}\|_{k}^{2}\right) \tag{5.123}
\end{align*}
\]
where
\[
\begin{align*}
\|\mathbf{w}\|_{k}^{2}=\sum_{j \in \mathcal{W}_{k}} w_{j}^{2} \tag{5.124}
\end{align*}
\]
As a special case of this prior, if we choose the groups to correspond to the sets of weights associated with each of the input units, and we optimize the marginal likelihood with respect to the corresponding parameters \(\alpha_{k}\), we obtain automatic relevance determination as discussed in Section 7.2.2.
***** Early stopping
:PROPERTIES:
:CUSTOM_ID: early-stopping
:END:
An alternative to regularization as a way of controlling the effective complexity of a network is the procedure of early stopping. The training of nonlinear network models corresponds to an iterative reduction of the error function defined with respect to a set of training data. For many of the optimization algorithms used for network training, such as conjugate gradients, the error is a nonincreasing function of the iteration index. However, the error measured with respect to independent data, generally called a validation set, often shows a decrease at first, followed by an increase as the network starts to over-fit. Training can therefore be stopped at the point of smallest error with respect to the validation data set, as indicated in Figure 5.12, in order to obtain a network having good generalization performance.
The behaviour of the network in this case is sometimes explained qualitatively in terms of the effective number of degrees of freedom in the network, in which this number starts out small and then to grows during the training process, corresponding to a steady increase in the effective complexity of the model. Halting training before
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-280.jpg?height=922&width=1479&top_left_y=239&top_left_x=128]]
Figure 5.11 Illustration of the effect of the hyperparameters governing the prior distribution over weights and biases in a two-layer network having a single input, a single linear output, and 12 hidden units having 'tanh' activation functions. The priors are governed by four hyperparameters \(\alpha_{1}^{\mathrm{b}}, \alpha_{1}^{\mathrm{w}}, \alpha_{2}^{\mathrm{b}}\), and \(\alpha_{2}^{\mathrm{w}}\), which represent the precisions of the Gaussian distributions of the first-layer biases, first-layer weights, second-layer biases, and second-layer weights, respectively. We see that the parameter \(\alpha_{2}^{\mathrm{w}}\) governs the vertical scale of functions (note the different vertical axis ranges on the top two diagrams), \(\alpha_{1}^{\mathrm{w}}\) governs the horizontal scale of variations in the function values, and \(\alpha_{1}^{\mathrm{b}}\) governs the horizontal range over which variations occur. The parameter \(\alpha_{2}^{\mathrm{b}}\), whose effect is not illustrated here, governs the range of vertical offsets of the functions.
a minimum of the training error has been reached then represents a way of limiting the effective network complexity.
In the case of a quadratic error function, we can verify this insight, and show that early stopping should exhibit similar behaviour to regularization using a simple weight-decay term. This can be understood from Figure 5.13, in which the axes in weight space have been rotated to be parallel to the eigenvectors of the Hessian matrix. If, in the absence of weight decay, the weight vector starts at the origin and proceeds during training along a path that follows the local negative gradient vector, then the weight vector will move initially parallel to the \(w_{2}\) axis through a point corresponding roughly to \(\widetilde{\mathbf{w}}\) and then move towards the minimum of the error function \(\mathbf{w}_{\mathrm{ML}}\). This follows from the shape of the error surface and the widely differing eigenvalues of the Hessian. Stopping at a point near \(\widetilde{\mathbf{w}}\) is therefore similar to weight decay. The relationship between early stopping and weight decay can be made quantitative, thereby showing that the quantity \(\tau \eta\) (where \(\tau\) is the iteration index, and \(\eta\) is the learning rate parameter) plays the role of the reciprocal of the regularization
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-281.jpg?height=510&width=1487&top_left_y=244&top_left_x=124]]
Figure 5.12 An illustration of the behaviour of training set error (left) and validation set error (right) during a typical training session, as a function of the iteration step, for the sinusoidal data set. The goal of achieving the best generalization performance suggests that training should be stopped at the point shown by the vertical dashed lines, corresponding to the minimum of the validation set error.
parameter \(\lambda\). The effective number of parameters in the network therefore grows during the course of training.
***** Invariances
:PROPERTIES:
:CUSTOM_ID: invariances
:END:
In many applications of pattern recognition, it is known that predictions should be unchanged, or invariant, under one or more transformations of the input variables. For example, in the classification of objects in two-dimensional images, such as handwritten digits, a particular object should be assigned the same classification irrespective of its position within the image (translation invariance) or of its size (scale invariance). Such transformations produce significant changes in the raw data, expressed in terms of the intensities at each of the pixels in the image, and yet should give rise to the same output from the classification system. Similarly in speech recognition, small levels of nonlinear warping along the time axis, which preserve temporal ordering, should not change the interpretation of the signal.
If sufficiently large numbers of training patterns are available, then an adaptive model such as a neural network can learn the invariance, at least approximately. This involves including within the training set a sufficiently large number of examples of the effects of the various transformations. Thus, for translation invariance in an image, the training set should include examples of objects at many different positions.
This approach may be impractical, however, if the number of training examples is limited, or if there are several invariants (because the number of combinations of transformations grows exponentially with the number of such transformations). We therefore seek alternative approaches for encouraging an adaptive model to exhibit the required invariances. These can broadly be divided into four categories:
1. The training set is augmented using replicas of the training patterns, transformed according to the desired invariances. For instance, in our digit recognition example, we could make multiple copies of each example in which the
Figure 5.13 A schematic illustration of why early stopping can give similar results to weight decay in the case of a quadratic error function. The ellipse shows a contour of constant error, and \(\mathrm{w}_{\mathrm{ML}}\) denotes the minimum of the error function. If the weight vector starts at the origin and moves according to the local negative gradient direction, then it will follow the path shown by the curve. By stopping training early, a weight vector \(\widetilde{\mathbf{w}}\) is found that is qualitatively similar to that obtained with a simple weight-decay regularizer and training to the mini-
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-282.jpg?height=601&width=756&top_left_y=235&top_left_x=887]] mum of the regularized error, as can be seen by comparing with Figure 3.15.
digit is shifted to a different position in each image.
2. [@2] A regularization term is added to the error function that penalizes changes in the model output when the input is transformed. This leads to the technique of tangent propagation, discussed in Section 5.5.4.
3. Invariance is built into the pre-processing by extracting features that are invariant under the required transformations. Any subsequent regression or classification system that uses such features as inputs will necessarily also respect these invariances.
4. The final option is to build the invariance properties into the structure of a neural network (or into the definition of a kernel function in the case of techniques such as the relevance vector machine). One way to achieve this is through the use of local receptive fields and shared weights, as discussed in the context of convolutional neural networks in Section 5.5.6.
Approach 1 is often relatively easy to implement and can be used to encourage complex invariances such as those illustrated in Figure 5.14. For sequential training algorithms, this can be done by transforming each input pattern before it is presented to the model so that, if the patterns are being recycled, a different transformation (drawn from an appropriate distribution) is added each time. For batch methods, a similar effect can be achieved by replicating each data point a number of times and transforming each copy independently. The use of such augmented data can lead to significant improvements in generalization (Simard et al., 2003), although it can also be computationally costly.
Approach 2 leaves the data set unchanged but modifies the error function through the addition of a regularizer. In Section 5.5.5, we shall show that this approach is closely related to approach 2 . [[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-283.jpg?height=542&width=1482&top_left_y=248&top_left_x=126]]
Figure 5.14 Illustration of the synthetic warping of a handwritten digit. The original image is shown on the left. On the right, the top row shows three examples of warped digits, with the corresponding displacement fields shown on the bottom row. These displacement fields are generated by sampling random displacements \(\Delta x, \Delta y \in(0,1)\) at each pixel and then smoothing by convolution with Gaussians of width \(0.01,30\) and 60 respectively.
One advantage of approach 3 is that it can correctly extrapolate well beyond the range of transformations included in the training set. However, it can be difficult to find hand-crafted features with the required invariances that do not also discard information that can be useful for discrimination.
***** Tangent propagation
:PROPERTIES:
:CUSTOM_ID: tangent-propagation
:END:
We can use regularization to encourage models to be invariant to transformations of the input through the technique of tangent propagation (Simard et al., 1992). Consider the effect of a transformation on a particular input vector \(\mathbf{x}_{n}\). Provided the transformation is continuous (such as translation or rotation, but not mirror reflection for instance), then the transformed pattern will sweep out a manifold \(\mathcal{M}\) within the \(D\)-dimensional input space. This is illustrated in Figure 5.15, for the case of \(D=\) 2 for simplicity. Suppose the transformation is governed by a single parameter \(\xi\) (which might be rotation angle for instance). Then the subspace \(\mathcal{M}\) swept out by \(\mathbf{x}_{n}\)
Figure 5.15 Illustration of a two-dimensional input space showing the effect of a continuous transformation on a particular input vector \(\mathbf{x}_{n}\). A onedimensional transformation, parameterized by the continuous variable \(\xi\), applied to \(\mathbf{x}_{n}\) causes it to sweep out a one-dimensional manifold \(\mathcal{M}\). Locally, the effect of the transformation can be approximated by the tangent vector \(\tau_{n}\).
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-283.jpg?height=402&width=560&top_left_y=1721&top_left_x=1082]] will be one-dimensional, and will be parameterized by \(\xi\). Let the vector that results from acting on \(\mathbf{x}_{n}\) by this transformation be denoted by \(\mathbf{s}\left(\mathbf{x}_{n}, \xi\right)\), which is defined so that \(\mathbf{s}(\mathbf{x}, 0)=\mathbf{x}\). Then the tangent to the curve \(\mathcal{M}\) is given by the directional derivative \(\boldsymbol{\tau}=\partial \mathbf{s} / \partial \xi\), and the tangent vector at the point \(\mathbf{x}_{n}\) is given by
\[
\begin{align*}
\boldsymbol{\tau}_{n}=\left.\frac{\partial \mathbf{s}\left(\mathbf{x}_{n}, \xi\right)}{\partial \xi}\right|_{\xi=0} \tag{5.125}
\end{align*}
\]
Under a transformation of the input vector, the network output vector will, in general, change. The derivative of output \(k\) with respect to \(\xi\) is given by
\[
\begin{align*}
\left.\frac{\partial y_{k}}{\partial \xi}\right|_{\xi=0}=\left.\sum_{i=1}^{D} \frac{\partial y_{k}}{\partial x_{i}} \frac{\partial x_{i}}{\partial \xi}\right|_{\xi=0}=\sum_{i=1}^{D} J_{k i} \tau_{i} \tag{5.126}
\end{align*}
\]
where \(J_{k i}\) is the \((k, i)\) element of the Jacobian matrix \(\mathbf{J}\), as discussed in Section 5.3.4. The result (5.126) can be used to modify the standard error function, so as to encourage local invariance in the neighbourhood of the data points, by the addition to the original error function \(E\) of a regularization function \(\Omega\) to give a total error function of the form
\[
\begin{align*}
\widetilde{E}=E+\lambda \Omega \tag{5.127}
\end{align*}
\]
where \(\lambda\) is a regularization coefficient and
\[
\begin{align*}
\Omega=\frac{1}{2} \sum_{n} \sum_{k}\left(\left.\frac{\partial y_{n k}}{\partial \xi}\right|_{\xi=0}\right)^{2}=\frac{1}{2} \sum_{n} \sum_{k}\left(\sum_{i=1}^{D} J_{n k i} \tau_{n i}\right)^{2} \tag{5.128}
\end{align*}
\]
The regularization function will be zero when the network mapping function is invariant under the transformation in the neighbourhood of each pattern vector, and the value of the parameter \(\lambda\) determines the balance between fitting the training data and learning the invariance property.
In a practical implementation, the tangent vector \(\tau_{n}\) can be approximated using finite differences, by subtracting the original vector \(\mathbf{x}_{n}\) from the corresponding vector after transformation using a small value of \(\xi\), and then dividing by \(\xi\). This is illustrated in Figure 5.16.
The regularization function depends on the network weights through the Jacobian \(\mathbf{J}\). A backpropagation formalism for computing the derivatives of the regularizer with respect to the network weights is easily obtained by extension of the techniques introduced in Section 5.3.
If the transformation is governed by \(L\) parameters (e.g., \(L=3\) for the case of translations combined with in-plane rotations in a two-dimensional image), then the manifold \(\mathcal{M}\) will have dimensionality \(L\), and the corresponding regularizer is given by the sum of terms of the form (5.128), one for each transformation. If several transformations are considered at the same time, and the network mapping is made invariant to each separately, then it will be (locally) invariant to combinations of the transformations (Simard et al., 1992).
Figure 5.16 Illustration showing (a) the original image \(x\) of a handwritten digit, (b) the tangent vector \(\tau\) corresponding to an infinitesimal clockwise rotation, (c) the result of adding a small contribution from the tangent vector to the original image giving \(\mathrm{x}+\epsilon \tau\) with \(\epsilon=15\) degrees, and (d) the true image rotated for comparison. [[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-285.jpg?height=998&width=998&top_left_y=240&top_left_x=631]]
A related technique, called tangent distance, can be used to build invariance properties into distance-based methods such as nearest-neighbour classifiers (Simard et al., 1993).
***** Training with transformed data
:PROPERTIES:
:CUSTOM_ID: training-with-transformed-data
:END:
We have seen that one way to encourage invariance of a model to a set of transformations is to expand the training set using transformed versions of the original input patterns. Here we show that this approach is closely related to the technique of tangent propagation (Bishop, 1995b; Leen, 1995).
As in Section 5.5.4, we shall consider a transformation governed by a single parameter \(\xi\) and described by the function \(\mathbf{s}(\mathbf{x}, \xi)\), with \(\mathbf{s}(\mathbf{x}, 0)=\mathbf{x}\). We shall also consider a sum-of-squares error function. The error function for untransformed inputs can be written (in the infinite data set limit) in the form
\[
\begin{align*}
E=\frac{1}{2} \iint\{y(\mathbf{x})-t\}^{2} p(t \mid \mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x} \mathrm{d} t \tag{5.129}
\end{align*}
\]
as discussed in Section 1.5.5. Here we have considered a network having a single output, in order to keep the notation uncluttered. If we now consider an infinite number of copies of each data point, each of which is perturbed by the transformation in which the parameter \(\xi\) is drawn from a distribution \(p(\xi)\), then the error function defined over this expanded data set can be written as
\[
\begin{align*}
\widetilde{E}=\frac{1}{2} \iiint\{y(\mathbf{s}(\mathbf{x}, \xi))-t\}^{2} p(t \mid \mathbf{x}) p(\mathbf{x}) p(\xi) \mathrm{d} \mathbf{x} \mathrm{d} t \mathrm{~d} \xi \tag{5.130}
\end{align*}
\]
We now assume that the distribution \(p(\xi)\) has zero mean with small variance, so that we are only considering small transformations of the original input vectors. We can then expand the transformation function as a Taylor series in powers of \(\xi\) to give
\[
\begin{align*}
\begin{aligned}
\mathbf{s}(\mathbf{x}, \xi) & =\mathbf{s}(\mathbf{x}, 0)+\left.\xi \frac{\partial}{\partial \xi} \mathbf{s}(\mathbf{x}, \xi)\right|_{\xi=0}+\left.\frac{\xi^{2}}{2} \frac{\partial^{2}}{\partial \xi^{2}} \mathbf{s}(\mathbf{x}, \xi)\right|_{\xi=0}+O\left(\xi^{3}\right) \\
& =\mathbf{x}+\xi \boldsymbol{\tau}+\frac{1}{2} \xi^{2} \boldsymbol{\tau}^{\prime}+O\left(\xi^{3}\right)
\end{aligned}
\end{align*}
\]
where \(\boldsymbol{\tau}^{\prime}\) denotes the second derivative of \(\mathbf{s}(\mathbf{x}, \xi)\) with respect to \(\xi\) evaluated at \(\xi=0\). This allows us to expand the model function to give
\[
\begin{align*}
y(\mathbf{s}(\mathbf{x}, \xi))=y(\mathbf{x})+\xi \boldsymbol{\tau}^{\mathrm{T}} \nabla y(\mathbf{x})+\frac{\xi^{2}}{2}\left[\left(\boldsymbol{\tau}^{\prime}\right)^{\mathrm{T}} \nabla y(\mathbf{x})+\boldsymbol{\tau}^{\mathrm{T}} \nabla \nabla y(\mathbf{x}) \boldsymbol{\tau}\right]+O\left(\xi^{3}\right)
\end{align*}
\]
Substituting into the mean error function (5.130) and expanding, we then have
\[
\begin{align*}
\begin{aligned}
\widetilde{E}= & \frac{1}{2} \iint\{y(\mathbf{x})-t\}^{2} p(t \mid \mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x} \mathrm{d} t \\
+ & \mathbb{E}[\xi] \iint\{y(\mathbf{x})-t\} \boldsymbol{\tau}^{\mathrm{T}} \nabla y(\mathbf{x}) p(t \mid \mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x} \mathrm{d} t \\
+ & \mathbb{E}\left[\xi^{2}\right] \iint\left[\{y(\mathbf{x})-t\} \frac{1}{2}\left\{\left(\boldsymbol{\tau}^{\prime}\right)^{\mathrm{T}} \nabla y(\mathbf{x})+\boldsymbol{\tau}^{\mathrm{T}} \nabla \nabla y(\mathbf{x}) \boldsymbol{\tau}\right\}\right. \\
& \left.+\left(\boldsymbol{\tau}^{\mathrm{T}} \nabla y(\mathbf{x})\right)^{2}\right] p(t \mid \mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x} \mathrm{d} t+O\left(\xi^{3}\right)
\end{aligned}
\end{align*}
\]
Because the distribution of transformations has zero mean we have \(\mathbb{E}[\xi]=0\). Also, we shall denote \(\mathbb{E}\left[\xi^{2}\right]\) by \(\lambda\). Omitting terms of \(O\left(\xi^{3}\right)\), the average error function then becomes
\[
\begin{align*}
\widetilde{E}=E+\lambda \Omega \tag{5.131}
\end{align*}
\]
where \(E\) is the original sum-of-squares error, and the regularization term \(\Omega\) takes the form
\[
\begin{align*}
\begin{align*}
\Omega= & \int\left[\{y(\mathbf{x})-\mathbb{E}[t \mid \mathbf{x}]\} \frac{1}{2}\left\{\left(\boldsymbol{\tau}^{\prime}\right)^{\mathrm{T}} \nabla y(\mathbf{x})+\boldsymbol{\tau}^{\mathrm{T}} \nabla \nabla y(\mathbf{x}) \boldsymbol{\tau}\right\}\right. \\
& \left.+\left(\boldsymbol{\tau}^{T} \nabla y(\mathbf{x})\right)^{2}\right] p(\mathbf{x}) \mathrm{d} \mathbf{x}
\end{align*} \tag{5.132}
\end{align*}
\]
in which we have performed the integration over \(t\).
We can further simplify this regularization term as follows. In Section 1.5.5 we saw that the function that minimizes the sum-of-squares error is given by the conditional average \(\mathbb{E}[t \mid \mathbf{x}]\) of the target values \(t\). From (5.131) we see that the regularized error will equal the unregularized sum-of-squares plus terms which are \(O(\xi)\), and so the network function that minimizes the total error will have the form
\[
\begin{align*}
y(\mathbf{x})=\mathbb{E}[t \mid \mathbf{x}]+O(\xi) \tag{5.133}
\end{align*}
\]
Thus, to leading order in \(\xi\), the first term in the regularizer vanishes and we are left with
\[
\begin{align*}
\Omega=\frac{1}{2} \int\left(\boldsymbol{\tau}^{T} \nabla y(\mathbf{x})\right)^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x} \tag{5.134}
\end{align*}
\]
which is equivalent to the tangent propagation regularizer (5.128).
If we consider the special case in which the transformation of the inputs simply consists of the addition of random noise, so that \(\mathbf{x} \rightarrow \mathbf{x}+\boldsymbol{\xi}\), then the regularizer
\[
\begin{align*}
\Omega=\frac{1}{2} \int\|\nabla y(\mathbf{x})\|^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x} \tag{5.135}
\end{align*}
\]
which is known as Tikhonov regularization (Tikhonov and Arsenin, 1977; Bishop, 1995b). Derivatives of this regularizer with respect to the network weights can be found using an extended backpropagation algorithm (Bishop, 1993). We see that, for small noise amplitudes, Tikhonov regularization is related to the addition of random noise to the inputs, which has been shown to improve generalization in appropriate circumstances (Sietsma and Dow, 1991).
***** Convolutional networks
:PROPERTIES:
:CUSTOM_ID: convolutional-networks
:END:
Another approach to creating models that are invariant to certain transformation of the inputs is to build the invariance properties into the structure of a neural network. This is the basis for the convolutional neural network (Le Cun et al., 1989; LeCun et al., 1998), which has been widely applied to image data.
Consider the specific task of recognizing handwritten digits. Each input image comprises a set of pixel intensity values, and the desired output is a posterior probability distribution over the ten digit classes. We know that the identity of the digit is invariant under translations and scaling as well as (small) rotations. Furthermore, the network must also exhibit invariance to more subtle transformations such as elastic deformations of the kind illustrated in Figure 5.14. One simple approach would be to treat the image as the input to a fully connected network, such as the kind shown in Figure 5.1. Given a sufficiently large training set, such a network could in principle yield a good solution to this problem and would learn the appropriate invariances by example.
However, this approach ignores a key property of images, which is that nearby pixels are more strongly correlated than more distant pixels. Many of the modern approaches to computer vision exploit this property by extracting local features that depend only on small subregions of the image. Information from such features can then be merged in later stages of processing in order to detect higher-order features
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-288.jpg?height=865&width=1213&top_left_y=232&top_left_x=414]]
Figure 5.17 Diagram illustrating part of a convolutional neural network, showing a layer of convolutional units followed by a layer of subsampling units. Several successive pairs of such layers may be used.
and ultimately to yield information about the image as whole. Also, local features that are useful in one region of the image are likely to be useful in other regions of the image, for instance if the object of interest is translated.
These notions are incorporated into convolutional neural networks through three mechanisms: (i) local receptive fields, (ii) weight sharing, and (iii) subsampling. The structure of a convolutional network is illustrated in Figure 5.17. In the convolutional layer the units are organized into planes, each of which is called a feature map. Units in a feature map each take inputs only from a small subregion of the image, and all of the units in a feature map are constrained to share the same weight values. For instance, a feature map might consist of 100 units arranged in a \(10 \times 10\) grid, with each unit taking inputs from a \(5 \times 5\) pixel patch of the image. The whole feature map therefore has 25 adjustable weight parameters plus one adjustable bias parameter. Input values from a patch are linearly combined using the weights and the bias, and the result transformed by a sigmoidal nonlinearity using (5.1). If we think of the units as feature detectors, then all of the units in a feature map detect the same pattern but at different locations in the input image. Due to the weight sharing, the evaluation of the activations of these units is equivalent to a convolution of the image pixel intensities with a 'kernel' comprising the weight parameters. If the input image is shifted, the activations of the feature map will be shifted by the same amount but will otherwise be unchanged. This provides the basis for the (approximate) invariance of the network outputs to translations and distortions of the input image. Because we will typically need to detect multiple features in order to build an effective model, there will generally be multiple feature maps in the convolutional layer, each having its own set of weight and bias parameters.
The outputs of the convolutional units form the inputs to the subsampling layer of the network. For each feature map in the convolutional layer, there is a plane of units in the subsampling layer and each unit takes inputs from a small receptive field in the corresponding feature map of the convolutional layer. These units perform subsampling. For instance, each subsampling unit might take inputs from a \(2 \times 2\) unit region in the corresponding feature map and would compute the average of those inputs, multiplied by an adaptive weight with the addition of an adaptive bias parameter, and then transformed using a sigmoidal nonlinear activation function. The receptive fields are chosen to be contiguous and nonoverlapping so that there are half the number of rows and columns in the subsampling layer compared with the convolutional layer. In this way, the response of a unit in the subsampling layer will be relatively insensitive to small shifts of the image in the corresponding regions of the input space.
In a practical architecture, there may be several pairs of convolutional and subsampling layers. At each stage there is a larger degree of invariance to input transformations compared to the previous layer. There may be several feature maps in a given convolutional layer for each plane of units in the previous subsampling layer, so that the gradual reduction in spatial resolution is then compensated by an increasing number of features. The final layer of the network would typically be a fully connected, fully adaptive layer, with a softmax output nonlinearity in the case of multiclass classification.
The whole network can be trained by error minimization using backpropagation to evaluate the gradient of the error function. This involves a slight modification of the usual backpropagation algorithm to ensure that the shared-weight constraints are satisfied. Due to the use of local receptive fields, the number of weights in the network is smaller than if the network were fully connected. Furthermore, the number of independent parameters to be learned from the data is much smaller still, due to the substantial numbers of constraints on the weights.
***** Soft weight sharing
:PROPERTIES:
:CUSTOM_ID: soft-weight-sharing
:END:
One way to reduce the effective complexity of a network with a large number of weights is to constrain weights within certain groups to be equal. This is the technique of weight sharing that was discussed in Section 5.5.6 as a way of building translation invariance into networks used for image interpretation. It is only applicable, however, to particular problems in which the form of the constraints can be specified in advance. Here we consider a form of soft weight sharing (Nowlan and Hinton, 1992) in which the hard constraint of equal weights is replaced by a form of regularization in which groups of weights are encouraged to have similar values. Furthermore, the division of weights into groups, the mean weight value for each group, and the spread of values within the groups are all determined as part of the learning process.
Recall that the simple weight decay regularizer, given in (5.112), can be viewed as the negative log of a Gaussian prior distribution over the weights. We can encourage the weight values to form several groups, rather than just one group, by considering instead a probability distribution that is a mixture of Gaussians. The centres and variances of the Gaussian components, as well as the mixing coefficients, will be considered as adjustable parameters to be determined as part of the learning process. Thus, we have a probability density of the form
\[
\begin{align*}
p(\mathbf{w})=\prod_{i} p\left(w_{i}\right) \tag{5.136}
\end{align*}
\]
where
\[
\begin{align*}
p\left(w_{i}\right)=\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right) \tag{5.137}
\end{align*}
\]
and \(\pi_{j}\) are the mixing coefficients. Taking the negative logarithm then leads to a regularization function of the form
\[
\begin{align*}
\Omega(\mathbf{w})=-\sum_{i} \ln \left(\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)\right) \tag{5.138}
\end{align*}
\]
The total error function is then given by
\[
\begin{align*}
\widetilde{E}(\mathbf{w})=E(\mathbf{w})+\lambda \Omega(\mathbf{w}) \tag{5.139}
\end{align*}
\]
where \(\lambda\) is the regularization coefficient. This error is minimized both with respect to the weights \(w_{i}\) and with respect to the parameters \(\left\{\pi_{j}, \mu_{j}, \sigma_{j}\right\}\) of the mixture model. If the weights were constant, then the parameters of the mixture model could be determined by using the EM algorithm discussed in Chapter 9. However, the distribution of weights is itself evolving during the learning process, and so to avoid numerical instability, a joint optimization is performed simultaneously over the weights and the mixture-model parameters. This can be done using a standard optimization algorithm such as conjugate gradients or quasi-Newton methods.
In order to minimize the total error function, it is necessary to be able to evaluate its derivatives with respect to the various adjustable parameters. To do this it is convenient to regard the \(\left\{\pi_{j}\right\}\) as prior probabilities and to introduce the corresponding posterior probabilities which, following (2.192), are given by Bayes' theorem in the form
\[
\begin{align*}
\gamma_{j}(w)=\frac{\pi_{j} \mathcal{N}\left(w \mid \mu_{j}, \sigma_{j}^{2}\right)}{\sum_{k} \pi_{k} \mathcal{N}\left(w \mid \mu_{k}, \sigma_{k}^{2}\right)} \tag{5.140}
\end{align*}
\]
The derivatives of the total error function with respect to the weights are then given by
\[
\begin{align*}
\frac{\partial \widetilde{E}}{\partial w_{i}}=\frac{\partial E}{\partial w_{i}}+\lambda \sum_{j} \gamma_{j}\left(w_{i}\right) \frac{\left(w_{i}-\mu_{j}\right)}{\sigma_{j}^{2}} \tag{5.141}
\end{align*}
\]
The effect of the regularization term is therefore to pull each weight towards the centre of the \(j^{\text {th }}\) Gaussian, with a force proportional to the posterior probability of that Gaussian for the given weight. This is precisely the kind of effect that we are seeking.
Derivatives of the error with respect to the centres of the Gaussians are also
Exercise 5.30
Exercise 5.31 easily computed to give
\[
\begin{align*}
\frac{\partial \widetilde{E}}{\partial \mu_{j}}=\lambda \sum_{i} \gamma_{j}\left(w_{i}\right) \frac{\left(\mu_{i}-w_{j}\right)}{\sigma_{j}^{2}} \tag{5.142}
\end{align*}
\]
which has a simple intuitive interpretation, because it pushes \(\mu_{j}\) towards an average of the weight values, weighted by the posterior probabilities that the respective weight parameters were generated by component \(j\). Similarly, the derivatives with respect to the variances are given by
\[
\begin{align*}
\frac{\partial \widetilde{E}}{\partial \sigma_{j}}=\lambda \sum_{i} \gamma_{j}\left(w_{i}\right)\left(\frac{1}{\sigma_{j}}-\frac{\left(w_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{3}}\right) \tag{5.143}
\end{align*}
\]
which drives \(\sigma_{j}\) towards the weighted average of the squared deviations of the weights around the corresponding centre \(\mu_{j}\), where the weighting coefficients are again given by the posterior probability that each weight is generated by component \(j\). Note that in a practical implementation, new variables \(\eta_{j}\) defined by
\[
\begin{align*}
\sigma_{j}^{2}=\exp \left(\eta_{j}\right) \tag{5.144}
\end{align*}
\]
are introduced, and the minimization is performed with respect to the \(\eta_{j}\). This ensures that the parameters \(\sigma_{j}\) remain positive. It also has the effect of discouraging pathological solutions in which one or more of the \(\sigma_{j}\) goes to zero, corresponding to a Gaussian component collapsing onto one of the weight parameter values. Such solutions are discussed in more detail in the context of Gaussian mixture models in Section 9.2.1.
For the derivatives with respect to the mixing coefficients \(\pi_{j}\), we need to take account of the constraints
\[
\begin{align*}
\sum_{j} \pi_{j}=1, \quad 0 \leqslant \pi_{i} \leqslant 1 \tag{5.145}
\end{align*}
\]
which follow from the interpretation of the \(\pi_{j}\) as prior probabilities. This can be done by expressing the mixing coefficients in terms of a set of auxiliary variables \(\left\{\eta_{j}\right\}\) using the softmax function given by
\[
\begin{align*}
\pi_{j}=\frac{\exp \left(\eta_{j}\right)}{\sum_{k=1}^{M} \exp \left(\eta_{k}\right)} \tag{5.146}
\end{align*}
\]
The derivatives of the regularized error function with respect to the \(\left\{\eta_{j}\right\}\) then take the form
Figure 5.18 The left figure shows a two-link robot arm, in which the Cartesian coordinates \(\left(x_{1}, x_{2}\right)\) of the end effector are determined uniquely by the two joint angles \(\theta_{1}\) and \(\theta_{2}\) and the (fixed) lengths \(L_{1}\) and \(L_{2}\) of the arms. This is know as the forward kinematics of the arm. In practice, we have to find the joint angles that will give rise to a desired end effector position and, as shown in the right figure, this inverse kinematics has two solutions corresponding to 'elbow up' and 'elbow down'. [[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-292.jpg?height=362&width=726&top_left_y=232&top_left_x=914]]
\[
\begin{align*}
\frac{\partial \widetilde{E}}{\partial \eta_{j}}=\sum_{i}\left\{\pi_{j}-\gamma_{j}\left(w_{i}\right)\right\} \tag{5.147}
\end{align*}
\]
We see that \(\pi_{j}\) is therefore driven towards the average posterior probability for component \(j\).
**** Mixture density networks
The goal of supervised learning is to model a conditional distribution \(p(\mathbf{t} \mid \mathbf{x})\), which for many simple regression problems is chosen to be Gaussian. However, practical machine learning problems can often have significantly non-Gaussian distributions. These can arise, for example, with inverse problems in which the distribution can be multimodal, in which case the Gaussian assumption can lead to very poor predictions.
As a simple example of an inverse problem, consider the kinematics of a robot arm, as illustrated in Figure 5.18. The forward problem involves finding the end effector position given the joint angles and has a unique solution. However, in practice we wish to move the end effector of the robot to a specific position, and to do this we must set appropriate joint angles. We therefore need to solve the inverse problem, which has two solutions as seen in Figure 5.18.
Forward problems often corresponds to causality in a physical system and generally have a unique solution. For instance, a specific pattern of symptoms in the human body may be caused by the presence of a particular disease. In pattern recognition, however, we typically have to solve an inverse problem, such as trying to predict the presence of a disease given a set of symptoms. If the forward problem involves a many-to-one mapping, then the inverse problem will have multiple solutions. For instance, several different diseases may result in the same symptoms.
In the robotics example, the kinematics is defined by geometrical equations, and the multimodality is readily apparent. However, in many machine learning problems the presence of multimodality, particularly in problems involving spaces of high dimensionality, can be less obvious. For tutorial purposes, however, we shall consider a simple toy problem for which we can easily visualize the multimodality. Data for this problem is generated by sampling a variable \(x\) uniformly over the interval \((0,1)\), to give a set of values \(\left\{x_{n}\right\}\), and the corresponding target values \(t_{n}\) are obtained
Figure 5.19 On the left is the data set for a simple 'forward problem' in which the red curve shows the result of fitting a two-layer neural network by minimizing the sum-of-squares error function. The corresponding inverse problem, shown on the right, is obtained by exchanging the roles of \(x\) and \(t\). Here the same network trained again by minimizing the sum-of-squares error function gives a very poor fit to the data due to the multimodality of the data set. [[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-293.jpg?height=474&width=992&top_left_y=246&top_left_x=632]]
by computing the function \(x_{n}+0.3 \sin \left(2 \pi x_{n}\right)\) and then adding uniform noise over the interval \((-0.1,0.1)\). The inverse problem is then obtained by keeping the same data points but exchanging the roles of \(x\) and \(t\). Figure 5.19 shows the data sets for the forward and inverse problems, along with the results of fitting two-layer neural networks having 6 hidden units and a single linear output unit by minimizing a sumof-squares error function. Least squares corresponds to maximum likelihood under a Gaussian assumption. We see that this leads to a very poor model for the highly non-Gaussian inverse problem.
We therefore seek a general framework for modelling conditional probability distributions. This can be achieved by using a mixture model for \(p(\mathbf{t} \mid \mathbf{x})\) in which both the mixing coefficients as well as the component densities are flexible functions of the input vector \(\mathbf{x}\), giving rise to the mixture density network. For any given value of \(\mathbf{x}\), the mixture model provides a general formalism for modelling an arbitrary conditional density function \(p(\mathbf{t} \mid \mathbf{x})\). Provided we consider a sufficiently flexible network, we then have a framework for approximating arbitrary conditional distributions.
Here we shall develop the model explicitly for Gaussian components, so that
\[
\begin{align*}
p(\mathbf{t} \mid \mathbf{x})=\sum_{k=1}^{K} \pi_{k}(\mathbf{x}) \mathcal{N}\left(\mathbf{t} \mid \boldsymbol{\mu}_{k}(\mathbf{x}), \sigma_{k}^{2}(\mathbf{x})\right) \tag{5.148}
\end{align*}
\]
This is an example of a heteroscedastic model since the noise variance on the data is a function of the input vector \(\mathbf{x}\). Instead of Gaussians, we can use other distributions for the components, such as Bernoulli distributions if the target variables are binary rather than continuous. We have also specialized to the case of isotropic covariances for the components, although the mixture density network can readily be extended to allow for general covariance matrices by representing the covariances using a Cholesky factorization (Williams, 1996). Even with isotropic components, the conditional distribution \(p(\mathbf{t} \mid \mathbf{x})\) does not assume factorization with respect to the components of \(\mathbf{t}\) (in contrast to the standard sum-of-squares regression model) as a consequence of the mixture distribution.
We now take the various parameters of the mixture model, namely the mixing coefficients \(\pi_{k}(\mathbf{x})\), the means \(\boldsymbol{\mu}_{k}(\mathbf{x})\), and the variances \(\sigma_{k}^{2}(\mathbf{x})\), to be governed by
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-294.jpg?height=400&width=1051&top_left_y=229&top_left_x=500]]
Figure 5.20 The mixture density network can represent general conditional probability densities \(p(\mathbf{t} \mid \mathbf{x})\) by considering a parametric mixture model for the distribution of \(t\) whose parameters are determined by the outputs of a neural network that takes \(\mathrm{x}\) as its input vector.
the outputs of a conventional neural network that takes \(\mathbf{x}\) as its input. The structure of this mixture density network is illustrated in Figure 5.20. The mixture density network is closely related to the mixture of experts discussed in Section 14.5.3. The principle difference is that in the mixture density network the same function is used to predict the parameters of all of the component densities as well as the mixing coefficients, and so the nonlinear hidden units are shared amongst the input-dependent functions.
The neural network in Figure 5.20 can, for example, be a two-layer network having sigmoidal ('tanh') hidden units. If there are \(L\) components in the mixture model (5.148), and if \(\mathrm{t}\) has \(K\) components, then the network will have \(L\) output unit activations denoted by \(a_{k}^{\pi}\) that determine the mixing coefficients \(\pi_{k}(\mathbf{x}), K\) outputs denoted by \(a_{k}^{\sigma}\) that determine the kernel widths \(\sigma_{k}(\mathbf{x})\), and \(L \times K\) outputs denoted by \(a_{k j}^{\mu}\) that determine the components \(\mu_{k j}(\mathbf{x})\) of the kernel centres \(\boldsymbol{\mu}_{k}(\mathbf{x})\). The total number of network outputs is given by \((K+2) L\), as compared with the usual \(K\) outputs for a network, which simply predicts the conditional means of the target variables.
The mixing coefficients must satisfy the constraints
\[
\begin{align*}
\sum_{k=1}^{K} \pi_{k}(\mathbf{x})=1, \quad 0 \leqslant \pi_{k}(\mathbf{x}) \leqslant 1 \tag{5.149}
\end{align*}
\]
which can be achieved using a set of softmax outputs
\[
\begin{align*}
\pi_{k}(\mathbf{x})=\frac{\exp \left(a_{k}^{\pi}\right)}{\sum_{l=1}^{K} \exp \left(a_{l}^{\pi}\right)} \tag{5.150}
\end{align*}
\]
Similarly, the variances must satisfy \(\sigma_{k}^{2}(\mathbf{x}) \geqslant 0\) and so can be represented in terms of the exponentials of the corresponding network activations using
\[
\begin{align*}
\sigma_{k}(\mathbf{x})=\exp \left(a_{k}^{\sigma}\right) \tag{5.151}
\end{align*}
\]
Finally, because the means \(\boldsymbol{\mu}_{k}(\mathbf{x})\) have real components, they can be represented directly by the network output activations
\[
\begin{align*}
\mu_{k j}(\mathbf{x})=a_{k j}^{\mu} \tag{5.152}
\end{align*}
\]
The adaptive parameters of the mixture density network comprise the vector \(\mathbf{w}\) of weights and biases in the neural network, that can be set by maximum likelihood, or equivalently by minimizing an error function defined to be the negative logarithm of the likelihood. For independent data, this error function takes the form
\[
\begin{align*}
E(\mathbf{w})=-\sum_{n=1}^{N} \ln \left\{\sum_{k=1}^{k} \pi_{k}\left(\mathbf{x}_{n}, \mathbf{w}\right) \mathcal{N}\left(\mathbf{t}_{n} \mid \boldsymbol{\mu}_{k}\left(\mathbf{x}_{n}, \mathbf{w}\right), \sigma_{k}^{2}\left(\mathbf{x}_{n}, \mathbf{w}\right)\right)\right\} \tag{5.153}
\end{align*}
\]
where we have made the dependencies on \(\mathbf{w}\) explicit.
In order to minimize the error function, we need to calculate the derivatives of the error \(E(\mathbf{w})\) with respect to the components of \(\mathbf{w}\). These can be evaluated by using the standard backpropagation procedure, provided we obtain suitable expressions for the derivatives of the error with respect to the output-unit activations. These represent error signals \(\delta\) for each pattern and for each output unit, and can be backpropagated to the hidden units and the error function derivatives evaluated in the usual way. Because the error function (5.153) is composed of a sum of terms, one for each training data point, we can consider the derivatives for a particular pattern \(n\) and then find the derivatives of \(E\) by summing over all patterns.
Because we are dealing with mixture distributions, it is convenient to view the mixing coefficients \(\pi_{k}(\mathbf{x})\) as \(\mathbf{x}\)-dependent prior probabilities and to introduce the corresponding posterior probabilities given by
\[
\begin{align*}
\gamma_{k}(\mathbf{t} \mid \mathbf{x})=\frac{\pi_{k} \mathcal{N}_{n k}}{\sum_{l=1}^{K} \pi_{l} \mathcal{N}_{n l}} \tag{5.154}
\end{align*}
\]
where \(\mathcal{N}_{n k}\) denotes \(\mathcal{N}\left(\mathbf{t}_{n} \mid \boldsymbol{\mu}_{k}\left(\mathbf{x}_{n}\right), \sigma_{k}^{2}\left(\mathbf{x}_{n}\right)\right)\).
The derivatives with respect to the network output activations governing the mix-
Exercise 5.34
Exercise 5.35
Exercise 5.36
\[
\begin{align*}
\frac{\partial E_{n}}{\partial a_{k}^{\pi}}=\pi_{k}-\gamma_{k} \tag{5.155}
\end{align*}
\]
Similarly, the derivatives with respect to the output activations controlling the component means are given by
\[
\begin{align*}
\frac{\partial E_{n}}{\partial a_{k l}^{\mu}}=\gamma_{k}\left\{\frac{\mu_{k l}-t_{l}}{\sigma_{k}^{2}}\right\} \tag{5.156}
\end{align*}
\]
Finally, the derivatives with respect to the output activations controlling the component variances are given by
\[
\begin{align*}
\frac{\partial E_{n}}{\partial a_{k}^{\sigma}}=-\gamma_{k}\left\{\frac{\left\|\mathbf{t}-\boldsymbol{\mu}_{k}\right\|^{2}}{\sigma_{k}^{3}}-\frac{1}{\sigma_{k}}\right\} \tag{5.157}
\end{align*}
\]
Figure 5.21 (a) Plot of the mixing coefficients \(\pi_{k}(x)\) as a function of \(x\) for the three kernel functions in a mixture density network trained on the data shown in Figure 5.19. The model has three Gaussian components, and uses a two-layer multilayer perceptron with five 'tanh' sigmoidal units in the hidden layer, and nine outputs (corresponding to the 3 means and 3 variances of the Gaussian components and the 3 mixing coefficients). At both small and large values of \(x\), where the conditional probability density of the target data is unimodal, only one of the kernels has a high value for its prior probability, while at intermediate values of \(x\), where the conditional density is trimodal, the three mixing coefficients have comparable values. (b) Plots of the means \(\mu_{k}(x)\) using the same colour coding as for the mixing coefficients. (c) Plot of the contours of the corresponding conditional probability density of the target data for the same mixture density network. (d) Plot of the approximate conditional mode, shown by the red points, of the conditional
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-296.jpg?height=420&width=418&top_left_y=251&top_left_x=667]]
1) 
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-296.jpg?height=424&width=418&top_left_y=775&top_left_x=665]]
3) [@3] density.
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-296.jpg?height=420&width=416&top_left_y=251&top_left_x=1172]]
2) [@2] 
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-296.jpg?height=422&width=416&top_left_y=774&top_left_x=1172]]
4) [@4] 
We illustrate the use of a mixture density network by returning to the toy example of an inverse problem shown in Figure 5.19. Plots of the mixing coefficients \(\pi_{k}(x)\), the means \(\mu_{k}(x)\), and the conditional density contours corresponding to \(p(t \mid x)\), are shown in Figure 5.21. The outputs of the neural network, and hence the parameters in the mixture model, are necessarily continuous single-valued functions of the input variables. However, we see from Figure 5.21(c) that the model is able to produce a conditional density that is unimodal for some values of \(x\) and trimodal for other values by modulating the amplitudes of the mixing components \(\pi_{k}(\mathbf{x})\).
Once a mixture density network has been trained, it can predict the conditional density function of the target data for any given value of the input vector. This conditional density represents a complete description of the generator of the data, so far as the problem of predicting the value of the output vector is concerned. From this density function we can calculate more specific quantities that may be of interest in different applications. One of the simplest of these is the mean, corresponding to the conditional average of the target data, and is given by
\[
\begin{align*}
\mathbb{E}[\mathbf{t} \mid \mathbf{x}]=\int \mathbf{t} p(\mathbf{t} \mid \mathbf{x}) \mathrm{d} \mathbf{t}=\sum_{k=1}^{K} \pi_{k}(\mathbf{x}) \boldsymbol{\mu}_{k}(\mathbf{x}) \tag{5.158}
\end{align*}
\] where we have used (5.148). Because a standard network trained by least squares is approximating the conditional mean, we see that a mixture density network can reproduce the conventional least-squares result as a special case. Of course, as we have already noted, for a multimodal distribution the conditional mean is of limited value.
We can similarly evaluate the variance of the density function about the conditional average, to give
\[
\begin{align*}
\begin{align*}
s^{2}(\mathbf{x}) & =\mathbb{E}\left[\|\mathbf{t}-\mathbb{E}[\mathbf{t} \mid \mathbf{x}]\|^{2} \mid \mathbf{x}\right]  \tag{5.159}\\
& =\sum_{k=1}^{K} \pi_{k}(\mathbf{x})\left\{\sigma_{k}^{2}(\mathbf{x})+\left\|\boldsymbol{\mu}_{k}(\mathbf{x})-\sum_{l=1}^{K} \pi_{l}(\mathbf{x}) \boldsymbol{\mu}_{l}(\mathbf{x})\right\|^{2}\right\}
\end{align*} \tag{5.160}
\end{align*}
\]
where we have used (5.148) and (5.158). This is more general than the corresponding least-squares result because the variance is a function of \(\mathbf{x}\).
We have seen that for multimodal distributions, the conditional mean can give a poor representation of the data. For instance, in controlling the simple robot arm shown in Figure 5.18, we need to pick one of the two possible joint angle settings in order to achieve the desired end-effector location, whereas the average of the two solutions is not itself a solution. In such cases, the conditional mode may be of more value. Because the conditional mode for the mixture density network does not have a simple analytical solution, this would require numerical iteration. A simple alternative is to take the mean of the most probable component (i.e., the one with the largest mixing coefficient) at each value of \(\mathbf{x}\). This is shown for the toy data set in Figure 5.21(d).
**** Bayesian neural networks
:PROPERTIES:
:CUSTOM_ID: bayesian-neural-networks
:END:
So far, our discussion of neural networks has focussed on the use of maximum likelihood to determine the network parameters (weights and biases). Regularized maximum likelihood can be interpreted as a MAP (maximum posterior) approach in which the regularizer can be viewed as the logarithm of a prior parameter distribution. However, in a Bayesian treatment we need to marginalize over the distribution of parameters in order to make predictions.
In Section 3.3, we developed a Bayesian solution for a simple linear regression model under the assumption of Gaussian noise. We saw that the posterior distribution, which is Gaussian, could be evaluated exactly and that the predictive distribution could also be found in closed form. In the case of a multilayered network, the highly nonlinear dependence of the network function on the parameter values means that an exact Bayesian treatment can no longer be found. In fact, the log of the posterior distribution will be nonconvex, corresponding to the multiple local minima in the error function.
The technique of variational inference, to be discussed in Chapter 10, has been applied to Bayesian neural networks using a factorized Gaussian approximation to the posterior distribution (Hinton and van Camp, 1993) and also using a fullcovariance Gaussian (Barber and Bishop, 1998a; Barber and Bishop, 1998b). The most complete treatment, however, has been based on the Laplace approximation (MacKay, 1992c; MacKay, 1992b) and forms the basis for the discussion given here. We will approximate the posterior distribution by a Gaussian, centred at a mode of the true posterior. Furthermore, we shall assume that the covariance of this Gaussian is small so that the network function is approximately linear with respect to the parameters over the region of parameter space for which the posterior probability is significantly nonzero. With these two approximations, we will obtain models that are analogous to the linear regression and classification models discussed in earlier chapters and so we can exploit the results obtained there. We can then make use of the evidence framework to provide point estimates for the hyperparameters and to compare alternative models (for example, networks having different numbers of hidden units). To start with, we shall discuss the regression case and then later consider the modifications needed for solving classification tasks.
***** Weight-space symmetries
:PROPERTIES:
:CUSTOM_ID: weight-space-symmetries
:END:
One property of feed-forward networks, which will play a role when we consider Bayesian model comparison, is that multiple distinct choices for the weight vector \(\mathbf{w}\) can all give rise to the same mapping function from inputs to outputs (Chen et al., 1993). Consider a two-layer network of the form shown in Figure 5.1 with \(M\) hidden units having 'tanh' activation functions and full connectivity in both layers. If we change the sign of all of the weights and the bias feeding into a particular hidden unit, then, for a given input pattern, the sign of the activation of the hidden unit will be reversed, because 'tanh' is an odd function, so that \(\tanh (-a)=-\tanh (a)\). This transformation can be exactly compensated by changing the sign of all of the weights leading out of that hidden unit. Thus, by changing the signs of a particular group of weights (and a bias), the input-output mapping function represented by the network is unchanged, and so we have found two different weight vectors that give rise to the same mapping function. For \(M\) hidden units, there will be \(M\) such 'sign-flip'
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-252.jpg?height=526&width=650&top_left_y=243&top_left_x=976]]
symmetries, and thus any given weight vector will be one of a set \(2^{M}\) equivalent weight vectors .
Similarly, imagine that we interchange the values of all of the weights (and the bias) leading both into and out of a particular hidden unit with the corresponding values of the weights (and bias) associated with a different hidden unit. Again, this clearly leaves the network input-output mapping function unchanged, but it corresponds to a different choice of weight vector. For \(M\) hidden units, any given weight vector will belong to a set of \(M\) ! equivalent weight vectors associated with this interchange symmetry, corresponding to the \(M\) ! different orderings of the hidden units. The network will therefore have an overall weight-space symmetry factor of \(M!2^{M}\). For networks with more than two layers of weights, the total level of symmetry will be given by the product of such factors, one for each layer of hidden units.
It turns out that these factors account for all of the symmetries in weight space (except for possible accidental symmetries due to specific choices for the weight values). Furthermore, the existence of these symmetries is not a particular property of the 'tanh' function but applies to a wide range of activation functions (Krkov and Kainen, 1994). In many cases, these symmetries in weight space are of little practical consequence, although in Section 5.7 we shall encounter a situation in which we need to take them into account.
***** Posterior parameter distribution
:PROPERTIES:
:CUSTOM_ID: posterior-parameter-distribution
:END:
Consider the problem of predicting a single continuous target variable \(t\) from a vector \(\mathbf{x}\) of inputs (the extension to multiple targets is straightforward). We shall suppose that the conditional distribution \(p(t \mid \mathbf{x})\) is Gaussian, with an \(\mathbf{x}\)-dependent mean given by the output of a neural network model \(y(\mathbf{x}, \mathbf{w})\), and with precision (inverse variance) \(\beta\)
\[
\begin{align*}
p(t \mid \mathbf{x}, \mathbf{w}, \beta)=\mathcal{N}\left(t \mid y(\mathbf{x}, \mathbf{w}), \beta^{-1}\right) \tag{5.161}
\end{align*}
\]
Similarly, we shall choose a prior distribution over the weights \(\mathbf{w}\) that is Gaussian of the form
\[
\begin{align*}
p(\mathbf{w} \mid \alpha)=\mathcal{N}\left(\mathbf{w} \mid \mathbf{0}, \alpha^{-1} \mathbf{I}\right) \tag{5.162}
\end{align*}
\]
For an i.i.d. data set of \(N\) observations \(\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\), with a corresponding set of target values \(\mathcal{D}=\left\{t_{1}, \ldots, t_{N}\right\}\), the likelihood function is given by
\[
\begin{align*}
p(\mathcal{D} \mid \mathbf{w}, \beta)=\prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid y\left(\mathbf{x}_{n}, \mathbf{w}\right), \beta^{-1}\right) \tag{5.163}
\end{align*}
\]
and so the resulting posterior distribution is then
\[
\begin{align*}
p(\mathbf{w} \mid \mathcal{D}, \alpha, \beta) \propto p(\mathbf{w} \mid \alpha) p(\mathcal{D} \mid \mathbf{w}, \beta) \tag{5.164}
\end{align*}
\]
which, as a consequence of the nonlinear dependence of \(y(\mathbf{x}, \mathbf{w})\) on \(\mathbf{w}\), will be nonGaussian.
We can find a Gaussian approximation to the posterior distribution by using the Laplace approximation. To do this, we must first find a (local) maximum of the posterior, and this must be done using iterative numerical optimization. As usual, it is convenient to maximize the logarithm of the posterior, which can be written in the form
\[
\begin{align*}
\ln p(\mathbf{w} \mid \mathcal{D})=-\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}-\frac{\beta}{2} \sum_{n=1}^{N}\left\{y\left(\mathbf{x}_{n}, \mathbf{w}\right)-t_{n}\right\}^{2}+\text { const } \tag{5.165}
\end{align*}
\]
which corresponds to a regularized sum-of-squares error function. Assuming for the moment that \(\alpha\) and \(\beta\) are fixed, we can find a maximum of the posterior, which we denote \(\mathbf{w}_{\text {MAP }}\), by standard nonlinear optimization algorithms such as conjugate gradients, using error backpropagation to evaluate the required derivatives.
Having found a mode \(\mathbf{w}_{\text {MAP }}\), we can then build a local Gaussian approximation by evaluating the matrix of second derivatives of the negative log posterior distribution. From (5.165), this is given by
\[
\begin{align*}
\mathbf{A}=-\nabla \nabla \ln p(\mathbf{w} \mid \mathcal{D}, \alpha, \beta)=\alpha \mathbf{I}+\beta \mathbf{H} \tag{5.166}
\end{align*}
\]
where \(\mathbf{H}\) is the Hessian matrix comprising the second derivatives of the sum-ofsquares error function with respect to the components of \(\mathbf{w}\). Algorithms for computing and approximating the Hessian were discussed in Section 5.4. The corresponding Gaussian approximation to the posterior is then given from (4.134) by
\[
\begin{align*}
q(\mathbf{w} \mid \mathcal{D})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{w}_{\mathrm{MAP}}, \mathbf{A}^{-1}\right) \tag{5.167}
\end{align*}
\]
Similarly, the predictive distribution is obtained by marginalizing with respect to this posterior distribution
\[
\begin{align*}
p(t \mid \mathbf{x}, \mathcal{D})=\int p(t \mid \mathbf{x}, \mathbf{w}) q(\mathbf{w} \mid \mathcal{D}) \mathrm{d} \mathbf{w} \tag{5.168}
\end{align*}
\]
However, even with the Gaussian approximation to the posterior, this integration is still analytically intractable due to the nonlinearity of the network function \(y(\mathbf{x}, \mathbf{w})\) as a function of \(\mathbf{w}\). To make progress, we now assume that the posterior distribution has small variance compared with the characteristic scales of \(\mathbf{w}\) over which \(y(\mathbf{x}, \mathbf{w})\) is varying. This allows us to make a Taylor series expansion of the network function around \(\mathbf{w}_{\text {MAP }}\) and retain only the linear terms
\[
\begin{align*}
y(\mathbf{x}, \mathbf{w}) \simeq y\left(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}}\right)+\mathbf{g}^{\mathbf{T}}\left(\mathbf{w}-\mathbf{w}_{\mathrm{MAP}}\right) \tag{5.169}
\end{align*}
\]
where we have defined
\[
\begin{align*}
\mathbf{g}=\left.\nabla_{\mathbf{w}} y(\mathbf{x}, \mathbf{w})\right|_{\mathbf{w}=\mathbf{w}_{\mathrm{MAP}}} \tag{5.170}
\end{align*}
\]
With this approximation, we now have a linear-Gaussian model with a Gaussian distribution for \(p(\mathbf{w})\) and a Gaussian for \(p(t \mid \mathbf{w})\) whose mean is a linear function of \(\mathrm{w}\) of the form
\[
\begin{align*}
p(t \mid \mathbf{x}, \mathbf{w}, \beta) \simeq \mathcal{N}\left(t \mid y\left(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}}\right)+\mathbf{g}^{\mathbf{T}}\left(\mathbf{w}-\mathbf{w}_{\mathrm{MAP}}\right), \beta^{-1}\right) \tag{5.171}
\end{align*}
\]
We can therefore make use of the general result (2.115) for the marginal \(p(t)\) to give
\[
\begin{align*}
p(t \mid \mathbf{x}, \mathcal{D}, \alpha, \beta)=\mathcal{N}\left(t \mid y\left(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}}\right), \sigma^{2}(\mathbf{x})\right) \tag{5.172}
\end{align*}
\]
where the input-dependent variance is given by
\[
\begin{align*}
\sigma^{2}(\mathbf{x})=\beta^{-1}+\mathbf{g}^{\mathrm{T}} \mathbf{A}^{-1} \mathbf{g} \tag{5.173}
\end{align*}
\]
We see that the predictive distribution \(p(t \mid \mathbf{x}, \mathcal{D})\) is a Gaussian whose mean is given by the network function \(y\left(\mathbf{x}, \mathbf{w}_{\text {MAP }}\right)\) with the parameter set to their MAP value. The variance has two terms, the first of which arises from the intrinsic noise on the target variable, whereas the second is an \(\mathrm{x}\)-dependent term that expresses the uncertainty in the interpolant due to the uncertainty in the model parameters w. This should be compared with the corresponding predictive distribution for the linear regression model, given by (3.58) and (3.59).
***** Hyperparameter optimization
:PROPERTIES:
:CUSTOM_ID: hyperparameter-optimization
:END:
So far, we have assumed that the hyperparameters \(\alpha\) and \(\beta\) are fixed and known. We can make use of the evidence framework, discussed in Section 3.5, together with the Gaussian approximation to the posterior obtained using the Laplace approximation, to obtain a practical procedure for choosing the values of such hyperparameters.
The marginal likelihood, or evidence, for the hyperparameters is obtained by integrating over the network weights
\[
\begin{align*}
p(\mathcal{D} \mid \alpha, \beta)=\int p(\mathcal{D} \mid \mathbf{w}, \beta) p(\mathbf{w} \mid \alpha) \mathrm{d} \mathbf{w} \tag{5.174}
\end{align*}
\]
Exercise 5.39 This is easily evaluated by making use of the Laplace approximation result (4.135). Taking logarithms then gives
\[
\begin{align*}
\ln p(\mathcal{D} \mid \alpha, \beta) \simeq-E\left(\mathbf{w}_{\mathrm{MAP}}\right)-\frac{1}{2} \ln |\mathbf{A}|+\frac{W}{2} \ln \alpha+\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi) \tag{5.175}
\end{align*}
\]
where \(W\) is the total number of parameters in \(\mathbf{w}\), and the regularized error function is defined by
\[
\begin{align*}
E\left(\mathbf{w}_{\mathrm{MAP}}\right)=\frac{\beta}{2} \sum_{n=1}^{N}\left\{y\left(\mathbf{x}_{n}, \mathbf{w}_{\mathrm{MAP}}\right)-t_{n}\right\}^{2}+\frac{\alpha}{2} \mathbf{w}_{\mathrm{MAP}}^{\mathrm{T}} \mathbf{w}_{\mathrm{MAP}} \tag{5.176}
\end{align*}
\]
We see that this takes the same form as the corresponding result (3.86) for the linear regression model.
In the evidence framework, we make point estimates for \(\alpha\) and \(\beta\) by maximizing \(\ln p(\mathcal{D} \mid \alpha, \beta)\). Consider first the maximization with respect to \(\alpha\), which can be done by analogy with the linear regression case discussed in Section 3.5.2. We first define the eigenvalue equation
\[
\begin{align*}
\beta \mathbf{H} \mathbf{u}_{i}=\lambda_{i} \mathbf{u}_{i} \tag{5.177}
\end{align*}
\]
where \(\mathbf{H}\) is the Hessian matrix comprising the second derivatives of the sum-ofsquares error function, evaluated at \(\mathbf{w}=\mathbf{w}_{\text {MAP }}\). By analogy with (3.92), we obtain
\[
\begin{align*}
\alpha=\frac{\gamma}{\mathbf{w}_{\mathrm{MAP}}^{\mathrm{T}} \mathbf{w}_{\mathrm{MAP}}} \tag{5.178}
\end{align*}
\] where \(\gamma\) represents the effective number of parameters and is defined by
\[
\begin{align*}
\gamma=\sum_{i=1}^{W} \frac{\lambda_{i}}{\alpha+\lambda_{i}} \tag{5.179}
\end{align*}
\]
Note that this result was exact for the linear regression case. For the nonlinear neural network, however, it ignores the fact that changes in \(\alpha\) will cause changes in the Hessian \(\mathbf{H}\), which in turn will change the eigenvalues. We have therefore implicitly ignored terms involving the derivatives of \(\lambda_{i}\) with respect to \(\alpha\).
Similarly, from (3.95) we see that maximizing the evidence with respect to \(\beta\) gives the re-estimation formula
\[
\begin{align*}
\frac{1}{\beta}=\frac{1}{N-\gamma} \sum_{n=1}^{N}\left\{y\left(\mathbf{x}_{n}, \mathbf{w}_{\mathrm{MAP}}\right)-t_{n}\right\}^{2} \tag{5.180}
\end{align*}
\]
As with the linear model, we need to alternate between re-estimation of the hyper-
Section 5.1.1 parameters \(\alpha\) and \(\beta\) and updating of the posterior distribution. The situation with a neural network model is more complex, however, due to the multimodality of the posterior distribution. As a consequence, the solution for \(\mathbf{w}_{\mathrm{MAP}}\) found by maximizing the log posterior will depend on the initialization of \(\mathbf{w}\). Solutions that differ only as a consequence of the interchange and sign reversal symmetries in the hidden units are identical so far as predictions are concerned, and it is irrelevant which of the equivalent solutions is found. However, there may be inequivalent solutions as well, and these will generally yield different values for the optimized hyperparameters.
In order to compare different models, for example neural networks having different numbers of hidden units, we need to evaluate the model evidence \(p(\mathcal{D})\). This can be approximated by taking (5.175) and substituting the values of \(\alpha\) and \(\beta\) obtained from the iterative optimization of these hyperparameters. A more careful evaluation is obtained by marginalizing over \(\alpha\) and \(\beta\), again by making a Gaussian approximation (MacKay, 1992c; Bishop, 1995a). In either case, it is necessary to evaluate the determinant \(|\mathbf{A}|\) of the Hessian matrix. This can be problematic in practice because the determinant, unlike the trace, is sensitive to the small eigenvalues that are often difficult to determine accurately.
The Laplace approximation is based on a local quadratic expansion around a mode of the posterior distribution over weights. We have seen in Section 5.1.1 that any given mode in a two-layer network is a member of a set of \(M!2^{M}\) equivalent modes that differ by interchange and sign-change symmetries, where \(M\) is the number of hidden units. When comparing networks having different numbers of hidden units, this can be taken into account by multiplying the evidence by a factor of \(M!2^{M}\).
***** Bayesian neural networks for classification
:PROPERTIES:
:CUSTOM_ID: bayesian-neural-networks-for-classification
:END:
So far, we have used the Laplace approximation to develop a Bayesian treatment of neural network regression models. We now discuss the modifications to this framework that arise when it is applied to classification. Here we shall consider a network having a single logistic sigmoid output corresponding to a two-class classification problem. The extension to networks with multiclass softmax outputs is straightforward. We shall build extensively on the analogous results for linear classification models discussed in Section 4.5, and so we encourage the reader to familiarize themselves with that material before studying this section.
The log likelihood function for this model is given by
\[
\begin{align*}
\ln p(\mathcal{D} \mid \mathbf{w})=\sum_{n}=1^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\} \tag{5.181}
\end{align*}
\]
where \(t_{n} \in\{0,1\}\) are the target values, and \(y_{n} \equiv y\left(\mathbf{x}_{n}, \mathbf{w}\right)\). Note that there is no hyperparameter \(\beta\), because the data points are assumed to be correctly labelled. As before, the prior is taken to be an isotropic Gaussian of the form (5.162).
The first stage in applying the Laplace framework to this model is to initialize the hyperparameter \(\alpha\), and then to determine the parameter vector \(\mathbf{w}\) by maximizing the \(\log\) posterior distribution. This is equivalent to minimizing the regularized error function
\[
\begin{align*}
E(\mathbf{w})=-\ln p(\mathcal{D} \mid \mathbf{w})+\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \tag{5.182}
\end{align*}
\]
and can be achieved using error backpropagation combined with standard optimization algorithms, as discussed in Section 5.3.
Having found a solution \(\mathbf{w}_{\text {MAP }}\) for the weight vector, the next step is to evaluate the Hessian matrix \(\mathbf{H}\) comprising the second derivatives of the negative log likelihood function. This can be done, for instance, using the exact method of Section 5.4.5, or using the outer product approximation given by (5.85). The second derivatives of the negative log posterior can again be written in the form (5.166), and the Gaussian approximation to the posterior is then given by (5.167).
To optimize the hyperparameter \(\alpha\), we again maximize the marginal likelihood, which is easily shown to take the form
\[
\begin{align*}
\ln p(\mathcal{D} \mid \alpha) \simeq-E\left(\mathbf{w}_{\mathrm{MAP}}\right)-\frac{1}{2} \ln |\mathbf{A}|+\frac{W}{2} \ln \alpha+\text { const } \tag{5.183}
\end{align*}
\]
where the regularized error function is defined by
\[
\begin{align*}
E\left(\mathbf{w}_{\mathrm{MAP}}\right)=-\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\}+\frac{\alpha}{2} \mathbf{w}_{\mathrm{MAP}}^{\mathrm{T}} \mathbf{w}_{\mathrm{MAP}} \tag{5.184}
\end{align*}
\]
in which \(y_{n} \equiv y\left(\mathbf{x}_{n}, \mathbf{w}_{\text {MAP }}\right)\). Maximizing this evidence function with respect to \(\alpha\) again leads to the re-estimation equation given by (5.178).
The use of the evidence procedure to determine \(\alpha\) is illustrated in Figure 5.22 for the synthetic two-dimensional data discussed in Appendix A.
Finally, we need the predictive distribution, which is defined by (5.168). Again, this integration is intractable due to the nonlinearity of the network function. The
Figure 5.22 Illustration of the evidence framework applied to a synthetic two-class data set. The green curve shows the optimal decision boundary, the black curve shows the result of fitting a two-layer network with 8 hidden units by maximum likelihood, and the red curve shows the result of including a regularizer in which \(\alpha\) is optimized using the evidence procedure, starting from the initial value \(\alpha=0\). Note that the evidence procedure greatly reduces the over-fitting of the network.
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-303.jpg?height=531&width=666&top_left_y=245&top_left_x=977]]
simplest approximation is to assume that the posterior distribution is very narrow and hence make the approximation
\[
\begin{align*}
p(t \mid \mathbf{x}, \mathcal{D}) \simeq p\left(t \mid \mathbf{x}, \mathbf{w}_{\mathrm{MAP}}\right) \tag{5.185}
\end{align*}
\]
We can improve on this, however, by taking account of the variance of the posterior distribution. In this case, a linear approximation for the network outputs, as was used in the case of regression, would be inappropriate due to the logistic sigmoid outputunit activation function that constrains the output to lie in the range \((0,1)\). Instead, we make a linear approximation for the output unit activation in the form
\[
\begin{align*}
a(\mathbf{x}, \mathbf{w}) \simeq a_{\mathrm{MAP}}(\mathbf{x})+\mathbf{b}^{\mathrm{T}}\left(\mathbf{w}-\mathbf{w}_{\mathrm{MAP}}\right) \tag{5.186}
\end{align*}
\]
where \(a_{\text {MAP }}(\mathbf{x})=a\left(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}}\right)\), and the vector \(\mathbf{b} \equiv \nabla a\left(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}}\right)\) can be found by backpropagation.
Because we now have a Gaussian approximation for the posterior distribution over \(\mathbf{w}\), and a model for \(a\) that is a linear function of \(\mathbf{w}\), we can now appeal to the results of Section 4.5.2. The distribution of output unit activation values, induced by the distribution over network weights, is given by
\[
\begin{align*}
p(a \mid \mathbf{x}, \mathcal{D})=\int \delta\left(a-a_{\mathrm{MAP}}(\mathbf{x})-\mathbf{b}^{\mathrm{T}}(\mathbf{x})\left(\mathbf{w}-\mathbf{w}_{\mathrm{MAP}}\right)\right) q(\mathbf{w} \mid \mathcal{D}) \mathrm{d} \mathbf{w} \tag{5.187}
\end{align*}
\]
where \(q(\mathbf{w} \mid \mathcal{D})\) is the Gaussian approximation to the posterior distribution given by (5.167). From Section 4.5.2, we see that this distribution is Gaussian with mean \(a_{\mathrm{MAP}} \equiv a\left(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}}\right)\), and variance
\[
\begin{align*}
\sigma_{a}^{2}(\mathbf{x})=\mathbf{b}^{\mathrm{T}}(\mathbf{x}) \mathbf{A}^{-1} \mathbf{b}(\mathbf{x}) \tag{5.188}
\end{align*}
\]
Finally, to obtain the predictive distribution, we must marginalize over \(a\) using
\[
\begin{align*}
p(t=1 \mid \mathbf{x}, \mathcal{D})=\int \sigma(a) p(a \mid \mathbf{x}, \mathcal{D}) \mathrm{d} a \tag{5.189}
\end{align*}
\]
[[https://cdn.mathpix.com/cropped/2024_06_14_39c7c62aee82c576fd07g-304.jpg?height=529&width=1427&top_left_y=244&top_left_x=145]]
Figure 5.23 An illustration of the Laplace approximation for a Bayesian neural network having 8 hidden units with 'tanh' activation functions and a single logistic-sigmoid output unit. The weight parameters were found using scaled conjugate gradients, and the hyperparameter \(\alpha\) was optimized using the evidence framework. On the left is the result of using the simple approximation (5.185) based on a point estimate \(\mathbf{w}_{\text {MAP }}\) of the parameters, in which the green curve shows the \(y=0.5\) decision boundary, and the other contours correspond to output probabilities of \(y=0.1,0.3,0.7\), and 0.9 . On the right is the corresponding result obtained using (5.190). Note that the effect of marginalization is to spread out the contours and to make the predictions less confident, so that at each input point \(\mathbf{x}\), the posterior probabilities are shifted towards 0.5 , while the \(y=0.5\) contour itself is unaffected.
The convolution of a Gaussian with a logistic sigmoid is intractable. We therefore apply the approximation (4.153) to (5.189) giving
\[
\begin{align*}
p(t=1 \mid \mathbf{x}, \mathcal{D})=\sigma\left(\kappa\left(\sigma_{a}^{2}\right) \mathbf{b}^{\mathrm{T}} \mathbf{w}_{\mathrm{MAP}}\right) \tag{5.190}
\end{align*}
\]
where \(\kappa(\cdot)\) is defined by (4.154). Recall that both \(\sigma_{a}^{2}\) and \(\mathbf{b}\) are functions of \(\mathbf{x}\).
Figure 5.23 shows an example of this framework applied to the synthetic classification data set described in Appendix A.
** Markov random fields (MRFs)
:LOGBOOK:
CLOCK: [2024-06-20 Thu 20:28]--[2024-06-20 Thu 21:32] =>  1:04
CLOCK: [2024-06-12 Wed 19:08]--[2024-06-12 Wed 21:14] =>  2:06
:END:
*** Definitions
**** Preliminaries
#+NAME: Unidirected graph
#+begin_definition latex
An undirected graph is a tuple \(G=(V, E)\), where \(V\) is a finite set of nodes and \(E\) is a set of undirected edges. An edge consists out of a pair of nodes from \(V\).
#+end_definition
#+NAME: Neighborhood
#+begin_definition latex
Let \(G=(V, E)\) be an undirected graph. The neighborhood \(\mathcal{N}_{v}:=\{w \in V:\{w, v\} \in E\}\) of \(v\) is defined by the set of nodes connected to \(v\). We have \(w \in \mathcal{N}_{v} \Longrightarrow v \in \mathcal{N}_{w}\).
#+end_definition
#+NAME: Clique
#+begin_definition latex
Let \(G=(V, E)\) be an undirected graph. A clique \( C \) is a subset of \(V\) such that if \(w \in C\) and \( v \in C \) then \( (w,\,v) \in E \). All nodes in a clique are pairwise connected.
#+end_definition
#+NAME: Maximal clique
#+begin_definition latex
A clique is called a maximal clique and is denoted using \( C_{m} \) if no node can be added such that the resulting set is still a clique.
#+end_definition
#+NAME: Path
#+begin_definition latex
Let \(G=(V, E)\) be an undirected graph. We call a sequence of nodes \(v_{1}, v_{2}, \ldots, v_{m} \in V\), with \(\left\{v_{i}, v_{i+1}\right\} \in E\) for \(i=1, \ldots, m-1\) a path from \(v_{1}\) to \(v_{m}\).
#+end_definition
#+NAME: Separation
#+begin_definition latex
Let \(G=(V, E)\) be an undirected graph. A set \(\mathcal{V} \subset V\) separates two nodes \(v \notin \mathcal{V}\) and \(w \notin \mathcal{V}\), if every path from \(v\) to \(w\) contains a node from \(\mathcal{V}\).
#+end_definition
#+NAME: Markov blanket (MB)
#+begin_definition latex
Let \(G=(V, E)\) be an undirected graph and suppose \( v \in V \). A set of nodes \(\mathrm{MB}(v)\) is called the Markov blanket of node \(v\), if for any set of nodes \(\mathcal{B}\) with \(v \notin \mathcal{B}\) we have \(p(v \mid \operatorname{MB}(v),\, \mathcal{B})=p(v \mid \operatorname{MB}(v))\). This means that \(v\) is conditional independent from any other variables given \(\operatorname{MB}(v)\).
#+end_definition
**** Conditional independence
#+NAME: Conditional independence
#+ATTR_LATEX: :environment definition
#+begin_definition latex
\(X\) and \(Y\) are said to be independent conditioned on \(Z\) if their joint class-conditional probability factorizes as the product of their marginal class-conditional probabilities:
\[
p(X,\, Y \mid Z) = p(X \mid Z) \cdot p(Y \mid Z).
\]
#+end_definition
#+NAME: Conditional independence
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
If events \( X \) and \( Y \) are independent conditioned on event \( Z \) then
\[p(X \mid Y, ~Z) = p(X \mid Z) \qquad \text{and} \qquad p(Y \mid X, ~Z) = p(Y \mid Z)\]
#+end_corollary
#+NAME: Conditional independence
#+ATTR_LATEX: :environment proof
#+begin_proof latex
The corollary follows from the definition of conditional independence after applying the rules of probability to the joint class-conditional probability \( p(X, ~Y \mid Z) \)
\begin{align*}
p(X, ~Y \mid Z) &= p(X \mid Y, ~ Z) ~ p(Y \mid Z) = p(Y \mid X, ~ Z) ~ p(X \mid Z)  = p(X \mid Z) p(Y \mid Z) \\
&\Longrightarrow p(X \mid Y, ~Z) = p(X \mid Z) \quad \text{and} \quad p(Y \mid X, ~Z) = p(Y \mid Z).
\end{align*}
#+end_proof
We will use the notation \(X \perp\!\!\!\perp Y \mid Z\) to denote conditional independence of \( X \) and \( Y \) given \( Z \). 
**** Markov properties
Let \(G=(V, E)\) be an undirected graph. Suppose that each node \(v \in V\) is associated a random variable \(X_{v}\) taking values in a state space \(\Lambda_{v}\). Further assume that \(\Lambda_{v}=\Lambda\) for all \(v \in V\).
#+NAME: Pairwise Markov property
#+ATTR_LATEX: :environment definition
#+begin_definition latex
For all pairs of non-adjacent nodes \( u \in V \) and \( w \in V \) such that \( (u, ~ w) \notin E \) the variables \((X_{u})_{u \in V}\) and \((X_{w})_{w \in V}\) are conditionally independent given \((X_{z})_{z \in V \backslash \{u, ~ w\}}\) i.e., for all \( \boldsymbol{x} \in \Lambda^{\lvert V \rvert} \) it holds 
\[
\boxed{
p \big((x_{u})_{u \in V} \mid (x_{t})_{t \in \{w\} \cup V \backslash \{u, w\}} \big) = p \big((x_{u})_{u \in V} \mid\left(x_{t}\right)_{t \in V \backslash \{u, w\}}\big)
}
\]
#+end_definition
#+NAME: Local Markov property
#+ATTR_LATEX: :environment definition
#+begin_definition latex
For any node \( v \in V \) with neighborhood \( \mathcal{N}_{v} \) such that \( r \in V \backslash \mathcal{N}_{v} \) is a non-neighbor, the variables \((X_{v})_{v \in V}\) and \((X_{r})_{r \in V\backslash \mathcal{N}_{v}}\) are conditionally independent given \((X_{n})_{n \in \mathcal{N}_{v}}\) i.e., for all \( \boldsymbol{x} \in \Lambda^{\lvert V \rvert} \) it holds 
\[
\boxed{
p \big((x_{v})_{v \in V} \mid (x_{t})_{t \in \mathcal{N}_{v} \cup V \backslash \mathcal{N}_{v}} \big) = p \big((x_{v})_{v \in V} \mid\left(x_{t}\right)_{t \in \mathcal{N}_{v}}\big)
}
\]
#+end_definition
#+NAME: Global Markov property
#+ATTR_LATEX: :environment definition
#+begin_definition latex
For all disjoint subsets \(\mathcal{A}, \mathcal{B}, \mathcal{S} \subset V\), where all nodes in \(\mathcal{A}\) and \(\mathcal{B}\) are separated by \(\mathcal{S}\) the variables \(\left(X_{a}\right)_{a \in \mathcal{A}}\) and \(\left(X_{b}\right)_{b \in \mathcal{B}}\) are conditional independent given \(\left(X_{s}\right)_{s \in \mathcal{S}}\), i.e. for all \(\boldsymbol{x} \in \Lambda^{|V|}\) it holds 
\[
\boxed{
p\left(\left(x_{a}\right)_{a \in \mathcal{A}} \mid\left(x_{t}\right)_{t \in \mathcal{S} \cup \mathcal{B}}\right)=p\left(\left(x_{a}\right)_{a \in \mathcal{A}} \mid\left(x_{t}\right)_{t \in \mathcal{S}}\right)
}
\]
#+end_definition
#+NAME: Markov random field (MRF)
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Let \(G=(V, E)\) be an undirected graph. Suppose that each node \(v \in V\) is associated a random variable \(X_{v}\) taking values in a state space \(\Lambda_{v}\). Further assume that \(\Lambda_{v}=\Lambda\) for all \(v \in V\). The random variables \(\boldsymbol{X}=\left(X_{v}\right)_{v \in V}\) are called a Markov random field (MRF) if the joint probability distribution \(p (\boldsymbol{X})\) fulfills all conditional independence properties the (global) Markov property w.r.t. the graph \( G \).
#+end_definition
#+NAME: Latent variable Markov random fields (MRFs)
#+begin_definition latex
Let \(\boldsymbol{X}=\left(X_{v}\right)_{v \in V}\) be an MRF over the \(G=(V, E)\). Suppose that each node \(v \in V\) is associated a random variable \(X_{v}\) taking values in a state space \(\Lambda_{v}\). Further suppose that the MRF \(\boldsymbol{X}\) admits a split \( \boldsymbol{X} = \{\boldsymbol{V},\, \boldsymbol{H}\} \) into \( m \) observed variables  \(\boldsymbol{V}=\left(V_1, \ldots, V_m\right)\) and \( n \) hidden variables \(\boldsymbol{H}=\left(H_1, \ldots, H_n\right)\) such that \(|V|=n + m\). The joint probability distribution \( p(\boldsymbol{v},\,\boldsymbol{h}) \) of the MRF describes the joint probability distribution of \((\boldsymbol{V},\,\boldsymbol{H})\). The marginal distribution of \(\boldsymbol{V}\) which is given by
\begin{align*}
p(\boldsymbol{v} \mid \mathbf{w}) = \sum_{\boldsymbol{h}} p(\boldsymbol{v},\,\boldsymbol{h} \mid \mathbf{w}) = \frac{1}{Z} \sum_{\boldsymbol{h}} \exp (-E(\boldsymbol{v},\,\boldsymbol{h},\, \mathbf{w})), \quad Z = \sum_{\boldsymbol{v}} \sum_{\boldsymbol{h}} \exp (-E(\boldsymbol{v},\,\boldsymbol{h},\, \mathbf{w})).
\end{align*}
\( \mathbf{w} \) is a parameter vector that parametrizes the energy function. The visible variables correspond to the components of an observation, the latent variables introduce dependencies between the visible variables.
#+end_definition
**** Hammersley-Clifford theorem
#+NAME: The Hammersley-Clifford theorem
#+ATTR_LATEX: :environment theorem
#+begin_theorem latex
Let \( C \) denote the (maximal) cliques of an undirected graph \( G \). A strictly positive distribution \(p\) (meaning \(p(\mathbf{x})>0\) for all \( \mathbf{x} \)) satisfies the conditional independence properties of an undirected graphical model if and only if \(p\) admits the following factorization:
\[p(\mathbf{x})=\frac{1}{Z} \prod_{C} \psi_{C} (\mathbf{x}_C), \quad Z=\sum_{\mathbf{x}} \prod_{C} \psi_{C}(\mathbf{x}_{C}).\]
#+end_theorem
#+NAME: The Hammersley-Clifford theorem
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Let \( \mathbf{x} = (x_1, x_2, \ldots, x_D) \) be a vector of discrete random variables and \( \mathbf{x}^* = (0, 0, \ldots, 0) \) represent a baseline state. Define the function \( Q(\mathbf{x}) \) as \(Q(\mathbf{x}) = \ln (p(\mathbf{x})/p(\mathbf{x}^{\ast}})\). Assuming \( p(\mathbf{x}) > 0 \) for all \( \mathbf{x} \), \( Q(\mathbf{x}) \) can be uniquely expressed as a multi-linear polynomial
\[
Q(\mathbf{x}) = \sum_i \mathbf{x}_i \mathcal{G}_i(\mathbf{x}_i) + \sum_{i<j} \mathbf{x}_i \mathbf{x}_j G_{i, j}(\mathbf{x}_i, \mathbf{x}_j) + \ldots + \mathbf{x}_1 \mathbf{x}_2 \ldots \mathbf{x}_D G_{1,2, \ldots, D}(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_D).
\]
The proof proceeds by induction. For the configuration \(\mathbf{x} = \mathbf{x}^{\ast}\), \(Q(\mathbf{x}^{\ast}) = \ln (p(\mathbf{x}^{\ast})/p(\mathbf{x}^{\ast}})\) vanishes, as dones the multi-linear polynomial on the right. This is the base case. Now suppose we change \( \mathbf{x} \) to \(\mathbf{x}=\left(\mathbf{x}_1, 0, \ldots, 0\right)\) so that a single component is non-zero. The right hand side is \(\mathbf{x}_1 G_1(\mathbf{x}_1)\), which means we can select a unique \(G_1\). Next, assume that for any configuration of \( \mathbf{x} \) with up to \( k-1 \) non-zero entries, the expansion for \( Q(\mathbf{x}) \) holds: it correctly matches the logarithmic ratio of probabilities, and each term \( G \) is uniquely determined by these configurations. Now consider a configuration where exactly \( k \) variables are non-zero, say \( \mathbf{x}_{i_1}, \mathbf{x}_{i_2}, \ldots, \mathbf{x}_{i_k} \). We need to show that:
\[
 Q(\mathbf{x}) = \sum_{\substack{S \subseteq \{i_1, i_2, \ldots, i_k\} \\ S \neq \emptyset}} \prod_{j \in S} \mathbf{x}_j G_S(\mathbf{x}_S)
\]
where \( G_S \) corresponds to the interaction among the variables in set \( S \).
Correctness of \( G_S \). Each term \( G_S \) for subsets of size \( k \) is determined based on the configurations where all variables in \( S \) are the only non-zero entries. This ensures that the terms involving interactions of fewer than \( k \) variables (as per the induction hypothesis) have been correctly accounted, and the new term must correct any discrepancy for configurations with \( k \) non-zero variables.
Uniqueness of \(G_S\). By isolating the configuration to just the variables in \( S \), and comparing the polynomial expansion to the computed \( Q(\mathbf{x}) \) from the probability distribution, the term \( G_S(\mathbf{x}_S) \) is the only new term that can adjust the expression to match the logarithmic ratio, given that all other terms are fixed by the induction hypothesis.
This proof establishes that the polynomial expansion of \( Q(\mathbf{x}) \) as a sum of interactions among subsets of variables uniquely matches the logarithmic ratio of probabilities, thereby aligning with the conditional independence structure implied by the graph in the context of the Hammersley-Clifford theorem.
Define \( \mathbf{x}^{i}=\left(\mathbf{x}_{1}, \mathbf{x}_2, \ldots, \mathbf{x}_{i-1}, 0, \mathbf{x}_{i+1}, \ldots, \mathbf{x}_D\right) \) where only the \( i \)-th component of \( \mathbf{x} \) vanishes. We now get
\begin{align*}
\exp (Q(\mathbf{x})-Q(\mathbf{x}^{i}))=\frac{p(\mathbf{x})}{p(\mathbf{x}^{i})}=\frac{p\left(\mathrm{x}_i=\mathbf{x}_{i} \mid \mathbf{x}_{-i}\right)}{p\left(\mathrm{x}_i=0 \mid \mathbf{x}_{-i}\right)}.
\end{align*}
The first equality uses the definition \(Q(\mathbf{x})=\ln \left(p(\mathbf{x}) / p\left(\mathbf{x}^{\ast}\right)\right)\). The second equality uses the product rule of probability to get \( p(\mathbf{x}) = p(\mathbf{x}_{i} \mid \mathbf{x}_{-i}) p(\mathbf{x}_{-i}) \) and  \( p(\mathbf{x}^{i}) = p(\mathbf{x}_{i} = 0 \mid \mathbf{x}_{-i}) p(\mathbf{x}_{-i}) \). Note \(\mathbf{x}_{-i}\) is notation for all but the \( i \)-th component of \( \mathbf{x} \).
Pick node 1, without loss of generality (WOLOG). Then we can write
\begin{align*}
\begin{gathered}
Q(\mathbf{x})-Q\left(\mathbf{x}^1\right)=\mathbf{x}_{1}\left(G_1\left(\mathbf{x}_{1}\right)+\sum_{1<j} \mathbf{x}_{j} G_{1, j}\left(\mathbf{x}_{1}, \mathbf{x}_{j}\right)+\sum_{1<j<k} \mathbf{x}_{j} \mathbf{x}_k G_{1, j, k}\left(\mathbf{x}_{1}, \mathbf{x}_{j}, \mathbf{x}_k\right)+\right. \\
\left.\cdots+\mathbf{x}_2 \ldots \mathbf{x}_D G_{1, \ldots, D}\left(\mathbf{x}_{1}, \ldots, \mathbf{x}_D\right)\right) .
\end{gathered}
\end{align*}

This we can show, by utilizing the definition of \(Q(\mathbf{x})\) and cancel out all the terms without \(\mathbf{x}_{1}\) because those terms would be identical in \(Q(\mathbf{x})\) and \(Q\left(\mathbf{x}^1\right)\).

Hint:
We will proceed with further proof in the next lecture. But here is a hint about the proof. In Step 2, we showed that \(\exp \left(Q(\mathbf{x})-Q\left(\mathbf{x}^{i}\right)\right)\) is equivalent to the ratio of two conditional distributions, \(\frac{p\left(\mathrm{X}_i=\mathbf{x}_{i} \mid \mathbf{x}_{-i}\right)}{p\left(\mathrm{X}_{\mathrm{i}}=0 \mid \mathbf{x}_{-i}\right)}\). By assumption, this RHS can only depend on the neighbours of \(\mathbf{x}_{1}\). Hence, LHS in Step 3, \(Q(\mathbf{x})-Q\left(\mathbf{x}^1\right)\), can only depend on \(\mathbf{x}_{1}\) and neighbours of \(\mathbf{x}_{1}\). So, we will leverage this, to show that most of the \(G(\cdot)\) s in the RHS are zero except for the ones involving \(\mathbf{x}_{1}\) and the neighbours of \(\mathbf{x}_{1}\). Hence, we will show that, other than cliques, everything else on the RHS must vanish.
Notation (revisit):


Step 4:
Claim: Suppose \(j\) is not a neighbor of 1 . Then all terms involving \(\mathbf{x}_{j}\) in the right hand side (RHS) of the equation in Step 3 must be zero.
a) \(Q(\mathbf{x})-Q\left(\mathbf{x}^1\right)\) is independent of \(\mathbf{x}_{j}\).

Reason: If \(j \notin \mathrm{nb}(1)\), then by the undirected graph assumption, \(p\left(\mathbf{x}_{i} \mid \mathbf{x}_{-i}\right)=p\left(\mathbf{x}_{i} \mid \mathbf{x}_{\mathrm{mb}(i)}\right)\) and hence the RHS of the equation in Step 2 does not involve \(\mathbf{x}_{j}\).
b) Set \(\mathbf{x}_k=0, \forall k \notin\{1, j\}\), then everything from the RHS of the equation in Step 3 goes away (becomes zero) except for the term involving only \(\mathbf{x}_{1}\) and/or \(\mathbf{x}_{j}\) as
\begin{align*}
\mathbf{x}_{1}\left(G_1\left(\mathbf{x}_{1}\right)+\mathbf{x}_{j} G_{1 j}\left(\mathbf{x}_{1}, \mathbf{x}_{j}\right)\right)
\end{align*}

And If \(G_{1, j} \neq 0\), then we would have a contradiction.
Reason: The Left Hand Side (LHS) of the equation in Step 3 is independent of \(\mathbf{x}_{j}\) as we proved in Step 4.a, so the RHS of the equation in Step 3 should also be independent of \(\mathbf{x}_{j}\). But in \(\mathbf{x}_{1}\left(G_1\left(\mathbf{x}_{1}\right)+\mathbf{x}_{j} G_{1 j}\left(\mathbf{x}_{1}, \mathbf{x}_{j}\right)\right)\), we have \(\mathbf{x}_{j}\) influencing the RHS if \(G_{1, j}\) is not zero. So we hav
\begin{align*}
G_{1, j}=0, \forall j \notin \mathrm{nb}(1) .
\end{align*}
c) We can apply the same reasoning as in Step \(4 . b\) to higher-order terms. Then the only terms \(G_{1, j, k, l}\) that can be nonzero are those where \(j, k, l\) are all neighbors of 1 . And by the logic of WOLOG, all nodes in \(\{1, j, k, l\}\) must be neighbors of other nodes in the group \(\{1, j, k, l\}\). And this means the terms that's non-zeros must be a clique.

Conclusion:
All \(G_A\) function in the RHS must be zero except for the ones with \(A\) being a set of nodes in a clique. And the Hammersley-Clifford Theorem is proven.
#+end_proof
Consider a distribution which follows \(p(\mathbf{x})\) as per the formula given above. The form of \( p(\mathbf{x}) \) is based on a set of potential functions which in turns are defined in terms of the (maximal) cliques of some graph. Therefore it is not surprising that \( p(\mathbf{x}) \) will respect the conditional independence properties of the graph. What's bold about the Hammersley-Clifford theorem is that any arbitrary distributions that obey the a certain set of conditional independence properties admits the factorization above in terms of the cliques of some appropriate graph.
*** Estimation
**** ML Estimator
#+NAME: ML estimator for latent variable MRFs
#+ATTR_LATEX: :environment theorem
#+begin_theorem
Assume that the marginal distribution of a latent variable Markov random field is given by
\begin{align*}
p(\boldsymbol{v} \mid \mathbf{w}) = \sum_{\boldsymbol{h}} p(\boldsymbol{v},\,\boldsymbol{h} \mid \mathbf{w}) = \frac{1}{Z} \sum_{\boldsymbol{h}} \exp (-E(\boldsymbol{v},\,\boldsymbol{h},\, \mathbf{w})), \quad Z = \sum_{\boldsymbol{v}} \sum_{\boldsymbol{h}} \exp (-E(\boldsymbol{v},\,\boldsymbol{h},\, \mathbf{w})).
\end{align*}
where \( \mathbf{w} \) is a parameter vector that parametrizes the energy function. The likelihood function given a single training example \( \boldsymbol{v} \) is
  \begin{equation*}
  \boxed{
  \mathcal{L} (\mathbf{w} \mid \boldsymbol{v}) = p(\boldsymbol{v} \mid \mathbf{w}) = \frac{1}{Z} \sum_{\boldsymbol{h}} \exp (-E(\boldsymbol{v},\,\,\boldsymbol{h},\, \mathbf{w})).
  }
  \end{equation*}
The log-likelihood function is
  \begin{equation*}
  \boxed{
\ln \mathcal{L}(\mathbf{w} \mid \boldsymbol{v}) = \ln p(\boldsymbol{v} \mid \mathbf{w}) = \ln \bigg( \sum_{\boldsymbol{h}} \exp(-E(\boldsymbol{v},\, \boldsymbol{h},\, \mathbf{w})) \bigg) - \ln \bigg( \sum_{\boldsymbol{v}} \sum_{\boldsymbol{h}} \exp(-E(\boldsymbol{v},\, \boldsymbol{h},\, \mathbf{w})) \bigg).
  }
  \end{equation*}
The gradient of the log-likelihood function with respect to the parameter vector \( \mathbf{w} \) is
  \begin{equation*}
  \boxed{
\partial_{\mathbf{w}} \ln \mathcal{L}(\mathbf{w} \mid \boldsymbol{v}) = -\sum_{\boldsymbol{h}} p(\boldsymbol{h} \mid \boldsymbol{v},\, \mathbf{w}) \, \partial_{\mathbf{w}} E(\boldsymbol{v},\, \boldsymbol{h},\, \mathbf{w}) + \sum_{\boldsymbol{v},\,\, \boldsymbol{h}} p(\boldsymbol{v},\, \boldsymbol{h} \mid \mathbf{w}) \, \partial_{\mathbf{w}} E(\boldsymbol{v},\, \boldsymbol{h},\, \mathbf{w}).
  }
  \end{equation*}
Assuming \( \partial_{\mathbf{w}} \ln \mathcal{L}(\mathbf{w} \mid \boldsymbol{v}) \) admits a tractible evaluation/approximation a sequential algorithm such as the stochastic gradient descent (SGD) can be used to approximate \( \mathbf{w}_{\text{ML}} \). The SGD update rule \( \mathbf{w} \) is given by
  \begin{equation*}
  \boxed{
\mathbf{w}^{(\tau+1)} \leftarrow \mathbf{w}^{(\tau)} - \eta \, \partial_{\mathbf{w}} \ln \mathcal{L}(\mathbf{w} \mid \boldsymbol{v}).
  }
  \end{equation*}
#+end_theorem
#+NAME: ML estimator for latent variable MRFs
#+ATTR_LATEX: :environment proof
#+begin_proof latex
The only non-trivial steps are the ones that yields the gradient of the log-likelihood function with respect to the parameter vector \( \mathbf{w} \):
\begin{align*}
\partial_{\mathbf{w}} \ln \mathcal{L}(\mathbf{w} \mid \boldsymbol{v}) &= \partial_{\mathbf{w}}\bigg[ \ln \bigg( \sum_{\boldsymbol{h}} \exp(-E(\boldsymbol{v}, \boldsymbol{h})) \bigg) \bigg] - \partial_{\mathbf{w}}\bigg[ \ln \bigg( \sum_{\boldsymbol{v}, \boldsymbol{h}} \exp(-E(\boldsymbol{v}, \boldsymbol{h})) \bigg) \bigg] \\
&= -\bigg( \sum_{\boldsymbol{h}} \exp(-E(\boldsymbol{v}, \boldsymbol{h})) \bigg)^{-1} \sum_{\boldsymbol{h}} \exp(-E(\boldsymbol{v}, \boldsymbol{h})) \, \partial_{\mathbf{w}} E(\boldsymbol{v}, \boldsymbol{h}) \\
&\qquad \quad + \bigg( \sum_{\boldsymbol{v}, \boldsymbol{h}} \exp(-E(\boldsymbol{v}, \boldsymbol{h})) \bigg)^{-1} \sum_{\boldsymbol{v}, \boldsymbol{h}} \exp(-E(\boldsymbol{v}, \boldsymbol{h})) \, \partial_{\mathbf{w}} E(\boldsymbol{v}, \boldsymbol{h}).
\end{align*}
Since
\[
p(\boldsymbol{h} \mid \boldsymbol{v},\, \mathbf{w}) = \frac{p(\boldsymbol{v},\, \boldsymbol{h} \mid \mathbf{w})}{p(\boldsymbol{v} \mid \mathbf{w})} = \frac{\exp(-E(\boldsymbol{v}, \boldsymbol{h},\, \mathbf{w}))}{\sum_{\boldsymbol{h}} \exp(-E(\boldsymbol{v}, \boldsymbol{h},\, \mathbf{w}))}
\]
and
\[
p(\boldsymbol{v},\, \boldsymbol{v} \mid \mathbf{w}) = \frac{\exp(-E(\boldsymbol{v}, \boldsymbol{h},\, \mathbf{w}))}{\sum_{\boldsymbol{v}, \, \boldsymbol{h}} \exp(-E(\boldsymbol{v}, \boldsymbol{h},\, \mathbf{w}))}
\]
we have
\begin{equation*}
\partial_{\mathbf{w}} \ln \mathcal{L}(\mathbf{w} \mid \boldsymbol{v}) = -\sum_{\boldsymbol{h}} p(\boldsymbol{h} \mid \boldsymbol{v},\, \mathbf{w}) \, \partial_{\mathbf{w}} E(\boldsymbol{v},\, \boldsymbol{h},\, \mathbf{w}) + \sum_{\boldsymbol{v},\,\, \boldsymbol{h}} p(\boldsymbol{v},\, \boldsymbol{h} \mid \mathbf{w}) \, \partial_{\mathbf{w}} E(\boldsymbol{v},\, \boldsymbol{h},\, \mathbf{w}).
\end{equation*}
#+end_proof
+ \( \partial_{\mathbf{w}} \ln \mathcal{L}(\mathbf{w} \mid \boldsymbol{v}) \) is given by the difference of two expectations: 1) the expected values of the energy function under the joint distribution distribution \( p(\boldsymbol{v},\,\boldsymbol{h} \mid \mathbf{w})\), 2) the expected values of the energy function under the conditional distribution \( p(\boldsymbol{h} \mid \boldsymbol{v},\, \mathbf{w}) \), of the hidden variables given the training example. The former does not depend on the training example \( \boldsymbol{v} \), the later does. Therefore, these are sometimes called data-independent and data-dependent expectations respectively. For a general energy function \( E(\boldsymbol{v},\, \boldsymbol{h},\, \mathbf{w}) \), evaluation of both the data-dependent and data-independent expectations is intractable since the computational complexity for both the sums is in general exponential in the number of variables of the MRF. One must thus resort to approximation schemes such as approximating the sums that define the expectations by drawing samples from the corresponding distributions using MCMC techniques.
*** Extensions
**** Conditional random fields (CRFs)
** Markov Chain Monte Carlo
:LOGBOOK:
CLOCK: [2024-06-12 Wed 21:18]--[2024-06-12 Wed 21:39] =>  0:21
:END:
In the previous section, we discussed the rejection sampling and importance sampling strategies for evaluating expectations of functions, and we saw that they suffer from severe limitations particularly in spaces of high dimensionality. We therefore turn in this section to a very general and powerful framework called Markov chain Monte Carlo (MCMC), which allows sampling from a large class of distributions, and which scales well with the dimensionality of the sample space. Markov chain Monte Carlo methods have their origins in physics (Metropolis and Ulam, 1949), and it was only towards the end of the 1980s that they started to have a significant impact in the field of statistics.
As with rejection and importance sampling, we again sample from a proposal distribution. This time, however, we maintain a record of the current state \(\mathbf{z}^{(\tau)}\), and the proposal distribution \(q\left(\mathbf{z} \mid \mathbf{z}^{(\tau)}\right)\) depends on this current state, and so the sequence of samples \(\mathbf{z}^{(1)}, \mathbf{z}^{(2)}, \ldots\) forms a Markov chain. Again, if we write \(p(\mathbf{z})=\widetilde{p}(\mathbf{z}) / Z_{p}\), we will assume that \(\widetilde{p}(\mathbf{z})\) can readily be evaluated for any given value of \(\mathbf{z}\), although the value of \(Z_{p}\) may be unknown. The proposal distribution itself is chosen to be sufficiently simple that it is straightforward to draw samples from it directly. At each cycle of the algorithm, we generate a candidate sample \(\mathbf{z}^{\star}\) from the proposal distribution and then accept the sample according to an appropriate criterion.
In the basic Metropolis algorithm (Metropolis et al., 1953), we assume that the proposal distribution is symmetric, that is \(q\left(\mathbf{z}_{A} \mid \mathbf{z}_{B}\right)=q\left(\mathbf{z}_{B} \mid \mathbf{z}_{A}\right)\) for all values of \(\mathbf{z}_{A}\) and \(\mathbf{z}_{B}\). The candidate sample is then accepted with probability
\begin{align*}
A\left(\mathbf{z}^{\star}, \mathbf{z}^{(\tau)}\right)=\min \left(1, \frac{\widetilde{p}\left(\mathbf{z}^{\star}\right)}{\widetilde{p}\left(\mathbf{z}^{(\tau)}\right)}\right) \tag{11.33}
\end{align*}
This can be achieved by choosing a random number \(u\) with uniform distribution over the unit interval \((0,1)\) and then accepting the sample if \(A\left(\mathbf{z}^{\star}, \mathbf{z}^{(\tau)}\right)>u\). Note that if the step from \(\mathbf{z}^{(\tau)}\) to \(\mathbf{z}^{\star}\) causes an increase in the value of \(p(\mathbf{z})\), then the candidate point is certain to be kept.
If the candidate sample is accepted, then \(\mathbf{z}^{(\tau+1)}=\mathbf{z}^{\star}\), otherwise the candidate point \(\mathbf{z}^{\star}\) is discarded, \(\mathbf{z}^{(\tau+1)}\) is set to \(\mathbf{z}^{(\tau)}\) and another candidate sample is drawn from the distribution \(q\left(\mathbf{z} \mid \mathbf{z}^{(\tau+1)}\right)\). This is in contrast to rejection sampling, where rejected samples are simply discarded. In the Metropolis algorithm when a candidate point is rejected, the previous sample is included instead in the final list of samples, leading to multiple copies of samples. Of course, in a practical implementation, only a single copy of each retained sample would be kept, along with an integer weighting factor recording how many times that state appears. As we shall see, as long as \(q\left(\mathbf{z}_{A} \mid \mathbf{z}_{B}\right)\) is positive for any values of \(\mathbf{z}_{A}\) and \(\mathbf{z}_{B}\) (this is a sufficient but not necessary condition), the distribution of \(\mathbf{z}^{(\tau)}\) tends to \(p(\mathbf{z})\) as \(\tau \rightarrow \infty\). It should be emphasized, however, that the sequence \(\mathbf{z}^{(1)}, \mathbf{z}^{(2)}, \ldots\) is not a set of independent samples from \(p(\mathbf{z})\) because successive samples are highly correlated. If we wish to obtain independent samples, then we can discard most of the sequence and just retain every \(M^{\text {th }}\) sample. For \(M\) sufficiently large, the retained samples will for all practical purposes be independent. Figure 11.9 shows a simple illustrative example of sampling from a two-dimensional Gaussian distribution using the Metropolis oalgorithm in which the proposal distribution is an isotropic Gaussian.
Further insight into the nature of Markov chain Monte Carlo algorithms can be gleaned by looking at the properties of a specific example, namely a simple random
Figure 11.9 A simple illustration using Metropolis algorithm to sample from a Gaussian distribution whose one standard-deviation contour is shown by the ellipse. The proposal distribution is an isotropic Gaussian distribution whose standard deviation is 0.2 . Steps that are accepted are shown as green lines, and rejected steps are shown in red. A total of 150 candidate samples are generated, of which 43 are rejected.
walk. Consider a state space \(z\) consisting of the integers, with probabilities
\begin{align*}
p\left(z^{(\tau+1)}=z^{(\tau)}\right) & =0.5  \tag{11.34}\\
p\left(z^{(\tau+1)}=z^{(\tau)}+1\right) & =0.25  \tag{11.35}\\
p\left(z^{(\tau+1)}=z^{(\tau)}-1\right) & =0.25
\tag{11.36}
\end{align*}
where \(z^{(\tau)}\) denotes the state at step \(\tau\). If the initial state is \(z^{(1)}=0\), then by symmetry the expected state at time \(\tau\) will also be zero \(\mathbb{E}\left[z^{(\tau)}\right]=0\), and similarly it is easily seen that \(\mathbb{E}\left[\left(z^{(\tau)}\right)^{2}\right]=\tau / 2\). Thus after \(\tau\) steps, the random walk has only travelled a distance that on average is proportional to the square root of \(\tau\). This square root dependence is typical of random walk behaviour and shows that random walks are very inefficient in exploring the state space. As we shall see, a central goal in designing Markov chain Monte Carlo methods is to avoid random walk behaviour.
**** Markov chains I
#+NAME: Markov chain
#+begin_definition latex
A Markov chain is a time discrete stochastic process for which the Markov property holds, that is, a family of random variables \(X=\left\{X^{(k)} \mid k \in \mathbb{N}_0\right\}\) which take values in a (in the following considerations finite) set \(\Omega\) and for which \(\forall k \geq 0\) and \(\forall j, i, i_0, \ldots, i_{k-1} \in \Omega\) it holds
\begin{align*}
\begin{aligned}
p_{i j}^{(k)} & :=P\left(X^{(k+1)}=j \mid X^{(k)}=i, X^{(k-1)}=i_{k-1}, \ldots, X^{(0)}=i_0\right) \\
& =P\left(X^{(k+1)}=j \mid X^{(k)}=i\right) .
\end{aligned}
\end{align*}
This means that the next state of the system depends only on the current state and not on the sequence of events that preceded it.
#+end_definition
#+NAME: Homogeneous Markov chain
#+begin_definition latex
A Markov chain is called homogeneous if for all \(k \geq 0\) the \(p_{i j}^{(k)}\) have the same value \(p_{i j}\). The matrix \(\mathbf{P}=\) \(\left(p_{i j}\right)_{i, j \in \Omega}\) is called transition matrix of the homogeneous Markov chain.
#+end_definition
#+begin_theorem latex
If the starting distribution \(\mu^{(0)}\) (i.e., the probability distribution of \(X^{(0)}\) ) of a homogeneous Markov chain is given by the probability vector \(\boldsymbol{\mu}^{(0)}= (\mu^{(0)}(i))_{i \in \Omega}\), with \(\mu^{(0)}(i)=P (X^{(0)}=i)\), then the distribution \(\boldsymbol{\mu}^{(k)}\) of \(X^{(k)}\) is given by \(\boldsymbol{\mu}^{(k) \mathrm{T}}=\boldsymbol{\mu}^{(0) \mathrm{T}} \mathbf{P}^k\).
#+end_theorem
***** Detailed balance
#+NAME: Detailed balance
#+begin_theorem latex
Consider homogeneous Markov chain described by the transition probabilities \(p_{i j}, i, j \in \Omega\) is that \(\forall i, j \in \Omega\). If a distribution \(\pi\) satisfies the detailed balance condition
\begin{align*}
\pi(i) p_{i j}=\pi(j) p_{j i} \text {. }
\end{align*}
then it is a stationary w.r.t. the Markov chain.
#+end_theorem
***** Stationarity
#+NAME: Stationary distribution
#+begin_definition latex
Consider homogeneous Markov chain described by the transition probabilities \(p_{i j}, i, j \in \Omega\) is that \(\forall i, j \in \Omega\). A distribution \(\pi\) for which it holds \(\boldsymbol{\pi}^{\mathrm{T}}=\boldsymbol{\pi}^{\mathrm{T}} \mathbf{P}\) is called stationary distribution of the Markov chain.
#+end_definition
#+NAME: Irreducible Markov chain
#+begin_definition latex
A Markov chain is irreducible if one can get from any state in \(\Omega\) to any other in a finite number of transitions or more formally \(\forall i, j \in \Omega\) \(\exists k>0\) with \(P\left(X^{(k)}=j \mid X^{(0)}=i\right)>0\).
#+end_definition
#+NAME: Aperiodic Markov chain
#+begin_definition latex
A chain is called aperiodic if for all \(i \in \Omega\) the greatest common divisor of \(\left\{k \mid P\left(X^{(k)}=i \mid X^{(0)}=i\right)>0 \wedge k \in \mathbb{N}_0\right\}\) is 1.
#+end_definition
#+NAME: Distance of variation
#+begin_definition latex
For two distributions \(\alpha\) and \(\beta\) on a finite state space \(\Omega\), the distance of variation is defined as
\[
d_V(\boldsymbol{\alpha},\,\boldsymbol{\beta})=\frac{1}{2}|\boldsymbol{\alpha}-\boldsymbol{\beta}|=\frac{1}{2} \sum_{x \in \Omega}|\alpha(x)-\beta(x)|.
\]
#+end_definition
#+NAME: Uniqueness of stationary distribution
#+begin_theorem latex
An irreducible and aperiodic Markov chain on a finite state space is guaranteed to converge to its stationary distribution, i.e., for an arbitrary starting distribution \(\mu\) it holds
\begin{align*}
\lim _{k \rightarrow \infty} d_V\left(\boldsymbol{\mu}^T \mathrm{P}^k, \boldsymbol{\pi}^T\right)=0
\end{align*}
where \(d_V\) is the distance of variation. 
#+end_theorem
***** Markov chain Monte Carlo (MCMC) methods
Markov chain Monte Carlo methods make use of this convergence theorem for producing samples from certain probability distribution by setting up a Markov chain that converges to the desired distributions. Suppose you want to sample from a distribution \(q\) with a finite state space. Then you construct an irreducible and aperiodic Markov chain with stationary distribution \(\pi=q\). This is a nontrivial task. If \(t\) is large enough, a sample \(X^{(t)}\) from the constructed chain is then approximately a sample from \(\pi\) and therefore from \(q\). Gibbs Sampling [13] is such a MCMC method and will be described in the following section.
**** Markov chains
:PROPERTIES:
:CUSTOM_ID: markov-chains
:END:
Before discussing Markov chain Monte Carlo methods in more detail, it is useful to study some general properties of Markov chains in more detail. In particular, we ask under what circumstances will a Markov chain converge to the desired distribution. A first-order Markov chain is defined to be a series of random variables \(\mathbf{z}^{(1)}, \ldots, \mathbf{z}^{(M)}\) such that the following conditional independence property holds for \(m \in\{1, \ldots, M-1\}\)
\[
\begin{align*}
p\left(\mathbf{z}^{(m+1)} \mid \mathbf{z}^{(1)}, \ldots, \mathbf{z}^{(m)}\right)=p\left(\mathbf{z}^{(m+1)} \mid \mathbf{z}^{(m)}\right) \tag{11.37}
\end{align*}
\]
This of course can be represented as a directed graph in the form of a chain, an example of which is shown in Figure 8.38. We can then specify the Markov chain by giving the probability distribution for the initial variable \(p\left(\mathbf{z}^{(0)}\right)\) together with the conditional probabilities for subsequent variables in the form of transition probabilities \(T_{m}\left(\mathbf{z}^{(m)}, \mathbf{z}^{(m+1)}\right) \equiv p\left(\mathbf{z}^{(m+1)} \mid \mathbf{z}^{(m)}\right)\). A Markov chain is called homogeneous if the transition probabilities are the same for all \(m\).
The marginal probability for a particular variable can be expressed in terms of the marginal probability for the previous variable in the chain in the form
\[
\begin{align*}
p\left(\mathbf{z}^{(m+1)}\right)=\sum_{\mathbf{z}^{(m)}} p\left(\mathbf{z}^{(m+1)} \mid \mathbf{z}^{(m)}\right) p\left(\mathbf{z}^{(m)}\right) \tag{11.38}
\end{align*}
\]
A distribution is said to be invariant, or stationary, with respect to a Markov chain if each step in the chain leaves that distribution invariant. Thus, for a homogeneous Markov chain with transition probabilities \(T\left(\mathbf{z}^{\prime}, \mathbf{z}\right)\), the distribution \(p^{\star}(\mathbf{z})\) is invariant if
\[
\begin{align*}
p^{\star}(\mathbf{z})=\sum_{\mathbf{z}^{\prime}} T\left(\mathbf{z}^{\prime}, \mathbf{z}\right) p^{\star}\left(\mathbf{z}^{\prime}\right) \tag{11.39}
\end{align*}
\]
Note that a given Markov chain may have more than one invariant distribution. For instance, if the transition probabilities are given by the identity transformation, then any distribution will be invariant.
A sufficient (but not necessary) condition for ensuring that the required distribution \(p(\mathbf{z})\) is invariant is to choose the transition probabilities to satisfy the property of detailed balance, defined by
\[
\begin{align*}
p^{\star}(\mathbf{z}) T\left(\mathbf{z}, \mathbf{z}^{\prime}\right)=p^{\star}\left(\mathbf{z}^{\prime}\right) T\left(\mathbf{z}^{\prime}, \mathbf{z}\right) \tag{11.40}
\end{align*}
\]
for the particular distribution \(p^{\star}(\mathbf{z})\). It is easily seen that a transition probability that satisfies detailed balance with respect to a particular distribution will leave that distribution invariant, because
\[
\begin{align*}
\sum_{\mathbf{z}^{\prime}} p^{\star}\left(\mathbf{z}^{\prime}\right) T\left(\mathbf{z}^{\prime}, \mathbf{z}\right)=\sum_{\mathbf{z}^{\prime}} p^{\star}(\mathbf{z}) T\left(\mathbf{z}, \mathbf{z}^{\prime}\right)=p^{\star}(\mathbf{z}) \sum_{\mathbf{z}^{\prime}} p\left(\mathbf{z}^{\prime} \mid \mathbf{z}\right)=p^{\star}(\mathbf{z}) \tag{11.41}
\end{align*}
\]
A Markov chain that respects detailed balance is said to be reversible.
Our goal is to use Markov chains to sample from a given distribution. We can achieve this if we set up a Markov chain such that the desired distribution is invariant. However, we must also require that for \(m \rightarrow \infty\), the distribution \(p\left(\mathbf{z}^{(m)}\right)\) converges to the required invariant distribution \(p^{\star}(\mathbf{z})\), irrespective of the choice of initial distribution \(p\left(\mathbf{z}^{(0)}\right)\). This property is called ergodicity, and the invariant distribution is then called the equilibrium distribution. Clearly, an ergodic Markov chain can have only one equilibrium distribution. It can be shown that a homogeneous Markov chain will be ergodic, subject only to weak restrictions on the invariant distribution and the transition probabilities (Neal, 1993).
In practice we often construct the transition probabilities from a set of 'base' transitions \(B_{1}, \ldots, B_{K}\). This can be achieved through a mixture distribution of the form
\[
\begin{align*}
T\left(\mathbf{z}^{\prime}, \mathbf{z}\right)=\sum_{k=1}^{K} \alpha_{k} B_{k}\left(\mathbf{z}^{\prime}, \mathbf{z}\right) \tag{11.42}
\end{align*}
\] for some set of mixing coefficients \(\alpha_{1}, \ldots, \alpha_{K}\) satisfying \(\alpha_{k} \geqslant 0\) and \(\sum_{k} \alpha_{k}=1\). Alternatively, the base transitions may be combined through successive application, so that
\[
\begin{align*}
T\left(\mathbf{z}^{\prime}, \mathbf{z}\right)=\sum_{\mathbf{z}_{1}} \ldots \sum_{\mathbf{z}_{n-1}} B_{1}\left(\mathbf{z}^{\prime}, \mathbf{z}_{1}\right) \ldots B_{K-1}\left(\mathbf{z}_{K-2}, \mathbf{z}_{K-1}\right) B_{K}\left(\mathbf{z}_{K-1}, \mathbf{z}\right) \tag{11.43}
\end{align*}
\]
If a distribution is invariant with respect to each of the base transitions, then obviously it will also be invariant with respect to either of the \(T\left(\mathbf{z}^{\prime}, \mathbf{z}\right)\) given by (11.42) or (11.43). For the case of the mixture (11.42), if each of the base transitions satisfies detailed balance, then the mixture transition \(T\) will also satisfy detailed balance. This does not hold for the transition probability constructed using (11.43), although by symmetrizing the order of application of the base transitions, in the form \(B_{1}, B_{2}, \ldots, B_{K}, B_{K}, \ldots, B_{2}, B_{1}\), detailed balance can be restored. A common example of the use of composite transition probabilities is where each base transition changes only a subset of the variables.
**** The Metropolis-Hastings algorithm
:PROPERTIES:
:CUSTOM_ID: the-metropolis-hastings-algorithm
:END:
Earlier we introduced the basic Metropolis algorithm, without actually demonstrating that it samples from the required distribution. Before giving a proof, we first discuss a generalization, known as the Metropolis-Hastings algorithm (Hastings, 1970), to the case where the proposal distribution is no longer a symmetric function of its arguments. In particular at step \(\tau\) of the algorithm, in which the current state is \(\mathbf{z}^{(\tau)}\), we draw a sample \(\mathbf{z}^{\star}\) from the distribution \(q_{k}\left(\mathbf{z} \mid \mathbf{z}^{(\tau)}\right)\) and then accept it with probability \(A_{k}\left(\mathbf{z}^{\star}, \mathbf{z}_{\tau}\right)\) where
\[
\begin{align*}
A_{k}\left(\mathbf{z}^{\star}, \mathbf{z}^{(\tau)}\right)=\min \left(1, \frac{\widetilde{p}\left(\mathbf{z}^{\star}\right) q_{k}\left(\mathbf{z}^{(\tau)} \mid \mathbf{z}^{\star}\right)}{\widetilde{p}\left(\mathbf{z}^{(\tau)}\right) q_{k}\left(\mathbf{z}^{\star} \mid \mathbf{z}^{(\tau)}\right)}\right) \tag{11.44}
\end{align*}
\]
Here \(k\) labels the members of the set of possible transitions being considered. Again, the evaluation of the acceptance criterion does not require knowledge of the normalizing constant \(Z_{p}\) in the probability distribution \(p(\mathbf{z})=\widetilde{p}(\mathbf{z}) / Z_{p}\). For a symmetric proposal distribution the Metropolis-Hastings criterion (11.44) reduces to the standard Metropolis criterion given by (11.33).
We can show that \(p(\mathbf{z})\) is an invariant distribution of the Markov chain defined by the Metropolis-Hastings algorithm by showing that detailed balance, defined by (11.40), is satisfied. Using (11.44) we have
\[
\begin{align*}
\begin{align*}
p(\mathbf{z}) q_{k}\left(\mathbf{z} \mid \mathbf{z}^{\prime}\right) A_{k}\left(\mathbf{z}^{\prime}, \mathbf{z}\right) & =\min \left(p(\mathbf{z}) q_{k}\left(\mathbf{z} \mid \mathbf{z}^{\prime}\right), p\left(\mathbf{z}^{\prime}\right) q_{k}\left(\mathbf{z}^{\prime} \mid \mathbf{z}\right)\right) \\
& =\min \left(p\left(\mathbf{z}^{\prime}\right) q_{k}\left(\mathbf{z}^{\prime} \mid \mathbf{z}\right), p(\mathbf{z}) q_{k}\left(\mathbf{z} \mid \mathbf{z}^{\prime}\right)\right) \\
& =p\left(\mathbf{z}^{\prime}\right) q_{k}\left(\mathbf{z}^{\prime} \mid \mathbf{z}\right) A_{k}\left(\mathbf{z}, \mathbf{z}^{\prime}\right)
\end{align*} \tag{11.45}
\end{align*}
\]
as required.
The specific choice of proposal distribution can have a marked effect on the performance of the algorithm. For continuous state spaces, a common choice is a Gaussian centred on the current state, leading to an important trade-off in determining the variance parameter of this distribution. If the variance is small, then the
Figure 11.10 Schematic illustration of the use of an isotropic Gaussian proposal distribution (blue circle) to sample from a correlated multivariate Gaussian distribution (red ellipse) having very different standard deviations in different directions, using the Metropolis-Hastings algorithm. In order to keep the rejection rate low, the scale \(\rho\) of the proposal distribution should be on the order of the smallest standard deviation \(\sigma_{\min }\), which leads to random walk behaviour in which the number of steps separating states that are approximately independent
[[https://cdn.mathpix.com/cropped/2024_06_14_cf740e9c517b2f6ee274g-562.jpg?height=402&width=542&top_left_y=228&top_left_x=1105]] is of order \(\left(\sigma_{\max } / \sigma_{\min }\right)^{2}\) where \(\sigma_{\max }\) is the largest standard deviation.
proportion of accepted transitions will be high, but progress through the state space takes the form of a slow random walk leading to long correlation times. However, if the variance parameter is large, then the rejection rate will be high because, in the kind of complex problems we are considering, many of the proposed steps will be to states for which the probability \(p(\mathbf{z})\) is low. Consider a multivariate distribution \(p(\mathbf{z})\) having strong correlations between the components of \(\mathbf{z}\), as illustrated in Figure 11.10. The scale \(\rho\) of the proposal distribution should be as large as possible without incurring high rejection rates. This suggests that \(\rho\) should be of the same order as the smallest length scale \(\sigma_{\min }\). The system then explores the distribution along the more extended direction by means of a random walk, and so the number of steps to arrive at a state that is more or less independent of the original state is of order \(\left(\sigma_{\max } / \sigma_{\min }\right)^{2}\). In fact in two dimensions, the increase in rejection rate as \(\rho\) increases is offset by the larger steps sizes of those transitions that are accepted, and more generally for a multivariate Gaussian the number of steps required to obtain independent samples scales like \(\left(\sigma_{\max } / \sigma_{2}\right)^{2}\) where \(\sigma_{2}\) is the second-smallest standard deviation (Neal, 1993). These details aside, it remains the case that if the length scales over which the distributions vary are very different in different directions, then the Metropolis Hastings algorithm can have very slow convergence.
*** Gibbs Sampling
:PROPERTIES:
:CUSTOM_ID: gibbs-sampling
:END:
Gibbs sampling (Geman and Geman, 1984) is a simple and widely applicable Markov chain Monte Carlo algorithm and can be seen as a special case of the MetropolisHastings algorithm.
Consider the distribution \(p(\mathbf{z})=p\left(z_{1}, \ldots, z_{M}\right)\) from which we wish to sample, and suppose that we have chosen some initial state for the Markov chain. Each step of the Gibbs sampling procedure involves replacing the value of one of the variables by a value drawn from the distribution of that variable conditioned on the values of the remaining variables. Thus we replace \(z_{i}\) by a value drawn from the distribution \(p\left(z_{i} \mid \mathbf{z}_{\backslash i}\right)\), where \(z_{i}\) denotes the \(i^{\text {th }}\) component of \(\mathbf{z}\), and \(\mathbf{z}_{\backslash i}\) denotes \(z_{1}, \ldots, z_{M}\) but with \(z_{i}\) omitted. This procedure is repeated either by cycling through the variables in some particular order or by choosing the variable to be updated at each step at random from some distribution.
For example, suppose we have a distribution \(p\left(z_{1}, z_{2}, z_{3}\right)\) over three variables, and at step \(\tau\) of the algorithm we have selected values \(z_{1}^{(\tau)}, z_{2}^{(\tau)}\) and \(z_{3}^{(\tau)}\). We first replace \(z_{1}^{(\tau)}\) by a new value \(z_{1}^{(\tau+1)}\) obtained by sampling from the conditional distribution
\[
\begin{align*}
p\left(z_{1} \mid z_{2}^{(\tau)}, z_{3}^{(\tau)}\right) \tag{11.46}
\end{align*}
\]
Next we replace \(z_{2}^{(\tau)}\) by a value \(z_{2}^{(\tau+1)}\) obtained by sampling from the conditional distribution
\[
\begin{align*}
p\left(z_{2} \mid z_{1}^{(\tau+1)}, z_{3}^{(\tau)}\right) \tag{11.47}
\end{align*}
\]
so that the new value for \(z_{1}\) is used straight away in subsequent sampling steps. Then we update \(z_{3}\) with a sample \(z_{3}^{(\tau+1)}\) drawn from
\[
\begin{align*}
p\left(z_{3} \mid z_{1}^{(\tau+1)}, z_{2}^{(\tau+1)}\right) \tag{11.48}
\end{align*}
\]
and so on, cycling through the three variables in turn.
Gibbs Sampling
\[
\begin{align*}
\begin{aligned}
& \text { 1. Initialize }\left\{z_{i}: i=1, \ldots, M\right\} \\
& \text { 2. For } \tau=1, \ldots, T \text { : } \\
& \text { - Sample } z_{1}^{(\tau+1)} \sim p\left(z_{1} \mid z_{2}^{(\tau)}, z_{3}^{(\tau)}, \ldots, z_{M}^{(\tau)}\right) \text {. } \\
& \text { - Sample } z_{2}^{(\tau+1)} \sim p\left(z_{2} \mid z_{1}^{(\tau+1)}, z_{3}^{(\tau)}, \ldots, z_{M}^{(\tau)}\right) \\
& \quad \vdots \\
& \text { - Sample } z_{j}^{(\tau+1)} \sim p\left(z_{j} \mid z_{1}^{(\tau+1)}, \ldots, z_{j-1}^{(\tau+1)}, z_{j+1}^{(\tau)}, \ldots, z_{M}^{(\tau)}\right) \\
& \quad \vdots \\
& \text { - Sample } z_{M}^{(\tau+1)} \sim p\left(z_{M} \mid z_{1}^{(\tau+1)}, z_{2}^{(\tau+1)}, \ldots, z_{M-1}^{(\tau+1)}\right)
\end{aligned}
\end{align*}
\]
[[https://cdn.mathpix.com/cropped/2024_06_14_cf740e9c517b2f6ee274g-563.jpg?height=386&width=1543&top_left_y=1664&top_left_x=105]]
To show that this procedure samples from the required distribution, we first of all note that the distribution \(p(\mathbf{z})\) is an invariant of each of the Gibbs sampling steps individually and hence of the whole Markov chain. This follows from the fact that when we sample from \(p\left(z_{i} \mid\left\{\mathbf{z}_{\backslash i}\right)\right.\), the marginal distribution \(p(\mathbf{z} \backslash i)\) is clearly invariant because the value of \(\mathbf{z}_{\backslash i}\) is unchanged. Also, each step by definition samples from the correct conditional distribution \(p\left(z_{i} \mid \mathbf{z}_{\backslash i}\right)\). Because these conditional and marginal distributions together specify the joint distribution, we see that the joint distribution is itself invariant.
The second requirement to be satisfied in order that the Gibbs sampling procedure samples from the correct distribution is that it be ergodic. A sufficient condition for ergodicity is that none of the conditional distributions be anywhere zero. If this is the case, then any point in \(z\) space can be reached from any other point in a finite number of steps involving one update of each of the component variables. If this requirement is not satisfied, so that some of the conditional distributions have zeros, then ergodicity, if it applies, must be proven explicitly.
The distribution of initial states must also be specified in order to complete the algorithm, although samples drawn after many iterations will effectively become independent of this distribution. Of course, successive samples from the Markov chain will be highly correlated, and so to obtain samples that are nearly independent it will be necessary to subsample the sequence.
We can obtain the Gibbs sampling procedure as a particular instance of the Metropolis-Hastings algorithm as follows. Consider a Metropolis-Hastings sampling step involving the variable \(z_{k}\) in which the remaining variables \(\mathbf{z}_{\backslash k}\) remain fixed, and for which the transition probability from \(\mathbf{z}\) to \(\mathbf{z}^{\star}\) is given by \(q_{k}\left(\mathbf{z}^{\star} \mid \mathbf{z}\right)=p\left(z_{k}^{\star} \mid \mathbf{z} \backslash k\right)\). We note that \(\mathbf{z}_{\backslash k}^{\star}=\mathbf{z}_{\backslash k}\) because these components are unchanged by the sampling step. Also, \(p(\mathbf{z})=p\left(z_{k} \mid \mathbf{z}_{\backslash k}\right) p\left(\mathbf{z}_{\backslash k}\right)\). Thus the factor that determines the acceptance probability in the Metropolis-Hastings (11.44) is given by
\[
\begin{align*}
A\left(\mathbf{z}^{\star}, \mathbf{z}\right)=\frac{p\left(\mathbf{z}^{\star}\right) q_{k}\left(\mathbf{z} \mid \mathbf{z}^{\star}\right)}{p(\mathbf{z}) q_{k}\left(\mathbf{z}^{\star} \mid \mathbf{z}\right)}=\frac{p\left(z_{k}^{\star} \mid \mathbf{z}_{\backslash k}^{\star}\right) p\left(\mathbf{z}_{\backslash k}^{\star}\right) p\left(z_{k} \mid \mathbf{z}_{\backslash k}^{\star}\right)}{p\left(z_{k} \mid \mathbf{z}_{\backslash k}\right) p\left(\mathbf{z}_{\backslash k}\right) p\left(z_{k}^{\star} \mid \mathbf{z}_{\backslash k}\right)}=1 \tag{11.49}
\end{align*}
\]
where we have used \(\mathbf{z}_{\backslash k}^{\star}=\mathbf{z}_{\backslash k}\). Thus the Metropolis-Hastings steps are always accepted.
As with the Metropolis algorithm, we can gain some insight into the behaviour of Gibbs sampling by investigating its application to a Gaussian distribution. Consider a correlated Gaussian in two variables, as illustrated in Figure 11.11, having conditional distributions of width \(l\) and marginal distributions of width \(L\). The typical step size is governed by the conditional distributions and will be of order \(l\). Because the state evolves according to a random walk, the number of steps needed to obtain independent samples from the distribution will be of order \((L / l)^{2}\). Of course if the Gaussian distribution were uncorrelated, then the Gibbs sampling procedure would be optimally efficient. For this simple problem, we could rotate the coordinate system in order to decorrelate the variables. However, in practical applications it will generally be infeasible to find such transformations.
One approach to reducing random walk behaviour in Gibbs sampling is called over-relaxation (Adler, 1981). In its original form, this applies to problems for which
Figure 11.11 Illustration of Gibbs sampling by alternate updates of two variables whose distribution is a correlated Gaussian. The step size is governed by the standard deviation of the conditional distribution (green curve), and is \(O(l)\), leading to slow progress in the direction of elongation of the joint distribution (red ellipse). The number of steps needed to obtain an independent sample from the distribution is \(O\left((L / l)^{2}\right)\).
[[https://cdn.mathpix.com/cropped/2024_06_14_cf740e9c517b2f6ee274g-565.jpg?height=662&width=660&top_left_y=225&top_left_x=987]]
the conditional distributions are Gaussian, which represents a more general class of distributions than the multivariate Gaussian because, for example, the non-Gaussian distribution \(p(z, y) \propto \exp \left(-z^{2} y^{2}\right)\) has Gaussian conditional distributions. At each step of the Gibbs sampling algorithm, the conditional distribution for a particular component \(z_{i}\) has some mean \(\mu_{i}\) and some variance \(\sigma_{i}^{2}\). In the over-relaxation framework, the value of \(z_{i}\) is replaced with
\[
\begin{align*}
z_{i}^{\prime}=\mu_{i}+\alpha\left(z_{i}-\mu_{i}\right)+\sigma_{i}\left(1-\alpha_{i}^{2}\right)^{1 / 2} \nu \tag{11.50}
\end{align*}
\]
where \(\nu\) is a Gaussian random variable with zero mean and unit variance, and \(\alpha\) is a parameter such that \(-1<\alpha<1\). For \(\alpha=0\), the method is equivalent to standard Gibbs sampling, and for \(\alpha<0\) the step is biased to the opposite side of the mean. This step leaves the desired distribution invariant because if \(z_{i}\) has mean \(\mu_{i}\) and variance \(\sigma_{i}^{2}\), then so too does \(z_{i}^{\prime}\). The effect of over-relaxation is to encourage directed motion through state space when the variables are highly correlated. The framework of ordered over-relaxation (Neal, 1999) generalizes this approach to nonGaussian distributions.
The practical applicability of Gibbs sampling depends on the ease with which samples can be drawn from the conditional distributions \(p\left(z_{k} \mid \mathbf{z}_{\backslash k}\right)\). In the case of probability distributions specified using graphical models, the conditional distributions for individual nodes depend only on the variables in the corresponding Markov blankets, as illustrated in Figure 11.12. For directed graphs, a wide choice of conditional distributions for the individual nodes conditioned on their parents will lead to conditional distributions for Gibbs sampling that are log concave. The adaptive rejection sampling methods discussed in Section 11.1.3 therefore provide a framework for Monte Carlo sampling from directed graphs with broad applicability.
If the graph is constructed using distributions from the exponential family, and if the parent-child relationships preserve conjugacy, then the full conditional distributions arising in Gibbs sampling will have the same functional form as the orig-
Figure 11.12 The Gibbs sampling method requires samples to be drawn from the conditional distribution of a variable conditioned on the remaining variables. For graphical models, this conditional distribution is a function only of the states of the nodes in the Markov blanket. For an undirected graph this comprises the set of neighbours, as shown on the left, while for a directed graph the Markov blanket comprises the parents, the children, and the co-parents, as shown on the right. [[https://cdn.mathpix.com/cropped/2024_06_14_cf740e9c517b2f6ee274g-566.jpg?height=374&width=634&top_left_y=234&top_left_x=1010]]
inal conditional distributions (conditioned on the parents) defining each node, and so standard sampling techniques can be employed. In general, the full conditional distributions will be of a complex form that does not permit the use of standard sampling algorithms. However, if these conditionals are log concave, then sampling can be done efficiently using adaptive rejection sampling (assuming the corresponding variable is a scalar).
If, at each stage of the Gibbs sampling algorithm, instead of drawing a sample from the corresponding conditional distribution, we make a point estimate of the variable given by the maximum of the conditional distribution, then we obtain the iterated conditional modes (ICM) algorithm discussed in Section 8.3.3. Thus ICM can be seen as a greedy approximation to Gibbs sampling.
Because the basic Gibbs sampling technique considers one variable at a time, there are strong dependencies between successive samples. At the opposite extreme, if we could draw samples directly from the joint distribution (an operation that we are supposing is intractable), then successive samples would be independent. We can hope to improve on the simple Gibbs sampler by adopting an intermediate strategy in which we sample successively from groups of variables rather than individual variables. This is achieved in the blocking Gibbs sampling algorithm by choosing blocks of variables, not necessarily disjoint, and then sampling jointly from the variables in each block in turn, conditioned on the remaining variables (Jensen et al., 1995).
*** Slice Sampling
:PROPERTIES:
:CUSTOM_ID: slice-sampling
:END:
We have seen that one of the difficulties with the Metropolis algorithm is the sensitivity to step size. If this is too small, the result is slow decorrelation due to random walk behaviour, whereas if it is too large the result is inefficiency due to a high rejection rate. The technique of slice sampling (Neal, 2003) provides an adaptive step size that is automatically adjusted to match the characteristics of the distribution. Again it requires that we are able to evaluate the unnormalized distribution \(\widetilde{p}(\mathbf{z})\).
Consider first the univariate case. Slice sampling involves augmenting \(z\) with an additional variable \(u\) and then drawing samples from the joint \((z, u)\) space. We shall see another example of this approach when we discuss hybrid Monte Carlo in Section 11.5. The goal is to sample uniformly from the area under the distribution
[[https://cdn.mathpix.com/cropped/2024_06_14_cf740e9c517b2f6ee274g-567.jpg?height=309&width=620&top_left_y=234&top_left_x=178]]
1) 
[[https://cdn.mathpix.com/cropped/2024_06_14_cf740e9c517b2f6ee274g-567.jpg?height=309&width=603&top_left_y=234&top_left_x=954]]
2) [@2] 
Figure 11.13 Illustration of slice sampling. (a) For a given value \(z^{(\tau)}\), a value of \(u\) is chosen uniformly in the region \(0 \leqslant u \leqslant \widetilde{p}\left(z^{(\tau)}\right)\), which then defines a 'slice' through the distribution, shown by the solid horizontal lines. (b) Because it is infeasible to sample directly from a slice, a new sample of \(z\) is drawn from a region \(z_{\min } \leqslant z \leqslant z_{\text {max }}\), which contains the previous value \(z^{(\tau)}\).
given by
\[
\begin{align*}
\widehat{p}(z, u)= \begin{cases}1 / Z_{p} & \text { if } 0 \leqslant u \leqslant \widetilde{p}(z)  \tag{11.51}\\ 0 & \text { otherwise }\end{cases}
\end{align*}
\]
where \(Z_{p}=\int \widetilde{p}(z) \mathrm{d} z\). The marginal distribution over \(z\) is given by
\[
\begin{align*}
\int \widehat{p}(z, u) \mathrm{d} u=\int_{0}^{\widetilde{p}(z)} \frac{1}{Z_{p}} \mathrm{~d} u=\frac{\widetilde{p}(z)}{Z_{p}}=p(z) \tag{11.52}
\end{align*}
\]
and so we can sample from \(p(z)\) by sampling from \(\widehat{p}(z, u)\) and then ignoring the \(u\) values. This can be achieved by alternately sampling \(z\) and \(u\). Given the value of \(z\) we evaluate \(\widetilde{p}(z)\) and then sample \(u\) uniformly in the range \(0 \leqslant u \leqslant \widetilde{p}(z)\), which is straightforward. Then we fix \(u\) and sample \(z\) uniformly from the 'slice' through the distribution defined by \(\{z: \widetilde{p}(z)>u\}\). This is illustrated in Figure 11.13(a).
In practice, it can be difficult to sample directly from a slice through the distribution and so instead we define a sampling scheme that leaves the uniform distribution under \(\widehat{p}(z, u)\) invariant, which can be achieved by ensuring that detailed balance is satisfied. Suppose the current value of \(z\) is denoted \(z^{(\tau)}\) and that we have obtained a corresponding sample \(u\). The next value of \(z\) is obtained by considering a region \(z_{\min } \leqslant z \leqslant z_{\max }\) that contains \(z^{(\tau)}\). It is in the choice of this region that the adaptation to the characteristic length scales of the distribution takes place. We want the region to encompass as much of the slice as possible so as to allow large moves in \(z\) space while having as little as possible of this region lying outside the slice, because this makes the sampling less efficient.
One approach to the choice of region involves starting with a region containing \(z^{(\tau)}\) having some width \(w\) and then testing each of the end points to see if they lie within the slice. If either end point does not, then the region is extended in that direction by increments of value \(w\) until the end point lies outside the region. A candidate value \(z^{\prime}\) is then chosen uniformly from this region, and if it lies within the slice, then it forms \(z^{(\tau+1)}\). If it lies outside the slice, then the region is shrunk such that \(z^{\prime}\) forms an end point and such that the region still contains \(z^{(\tau)}\). Then another candidate point is drawn uniformly from this reduced region and so on, until a value of \(z\) is found that lies within the slice.
Slice sampling can be applied to multivariate distributions by repeatedly sampling each variable in turn, in the manner of Gibbs sampling. This requires that we are able to compute, for each component \(z_{i}\), a function that is proportional to \(p\left(z_{i} \mid \mathbf{z}_{\backslash i}\right)\).
*** Estimating the Partition Function
:PROPERTIES:
:CUSTOM_ID: estimating-the-partition-function
:END:
As we have seen, most of the sampling algorithms considered in this chapter require only the functional form of the probability distribution up to a multiplicative constant. Thus if we write
\[
\begin{align*}
p_{E}(\mathbf{z})=\frac{1}{Z_{E}} \exp (-E(\mathbf{z})) \tag{11.71}
\end{align*}
\]
then the value of the normalization constant \(Z_{E}\), also known as the partition function, is not needed in order to draw samples from \(p(\mathbf{z})\). However, knowledge of the value of \(Z_{E}\) can be useful for Bayesian model comparison since it represents the model evidence (i.e., the probability of the observed data given the model), and so it is of interest to consider how its value might be obtained. We assume that direct evaluation by summing, or integrating, the function \(\exp (-E(\mathbf{z}))\) over the state space of \(\mathbf{z}\) is intractable.
For model comparison, it is actually the ratio of the partition functions for two models that is required. Multiplication of this ratio by the ratio of prior probabilities gives the ratio of posterior probabilities, which can then be used for model selection or model averaging.
One way to estimate a ratio of partition functions is to use importance sampling from a distribution with energy function \(G(\mathbf{z})\)
\begin{align*}
\frac{Z_{E}}{Z_{G}} & =\frac{\sum_{\mathbf{z}} \exp (-E(\mathbf{z}))}{\sum_{\mathbf{z}} \exp (-G(\mathbf{z}))} \\
& =\frac{\sum_{\mathbf{z}} \exp (-E(\mathbf{z})+G(\mathbf{z})) \exp (-G(\mathbf{z}))}{\sum_{\mathbf{z}} \exp (-G(\mathbf{z}))} \\
& =\mathbb{E}_{G(\mathbf{z})}[\exp (-E+G)] \\
& \simeq \sum_{l} \exp \left(-E\left(\mathbf{z}^{(l)}\right)+G\left(\mathbf{z}^{(l)}\right)\right)
\tag{11.72}
\end{align*}
where \(\left\{\mathbf{z}^{(l)}\right\}\) are samples drawn from the distribution defined by \(p_{G}(\mathbf{z})\). If the distribution \(p_{G}\) is one for which the partition function can be evaluated analytically, for example a Gaussian, then the absolute value of \(Z_{E}\) can be obtained.
This approach will only yield accurate results if the importance sampling distribution \(p_{G}\) is closely matched to the distribution \(p_{E}\), so that the ratio \(p_{E} / p_{G}\) does not have wide variations. In practice, suitable analytically specified importance sampling distributions cannot readily be found for the kinds of complex models considered in this book.
An alternative approach is therefore to use the samples obtained from a Markov chain to define the importance-sampling distribution. If the transition probability for the Markov chain is given by \(T\left(\mathbf{z}, \mathbf{z}^{\prime}\right)\), and the sample set is given by \(\mathbf{z}^{(1)}, \ldots, \mathbf{z}^{(L)}\), then the sampling distribution can be written as
\begin{align*}
\frac{1}{Z_{G}} \exp (-G(\mathbf{z}))=\sum_{l=1}^{L} T\left(\mathbf{z}^{(l)}, \mathbf{z}\right) \tag{11.73}
\end{align*}
which can be used directly in (11.72).
Methods for estimating the ratio of two partition functions require for their success that the two corresponding distributions be reasonably closely matched. This is especially problematic if we wish to find the absolute value of the partition function for a complex distribution because it is only for relatively simple distributions that the partition function can be evaluated directly, and so attempting to estimate the ratio of partition functions directly is unlikely to be successful. This problem can be tackled using a technique known as chaining (Neal, 1993; Barber and Bishop, 1997), which involves introducing a succession of intermediate distributions \(p_{2}, \ldots, p_{M-1}\) that interpolate between a simple distribution \(p_{1}(\mathbf{z})\) for which we can evaluate the normalization coefficient \(Z_{1}\) and the desired complex distribution \(p_{M}(\mathbf{z})\). We then have
\begin{align*}
\frac{Z_{M}}{Z_{1}}=\frac{Z_{2}}{Z_{1}} \frac{Z_{3}}{Z_{2}} \cdots \frac{Z_{M}}{Z_{M-1}} \tag{11.74}
\end{align*}
in which the intermediate ratios can be determined using Monte Carlo methods as discussed above. One way to construct such a sequence of intermediate systems is to use an energy function containing a continuous parameter \(0 \leqslant \alpha \leqslant 1\) that interpolates between the two distributions
\begin{align*}
E_{\alpha}(\mathbf{z})=(1-\alpha) E_{1}(\mathbf{z})+\alpha E_{M}(\mathbf{z}) \tag{11.75}
\end{align*}
If the intermediate ratios in (11.74) are to be found using Monte Carlo, it may be more efficient to use a single Markov chain run than to restart the Markov chain for each ratio. In this case, the Markov chain is run initially for the system \(p_{1}\) and then after some suitable number of steps moves on to the next distribution in the sequence. Note, however, that the system must remain close to the equilibrium distribution at each stage.
** Restricted Boltzmann machines (RBMs)
:LOGBOOK:
CLOCK: [2024-06-20 Thu 13:12]--[2024-06-20 Thu 18:44] =>  5:32
CLOCK: [2024-06-20 Thu 12:17]--[2024-06-20 Thu 13:08] =>  0:51
CLOCK: [2024-06-20 Thu 08:44]--[2024-06-20 Thu 11:07] =>  2:23
CLOCK: [2024-06-19 Wed 21:03]--[2024-06-19 Wed 22:16] =>  1:13
:END:
*** Definitions
**** Bipartite graph
#+NAME: Disjoint sets
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Let \( V \) and \( H \) be two sets. If \( V \cap H = \emptyset \), then \( V \) and \( H \) are called disjoint sets.
#+end_definition
#+NAME: Anti-clique
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Given a graph \( G = (V,~E) \), a subset \( A \subset V \) of the graph's nodes is called an anti-clique if:
\[
\forall (i,~j) \quad a_{i} \in A, ~a_{j} \in A \Longrightarrow (a_{i},~a_{j}) \notin E.
\]
#+end_definition
#+NAME: Bipartite graph
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Consider a graph \( G = (V,~H,~E) \) whose vertices admit a decomposition into two sets \( V \) and \(  \)\( H \). \( G \) is called a bipartite graph if \( V \) and \( H \) are disjoint and separately anti-cliques.
#+end_definition
#+NAME: Balanced bipartite graph
#+ATTR_LATEX: :environment definition
#+begin_definition latex
A bipartite graph is balanced if \( \lvert V \rvert = \lvert H \rvert \).
#+end_definition
#+NAME: Complete bipartite graph
#+ATTR_LATEX: :environment definition
#+begin_definition latex
A bipartite graph \( G(V,~H,~E) \) is called a complete bipartite graph if:
\[
\forall (i,~j) \quad v_{i} \in V, ~h_{j} \in H \Longrightarrow (v_{i},~h_{j}) \in E.
\]
A complete bipartite graph with \( \lvert V \rvert = m \) and \( \lvert H \rvert = n \) is denoted by \( K_{m,n} \). It has \( m + n \) vertices and \( m \times n \) edges.
#+end_definition
#+NAME: Clique
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Consider a bipartite graph \( G(V,~H,~E) \). Let \( C \) denotes its cliques \(\forall (i, ~ j) \quad \text{If } (v_{i}, ~ h_{j}) \in E \Longrightarrow \{v_{i}, ~ h_{j}\} \in C\).
#+end_proposition
#+NAME: Clique
#+ATTR_LATEX: :environment proof
#+begin_proof latex
This is trivial result; two vertices that are connected by an edge always form a clique.
#+end_proof
#+NAME: Maximal clique
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Consider a bipartite graph \( G(V,~H,~E) \). Let \( C \) and \( \mathcal{C} \) be the set of cliques and maximal cliques of \( G \) respectively. We have \( \lvert E \rvert = \lvert C \rvert = \lvert \mathcal{C} \rvert\) and every clique is a maximal clique. When the graph is a complete bipartite graph \( K_{m, n} \) we have \( \lvert E \rvert = \lvert C \rvert = \lvert \mathcal{C} \rvert = m \times n \).
#+end_proposition
#+NAME: Maximal clique
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Suppose \((v_{i}, ~ h_{j}) \in E\) so that \( \{v_{i}, ~ h_{j}\} \) is a clique, i.e., \( \{v_{i}, ~ h_{j}\} \in C \). Now suppose we add so arbitrary vertex \( v_{k} \in V \) to \( \{v_{i}, ~ h_{j}\} \) to obtain \( \{v_{i}, ~ h_{j}, ~ v_{k}\} \). We have \( \{v_{i}, ~ h_{j}, ~ v_{k}\} \notin C \) since \( (v_{i}, ~ v_{k}) \notin E \). Similarly, if we add an arbitrary \( h_{k} \) to obtain \( \{v_{i}, ~ h_{j}, ~ h_{k}\} \notin C \) since \( (h_{j}, ~ h_{k}) \notin E \). Clearly, the addition of any vertex in \( G \) to \( \{v_{i}, ~ h_{j}\}\) makes it lose its clique status. It must then be a maximal clique, i.e, \( \{v_{i}, ~ h_{j}\} \in \mathcal{C} \). In other words, if \( G(V, ~ H, ~E) \) is bipartite, \( \{v_{i}, ~ h_{j}\} \in C \Longrightarrow \{v_{i}, ~ h_{j}\} \in \mathcal{C} \). \( \lvert E \rvert = \lvert C \rvert = \lvert \mathcal{C} \rvert\) follows immediately. \( \lvert E \rvert = \lvert C \rvert = \lvert \mathcal{C} \rvert = m \times n \) follows from the fact that for a complete bipartite graph \( K_{m, n} \), \( \lvert E \rvert = m \times n \).
#+end_proof
**** Markov random field
Let \( G(V,~H,~E) \) denote a /complete bipartite graph/. Introduce a binary random variables for each vertex of the graph and denote the vector of random variables for the vertex set \( V \) and \( H \) using \(\mathbf{v} \in \{\pm 1\}^{\lvert V \rvert} \) and \( \(\mathbf{h} \in \{\pm 1\}^{\lvert H \rvert} \) respectively. Let \( p(\mathbf{v}, ~ \mathbf{h}) \) denote the joint distribution over these random variables.
Next, consider two arbitrarily chosen random variables \( v_{i} \) and \( v_{j} \) from \( \mathbf{v} \). By definition, these random variables are not connected by a link. If the rest of the variables are given, then all /paths/ between \( v_{i} \) and \( v_{j} \) are /blocked/: they must therefore be conditionally independent given all other variables in the graph \( G \). 
\[
p(v_{i}, ~ v_{j} \mid (v_{k})_{k \neq i, j}, ~ \mathbf{h}) = p(v_{i} \mid (v_{k})_{k \neq i, j}, ~ \mathbf{h}) ~ p(v_{j} \mid (v_{k})_{k \neq i, j}, ~ \mathbf{h}).
\]
This is the /pairwise Markov property/ for the random variables \( \mathbf{v} \) and \mathbf{h}: for it to hold the factorization of the joint distribution must be such that the random variables \( v_{i} \) and \( v_{j} \) never appear in the same factor. But from the previous result on maximal clique of the complete bipartite graph, it is clear that all the \( m n \) potential functions are functions of two random variables one each from \( V \) and \( H \) which are disjoint sets, and separately anti-clique. Therefore the bipartite structure ensures fulfills the /pairwise Markov property/. Next we assume that the joint distribution is strictly positive. It turns out that under this assumption, the pairwise, local, and global Markov properties are equivalent and therefore \(  \)\( \mathbf{v} \) and \( \mathbf{h} \) form a /Markov random field/.
**** Hammersley-Clifford theorem
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Let \(G=(V, E)\) be an undirected graph. Let \(\mathcal{C}\) denote the set of all maximal cliques of \( G \). A distribution \( p(\boldsymbol{x}) \) is said to factorize about the graph \( G \) if there exists a set of non-negative functions \(\left\{\psi_{C}\right\}_{C \subset \mathcal{C}}\), called potential functions, with
\begin{align*}
\forall \boldsymbol{x}, \hat{\boldsymbol{x}} \in \Lambda^{|V|}:\left(x_{c}\right)_{c \in C}=\left(\hat{x}_{c}\right)_{c \in C} \Rightarrow \psi_{C}(\boldsymbol{x})=\psi_{C}(\hat{\boldsymbol{x}})
\end{align*}
and
\begin{align*}
p(\boldsymbol{x})=\frac{1}{Z} \prod_{C \in \mathcal{C}} \psi_{C}(\boldsymbol{x}).
\end{align*}
The normalization constant \(Z=\sum_{\boldsymbol{x}} \prod_{C \in \mathcal{C}} \psi_{C}\left(\boldsymbol{x}_{C}\right)\) is called partition function.
#+end_definition
#+NAME: Hammersley-Clifford theorem
#+begin_theorem latex
Let \(G=(V, E)\) be an undirected graph. Let \(\mathcal{C}\) denote the set of all maximal cliques of \( G \). Suppose that each node \(v \in V\) is associated a random variable \(X_{v}\) taking values in a state space \(\Lambda_{v}\). Further assume that \(\Lambda_{v}=\Lambda\) for all \(v \in V\). A strictly positive distribution, i.e., \(0 < p(\boldsymbol{X} = \boldsymbol{x})\) for all \(\boldsymbol{x}\), over the random variables \(\boldsymbol{X}=\left(X_{v}\right)_{v \in V}\) forms an MRF if and only if \(p(\boldsymbol{x})\) factorizes over \(G\). If \(p(\boldsymbol{x})\) factorizes over \( G \), then
\begin{align*}
p(\boldsymbol{x})=\frac{1}{Z} \prod_{C \in \mathcal{C}} \psi_{C}\left(\boldsymbol{x}_{C}\right)=\frac{1}{Z} \exp \bigg( \sum_{C \in \mathcal{C}} \ln \psi_{C} (\boldsymbol{x}_{C}) \bigg) = \frac{1}{Z} \exp (-E(\boldsymbol{x}))
\end{align*}
where \(E \equiv \sum_{C \in \mathcal{C}} \ln \psi_{C} (\boldsymbol{x}_{C})\) is called the energy function. This is the Gibbs-Boltzmann distribution. Thus, the probability distribution of every MRF is a Gibbs-Boltzmann distribution.
#+end_theorem
**** Restricted Boltzmann machines
#+NAME: Restricted Boltzmann machines
#+begin_definition latex
An RBM is an MRF associated with a complete bipartite  Markov network. Let \( \mathbf{v} = (V_{i})_{i=1,\ldots,m} \) and \( \mathbf{h} = (H_{i})_{i=1,\ldots,n} \) denote \( m \) observed and \( n \) latent variables respectively. It consists of \(m\) visible units \(\mathbf{v} = (V_{1}, \ldots, V_{m})\) to represent observable data and \(n\) hidden units \(\mathbf{h} = (H_{1}, \ldots, H_{n})\) to capture dependencies between observed variables. \((\mathbf{v}, ~ \mathbf{h})\) take values \((\mathbf{v}, ~ \mathbf{h}) \in \{0,1\}^{m+n}\) and the joint probability distribution under the model is given by the Gibbs distribution \(p(\mathbf{v}, ~ \mathbf{h}) = \exp (-E(\mathbf{v}, ~ \mathbf{h})) / Z\) with the energy function
\[
\boxed{
E(\mathbf{v}, ~ ~ \mathbf{h}) = -\sum_{i=1}^{n} \sum_{j=1}^{m} w_{i j} h_{i} v_{j} - \sum_{j=1}^{m} b_{j} v_{j} - \sum_{i=1}^{n} c_{i} h_{i}
}
\]
For all \(i \in \{1, \ldots, n\}\) and \(j \in \{1, \ldots, m\}\), \(w_{i j}\) is a real-valued weight associated with the edge between units \(V_{j}\) and \(H_{i}\), and \(b_{j}\) and \(c_{i}\) are real-valued bias terms associated with the \(j\)th visible and the \(i\)th hidden variable, respectively.
#+end_definition
The graph of an RBM has only connections between the layer of hidden and visible variables but not between two variables of the same layer. In terms of probability, this means that the hidden variables are independent given the state of the visible variables and vice versa:
\[
p(\mathbf{h} \mid \mathbf{v}) = \prod_{i=1}^{n} p(h_{i} \mid \mathbf{v}) \quad \text{and} \quad p(\mathbf{v} \mid \mathbf{h}) = \prod_{j=1}^{m} p(v_{i} \mid \mathbf{h})
\]
The absence of connections between hidden variables makes the marginal distribution of the visible variables easy to calculate:
\begin{align*}
p(\mathbf{v}) &= \frac{1}{Z} \sum_{\mathbf{h}} p(\mathbf{v}, ~ \mathbf{h}) = \frac{1}{Z} \sum_{\mathbf{h}} \exp (-E(\mathbf{v}, ~ \mathbf{h}))  \\
&= \frac{1}{Z} \sum_{h_{1}} \sum_{h_{2}} \cdots \sum_{h_{n}} \exp \bigg(\sum_{j=1}^{m} b_{j} v_{j}\bigg) \prod_{i=1}^{n} \exp \bigg\{h_{i}\bigg(c_{i} + \sum_{j=1}^{m} w_{i j} v_{j}\bigg)\bigg\} \\
&= \frac{1}{Z} \exp \bigg(\sum_{j=1}^{m} b_{j} v_{j}\bigg) \sum_{h_{1}} \exp \bigg\{h_{1}\bigg(c_{1} + \sum_{j=1}^{m} w_{1 j} v_{j}\bigg)\bigg\} \sum_{h_{2}} \exp \bigg\{h_{2}\bigg(c_{2} + \sum_{j=1}^{m} w_{2 j} v_{j}\bigg)\bigg\} \ldots \sum_{h_{n}} \exp \bigg\{h_{n}\bigg(c_{n} + \sum_{j=1}^{m} w_{n j} v_{j}\bigg)\bigg\} \\
&= \frac{1}{Z} \exp \bigg(\sum_{j=1}^{m} b_{j} v_{j}\bigg) \prod_{i=1}^{n} \sum_{h_{i}} \exp \bigg\{h_{i}\bigg(c_{i} + \sum_{j=1}^{m} w_{i j} v_{j}\bigg)\bigg\} \\
&= \frac{1}{Z} \prod_{j=1}^{m} \exp (b_{j} v_{j}) \prod_{i=1}^{n} \bigg\{1 + \exp \bigg(c_{i} + \sum_{j=1}^{m} w_{i j} v_{j}\bigg)\bigg\}
\end{align*}
This equation shows why a (marginalized) RBM can be regarded as a product of experts model, in which a number of "experts" for individual components of the observations are combined multiplicatively.
Any distribution on \(\{0,1\}^{m}\) can be modeled arbitrarily well by an RBM with \(m\) visible and \(k+1\) hidden units, where \(k\) denotes the cardinality of the support set of the target distribution, that is, the number of input elements from \(\{0,1\}^{m}\) that have a non-zero probability of being observed. It has been shown recently that even fewer units can be sufficient depending on the patterns in the support set.
The RBM can be interpreted as a stochastic neural network, where nodes and edges correspond to neurons and synaptic connections, respectively. The conditional probability of a single variable being one can be interpreted as the firing rate of a (stochastic) neuron with sigmoid activation function \(\sigma(x) = \frac{1}{1+\exp (-x)}\), because it holds:
\[
p(H_{i} = 1 \mid \mathbf{v}) = \sigma \bigg(\sum_{j=1}^{m} w_{i j} v_{j} + c_{i}\bigg)
\]
and
\[
p(V_{j} = 1 \mid \mathbf{h}) = \sigma \bigg(\sum_{i=1}^{n} w_{i j} h_{i} + b_{j}\bigg)
\]
To see this, let \(\mathbf{v}_{-l}\) denote the state of all visible units except the \(l\)-th one and let us define
\[
\alpha_{l}(\mathbf{h}) = -\sum_{i=1}^{n} w_{i l} h_{i} - b_{l}, \quad \beta(\mathbf{v}_{-l}, \mathbf{h}) = -\sum_{i=1}^{n} \sum_{j=1, j \neq l}^{m} w_{i j} h_{i} v_{j} - \sum_{j=1, j \neq l}^{m} b_{i} v_{i} - \sum_{i=1}^{n} c_{i} h_{i}.
\]
Then \(E(\mathbf{v}, ~ \mathbf{h}) = \beta(\mathbf{v}_{-l}, \mathbf{h}) + v_{l} \alpha_{l}(\mathbf{h})\), where \(v_{l} \alpha_{l}(\mathbf{h})\) collects all terms involving \(v_{l}\) and we can write:
\begin{align*}
p(V_{l} &= 1 \mid \mathbf{h}) = p(V_{l} = 1 \mid \mathbf{v}_{-l}, \mathbf{h}) = \frac{p(V_{l} = 1, \mathbf{v}_{-l}, \mathbf{h})}{p(\mathbf{v}_{-l}, \mathbf{h})} \\
&= \exp \{-E(v_{l}=1, \mathbf{v}_{-l}, \mathbf{h})\} \bigg[\exp \{-E(v_{l}=1, \mathbf{v}_{-l}, \mathbf{h})\} + \exp \{-E(v_{l}=0, \mathbf{v}_{-l}, \mathbf{h})\}\bigg]^{-1} \\
&= \exp \{-\beta(\mathbf{v}_{-l}, \mathbf{h}) - 1 \cdot \alpha_{l}(\mathbf{h})\} \bigg[ \exp \{-\beta(\mathbf{v}_{-l}, \mathbf{h}) - 1 \cdot \alpha_{l}(\mathbf{h})\} + \exp \{-\beta(\mathbf{v}_{-l}, \mathbf{h}) - 0 \cdot \alpha_{l}(\mathbf{h})\} \bigg]^{-1} \\
&= \exp \{-\beta(\mathbf{v}_{-l}, \mathbf{h})\} \cdot \exp \{-\alpha_{l}(\mathbf{h})\} \bigg[ \exp \{-\beta(\mathbf{v}_{-l}, \mathbf{h})\} \cdot \exp \{-\alpha_{l}(\mathbf{h})\} + \exp \{-\beta(\mathbf{v}_{-l}, \mathbf{h}) \} \bigg]^{-1} \\
&= \exp \{-\beta(\mathbf{v}_{-l}, \mathbf{h})\} \cdot \exp \{-\alpha_{l}(\mathbf{h})\} \bigg[\exp \{-\beta(\mathbf{v}_{-l}, \mathbf{h})\} \cdot \bigg(\exp \{-\alpha_{l}(\mathbf{v}_{-l}, \mathbf{h})\} + 1\bigg) \bigg]^{-1} \\
&= \exp \{-\alpha_{l}(\mathbf{v}_{-l}, \mathbf{h}) \} \bigg[\exp (-\alpha_{l}(\mathbf{v}_{-l}, \mathbf{h})) + 1 \bigg]^{-1} = \frac{1}{1 + \exp (\alpha_{l}(\mathbf{v}_{-l}, \mathbf{h}))} \\
&= \sigma(-\alpha_{l}(\mathbf{h})) = \sigma \bigg(\sum_{i=1}^{n} w_{i l} h_{i} + b_{j}\bigg).
\end{align*}
The independence between the variables in one layer makes Gibbs sampling especially easy: Instead of sampling new values for all variables subsequently, the states of all variables in one layer can be sampled jointly. Thus, Gibbs sampling can be performed in just two sub-steps: sampling a new state \(\mathbf{h}\) for the hidden neurons based on \(p(\mathbf{h} \mid \mathbf{v})\) and sampling a state \(\mathbf{v}\) for the visible layer based on \(p(\mathbf{v} \mid \mathbf{h})\). This is also referred to as block Gibbs sampling.
As mentioned in the introduction, an RBM can be reinterpreted as a standard feed-forward neural network with one layer of non-linear processing units. From this perspective, the RBM is viewed as a deterministic function \(\{0,1\}^{m} \rightarrow \mathbb{R}^{n}\) that maps an input \(\mathbf{v} \in \{0,1\}^{m}\) to \(\boldsymbol{y} \in \mathbb{R}^{n}\) with \(y_{i} = p(H_{i} = 1 \mid \mathbf{v})\). That is, an observation is mapped to the expected value of the hidden neurons given the observation.
*** Estimation
**** ML estimation
The log-likelihood gradient of an MRF can be written as the sum of two expectations. For RBMs, the first term (i.e., the expectation of the energy gradient under the conditional distribution of the hidden variables given a training sample \(\mathbf{v}\)) can be computed efficiently because it factorizes nicely. For example, with respect to the parameter \(w_{i j}\), we get:
\begin{align*}
\sum_{\mathbf{h}} p(\mathbf{h} \mid \mathbf{v}) \frac{\partial E(\mathbf{v}, ~ \mathbf{h})}{\partial w_{i j}} &= \sum_{\mathbf{h}} p(\mathbf{h} \mid \mathbf{v}) h_{i} v_{j} = \sum_{\mathbf{h}} \prod_{k=1}^{n} p(h_{k} \mid \mathbf{v}) h_{i} v_{j} \\
&= \sum_{h_{i}} \sum_{\mathbf{h}_{-i}} p(h_{i} \mid \mathbf{v}) p(\mathbf{h}_{-i} \mid \mathbf{v}) h_{i} v_{j} = \sum_{h_{i}} p(h_{i} \mid \mathbf{v}) h_{i} v_{j} \underbrace{\sum_{\mathbf{h}_{-i}} p(\mathbf{h}_{-i} \mid \mathbf{v})}_{=1} = p(H_{i}=1 \mid \mathbf{v}) v_{j} = \sigma \bigg(\sum_{j=1}^{m} w_{i j} v_{j} + c_{i}\bigg) v_{j}
\end{align*}
Since the second term can also be written as \(\sum_{\mathbf{v}} p(\mathbf{v}) \sum_{\mathbf{h}} p(\mathbf{h} \mid \mathbf{v}) \frac{\partial E(\mathbf{v}, ~ \mathbf{h})}{\partial \boldsymbol{\theta}}\) or \(\sum_{\mathbf{h}} p(\mathbf{h}) \sum_{\mathbf{v}} p(\mathbf{v} \mid \mathbf{h}) \frac{\partial E(\mathbf{v}, ~ \mathbf{h})}{\partial \boldsymbol{\theta}}\), we can also reduce its computational complexity by applying the same kind of factorization to the inner sum, either factorizing over the hidden variables as shown above or factorizing over the visible variables in an analogous way. However, the computation remains intractable for regular sized RBMs because its complexity is still exponential in the size of the smallest layer (the outer sum still runs over either \(2^{m}\) or \(2^{n}\) states).
Using the factorization trick, the derivative of the log-likelihood of a single training pattern \(\mathbf{v}\) with respect to the weight \(w_{i j}\) becomes:
\begin{align*}
\frac{\partial \ln \mathcal{L}(\boldsymbol{\theta} \mid \mathbf{v})}{\partial w_{i j}} &= -\sum_{\mathbf{h}} p(\mathbf{h} \mid \mathbf{v}) \frac{\partial E(\mathbf{v}, ~ \mathbf{h})}{\partial w_{i j}} + \sum_{\mathbf{v}, ~ \mathbf{h}} p(\mathbf{v}, ~ \mathbf{h}) \frac{\partial E(\mathbf{v}, ~ \mathbf{h})}{\partial w_{i j}} \\
&= \sum_{\mathbf{h}} p(\mathbf{h} \mid \mathbf{v}) h_{i} v_{j} - \sum_{\mathbf{v}} p(\mathbf{v}) \sum_{\mathbf{h}} p(\mathbf{h} \mid \mathbf{v}) h_{i} v_{j} \\
&= p(H_{i}=1 \mid \mathbf{v}) v_{j} - \sum_{\mathbf{v}} p(\mathbf{v}) p(H_{i}=1 \mid \mathbf{v}) v_{j} \\
\end{align*}
For the mean of this derivative over a training set \(S=\{\mathbf{v}_{1}, \ldots, \mathbf{v}_{\ell}\}\), often the following notations are used:
\begin{align*}
\frac{1}{\ell} \sum_{\mathbf{v} \in S} \frac{\partial \ln \mathcal{L}(\boldsymbol{\theta} \mid \mathbf{v})}{\partial w_{i j}} &= \frac{1}{\ell} \sum_{\mathbf{v} \in S} \bigg[-\mathbb{E}_{p(\mathbf{h} \mid \mathbf{v})} \bigg[\frac{\partial E(\mathbf{v}, ~ \mathbf{h})}{\partial w_{i j}}\bigg] + \mathbb{E}_{p(\mathbf{h}, \mathbf{v})} \bigg[\frac{\partial E(\mathbf{v}, ~ \mathbf{h})}{\partial w_{i j}}\bigg]\bigg] \\
&= \frac{1}{\ell} \sum_{\mathbf{v} \in S} \bigg[\mathbb{E}_{p(\mathbf{h} \mid \mathbf{v})} \bigg[v_{i} h_{j}\bigg] - \mathbb{E}_{p(\mathbf{h}, \mathbf{v})} \bigg[v_{i} h_{j}\bigg]\bigg] \\
&= \bigg\langle v_{i} h_{j}\bigg\rangle_{p(\mathbf{h} \mid \mathbf{v}) q(\mathbf{v})} - \bigg\langle v_{i} h_{j}\bigg\rangle_{p(\mathbf{h}, \mathbf{v})}  \\
\end{align*}
with \(q\) denoting the empirical distribution. This gives the often stated rule:
\[
\sum_{\mathbf{v} \in S} \frac{\partial \ln \mathcal{L}(\boldsymbol{\theta} \mid \mathbf{v})}{\partial w_{i j}} \propto \bigg\langle v_{i} h_{j}\bigg\rangle_{\text{data}} - \bigg\langle v_{i} h_{j}\bigg\rangle_{\text{model}}
\]
Analogously, we get the derivatives with respect to the bias parameter \(b_{j}\) of the \(j\)th visible variable:
\[
\frac{\partial \ln \mathcal{L}(\boldsymbol{\theta} \mid \mathbf{v})}{\partial b_{j}} = v_{j} - \sum_{\mathbf{v}} p(\mathbf{v}) v_{j}
\]
and with respect to the bias parameter \(c_{i}\) of the \(i\)th hidden variable:
\[
\frac{\partial \ln \mathcal{L}(\boldsymbol{\theta} \mid \mathbf{v})}{\partial c_{i}} = p(H_{i}=1 \mid \mathbf{v}) - \sum{\mathbf{v}} p(\mathbf{v}) p(H_{i}=1 \mid \mathbf{v})
\]
To avoid the exponential complexity of summing over all values of the visible variables (or all values of the hidden if one decides to factorize over the visible variables beforehand) when calculating the second term of the log-likelihood gradient, one can approximate this expectation by samples from the model distribution. These samples can be obtained by Gibbs sampling. This requires running the Markov chain "long enough" to ensure convergence to stationarity. Since the computational costs of such an MCMC approach are still too large to yield an efficient learning algorithm, common RBM learning techniques introduce additional approximations.
***** Algorithms
All common training algorithms for RBMs approximate the log-likelihood gradient given some data and perform gradient ascent on these approximations. Selected learning algorithms will be described in the following section, starting with contrastive divergence learning.
******* Contrastive divergence (CD)
Obtaining unbiased estimates of log-likelihood gradient using MCMC methods typically requires many sampling steps. However, recently it was shown that estimates obtained after running the chain for just a few steps can be sufficient for model training. This leads to contrastive divergence (CD) learning, which has become a standard way to train RBMs.
The idea of \(k\)-step contrastive divergence learning (CD-\(k\)) is quite simple: Instead of approximating the second term in the log-likelihood gradient by a sample from the RBM-distribution (which would require running a Markov chain until the stationary distribution is reached), a Gibbs chain is run for only \(k\) steps (and usually \(k=1\)). The Gibbs chain is initialized with a training example \(\mathbf{v}^{(0)}\) of the training set and yields the sample \(\mathbf{v}^{(k)}\) after \(k\) steps. Each step \(t\) consists of sampling \(\mathbf{h}^{(t)}\) from \(p(\mathbf{h} \mid \mathbf{v}^{(t)})\) and sampling \(\mathbf{v}^{(t+1)}\) from \(p(\mathbf{v} \mid \mathbf{h}^{(t)})\) subsequently. The gradient with respect to \(\boldsymbol{\theta}\) of the log-likelihood for one training pattern \(\mathbf{v}^{(0)}\) is then approximated by:
\[
\mathrm{CD}_{k}(\boldsymbol{\theta}, \mathbf{v}^{(0)}) = -\sum_{\mathbf{h}} p(\mathbf{h} \mid \mathbf{v}^{(0)}) \frac{\partial E(\mathbf{v}^{(0)}, \mathbf{h})}{\partial \boldsymbol{\theta}} + \sum_{\mathbf{h}} p(\mathbf{h} \mid \mathbf{v}^{(k)}) \frac{\partial E(\mathbf{v}^{(k)}, \mathbf{h})}{\partial \boldsymbol{\theta}}
\]
The derivatives in direction of the single parameters are obtained by "estimating" the expectations over \(p(\mathbf{v})\) by the single sample \(\mathbf{v}^{(k)}\). A batch version of CD-\(k\) can be seen in algorithm 1.
\begin{algorithm}
\caption{k-step contrastive divergence}
\label{alg:contrast_div}
\begin{algorithmic}[1]
\REQUIRE RBM \((V_1, \dots, V_m, H_1, \dots, H_n)\), training batch \(S\)
\ENSURE Gradient approximation \(\Delta w_{ij}\), \(\Delta b_j\), and \(\Delta c_i\) for \(i = 1, \dots, n\), \(j = 1, \dots, m\)
\STATE Initialize \(\Delta w_{ij} = \Delta b_j = \Delta c_i = 0\) for \(i = 1, \dots, n\), \(j = 1, \dots, m\)
\FORALL{\(v \in S\)}
    \STATE \(v^{(0)} \leftarrow v\)
    \FOR{\(t = 0\) to \(k-1\)}
        \FOR{\(i = 1\) to \(n\)}
            \STATE Sample \(h_i^{(t)} \sim p(h_i | v^{(t)})\)
            \FOR{\(j = 1\) to \(m\)}
                \STATE Sample \(v_j^{(t+1)} \sim p(v_j | h^{(t)})\)
            \ENDFOR
        \ENDFOR
        \FOR{\(i = 1\) to \(n\), \(j = 1\) to \(m\)}
            \STATE \(\Delta w_{ij} \leftarrow \Delta w_{ij} + p(H_i = 1 | v^{(0)}) \cdot v_j^{(0)} - p(H_i = 1 | v^{(k)}) \cdot v_j^{(k)}\)
        \ENDFOR
        \STATE \(\Delta b_j \leftarrow \Delta b_j + v_j^{(0)} - v_j^{(k)}\)
        \STATE \(\Delta c_i \leftarrow \Delta c_i + p(H_i = 1 | v^{(0)}) - p(H_i = 1 | v^{(k)})\)
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
Since \(\mathbf{v}^{(k)}\) is not a sample from the stationary model distribution, the approximation is biased. Obviously, the bias vanishes as \(k \rightarrow \infty\). That CD is a biased approximation becomes clear by realizing that it does not maximize the likelihood of the data under the model but the difference of two KL-divergences:
\[
\mathrm{KL}(q \mid p) - \mathrm{KL}(p_{k} \mid p)
\]
where \(q\) is the empirical distribution and \(p_{k}\) is the distribution of the visible variables after \(k\) steps of the Markov chain. If the chain has already reached stationarity, it holds \(p_{k} = p\) and thus \(\mathrm{KL}(p_{k} \mid p) = 0\) and the approximation error of \(\mathrm{CD}\) vanishes.
The theoretical results give a good understanding of the CD approximation and the corresponding bias by showing that the log-likelihood gradient can, based on a Markov chain, be expressed as a sum of terms containing the \(k\)-th sample:
#+NAME: (Bengio and Delalleau)
#+begin_theorem latex
For a converging Gibbs chain
\[
\mathbf{v}^{(0)} \Rightarrow \mathbf{h}^{(0)} \Rightarrow \mathbf{v}^{(1)} \Rightarrow \mathbf{h}^{(1)} \ldots
\]
starting at data point \(\mathbf{v}^{(0)}\), the log-likelihood gradient can be written as
\[
\frac{\partial}{\partial \boldsymbol{\theta}} \ln p(\mathbf{v}^{(0)}) = -\sum_{\mathbf{h}} p(\mathbf{h} \mid \mathbf{v}^{(0)}) \frac{\partial E(\mathbf{v}^{(0)}, \mathbf{h})}{\partial \boldsymbol{\theta}} + E_{p(\mathbf{v}^{(k)} \mid \mathbf{v}^{(0)})} \bigg[\sum_{\mathbf{h}} p(\mathbf{h} \mid \mathbf{v}^{(k)}) \frac{\partial E(\mathbf{v}^{(k)}, \mathbf{h})}{\partial \boldsymbol{\theta}}\bigg] + E_{p(\mathbf{v}^{(k)} \mid \mathbf{v}^{(0)})} \bigg[\frac{\partial \ln p(\mathbf{v}^{(k)})}{\partial \boldsymbol{\theta}}\bigg]
\]
and the final term converges to zero as \(k\) goes to infinity.
#+end_theorem
The first two terms in the equation just correspond to the expectation of the \(\mathrm{CD}\) approximation (under \(p_{k}\)) and the bias is given by the final term. The approximation error does not only depend on the value of \(k\) but also on the rate of convergence or the mixing rate of the Gibbs chain. The rate describes how fast the Markov chain approaches the stationary distribution. The mixing rate of the Gibbs chain of an RBM is up to the magnitude of the model parameters. This becomes clear by considering that the conditional probabilities \(p(v_{j} \mid \mathbf{h})\) and \(p(h_{i} \mid \mathbf{v})\) are given by thresholding \(\sum_{i=1}^{n} w_{i j} h_{i} + b_{j}\) and \(\sum_{j=1}^{m} w_{i j} v_{j} + c_{i}\), respectively. If the absolute values of the parameters are high, the conditional probabilities can get close to one or zero. If this happens, the states get more and more "predictable" and the Markov chain changes its state slowly. 
An empirical analysis of the dependency between the size of the bias and magnitude of the parameters can be found. An upper bound on the expectation of the CD approximation error under the empirical distribution is given by the following theorem:
#+NAME: Fischer and Igel
#+begin_theorem latex
Let \(p\) denote the marginal distribution of the visible units of an RBM and let \(q\) be the empirical distribution defined by a set of samples \(\mathbf{v}_{1}, \ldots, \mathbf{v}_{\ell}\). Then an upper bound on the expectation of the error of the CD-\(k\) approximation of the log-likelihood derivative with respect to some RBM parameter \(\theta_{a}\) is given by:
\[
\left|E_{q(\mathbf{v}^{(0)})}\bigg[E_{p(\mathbf{v}^{(k)} \mid \mathbf{v}^{(0)})}\bigg[\frac{\partial \ln p(\mathbf{v}^{(k)})}{\partial \theta_{a}}\bigg]\bigg]\right| \leq \frac{1}{2}\|q - p\|\bigg(1 - \exp (-(m+n) \Delta)\bigg)^{k}
\]
with
\[
\Delta = \max \bigg\{\max_{l \in \{1, \ldots, m\}} \vartheta_{l}, \max_{l \in \{1, \ldots, n\}} \xi_{l}\bigg\}
\]
where
\[
\vartheta_{l} = \max \bigg\{\bigg|\sum_{i=1}^{n} I_{\{w_{i l}>0\}} w_{i l} + b_{l}\bigg|, \bigg|\sum_{i=1}^{n} I_{\{w_{i l}<0\}} w_{i l} + b_{l}\bigg|\bigg\}
\]
and
\[
\xi_{l} = \max \bigg\{\bigg|\sum_{j=1}^{m} I_{\{w_{l j}>0\}} w_{l j} + c_{l}\bigg|, \bigg|\sum_{j=1}^{m} I_{\{w_{l j}<0\}} w_{l j} + c_{l}\bigg|\bigg\}.
\]
#+end_theorem
The bound (and probably also the bias) depends on the absolute values of the RBM parameters, on the size of the RBM (the number of variables in the graph), and on the distance in variation between the modeled distribution and the starting distribution of the Gibbs chain.
As a consequence of the approximation error, CD-learning does not necessarily lead to a maximum likelihood estimate of the model parameters. Yuille specifies conditions under which CD learning is guaranteed to converge to the maximum likelihood solution, which need not hold for RBM training in general. Examples of energy functions and Markov chains for which CD-1 learning does not converge are given. The empirical comparisons of the CD approximation and the true gradient for RBMs small enough that the gradient is still tractable show that the bias can lead to a convergence to parameters that do not reach the maximum likelihood.
The bias, however, can also lead to a distortion of the learning process: After some learning iterations, the likelihood can start to diverge in the sense that the model systematically gets worse if \(k\) is not large. This is especially bad because the log-likelihood is not tractable in reasonably sized RBMs, and so the misbehavior cannot be displayed and used as a stopping criterion. Because the effect depends on the magnitude of the weights, weight decay can help to prevent it. However, the weight decay parameter \(\lambda\) is difficult to tune. If it is too small, weight decay has no effect. If it is too large, the learning converges to models with low likelihood.
More recently proposed learning algorithms try to yield better approximations of the log-likelihood gradient by sampling from Markov chains with increased mixing rate.
******* Persistent contrastive divergence (PCD)
The idea of persistent contrastive divergence (PCD) is described for log-likelihood maximization of general MRFs and is applied to RBMs. The PCD approximation is obtained from the CD approximation by replacing the sample \(v^{(k)}\) by a sample from a Gibbs chain that is independent of the sample \(v^{(0)}\) of the training distribution. The algorithm corresponds to standard CD learning without reinitializing the visible units of the Markov chain with a training sample each time we want to draw a sample \(v^{(k)}\) approximately from the RBM distribution. Instead, one keeps "persistent" chains which are run for \(k\) Gibbs steps after each parameter update (i.e., the initial state of the current Gibbs chain is equal to \(v^{(k)}\) from the previous update step). The fundamental idea underlying PCD is that one could assume that the chains stay close to the stationary distribution if the learning rate is sufficiently small and thus the model changes only slightly between parameter updates. The number of persistent chains used for sampling (or the number of samples used to approximate the second term of gradient) is a hyperparameter of the algorithm. In the canonical form, there exists one Markov chain per training example in a batch.
The PCD algorithm was further refined in a variant called fast persistent contrastive divergence (FPCD). Fast PCD tries to reach faster mixing of the Gibbs chain by introducing additional parameters \(w_{i j}^{f}, b_{j}^{f}, c_{i}^{f}\) (for \(i=1, \ldots, n\) and \(j=1, \ldots, m\)) referred to as fast parameters. These new set of parameters is only used for sampling and not in the model itself. When calculating the conditional distributions for Gibbs sampling, the regular parameters are replaced by the sum of the regular and the fast parameters, i.e., Gibbs sampling is based on the probabilities \(\tilde{p}(H_{i}=1 \mid \mathbf{v}) = \sigma \bigg(\sum_{j=1}^{m} (w_{i j} + w_{i j}^{f}) v_{j} + (c_{i} + c_{i}^{f})\bigg)\) and \(\tilde{p}(V_{j}=1 \mid \mathbf{h}) = \sigma \bigg(\sum_{i=1}^{n} (w_{i j} + w_{i j}^{f}) h_{i} + (b_{j} + b_{j}^{f})\bigg)\) instead of the conditional probabilities given by the original equations. The learning update rule for the fast parameters equals the one for the regular parameters, but with an independent, large learning rate leading to faster changes as well as a large weight decay parameter. Weight decay can also be used for the regular parameters, but it was suggested that regularizing just the fast weights is sufficient.
Neither PCD nor FPCD seem to enlarge the mixing rate (or decrease the bias of the approximation) sufficiently to avoid the divergence problem as can be seen in the empirical analysis.
******* Parallel tempering (PT)
One of the most promising sampling techniques used for RBM-training so far is parallel tempering (PT). It introduces supplementary Gibbs chains that sample from more and more smoothed replicas of the original distribution. This can be formalized in the following way: Given an ordered set of \(M\) temperatures \(T_{1}, T_{2}, \ldots, T_{M}\) with \(1 = T_{1} < T_{2} < \cdots < T_{M}\), we define a set of \(M\) Markov chains with stationary distributions
\[
p_{r}(\mathbf{v}, ~ \mathbf{h}) = \frac{1}{Z_{r}} \exp \bigg(-\frac{1}{T_{r}} E(\mathbf{v}, ~ \mathbf{h})\bigg)
\]
for \(r = 1, \ldots, M\), where \(Z_{r} = \sum_{\mathbf{v}, ~ \mathbf{h}} \exp \bigg(-\frac{1}{T_{r}} E(\mathbf{v}, ~ \mathbf{h})\bigg)\) is the corresponding partition function, and \(p_{1}\) is exactly the model distribution.
In each step of the algorithm, we run \(k\) (usually \(k=1\)) Gibbs sampling steps in each tempered Markov chain yielding samples \((\mathbf{v}_{1}, ~ \mathbf{h}_{1}), \ldots, (\mathbf{v}_{M}, ~ \mathbf{h}_{M})\). After this, two neighboring Gibbs chains with temperatures \(T_{r}\) and \(T_{r-1}\) may exchange particles \((\mathbf{v}_{r}, \mathbf{h}_{r})\) and \((\mathbf{v}_{r-1}, ~ \mathbf{h}_{r-1})\) with an exchange probability based on the Metropolis ratio,
\[
\min \left\{1, \frac{p_{r}(\mathbf{v}_{r-1}, \mathbf{h}_{r-1}) p_{r-1}(\mathbf{v}_{r}, \mathbf{h}_{r})}{p_{r}(\mathbf{v}_{r}, \mathbf{h}_{r}) p_{r-1}(\mathbf{v}_{r-1}, \mathbf{h}_{r-1})}\right\}
\]
which gives for RBMs
\[
\min \left\{1, \exp \bigg(\bigg(\frac{1}{T_{r}} - \frac{1}{T_{r-1}}\bigg) * \bigg(E(\mathbf{v}_{r}, \mathbf{h}_{r}) - E(\mathbf{v}_{r-1}, \mathbf{h}_{r-1})\bigg)\bigg)\right\}
\]
After performing these swaps between chains, which enlarge the mixing rate, we take the (eventually exchanged) sample \(\mathbf{v}_{1}\) of the original chain (with temperature \(T_{1} = 1\)) as a sample from the model distribution. This procedure is repeated \(L\) times yielding samples \(\mathbf{v}_{1,1}, \ldots, \mathbf{v}_{1,L}\) used for the approximation of the expectation under the RBM distribution in the log-likelihood gradient (i.e., for the approximation of the second term). Usually \(L\) is set to the number of samples in the (mini) batch of training data as shown in algorithm 2.
Compared to CD, PT introduces computational overhead, but results in a faster mixing Markov chain and thus a less biased gradient approximation. The evolution of the log-likelihood during training using PT with different values of \(M\) can be seen in figure 2.
\begin{algorithm}
\caption{k-step parallel tempering with \(M\) temperatures}
\label{alg:parallel_tempering}
\begin{algorithmic}[1]
\REQUIRE RBM \((V_1, \dots, V_m, H_1, \dots, H_n)\), training batch \(S\), current state \(v_r\) of Markov chain with stationary distribution \(p_r\) for \(r = 1, \dots, M\)
\ENSURE Gradient approximation \(\Delta w_{ij}\), \(\Delta b_j\), and \(\Delta c_i\) for \(i = 1, \dots, n\), \(j = 1, \dots, m\)
\STATE Initialize \(\Delta w_{ij} = \Delta b_j = \Delta c_i = 0\) for \(i = 1, \dots, n\), \(j = 1, \dots, m\)
\FORALL{\(v \in S\)}
    \FOR{\(r=1\) to \(M\)}
        \STATE \(v_r^{(0)} \leftarrow v_r\)
        \FOR{\(i = 1\) to \(n\)}
            \STATE Sample \(h_{r,i}^{(0)} \sim p(h_{r,i} | v_r^{(0)})\)
        \ENDFOR
        \FOR{\(t = 0\) to \(k-1\)}
            \FOR{\(j = 1\) to \(m\)}
                \STATE Sample \(v_{r,j}^{(t+1)} \sim p(v_{r,j} | h_r^{(t)})\)
            \ENDFOR
            \FOR{\(i = 1\) to \(n\)}
                \STATE Sample \(h_{r,i}^{(t+1)} \sim p(h_{r,i} | v_r^{(t+1)})\)
            \ENDFOR
            \STATE \(v_r^{(k)} \leftarrow v_r^{(t+1)}\)
        \ENDFOR
    \ENDFOR
    \STATE /* swapping order below works well in practice */
    \FOR{\(r=2\) to \(M\) \textbf{step} 2}
        \STATE Swap \((v_r^{(k)}, h_r^{(k)})\) and \((v_{r-1}^{(k)}, h_{r-1}^{(k)})\) with probability given by [26]
    \ENDFOR
    \FOR{\(r=3\) to \(M\) \textbf{step} 2}
        \STATE Swap \((v_r^{(k)}, h_r^{(k)})\) and \((v_{r-1}^{(k)}, h_{r-1}^{(k)})\) with probability given by [40]
    \ENDFOR
    \FOR{\(i = 1\) to \(n\), \(j = 1\) to \(m\)}
        \STATE \(\Delta w_{ij} \leftarrow \Delta w_{ij} + p(H_i = 1 | v_1^{(0)}) \cdot v_{1,j}^{(0)} - p(H_i = 1 | v_1^{(k)}) \cdot v_{1,j}^{(k)}\)
        \STATE \(\Delta b_j \leftarrow \Delta b_j + v_{1,j}^{(0)} - v_{1,j}^{(k)}\)
        \STATE \(\Delta c_i \leftarrow \Delta c_i + p(H_i = 1 | v_1^{(0)}) - p(H_i = 1 | v_1^{(k)})\)
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
*** Extensions
**** Real-valued variables
**** Conditional RBMs
** Expectation maximization (EM)
:LOGBOOK:
CLOCK: [2024-06-19 Wed 00:58]--[2024-06-19 Wed 12:00] => 11:02
:END:
/The expectation maximization algorithm, or EM algorithm, is a general technique for finding maximum likelihood solutions for probabilistic models having latent variables./
*** Definitions
Consider a density estimation task for a distribution \( p(\mathbf{X} \mid \boldsymbol{\theta}) \), where  \(\mathbf{X}\) is an arrangement of all observed data and \( \boldsymbol{\theta} \) is the parameter vector. Now suppose we introduce latent variables \(\mathbf{Z}\) and define a density estimation task for the distribution \( p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta}) \). If an estimator of \( \boldsymbol{\theta} \) for \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta}) \) can be obtained, the original task is solved as a consequence: \( p(\mathbf{X} \mid \boldsymbol{\theta}) \) is obtained from \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\) via marginalization \( p(\mathbf{X} \mid \boldsymbol{\theta}) = \sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta}) \). The distribution \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\) is called a latent variable model. The use of latent variable models is justified when the following postulate is defensible.
#+NAME: Latent variable models
#+begin_postulate latex
Estimation of \(p(\mathbf{X} \mid \boldsymbol{\theta})\) is difficult, but estimation of \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\) is easy.
#+end_postulate
For the rest of this section, we will use the above problem setup and assume without justification that the postulate above is valid.
**** Log-likelihood
#+NAME: Log-likelihood
#+begin_definition latex
Consider the joint distribution \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\) where \(\mathbf{X}\), \( \mathbf{Z} \), and \( \boldsymbol{\theta} \) denote the set of observed data, the latent variables and the model parameters respectively. If \( \mathbf{Z} \) is discrete, the log-likelihood function for an estimation of the marginal distribution \(p(\mathbf{X} \mid \boldsymbol{\theta}) \) is
\[
\ln p(\mathbf{X} \mid \boldsymbol{\theta})= \ln \bigg(\sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta}) \bigg).
\]
If \( \mathbf{Z} \) is continuous \( \sum_{\mathbf{Z}} \to \int \mathrm{d} \mathbf{Z} \). If the latent variables decompose into \( \{\mathbf{Z}_{c}, \, \mathbf{Z}_{d}\} \) so that \( \mathbf{Z}_{c} \) is continuous and \( \mathbf{Z}_{d} \) is discrete \( \sum_{\mathbf{Z}} \to \int \mathrm{d} \mathbf{Z}_{c} \, \sum_{\mathbf{Z}_{d}}\).
#+end_definition
**** Evidence lower bound (ELBO)
#+NAME: Evidence lower bound (ELBO)
#+begin_definition latex
Consider the joint distribution \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\) where \(\mathbf{X}\), \( \mathbf{Z} \), and \( \boldsymbol{\theta} \) denote the set of observed data, the latent variables and the model parameters respectively. Let \(q\) be a distribution defined over the latent variables \( Z \) such that \( \mathbf{Z} \) has density \( q(\mathbf{Z}) \). The evidence lower bound objective (ELBO) is defined as
\[
\mathcal{L}[q](\boldsymbol{\theta}) =\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \bigg(\frac{p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}{q(\mathbf{Z})}\bigg).
\]
ELBO is a functional of the distribution \(q\) and a function of the parameters \( \boldsymbol{\theta} \). Note that \( \mathbf{X} \) is fixed.
#+end_definition
**** Kullback-Liebler (KL) divergence
#+NAME: KL divergence
#+begin_definition latex
Consider the joint distribution \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\) where \(\mathbf{X}\), \( \mathbf{Z} \), and \( \boldsymbol{\theta} \) denote the set of observed data, the latent variables and the model parameters respectively. Let \(q\) be a distribution defined over the latent variables \( Z \) such that \( \mathbf{Z} \) has density \( q(\mathbf{Z}) \). The Kullback-Liebler (KL) divergence of \( q(\mathbf{Z}) \) from \( p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) \) is defined as
\[
\mathrm{KL}[q \| p] & =-\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \bigg(\frac{p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})}{q(\mathbf{Z})}\bigg).
\]
Like ELBO, the KL divergence is a functional of the distribution \(q\) and a function of the parameters \( \boldsymbol{\theta} \). Note that \( \mathbf{X} \) is fixed.
#+end_definition
**** EM decomposition
#+NAME: EM decomposition
#+begin_theorem latex
Consider the joint distribution \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\) where \(\mathbf{X}\), \( \mathbf{Z} \), and \( \boldsymbol{\theta} \) denote the set of observed data, the latent variables and the model parameters respectively. Let \(q\) be a distribution defined over the latent variables \( Z \) such that \( \mathbf{Z} \) has density \( q(\mathbf{Z}) \). The log-likelihood function for an estimation of the marginal distribution \(p(\mathbf{X} \mid \boldsymbol{\theta}) \) admits the following decomposition in terms of the ELBO and KL divergence which we will call the EM decomposition
\[
\ln p(\mathbf{X} \mid \boldsymbol{\theta}) = \mathcal{L}[q](\boldsymbol{\theta}) + \mathrm{KL}[q \| p],
\]
where
\[
\mathcal{L}[q](\boldsymbol{\theta}) =\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \bigg(\frac{p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}{q(\mathbf{Z})}\bigg),
\]
and
\[
\mathrm{KL}[q \| p] & =-\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \bigg(\frac{p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})}{q(\mathbf{Z})}\bigg).
\]
#+end_theorem
#+NAME: EM decomposition
#+begin_proof latex
On substituting \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})= p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) p(\mathbf{X} \mid \boldsymbol{\theta})\) into the expression for \(\mathcal{L}[q](\boldsymbol{\theta})\)
\begin{align*}
\mathcal{L}[q](\boldsymbol{\theta}) &= \sum_{\mathbf{Z}} q(\mathbf{Z}) \ln p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) - \sum_{\mathbf{Z}} q(\mathbf{Z}) \ln q(\mathbf{Z}) + \ln p(\mathbf{X} \mid \boldsymbol{\theta}) \sum_{\mathbf{Z}} q(\mathbf{Z}) \\
&= \sum_{\mathbf{Z}} \bigg[ q(\mathbf{Z}) \ln p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) - q(\mathbf{Z}) \ln q(\mathbf{Z}) \bigg] + \ln p(\mathbf{X} \mid \boldsymbol{\theta}) \\
&= - \mathrm{KL}[q \| p] + \ln p(\mathbf{X} \mid \boldsymbol{\theta}).
\end{align*}
We have used \(\sum_{\mathbf{Z}} q(\mathbf{Z}) = 1\) and identified 
\[
\mathrm{KL}[q \| p] & =-\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \bigg(\frac{p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})}{q(\mathbf{Z})}\bigg) = - \sum_{\mathbf{Z}} \bigg[ q(\mathbf{Z}) \ln p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) - q(\mathbf{Z}) \ln q(\mathbf{Z}) \bigg].
\]
The rewrite
\[
\ln p(\mathbf{X} \mid \boldsymbol{\theta}) = \mathcal{L}[q](\boldsymbol{\theta}) + \mathrm{KL}[q \| p]
\]
completes the proof.
#+end_proof
#+NAME: ELBO bounds log-likelihood function
#+begin_corollary latex
\(\mathcal{L}[q](\boldsymbol{\theta})\) is a lower bound on the log likelihood function \(\ln p(\mathbf{X} \mid \boldsymbol{\theta})\).
#+end_corollary
#+NAME: ELBO bounds log-likelihood function
#+begin_proof latex
\(\mathcal{L}[q](\boldsymbol{\theta}) \leqslant \ln p(\mathbf{X} \mid \boldsymbol{\theta})\) follows immediately from the form of the EM decomposition \( \ln p(\mathbf{X} \mid \boldsymbol{\theta})=\mathcal{L}[q](\boldsymbol{\theta}) + \mathrm{KL}(q \| p) \) and the non-negativity of the KL-divergence \(\mathrm{KL}[q \| p] \geqslant 0\). The inequality is saturated when \(\mathrm{KL}[q \| p] = 0\) and the bound is tight, i.e., \(\ln p(\mathbf{X} \mid \boldsymbol{\theta}) = \mathcal{L}[q](\boldsymbol{\theta})\).
#+end_proof
*** Estimation
**** Expectation maximization
#+HTML_ATTR: :width 100px
[[file:~/.local/images/prml-9-11.png]]
#+CAPTION: Illustration of the EM decomposition, which holds for any choice of distribution q(Z). Because the Kullback-Leibler divergence satisfies KL[q  p]  0, we see that the quantity  [q]() is a lower bound on the log likelihood function ln p(X | ).
The EM decomposition and the ELBO bound on the log-likelihood function suggest a 2-step iterative scheme for the estimation of \( p(\mathbf{X} \mid \boldsymbol{\theta}) \) by maximizing the log-likelihood \( \ln p(\mathbf{X} \mid \boldsymbol{\theta}) \):
  + E-step :: During this step, \(\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{old}} \) is held fixed and the density \( q \) is updated \( q \leftarrow q^{\text{new}} \) in a way that the KL divergence \( \mathrm{KL} [q \| p] \) vanishes. This tightens the ELBO around the log-likelihood by saturating the inequality \(\mathcal{L}(q, \boldsymbol{\theta}) \leqslant \ln p(\mathbf{X} \mid \boldsymbol{\theta})\) so that the EM decomposition reduces to \( \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{old}}) = \mathcal{L}[q^{\text{new}}](\boldsymbol{\theta}^{\text{old}}) \). 
  + M-step :: During this step, \( q = q^{\text{new}} \) is held fixed and the model parameters \( \boldsymbol{\theta} \) are updated \( \boldsymbol{\theta} \leftarrow \boldsymbol{\theta}^{\text{new}} \) in a way that the new ELBO \( \mathcal{L}[q^{\text{new}}](\boldsymbol{\theta}^{\text{new}})\) is greater than the old ELBO \( \mathcal{L}[q^{\text{new}}](\boldsymbol{\theta}^{\text{old}}) \). This leads to a new log-likelihood \( \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{new}}) \) that is greater that the old log-likelihood \( \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{old}}) \).
Repeated application of these two steps iteratively maximizes the log-likelihood \( \ln p(\mathbf{X} \mid \boldsymbol{\theta}) \). Note that there in an increase in the log-likelihood only during the M-step.
***** E step
The E-step follows the M-step (see next section) of the /previous iteration/. During the E-step, \(\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{new}}\) which solves the optimization problem \( \arg \max_{\boldsymbol{\theta}} \mathcal{L}[q=q^{\text{old}}](\boldsymbol{\theta}) \) of the M-step of the previous iteration is referred to as \( \boldsymbol{\theta}^{\text{old}} \), i.e., \( \boldsymbol{\theta}^{\text{old}} = \boldsymbol{\theta}^{\text{new}}\).
#+NAME: E-step
#+begin_definition latex
The E-step refers to the following optimization problem involving the ELBO: given \( \boldsymbol{\theta}^{\text{old}} \)
\[
q^{\text{new}} = \arg \max_{q} \mathcal{L}[q](\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{old}}).
\]
#+end_definition
#+NAME: E-step optimization
#+begin_theorem latex
\(q (\mathbf{Z}) = q^{\text{new}}(\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}})\) solves the optimization problem \( \arg \max_{q} \mathcal{L}[q](\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{old}}) \) of the E-step.
#+end_theorem
#+NAME: E-step optimization
#+begin_proof latex
Rearranging the EM decomposition, we have \(\mathcal{L}[q](\boldsymbol{\theta}^{\text{old}}) = \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{old}})-\mathrm{KL}[q \| p]\). Since \( \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{old}}) \) is independent of \( q(\mathbf{Z}) \) and \(\mathrm{KL}[q \| p] \geq 0\), \( \mathcal{L}[q](\boldsymbol{\theta}^{\text{old}})\) is maximal when \(\mathrm{KL}[q \| p] = 0\). Since
\[
\mathrm{KL}[q \| p] & = - \sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \bigg(\frac{p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text{old}})}{q(\mathbf{Z})}\bigg)
\]
it is clear that this happens when \(q(\mathbf{Z}) = q^{\text{new}}(\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X}, \, \boldsymbol{\theta}^{\text{old}})\).
#+end_proof
#+NAME: Maximum of ELBO for E-step
#+begin_corollary latex
The maximum ELBO for the optimization problem of the E-step is \(\mathcal{L} = \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{old}})\).
#+end_corollary
#+NAME: Maximum of ELBO for E-step
#+begin_proof latex
The solution \(q^{\text{new}}(\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}})\) for the optimization problem of the E-step results in a vanishing KL divergence. \(\mathcal{L} = \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{old}})\) follows immediately from the form of the EM decomposition \(\mathcal{L}[q](\boldsymbol{\theta}^{\text{old}}) = \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{old}})-\mathrm{KL}[q \| p]\).
#+end_proof
#+HTML_ATTR: :width 500px
[[file:~/.local/images/prml-9-12.png]]
#+CAPTION: Illustration of the E step of the EM algorithm. The q distribution is set equal to the posterior distribution for the current parameter values , causing the lower bound to move up to the same value as the log likelihood function, with the KL divergence vanishing.
#+CAPTION: Illustration of the E step of the EM algorithm. The q distribution is set equal to the posterior distribution for the current parameter values , causing the lower bound to move up to the same value as the log likelihood function, with the KL divergence vanishing.
\begin{algorithm}
\caption{E-Step of the EM Algorithm}
\begin{algorithmic}[1]
    \State \textbf{Input:} Parameter vector \(\boldsymbol{\theta}^{\text{new}}\) from previous M-step, data \(\mathbf{X}\)
    \State \(\boldsymbol{\theta}^{\text{old}} \leftarrow \boldsymbol{\theta}^{\text{new}}\)
    \State \(q^{\text{new}}(\mathbf{Z}) \leftarrow p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text{old}})\)
    \State \textbf{Output:} Updated distribution \(q^{\text{new}} = \arg \max_{q} \mathcal{L}[q](\boldsymbol{\theta}^{\text{old}})\)
\end{algorithmic}
\end{algorithm}
***** M step
The M-step follows the E-step of the /same iteration/. During the M-step, \(q (\mathbf{Z}) = q^{\text{new}}(\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}})\) which solves the optimization problem \( \arg \max_{q} \mathcal{L}[q](\boldsymbol{\theta}^{\text{old}}) \) of the E-step is referred to as \( q^{\text{old}} \), i.e., \( q^{\text{old}}(\mathbf{Z}) = q^{\text{new}}(\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}}) \).
#+NAME: M-step
#+begin_definition latex
The M-step refers to the following optimization problem involving the ELBO: given \(q^{\text{old}}\)
\[
\boldsymbol{\theta}^{\text{new}} = \arg \max_{\boldsymbol{\theta}} \mathcal{L}[q=q^{\text{old}}](\boldsymbol{\theta}).
\]
#+end_definition
#+NAME: M-step optimization
#+begin_theorem latex
\[
\boldsymbol{\theta}^{\text{new}} = \arg \max_{\boldsymbol{\theta}} \mathcal{Q}(\boldsymbol{\theta}, \, \boldsymbol{\theta}^{\text{old}}) \Longrightarrow \arg \max_{\boldsymbol{\theta}} \mathcal{L}[q=q^{\text{old}}](\boldsymbol{\theta}) = \boldsymbol{\theta}^{\text{new}}
\]
where
\[
\mathcal{Q}(\boldsymbol{\theta},\,\boldsymbol{\theta}^{\text{old}}) \equiv \sum_{\mathbf{Z}} p(\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}}) \ln p(\mathbf{X},\, \mathbf{Z} \mid \boldsymbol{\theta} ).
\]
In other words, solving the optimization problem of the M-step is equivalent to solving the optimization problem \( \arg \max_{\boldsymbol{\theta}} \mathcal{Q}(\boldsymbol{\theta}, \, \boldsymbol{\theta}^{\text{old}}) \).
#+end_theorem
#+NAME: M-step optimization
#+begin_proof latex
On substituting \(q(\mathbf{Z}) = q^{\text{old}} (\mathbf{Z}) = p (\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}})\) into
\[
\mathcal{L}[q = q^{\text{old}}](\boldsymbol{\theta}) =\sum_{\mathbf{Z}} q^{\text{old}}(\mathbf{Z}) \ln \bigg(\frac{p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}{q^{\text{old}}(\mathbf{Z})}\bigg).
\]
we obtain
\[
\mathcal{L}[q=q^{\text{old}}](\boldsymbol{\theta}) = \overbrace{\sum_{\mathbf{Z}} p (\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \ln p(\mathbf{X},\, \mathbf{Z} \mid \boldsymbol{\theta})}^{\equiv \mathcal{Q}(\boldsymbol{\theta},\, \boldsymbol{\theta}^{\text{old}})} - \underbrace{\sum_{\mathbf{Z}} p (\mathbf{Z} \mid \mathbf{X}, \, \boldsymbol{\theta}^{\text{old}} ) \ln p (\mathbf{Z} \mid \mathbf{X}, \, \boldsymbol{\theta}^{\text{old}})}_{\text{no dependence on } \( \boldsymbol{\theta} \)}.
\]
The second term is simply the negative entropy of the \(q^{\text{old}}\) and is therefore independent of \(\boldsymbol{\theta}\). It follows that
\[
\boldsymbol{\theta}^{\text{new}} = \arg \max_{\boldsymbol{\theta}} \mathcal{Q}(\boldsymbol{\theta}, \, \boldsymbol{\theta}^{\text{old}}) \Longrightarrow \arg \max_{\boldsymbol{\theta}} \mathcal{L}[q=q^{\text{old}}](\boldsymbol{\theta}) = \boldsymbol{\theta}^{\text{new}}.
\]
#+end_proof
#+NAME: Complete-data and incomplete-data log-likelihood functions
#+begin_remark latex
The quantity that is being maximized during the M-step is the expectation of the \textit{complete-data log-likelihood function} \( \ln p(\mathbf{X},\, \mathbf{Z} \mid \boldsymbol{\theta} ) \). The variable \(\boldsymbol{\theta}\) over which we are optimizing appears only inside the logarithm. If the joint distribution \(p(\mathbf{Z}, \mathbf{X} \mid \boldsymbol{\theta})\) comprises a member of the exponential family, or a product of such members, then use of the identity \( \ln \exp (x) = x \) yields an M-step that is typically much simpler than the potentially expensive maximization of the corresponding \text{incomplete-data log-likelihood function} \(p(\mathbf{X} \mid \boldsymbol{\theta})\). Thus the appeal of latent variable models is apparent in the face of an estimation task for the density \(p(\mathbf{X} \mid \boldsymbol{\theta})\) that is intractable; it is contingent on the introduction of latent variables \( \mathbf{Z} \) and a joint density \(p(\mathbf{Z}, \mathbf{X} \mid \boldsymbol{\theta})\) whose estimation is tractable.
#+end_remark
#+NAME: ELBO inequalities for M-step
#+begin_corollary latex
Let \(\boldsymbol{\theta}^{\text{old}}\) denote the solutions of the optimization problem of the M step of the previous iteration. Let \( q^{\text{new}} \) and \(\boldsymbol{\theta}^{\text{new}}\) denote the solutions of the optimization problem of the E-step and M-step respectively during the current iteration. Assume that the ELBO is a concave function of \( \boldsymbol{\theta} \). In the current iteration, the ELBO after the E-step and M-step is given by \( \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{old}}) \) and \( \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{new}}) \) respectively. The KL divergence vanishes after the E-step; let \( \mathrm{KL}[q\|p] \) denote the its value after the M-step. Denote the difference in ELBO with \( \Delta \mathcal{L}[q](\boldsymbol{\theta}) \equiv \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{new}}) - \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{old}}) \). We have
\[
\Delta \mathcal{L}[q](\boldsymbol{\theta}) \begin{cases} 
> 0 & \Longrightarrow \boldsymbol{\theta}^{\text{old}} \neq \boldsymbol{\theta}^{\text{new}}, \quad \mathrm{KL}[q\|p] \neq 0, \\
= 0 & \Longrightarrow \boldsymbol{\theta}^{\text{old}} = \boldsymbol{\theta}^{\text{new}}, \quad \mathrm{KL}[q\|p] = 0.
\end{cases}
\]
#+end_corollary
#+NAME: ELBO inequalities for M-step
#+begin_proof latex
Consider the optimization problem of the M-step of the current iteration \(\arg \max_{\boldsymbol{\theta}} \mathcal{L}[q=q^{\text{new}}](\boldsymbol{\theta})\). Since \( q \) is held fixed during the M-step, \(\boldsymbol{\theta}^{\text{old}} = \boldsymbol{\theta}^{\text{new}} \Longrightarrow \Delta \mathcal{L}[q](\boldsymbol{\theta}) = 0\). Since the ELBO is a concave function of \( \boldsymbol{\theta} \), it has a single root, i.e., \(\Delta \mathcal{L}[q](\boldsymbol{\theta}) = 0 \Longrightarrow \boldsymbol{\theta}^{\text{old}} = \boldsymbol{\theta}^{\text{new}} \). Thus \( \Delta \mathcal{L}[q](\boldsymbol{\theta}) \neq 0 \) if and only if \( \boldsymbol{\theta}^{\text{old}} \neq \boldsymbol{\theta}^{\text{new}}\). The E-step of the current iteration uses \( \boldsymbol{\theta}^{\text{old}} \) from the previous iteration to yield \(q(\mathbf{Z}) = q^{\text{new}}(\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}})\). Recall that the KL-divergence, by definition, is given by
\[
\mathrm{KL}[q \| p] =-\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \bigg(\frac{p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})}{q(\mathbf{Z})}\bigg).
\]
\( \boldsymbol{\theta} \) is held fixed during the E-step, so that at the end of the E-step we have \( p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) = p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \) and \(q (\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}})\): the KL divergence vanishes. Now suppose \( \Delta \mathcal{L}[q](\boldsymbol{\theta}) > 0 \) which indicates an increase in ELBO during the M-step of the current iteration. The M-step of the current iteration uses \( q^{\text{new}} \) from the E-step of the current iteration to yield \( \boldsymbol{\theta}^{\text{new}} \), so that at the end of the M-step we have \( p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) = p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text{new}}) \) and \(q (\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}})\); in other words \( \mathrm{KL}[q \| p] \neq 0 \). Now suppose \( \Delta \mathcal{L}[q](\boldsymbol{\theta}) = 0 \). Previously, we saw that the lack of change in the ELBO during the M-step must mean that the parameters were not updated in any way, i.e., \( \boldsymbol{\theta}^{\text{new}} = \boldsymbol{\theta}^{\text{old}} \). We then have \( p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) = p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text{new}}) = p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \) and \(q (\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}})\); so that \( \mathrm{KL}[q \| p] = 0 \). This completes the proof.
#+end_proof
#+NAME: ELBO bounds the log-likelihood
#+begin_corollary latex
Let \(\boldsymbol{\theta}^{\text{old}}\) denote the solutions of the optimization problem of the M step of the previous iteration. Let \( q^{\text{new}} \) and \(\boldsymbol{\theta}^{\text{new}}\) denote the solutions of the optimization problem of the E-step and M-step respectively during the current iteration. Assume that the ELBO is a concave function of \( \boldsymbol{\theta} \). In the current iteration, the ELBO after the E-step and M-step is given by \( \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{old}}) \) and \( \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{new}}) \) respectively. Denote the difference in ELBO with \( \Delta \mathcal{L}[q](\boldsymbol{\theta}) \equiv \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{new}}) - \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{old}}) \). We have
\[
\Delta \mathcal{L}[q](\boldsymbol{\theta}) \begin{cases} 
> 0 & \Longrightarrow \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{new}}) > \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{new}}), \\
= 0 & \Longrightarrow \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{new}}) = \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{new}}) = \mathcal{L} [q = q^{\text{new}}](\boldsymbol{\theta}^{\text{old}}).
\end{cases}
\]
#+end_corollary
#+NAME: ELBO bounds the log-likelihood
#+begin_proof latex
The result follows immediately on considering the variations of either side of the EM decomposition with respect to \( \boldsymbol{\theta} \)
\[
\Delta \ln p(\mathbf{X} \mid \boldsymbol{\theta}) = \Delta \mathcal{L}[q](\boldsymbol{\theta}) + \Delta \mathrm{KL}[q \| p],
\]
and using the previous result
\[
\Delta \mathcal{L}[q](\boldsymbol{\theta}) \begin{cases} 
> 0 & \Longrightarrow \boldsymbol{\theta}^{\text{old}} \neq \boldsymbol{\theta}^{\text{new}}, \quad \mathrm{KL}[q\|p] \neq 0, \\
= 0 & \Longrightarrow \boldsymbol{\theta}^{\text{old}} = \boldsymbol{\theta}^{\text{new}}, \quad \mathrm{KL}[q\|p] = 0.
\end{cases}
\]
#+end_proof
#+HTML_ATTR: :width 100px
[[file:~/.local/images/prml-9-13.png]]
#+CAPTION: Illustration of the M step of the EM algorithm. The distribution q(Z) is held fixed and the lower bound (q, ) is maximized with respect to the parameter vector  to give a revised value . Because the KL divergence is nonnegative, this causes the log likelihood ln p(X | ) to increase by at least as much as the lower determined.
\begin{algorithm}
\caption{M-Step of the EM Algorithm}
\begin{algorithmic}[1]
    \State \textbf{Input:} Distribution \(q^{\text{new}}(\mathbf{Z})\) from previous E-step, data \(\mathbf{X}\)
    \State \(q^{\text{old}} \leftarrow q^{\text{new}}\)
    \State \(\boldsymbol{\theta}^{\text{new}} \leftarrow \arg \max_{\boldsymbol{\theta}} \sum_{\mathbf{Z}} q^{\text{old}} (\mathbf{Z}) \ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta} ),\)
    \State \textbf{Output:} Updated parameters \(\boldsymbol{\theta}^{\text{new}} = \arg \max_{\boldsymbol{\theta}} \mathcal{L}(q^{\text{old}},\, \boldsymbol{\theta})\)
\end{algorithmic}
\end{algorithm}
**** EM algorithm
The full EM algorithm with iterations of the E and M steps is presented below:
\begin{algorithm}
\caption{Expectation-Maximization (EM) Algorithm}
\begin{algorithmic}[1]
    \State \textbf{Input:} Data \(\mathbf{X}\), initial parameters \(\boldsymbol{\theta}^{\text{init}}\)
    \State \textbf{Initialize:} 
        \State \quad \( \Delta \mathcal{L} \leftarrow \infty \)
        \State \quad \( \varepsilon \leftarrow \text{tolerance} \)
        \State \quad \(\boldsymbol{\theta}^{\text{new}} \leftarrow \boldsymbol{\theta}^{\text{init}}\)
    \While{\(\Delta \mathcal{L} > \varepsilon\)}
        \State \textbf{E-step:}
            \State \quad \(\boldsymbol{\theta}^{\text{old}} \leftarrow \boldsymbol{\theta}^{\text{new}}\)
            \State \quad \(q^{\text{new}}(\mathbf{Z}) \leftarrow p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text{old}})\)
            \State \quad \(\mathcal{L}^{\text{old}} = \mathcal{L}(q^{\text{new}}, \boldsymbol{\theta}^{\text{old}})\)  % Calculation of ELBO before update
        \State \textbf{M-step:}
            \State \quad \(q^{\text{old}} \leftarrow q^{\text{new}}\)
            \State \quad \(\boldsymbol{\theta}^{\text{new}} \leftarrow \arg \max_{\boldsymbol{\theta}} \sum_{\mathbf{Z}} q^{\text{old}}(\mathbf{Z}) \ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\)
            \State \quad \(\mathcal{L}^{\text{new}} = \mathcal{L}(q^{\text{old}}, \boldsymbol{\theta}^{\text{new}})\)  % Recalculation of ELBO after update
        \State \(\Delta \mathcal{L} \leftarrow \operatorname{abs}(\mathcal{L}^{\text{new}} - \mathcal{L}^{\text{old}})\)
    \EndWhile
    \State \textbf{Output:} Optimized parameters \(\boldsymbol{\theta}^{\text{new}}\)
\end{algorithmic}
\end{algorithm}
*** Interpretation
The operation of the EM algorithm has the following interpretation when viewed in the space of parameters:
  1) In the E-step, we start with the previous parameter value \(\boldsymbol{\theta}^{\text{old}}\) and evaluate the posterior distribution over latent variables, which gives rise to a lower bound \( \mathcal{L}[q^{\text{new}}](\boldsymbol{\theta}^{\text{old}}) \) whose value equals the log-likelihood  \( \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{old}}) \) at \(\boldsymbol{\theta}^{\text{old}}\). /Note that the bound makes a tangential contact with the log-likelihood at/ \(\boldsymbol{\theta}^{\text{old}}\), /so that both curves have the same gradient. For mixture components from the exponential family, this bound is a convex function having a unique maximum./
  2) In the M-step, the bound is maximized giving the value \(\boldsymbol{\theta}^{\text{new}}\), which gives a larger value of log-likelihood \( \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{new}}) \) than what was obtained (\( \ln p(\mathbf{X} \mid \boldsymbol{\theta}^{\text{old}}) \)) at \(\boldsymbol{\theta}^{\text{old}}\). 
  3) The subsequent E-step then constructs a bound that is tangential at \(\boldsymbol{\theta}^{\text{new}}\) and on it goes.
#+HTML_ATTR: :width 100px
[[file:~/.local/images/prml-9-14.png]]
#+CAPTION: The EM algorithm involves alternately computing a lower bound on the log likelihood for the current parameter values and then maximizing this bound to obtain the new parameter values. Here the red curve depicts the incomplete-data log likelihood function whose value we wish to maximize. The blue curve shows (q, ) obtained from the first E-step. The green curve shows  (q, ) obtained from the subsequent E-step.
*** Applications
**** ML estimation
EM, as described above is an /approximate ML estimator/. Here we describe a special cases where the /independence assumption/ holds for the dataset \( \mathbf{X} \).
#+NAME: IID data set
#+begin_theorem latex
Consider the joint distribution \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\) where \(\mathbf{X}\), \( \mathbf{Z} \), and \( \boldsymbol{\theta} \) denote the set of observed data, the latent variables and the model parameters respectively.  \(\mathbf{X}\) is comprised of \(N\) data points \(\left\{\mathbf{x}_{n}\right\}\) while \(\mathbf{Z}\) will comprise \(N\) corresponding latent variables \(\left\{\mathbf{z}_{n}\right\}\), where \(n=1, \ldots, N\). If the data set \(\left\{\mathbf{x}_{n}\right\}\) is independent and identically distributed (IID), then the posterior \( p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}) \) factorizes:
\[
p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})=\frac{p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}{\sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}=\frac{\prod_{n=1}^{N} p\left(\mathbf{x}_{n}, \mathbf{z}_{n} \mid \boldsymbol{\theta}\right)}{\sum_{\mathbf{Z}} \prod_{n=1}^{N} p\left(\mathbf{x}_{n}, \mathbf{z}_{n} \mid \boldsymbol{\theta}\right)}=\prod_{n=1}^{N} p\left(\mathbf{z}_{n} \mid \mathbf{x}_{n}, \boldsymbol{\theta}\right).
\]
#+end_theorem
#+NAME: IID data set
#+begin_proof latex
For the particular case of an independent, identically distributed data set (IID), \(\mathbf{X}\) will comprise \(N\) data points \(\left\{\mathbf{x}_{n}\right\}\) while \(\mathbf{Z}\) will comprise \(N\) corresponding latent variables \(\left\{\mathbf{z}_{n}\right\}\), where \(n=1, \ldots, N\). From the independence assumption, we have \(p(\mathbf{X}, \mathbf{Z})=\prod_{n} p\left(\mathbf{x}_{n}, \mathbf{z}_{n}\right)\) and, by marginalizing over the \(\left\{\mathbf{z}_{n}\right\}\) we have \(p(\mathbf{X})=\) \(\prod_{n} p\left(\mathbf{x}_{n}\right)\). Using the sum and product rules, we see that the posterior probability that is evaluated in the E step takes the form
\[
p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})=\frac{p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}{\sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}=\frac{\prod_{n=1}^{N} p\left(\mathbf{x}_{n}, \mathbf{z}_{n} \mid \boldsymbol{\theta}\right)}{\sum_{\mathbf{Z}} \prod_{n=1}^{N} p\left(\mathbf{x}_{n}, \mathbf{z}_{n} \mid \boldsymbol{\theta}\right)}=\prod_{n=1}^{N} p\left(\mathbf{z}_{n} \mid \mathbf{x}_{n}, \boldsymbol{\theta}\right).
\]
and so the posterior distribution also factorizes with respect to \(n\).
#+end_proof
In the case of the Gaussian mixture model this simply says that the responsibility that each of the mixture components takes for a particular data point \(\mathbf{x}_{n}\) depends only on the value of \(\mathbf{x}_{n}\) and on the parameters \(\boldsymbol{\theta}\) of the mixture components, not on the values of the other data points.
**** MAP estimation
EM as described above can be turned into an /approximate MAP estimator/ by introducing a prior \( p(\boldsymbol{\theta}) \) over the model parameters. We have the new model evidence \(p(\boldsymbol{\theta} \mid \mathbf{X})=p(\mathbf{X} \mid \boldsymbol{\theta}) \cdot p(\boldsymbol{\theta}) / p(\mathbf{X})\) which yields
\[\ln p(\boldsymbol{\theta} \mid \mathbf{X}) = \ln p(\mathbf{X} \mid \boldsymbol{\theta}) + \ln p(\boldsymbol{\theta}) - \ln p(\mathbf{X}).\]
On substituting the EM decomposition \( \ln p(\mathbf{X} \mid \boldsymbol{\theta}) = \mathcal{L}[q](\boldsymbol{\theta}) + \mathrm{KL}[q \| p] \), we get 
\[\ln p(\boldsymbol{\theta} \mid \mathbf{X}) = \mathcal{L}(q, \boldsymbol{\theta})+\mathrm{KL}(q \| p)+\ln p(\boldsymbol{\theta})-\ln p(\mathbf{X}) \propto \mathcal{L}(q, \boldsymbol{\theta})+\mathrm{KL}(q \| p)+\ln p(\boldsymbol{\theta}).
\]
Since \(\ln p(\mathbf{X})\) is a constant, we will drop it henceforth to write
\[\ln p(\boldsymbol{\theta} \mid \mathbf{X}) = \mathcal{L}(q, \boldsymbol{\theta})+\mathrm{KL}(q \| p)+\ln p(\boldsymbol{\theta})
\]
with the understanding that \( \ln p(\boldsymbol{\theta} \mid \mathbf{X}) \) is defined only up to addition of an arbitrary constant. We also have the analogous inequality
\[\ln p(\boldsymbol{\theta} \mid \mathbf{X}) & = \mathcal{L}(q, \boldsymbol{\theta})+\mathrm{KL}(q \| p)+\ln p(\boldsymbol{\theta}) \geqslant \mathcal{L}(q, \boldsymbol{\theta})+\ln p(\boldsymbol{\theta}).
\]
The E-step remains the same and all of the results derived above continue to hold. The simplest way to see this to appreciate that \( \boldsymbol{\theta} \) is held fixed as a new \( q \) is estimated: the introduction of a prior over some parameter cannot inform an estimator that considers it a constant. The M-step must however now maximize \( \ln p(\boldsymbol{\theta} \mid \mathbf{X}) \) which motivates the following redefinition
#+NAME: MAP Evidence lower bound (MAP ELBO)
#+begin_definition latex
Consider the joint distribution \(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\) where \(\mathbf{X}\), \( \mathbf{Z} \), and \( \boldsymbol{\theta} \) denote the set of observed data, the latent variables and the model parameters respectively. Let \(q\) be a distribution defined over the latent variables \( Z \) such that \( \mathbf{Z} \) has density \( q(\mathbf{Z}) \). Let \( p(\boldsymbol{\theta}) \) denote a prior over the model parameters. The MAP evidence lower bound objective (MAP ELBO) is defined as
\[
\mathcal{L}_{\text{MAP}}[q](\boldsymbol{\theta}) &\equiv \overbrace{\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \bigg(\frac{p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}{q(\mathbf{Z})}\bigg)}^{\mathcal{L}_{\text{ML}}[q](\boldsymbol{\theta}) + \ln p (\boldsymbol{\theta})} + \ln p(\boldsymbol{\theta}).
\]
Like the standard ELBO (denoted with \( \mathcal{\text{ML}} \)), the MAP ELBO is a functional of the distribution \(q\) and a function of the parameters \( \boldsymbol{\theta} \).
#+end_definition
#+NAME: M-step (MAP)
#+begin_definition latex
The M-step (MAP) refers to the following optimization problem involving the MAP ELBO: given \(q^{\text{old}}\)
\[
\boldsymbol{\theta}^{\text{new}} = \arg \max_{\boldsymbol{\theta}} \mathcal{L}_{\text{MAP}}[q=q^{\text{old}}](\boldsymbol{\theta}) \equiv \arg \max_{\boldsymbol{\theta}} \bigg\{ \mathcal{L}[q=q^{\text{old}}](\boldsymbol{\theta}) + \ln p(\boldsymbol{\theta})\bigg\}.
\]
#+end_definition
#+NAME: M-step optimization (MAP)
#+begin_theorem latex
\[
\boldsymbol{\theta}^{\text{new}} = \arg \max_{\boldsymbol{\theta}} \mathcal{Q}_{\text{MAP}}(\boldsymbol{\theta}, \, \boldsymbol{\theta}^{\text{old}}) \Longrightarrow \arg \max_{\boldsymbol{\theta}} \mathcal{L}_{\text{MAP}}[q=q^{\text{old}}](\boldsymbol{\theta}) = \boldsymbol{\theta}^{\text{new}}
\]
where
\[
\mathcal{Q}_{\text{MAP}}(\boldsymbol{\theta},\,\boldsymbol{\theta}^{\text{old}}) = \sum_{\mathbf{Z}} p(\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}}) \ln p(\mathbf{X},\, \mathbf{Z} \mid \boldsymbol{\theta} ) + \ln p(\boldsymbol{\theta}).
\]
In other words, solving the optimization problem of the M-step is equivalent to solving the optimization problem \( \arg \max_{\boldsymbol{\theta}} \mathcal{Q}_{\text{MAP}}(\boldsymbol{\theta}, \, \boldsymbol{\theta}^{\text{old}}) \).
#+end_theorem
#+NAME: M-step optimization (MAP)
#+begin_proof latex
On substituting \(q(\mathbf{Z}) = q^{\text{old}} (\mathbf{Z}) = p (\mathbf{Z} \mid \mathbf{X},\, \boldsymbol{\theta}^{\text{old}})\) into
\[
\mathcal{L}_{\text{MAP}}[q = q^{\text{old}}](\boldsymbol{\theta}) =\sum_{\mathbf{Z}} q^{\text{old}}(\mathbf{Z}) \ln \bigg(\frac{p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}{q^{\text{old}}(\mathbf{Z})}\bigg) + \ln p(\boldsymbol{\theta})
\]
we obtain
\[
\mathcal{L}_{\text{MAP}}[q=q^{\text{old}}](\boldsymbol{\theta}) = \underbrace{\overbrace{\sum_{\mathbf{Z}} p (\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text{old}}) \ln p(\mathbf{X},\, \mathbf{Z} \mid \boldsymbol{\theta})}^{\equiv \mathcal{Q}_{\text{ML}}(\boldsymbol{\theta},\, \boldsymbol{\theta}^{\text{old}})} + \ln p (\boldsymbol{\theta})}_{\mathcal{Q}_{\text{MAP}} (\boldsymbol{\theta},\, \boldsymbol{\theta}^{\text{old}})} - \underbrace{\sum_{\mathbf{Z}} p (\mathbf{Z} \mid \mathbf{X}, \, \boldsymbol{\theta}^{\text{old}} ) \ln p (\mathbf{Z} \mid \mathbf{X}, \, \boldsymbol{\theta}^{\text{old}})}_{\text{no dependence on } \( \boldsymbol{\theta} \)}.
\]
The second term is simply the negative entropy of the \(q^{\text{old}}\) and is therefore independent of \(\boldsymbol{\theta}\). It follows that
\[
\boldsymbol{\theta}^{\text{new}} = \arg \max_{\boldsymbol{\theta}} \mathcal{Q}_{\text{MAP}}(\boldsymbol{\theta}, \, \boldsymbol{\theta}^{\text{old}}) \Longrightarrow \arg \max_{\boldsymbol{\theta}} \mathcal{L}_{\text{MAP}}[q=q^{\text{old}}](\boldsymbol{\theta}) = \boldsymbol{\theta}^{\text{new}}.
\]
#+end_proof
The rest of the results about the M-step and their proofs continues to hold under the transformations \(\mathcal{L}[q](\boldsymbol{\theta}) \to \mathcal{L}[q](\boldsymbol{\theta}) + \ln p (\boldsymbol{\theta}) \) and \( \mathcal{Q} (\boldsymbol{\theta}, \, \boldsymbol{\theta}^{\text{old}}) \to \mathcal{Q} (\boldsymbol{\theta}, \, \boldsymbol{\theta}^{\text{old}}) + \ln p (\boldsymbol{\theta}) \). Thus using the MAP ELBO as opposed to the standard ELBO in the M-step of an EM yields an /approximate MAP estimator/ as opposed to an /approximate ML estimator/.
** Mixture models
:LOGBOOK:
CLOCK: [2024-06-18 Tue 20:02]--[2024-06-18 Tue 20:23] =>  0:21
CLOCK: [2024-06-18 Tue 08:00]--[2024-06-18 Tue 09:53] =>  1:53
:END:
If we define a joint distribution over observed and latent variables, the corresponding distribution of the observed variables alone is obtained by marginalization. This allows relatively complex marginal distributions over observed variables to be expressed in terms of more tractable joint distributions over the expanded space of observed and latent variables.
As well as providing a framework for building more complex probability distributions, mixture models can also be used to cluster data. We therefore begin our discussion of mixture distributions by considering the problem of finding clusters in a set of data points, which we approach first using a non-probabilistic technique called the \(K\)-means algorithm (Lloyd, 1982). Then we introduce the latent variable view of mixture distributions in which the discrete latent variables can be interpreted as defining assignments of data points to specific components of the mixture. 
A general technique for finding maximum likelihood estimators in latent variable models is the expectation-maximization (EM) algorithm.
Gaussian mixture models are widely used in data mining, pattern recognition, machine learning, and statistical analysis. In many applications, their parameters are determined by maximum likelihood, typically using the EM algorithm.
However, as we shall see there are some significant limitations to the maximum likelihood approach, and in Chapter 10 we shall show that an elegant Bayesian treatment can be given using the framework of variational inference. This requires little additional computation compared with EM, and it resolves the principal difficulties of maximum likelihood while also allowing the number of components in the mixture to be inferred automatically from the data.
*** Mixtures of Gaussians
The Gaussian mixture distribution is expressed as a linear superposition of Gaussians:
\[
p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k).
\]
Introduce a \(K\)-dimensional binary random variable \(\mathbf{z}\) with a 1-of-\(K\) representation. A specific element \(z_k\) equals 1 while others are 0, satisfying \(z_k \in \{0, 1\}\) and \(\sum_k z_k = 1\).
Define the joint distribution \(p(\mathbf{x}, \mathbf{z})\) using the marginal \(p(\mathbf{z})\) and conditional \(p(\mathbf{x} \mid \mathbf{z})\) distributions, aligned with the graphical model in Figure 9.4. The marginal distribution over \(\mathbf{z}\) is:
\[
p(z_k=1) = \pi_k
\]
with constraints:
\[
0 \leq \pi_k \leq 1, \quad \sum_{k=1}^{K} \pi_k = 1.
\]
Using a 1-of-\(K\) representation, write the distribution as:
\[
p(\mathbf{z}) = \prod_{k=1}^{K} \pi_k^{z_k}.
\]
The conditional distribution of \(\mathbf{x}\) given \(\mathbf{z}\) is Gaussian:
\[
p(\mathbf{x} \mid z_k = 1) = \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\]
This can be expressed as:
\[
p(\mathbf{x} \mid \mathbf{z}) = \prod_{k=1}^{K} \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)^{z_k}.
\]
The joint distribution is:
\[
p(\mathbf{x}, \mathbf{z}) = p(\mathbf{z}) p(\mathbf{x} \mid \mathbf{z})
\]
The marginal distribution of \(\mathbf{x}\) is:
\[
p(\mathbf{x}) = \sum_{\mathbf{z}} p(\mathbf{z}) p(\mathbf{x} \mid \mathbf{z}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k).
\]
For multiple observations \(\mathbf{x}_1, \ldots, \mathbf{x}_N\), each data point \(\mathbf{x}_n\) has a corresponding latent variable \(\mathbf{z}_n\). This reformulation allows working with the joint distribution \(p(\mathbf{x}, \mathbf{z})\) instead of the marginal \(p(\mathbf{x})\), facilitating methods such as the expectation-maximization (EM) algorithm.
The conditional probability of \(\mathbf{z}\) given \(\mathbf{x}\) is denoted by \(\gamma(z_k) = p(z_k=1 \mid \mathbf{x})\), computed using Bayes' theorem:
\[
\gamma(z_k) \equiv p(z_k=1 \mid \mathbf{x}) = \frac{p(z_k=1) p(\mathbf{x} \mid z_k=1)}{\sum_{j=1}^{K} p(z_j=1) p(\mathbf{x} \mid z_j=1)} = \frac{\pi_k \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}.
\]
The \(\pi_k\) are the prior probabilities of \(z_k=1\), and \(\gamma(z_k)\) are the posterior probabilities, representing the responsibility component \(k\) has for explaining \(\mathbf{x}\).
Ancestral sampling for generating random samples follows: first generate \(\widehat{\mathbf{z}}\) from \(p(\mathbf{z})\), then generate \(\mathbf{x}\) from \(p(\mathbf{x} \mid \widehat{\mathbf{z}})\). Depicting samples from \(p(\mathbf{x}, \mathbf{z})\) involves plotting \(\mathbf{x}\) colored by \(\mathbf{z}\), while samples from \(p(\mathbf{x})\) ignore \(\mathbf{z}\). Responsibilities \(\gamma(z_{nk})\) for data point \(\mathbf{x}_n\) can be plotted with proportions of red, blue, and green ink corresponding to \(k=1, 2, 3\), respectively, illustrating the posterior probabilities.
**** Maximum likelihood
Given a dataset \(\{\mathbf{x}_1, \ldots, \mathbf{x}_N\}\) modeled using a Gaussian mixture, represent the data as an \(N \times D\) matrix \(\mathbf{X}\) with \(\mathbf{x}_n^{\mathrm{T}}\) as the \(n\)-th row. Corresponding latent variables form an \(N \times K\) matrix \(\mathbf{Z}\) with rows \(\mathbf{z}_n^{\mathrm{T}}\). For i.i.d. data, the Gaussian mixture model, as per Figure 9.6, has the log-likelihood function:
***** Limitations of Maximum Likelihood in Gaussian Mixture Models
1. *Singularities in the Log-Likelihood Function*:
    - For components with \(\boldsymbol{\Sigma}_k = \sigma_k^2 \mathbf{I}\), if \(\boldsymbol{\mu}_j = \mathbf{x}_n\):
    - As \(\sigma_j \to 0\), this term \(\to \infty\), causing the log-likelihood to \(\to \infty\).
    - This occurs when a Gaussian component 'collapses' onto a data point, creating singularities.
2. *Overfitting*:
    - In single Gaussian distributions, collapsing components contribute multiplicative factors to the likelihood, leading it to zero.
    - In mixtures, one component can maintain finite variance, assigning finite probability to all data points, while another collapses, driving the log-likelihood up.
    - Overfitting occurs due to this behavior.
3. *Identifiability Issues*:
    - A \(K\)-component mixture has \(K!\) equivalent solutions, each assigning \(K\) sets of parameters to \(K\) components differently.
    - Interpretation of parameter values becomes difficult due to these equivalent solutions.
    - Identifiability is crucial for parameter interpretation but irrelevant for density modeling.
***** Mitigating Limitations
- *Heuristics*:
    - Detect collapsing components and reset their means and covariances to avoid singularities.
    - This ensures seeking well-behaved local maxima of the likelihood function.
***** Complexity of Log-Likelihood Maximization
- *No Closed-Form Solution*:
    - The summation inside the logarithm in (9.14) complicates setting derivatives to zero, yielding no closed-form solutions.
- *Optimization Techniques*:
    - Gradient-based optimization (Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008) is feasible but complex.
    - The EM algorithm provides a simpler alternative, foundational for variational inference (Chapter 10).
**** Algorithm
***** Expectation maximization (EM)
An elegant and powerful method for finding maximum likelihood solutions for models with latent variables is the expectation-maximization (EM) algorithm (Dempster et al., 1977; McLachlan and Krishnan, 1997). We will initially motivate the EM algorithm in the context of the Gaussian mixture model.
### Maximum Likelihood Conditions
To maximize the likelihood function, set the derivatives of \(\ln p(\mathbf{X} \mid \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})\) in (9.14) with respect to \(\boldsymbol{\mu}_k\) to zero:
Multiplying by \(\boldsymbol{\Sigma}_k^{-1}\) and rearranging gives:
where:
\(N_k\) is the effective number of points assigned to cluster \(k\).
Similarly, setting the derivative with respect to \(\boldsymbol{\Sigma}_k\) to zero gives:
\[
\boldsymbol{\Sigma}_k = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk})(\mathbf{x}_n - \boldsymbol{\mu}_k)(\mathbf{x}_n - \boldsymbol{\mu}_k)^{\mathrm{T}} \tag{9.19}
\]
Maximizing with respect to \(\pi_k\) while considering the constraint \(\sum_{k=1}^{K} \pi_k = 1\) (using a Lagrange multiplier \(\lambda\)):
\[
\ln p(\mathbf{X} \mid \boldsymbol{\pi}, \bold. \mu, \bold. \Sigma) + \lambda\left(\sum_{k=1}^{K} \pi_k - 1\right) \tag{9.20}
\]
yields:
\[
0 = \sum_{n=1}^{N} \frac{\mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j} \pi_j \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)} + \lambda \tag{9.21}
\]
Multiplying both sides by \(\pi_k\) and summing over \(k\), using \(\sum_{k=1}^{K} \pi_k = 1\), gives \(\lambda = -N\):
\[
\pi_k = \frac{N_k}{N} \tag{9.22}
\]
### EM Algorithm Steps
The results (9.17), (9.19), and (9.22) do not form a closed solution due to the complex dependency of \(\gamma(z_{nk})\) on the parameters. However, they suggest an iterative scheme:
1. **E-Step**: Evaluate posterior probabilities (responsibilities) \(\gamma(z_{nk})\) using current parameter values:
   \[
   \gamma(z_{nk}) = \frac{\pi_k \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)} \tag{9.13}
   \]
2. **M-Step**: Update parameters using responsibilities:
   \[
   \boldsymbol{\mu}_k = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk}) \mathbf{x}_n \tag{9.17}
   \]
   \[
   \boldsymbol{\Sigma}_k = \frac{1}{N_k} \sum_{n=1}^{N} \gamma(z_{nk})(\mathbf{x}_n - \boldsymbol{\mu}_k)(\mathbf{x}_n - \boldsymbol{\mu}_k)^{\mathrm{T}} \tag{9.19}
   \]
   \[
   \pi_k = \frac{N_k}{N} \tag{9.22}
   \]
### Convergence
Each E-step followed by an M-step increases the log likelihood. The algorithm converges when the change in the log likelihood or parameters falls below a threshold.
### Example
Figure 9.8 illustrates the EM algorithm using the Old Faithful dataset with two Gaussians. Initial parameters are set as in the \(K\)-means algorithm. After several iterations, the algorithm converges.
### Initialization and Computational Considerations
- **Initialization**: The \(K\)-means algorithm is often used for initialization. Covariance matrices can be set to the sample covariances of clusters found by \(K\)-means, and mixing coefficients to the fractions of data points in each cluster.
- **Avoiding Singularities**: Techniques to avoid singularities include detecting collapsing components and resetting their parameters.
- **Local Maxima**: EM may find local maxima, not necessarily the global maximum.
The EM algorithm, while more computationally intensive than \(K\)-means, provides a robust framework for maximum likelihood estimation in models with latent variables.
 Because the EM algorithm for Gaussian mixtures plays such an important role, we summarize it below.
\begin{algorithm}
\caption{Expectation-Maximization for Gaussian Mixture Models}
\begin{algorithmic}[1]
    \State Initialize means \(\boldsymbol{\mu}_{k}\), covariances \(\boldsymbol{\Sigma}_{k}\), and mixing coefficients \(\pi_{k}\)
    \State Evaluate the initial log likelihood
    \Repeat
        \State \textbf{E-step:}
        \For{each data point \(n\) and each component \(k\)}
            \State Evaluate responsibilities:
            \[
            \gamma(z_{nk}) \leftarrow \frac{\pi_k \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}
            \]
        \EndFor
        \State \textbf{M-step:}
        \For{each component \(k\)}
            \State Calculate \(N_k \leftarrow \sum_{n=1}^N \gamma(z_{nk})\)
            \State Update means:
            \[
            \boldsymbol{\mu}_k^{\text{new}} \leftarrow \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) \mathbf{x}_n
            \]
            \State Update covariances:
            \[
            \boldsymbol{\Sigma}_k^{\text{new}} \leftarrow \frac{1}{N_k} \sum_{n=1}^N \gamma(z_{nk}) (\mathbf{x}_n - \boldsymbol{\mu}_k^{\text{new}})(\mathbf{x}_n - \boldsymbol{\mu}_k^{\text{new}})^T
            \]
            \State Update mixing coefficients:
            \[
            \pi_k^{\text{new}} \leftarrow \frac{N_k}{N}
            \]
        \EndFor
        \State Evaluate the log likelihood:
        \[
        \ln p(\mathbf{X} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) \leftarrow \sum_{n=1}^N \ln \left\{\sum_{k=1}^K \pi_k \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\right\}
        \]
        \State Check for convergence
    \Until{convergence criterion is met}
\end{algorithmic}
\end{algorithm}
**** Latent variable view of EM for Gaussian mixtures
We now consider the application of this latent variable view of EM to the specific case of a Gaussian mixture model. Recall that our goal is to maximize the log likelihood function (9.14), which is computed using the observed data set \(\mathbf{X}\), and we saw that this was more difficult than for the case of a single Gaussian distribution due to the presence of the summation over \(k\) that occurs inside the logarithm. Suppose then that in addition to the observed data set \(\mathbf{X}\), we were also given the values of the corresponding discrete variables Z. Recall that Figure 9.5(a) shows a 'complete' data set (i.e., one that includes labels showing which component generated each data point) while Figure 9.5(b) shows the corresponding 'incomplete' data set. The graphical model for the complete data is shown in Figure 9.9.
Figure 9.9 This shows the same graph as in Figure 9.6 except that we now suppose that the discrete variables \(\mathbf{z}_{n}\) are observed, as well as the data variables \(\mathbf{x}_{n}\).
Now consider the problem of maximizing the likelihood for the complete data set \(\{\mathbf{X}, \mathbf{Z}\}\). From (9.10) and (9.11), this likelihood function takes the form
\begin{align*}
p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})=\prod_{n=1}^{N} \prod_{k=1}^{K} \pi_{k}^{z_{n k}} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)^{z_{n k}} \tag{9.35}
\end{align*}
where \(z_{n k}\) denotes the \(k^{\text {th }}\) component of \(\mathbf{z}_{n}\). Taking the logarithm, we obtain
\begin{align*}
\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k}\left\{\ln \pi_{k}+\ln \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\} \tag{9.36}
\end{align*}
Comparison with the log likelihood function (9.14) for the incomplete data shows that the summation over \(k\) and the logarithm have been interchanged. The logarithm now acts directly on the Gaussian distribution, which itself is a member of the exponential family. Not surprisingly, this leads to a much simpler solution to the maximum likelihood problem, as we now show. Consider first the maximization with respect to the means and covariances. Because \(\mathbf{z}_{n}\) is a \(K\)-dimensional vector with all elements equal to 0 except for a single element having the value 1 , the complete-data log likelihood function is simply a sum of \(K\) independent contributions, one for each mixture component. Thus the maximization with respect to a mean or a covariance is exactly as for a single Gaussian, except that it involves only the subset of data points that are 'assigned' to that component. For the maximization with respect to the mixing coefficients, we note that these are coupled for different values of \(k\) by virtue of the summation constraint (9.9). Again, this can be enforced using a Lagrange multiplier as before, and leads to the result
\begin{align*}
\pi_{k}=\frac{1}{N} \sum_{n=1}^{N} z_{n k} \tag{9.37}
\end{align*}
so that the mixing coefficients are equal to the fractions of data points assigned to the corresponding components.
Thus we see that the complete-data log likelihood function can be maximized trivially in closed form. In practice, however, we do not have values for the latent variables so, as discussed earlier, we consider the expectation, with respect to the posterior distribution of the latent variables, of the complete-data log likelihood.
Using (9.10) and (9.11) together with Bayes' theorem, we see that this posterior distribution takes the form
\begin{align*}
p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) \propto \prod_{n=1}^{N} \prod_{k=1}^{K}\left[\pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right]^{z_{n k}} \tag{9.38}
\end{align*}
and hence factorizes over \(n\) so that under the posterior distribution the \(\left\{\mathbf{z}_{n}\right\}\) are independent. This is easily verified by inspection of the directed graph in Figure 9.6 and making use of the d-separation criterion. The expected value of the indicator variable \(z_{n k}\) under this posterior distribution is then given by
\begin{align*}
\begin{align*}
\mathbb{E}\left[z_{n k}\right]= & \frac{\sum_{z_{n k}} z_{n k}\left[\pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right]^{z_{n k}}}{\sum_{z_{n j}}\left[\pi_{j} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)\right]^{z_{n j}}} \\
= & \frac{\pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)}=\gamma\left(z_{n k}\right)
\end{align*} \tag{9.39}
\end{align*}
which is just the responsibility of component \(k\) for data point \(\mathbf{x}_{n}\). The expected value of the complete-data log likelihood function is therefore given by
\begin{align*}
\mathbb{E}_{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]=\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left\{\ln \pi_{k}+\ln \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\} \tag{9.40}
\end{align*}
We can now proceed as follows. First we choose some initial values for the parameters \(\boldsymbol{\mu}^{\text {old }}, \boldsymbol{\Sigma}^{\text {old }}\) and \(\boldsymbol{\pi}^{\text {old }}\), and use these to evaluate the responsibilities (the E step). We then keep the responsibilities fixed and maximize (9.40) with respect to \(\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\) and \(\pi_{k}\) (the M step). This leads to closed form solutions for \(\boldsymbol{\mu}^{\text {new }}, \boldsymbol{\Sigma}^{\text {new }}\) and \(\boldsymbol{\pi}^{\text {new }}\) given by (9.17), (9.19), and (9.22) as before. This is precisely the EM algorithm for Gaussian mixtures as derived earlier. We shall gain more insight into the role of the expected complete-data log likelihood function when we give a proof of convergence of the EM algorithm in Section 9.4.
*** K-means clustering
**** Definition
Consider a dataset \(\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\right\}\) consisting of \(N\) observations of a random \(D\)-dimensional Euclidean variable \(\mathbf{x}\). The goal is partition the dataset into some number \(K\) of clusters, assuming that \( K \) is /given/.
One way to do this is to minimize the sum of squared distances from each data point to its closest cluster center \(\boldsymbol{\mu}_{k}\). Introduce binary indicators \(r_{n k} \in \{0,1\}\) for \(k=1, \ldots, K\), where \(r_{n k} = 1\) if \(\mathbf{x}_{n}\) is assigned to cluster \(k\) and \(r_{n j} = 0\) for \(j \neq k\). The objective function \(J\) is defined as:
\[ 
J = (1/2) \sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \|\mathbf{x}_{n} - \boldsymbol{\mu}_{k}\|^2 
\]
The condition of vanishing derivative of \( J \) with respect to \(\boldsymbol{\mu}_{k}\) yields
\begin{align*}
\sum_{n=1}^{N} r_{n k}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right)=0 \Longrightarrow \boldsymbol{\mu}_{k}=\frac{\sum_{n} r_{n k} \mathbf{x}_{n}}{\sum_{n} r_{n k}}.
\end{align*}
To minimize \(J\), we use an iterative two-step process. Initially, choose \(\boldsymbol{\mu}_{k}\). Then:
1. *E-step*: Minimize \(J\) with respect to \(r_{n k}\), fixing \(\boldsymbol{\mu}_{k}\). Assign each \(\mathbf{x}_{n}\) to the nearest \(\boldsymbol{\mu}_{k}\):
  \[
  r_{n k} =\begin{cases} 
  1 & \text{if } k = \arg \min_{j} \|\mathbf{x}_{n} - \boldsymbol{\mu}_{j}\|^2 \\
  0 & \text{otherwise} 
  \end{cases}
  \]
2. *M-step*: Minimize \(J\) with respect to \(\boldsymbol{\mu}_{k}\), fixing \(r_{n k}\). Update \(\boldsymbol{\mu}_{k}\):
  \[
  \boldsymbol{\mu}_{k} = \frac{\sum_{n} r_{n k} \mathbf{x}_{n}}{\sum_{n} r_{n k}}
  \]
This iterative process continues until convergence (there is no further change in the assignments or until some maximum number of iterations is exceeded), reducing \(J\) in each step, ensuring convergence to a local minimum.  Because each phase reduces the value of the objective function \(J\), convergence of the algorithm is assured. However, it may converge to a local rather than global minimum of \(J\). This procedure is the \(K\)-means algorithm.
The \(K\)-means algorithm itself is often used to initialize the parameters in a Gaussian mixture model before applying the EM algorithm.
**** Application
***** Image segmentation and compression
:PROPERTIES:
:CUSTOM_ID: image-segmentation-and-compression
:END:
As an illustration of the application of the \(K\)-means algorithm, we consider the related problems of image segmentation and image compression. The goal of segmentation is to partition an image into regions each of which has a reasonably homogeneous visual appearance or which corresponds to objects or parts of objects (Forsyth and Ponce, 2003). Each pixel in an image is a point in a 3-dimensional space comprising the intensities of the red, blue, and green channels, and our segmentation algorithm simply treats each pixel in the image as a separate data point. Note that strictly this space is not Euclidean because the channel intensities are bounded by the interval \([0,1]\). Nevertheless, we can apply the \(K\)-means algorithm without difficulty. We illustrate the result of running \(K\)-means to convergence, for any particular value of \(K\), by re-drawing the image replacing each pixel vector with the \(\{R, G, B\}\) intensity triplet given by the centre \(\boldsymbol{\mu}_{k}\) to which that pixel has been assigned. Results for various values of \(K\) are shown in Figure 9.3. We see that for a given value of \(K\), the algorithm is representing the image using a palette of only \(K\) colours. It should be emphasized that this use of \(K\)-means is not a particularly sophisticated approach to image segmentation, not least because it takes no account of the spatial proximity of different pixels. The image segmentation problem is in general extremely difficult and remains the subject of active research and is introduced here simply to illustrate the behaviour of the \(K\)-means algorithm.
Figure 9.3 Two examples of the application of the \(K\)-means clustering algorithm to image segmentation showing the initial images together with their \(K\)-means segmentations obtained using various values of \(K\). This also illustrates of the use of vector quantization for data compression, in which smaller values of \(K\) give higher compression at the expense of poorer image quality.
We can also use the result of a clustering algorithm to perform data compression. It is important to distinguish between lossless data compression, in which the goal is to be able to reconstruct the original data exactly from the compressed representation, and lossy data compression, in which we accept some errors in the reconstruction in return for higher levels of compression than can be achieved in the lossless case. We can apply the \(K\)-means algorithm to the problem of lossy data compression as follows. For each of the \(N\) data points, we store only the identity \(k\) of the cluster to which it is assigned. We also store the values of the \(K\) cluster centres \(\boldsymbol{\mu}_{k}\), which typically requires significantly less data, provided we choose \(K \ll N\). Each data point is then approximated by its nearest centre \(\boldsymbol{\mu}_{k}\). New data points can similarly be compressed by first finding the nearest \(\boldsymbol{\mu}_{k}\) and then storing the label \(k\) instead of the original data vector. This framework is often called vector quantization, and the vectors \(\boldsymbol{\mu}_{k}\) are called code-book vectors.
The image segmentation problem discussed above also provides an illustration of the use of clustering for data compression. Suppose the original image has \(N\) pixels comprising \(\{R, G, B\}\) values each of which is stored with 8 bits of precision. Then to transmit the whole image directly would cost \(24 N\) bits. Now suppose we first run \(K\)-means on the image data, and then instead of transmitting the original pixel intensity vectors we transmit the identity of the nearest vector \(\boldsymbol{\mu}_{k}\). Because there are \(K\) such vectors, this requires \(\log _{2} K\) bits per pixel. We must also transmit the \(K\) code book vectors \(\mu_{k}\), which requires \(24 K\) bits, and so the total number of bits required to transmit the image is \(24 K+N \log _{2} K\) (rounding up to the nearest integer). The original image shown in Figure 9.3 has \(240 \times 180=43,200\) pixels and so requires \(24 \times 43,200=1,036,800\) bits to transmit directly. By comparison, the compressed images require 43,248 bits \((K=2), 86,472\) bits \((K=3)\), and 173,040 bits \((K=10)\), respectively, to transmit. These represent compression ratios compared to the original image of \(4.2 \%, 8.3 \%\), and \(16.7 \%\), respectively. We see that there is a trade-off between degree of compression and image quality. Note that our aim in this example is to illustrate the \(K\)-means algorithm. If we had been aiming to produce a good image compressor, then it would be more fruitful to consider small blocks of adjacent pixels, for instance \(5 \times 5\), and thereby exploit the correlations that exist in natural images between nearby pixels.
**** Interpretation
***** Relation to Gaussian mixture
Comparison of the \(K\)-means algorithm with the EM algorithm for Gaussian mixtures shows that there is a close similarity. Whereas the \(K\)-means algorithm performs a hard assignment of data points to clusters, in which each data point is associated uniquely with one cluster, the EM algorithm makes a soft assignment based on the posterior probabilities. In fact, we can derive the \(K\)-means algorithm as a particular limit of EM for Gaussian mixtures as follows.
Consider a Gaussian mixture model in which the covariance matrices of the mixture components are given by \(\epsilon \mathbf{I}\), where \(\epsilon\) is a variance parameter that is shared by all of the components, and \(\mathbf{I}\) is the identity matrix, so that
\begin{align*}
p\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)=\frac{1}{(2 \pi \epsilon)^{1 / 2}} \exp \left\{-\frac{1}{2 \epsilon}\left\|\mathbf{x}-\boldsymbol{\mu}_{k}\right\|^{2}\right\} \tag{9.41}
\end{align*}
We now consider the EM algorithm for a mixture of \(K\) Gaussians of this form in which we treat \(\epsilon\) as a fixed constant, instead of a parameter to be re-estimated. From (9.13) the posterior probabilities, or responsibilities, for a particular data point \(\mathbf{x}_{n}\), are given by
\begin{align*}
\gamma\left(z_{n k}\right)=\frac{\pi_{k} \exp \left\{-\left\|\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right\|^{2} / 2 \epsilon\right\}}{\sum_{j} \pi_{j} \exp \left\{-\left\|\mathbf{x}_{n}-\boldsymbol{\mu}_{j}\right\|^{2} / 2 \epsilon\right\}} \tag{9.42}
\end{align*}
If we consider the limit \(\epsilon \rightarrow 0\), we see that in the denominator the term for which \(\left\|\mathbf{x}_{n}-\boldsymbol{\mu}_{j}\right\|^{2}\) is smallest will go to zero most slowly, and hence the responsibilities \(\gamma\left(z_{n k}\right)\) for the data point \(\mathbf{x}_{n}\) all go to zero except for term \(j\), for which the responsibility \(\gamma\left(z_{n j}\right)\) will go to unity. Note that this holds independently of the values of the \(\pi_{k}\) so long as none of the \(\pi_{k}\) is zero. Thus, in this limit, we obtain a hard assignment of data points to clusters, just as in the \(K\)-means algorithm, so that \(\gamma\left(z_{n k}\right) \rightarrow r_{n k}\) where \(r_{n k}\) is defined by (9.2). Each data point is thereby assigned to the cluster having the closest mean.
The EM re-estimation equation for the \(\boldsymbol{\mu}_{k}\), given by (9.17), then reduces to the \(K\)-means result (9.4). Note that the re-estimation formula for the mixing coefficients (9.22) simply re-sets the value of \(\pi_{k}\) to be equal to the fraction of data points assigned to cluster \(k\), although these parameters no longer play an active role in the algorithm.
Finally, in the limit \(\epsilon \rightarrow 0\) the expected complete-data log likelihood, given by (9.40), becomes
\begin{align*}
\mathbb{E}_{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})] \rightarrow-\frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k}\left\|\mathbf{x}_{n}-\boldsymbol{\mu}_{k}\right\|^{2}+\text { const } \tag{9.43}
\end{align*}
Thus we see that in this limit, maximizing the expected complete-data log likelihood is equivalent to minimizing the distortion measure \(J\) for the \(K\)-means algorithm given by (9.1).
Note that the \(K\)-means algorithm does not estimate the covariances of the clusters but only the cluster means. A hard-assignment version of the Gaussian mixture model with general covariance matrices, known as the elliptical \(K\)-means algorithm, has been considered by Sung and Poggio (1994).
**** Extensions
***** Robbins-Monro procedure
So far, we have considered a batch version of \(K\)-means in which the whole data set is used together to update the prototype vectors. We can also derive an on-line stochastic algorithm (MacQueen, 1967) by applying the Robbins-Monro procedure to the problem of finding the roots of the regression function given by the derivatives of \(J\) in (9.1) with respect to \(\boldsymbol{\mu}_{k}\). This leads to a sequential update in which, for each data point \(\mathbf{x}_{n}\) in turn, we update the nearest prototype \(\boldsymbol{\mu}_{k}\) using
\begin{align*}
\boldsymbol{\mu}_{k}^{\text {new }}=\boldsymbol{\mu}_{k}^{\text {old }}+\eta_{n}\left(\mathbf{x}_{n}-\boldsymbol{\mu}_{k}^{\text {old }}\right) \tag{9.5}
\end{align*}
where \(\eta_{n}\) is the learning rate parameter, which is typically made to decrease monotonically as more data points are considered.
The Robbins-Monro procedure is a stochastic approximation method used to find the roots of a function that cannot be directly observed, but can be estimated via noisy observations. It's particularly suited to problems where you want to find a parameter value \(\theta\) that sets a certain expectation \(E[g(\theta, X)]\) to zero, where \(g\) is some function of the parameter and random variable \(X\). This expectation is typically unknown or difficult to compute exactly but can be estimated from available data.
In the context of \(K\)-means clustering, we're interested in finding the cluster centers \(\mu_k\) that minimize the objective function \(J\), which is the sum of squared distances between each data point and its closest cluster center. The objective function can be written as:
\begin{align*}
J=\frac{1}{2} \sum_{n=1}^N \sum_{k=1}^K r_{n k}\left\|\mathbf{x}_n-\boldsymbol{\mu}_k\right\|^2
\end{align*}
Gradient of \(J\) with respect to \(\mu_k\)
To use the Robbins-Monro procedure, we first need the gradient of \(J\) with respect to each cluster center \(\boldsymbol{\mu}_k\). Taking the derivative, assuming \(r_{n k}\) are fixed and known, we get:
\begin{align*}
\frac{\partial J}{\partial \boldsymbol{\mu}_k}=\sum_{n=1}^N r_{n k}\left(\boldsymbol{\mu}_k-\mathbf{x}_n\right)
\end{align*}
Gradient of \(J\) with respect to \(\mu_k\)
To use the Robbins-Monro procedure, we first need the gradient of \(J\) with respect to each cluster center \(\boldsymbol{\mu}_k\). Taking the derivative, assuming \(r_{n k}\) are fixed and known, we get:
\begin{align*}
\frac{\partial J}{\partial \boldsymbol{\mu}_k}=\sum_{n=1}^N r_{n k}\left(\boldsymbol{\mu}_k-\mathbf{x}_n\right)
\end{align*}
Robbins-Monro Update Rule
The goal is to find \(\boldsymbol{\mu}_k\) such that \(\frac{\partial J}{\partial \mu_k}=0\). The update rule from the Robbins-Monro procedure for the \(k\)-th cluster center after observing the \(n\)-th data point is:
\begin{align*}
\boldsymbol{\mu}_k^{\mathrm{new}}=\boldsymbol{\mu}_k^{\text {old }}-\eta_n \frac{\partial J}{\partial \boldsymbol{\mu}_k}
\end{align*}
Where \(\eta_n\) is the learning rate at step \(n\). Since the Robbins-Monro procedure suggests updating the parameters in the direction that would decrease the error (or here, minimize the gradient), and given our earlier derivative, the update rule becomes:
\begin{align*}
\boldsymbol{\mu}_k^{\text {new }}=\boldsymbol{\mu}_k^{\text {old }}+\eta_n\left(\mathbf{x}_n-\boldsymbol{\mu}_k^{\text {old }}\right)
\end{align*}
Implementation Details
1. Initialization: \(\boldsymbol{\mu}_k\) is initialized possibly as a random data point from the dataset or using another heuristic.
2. Learning Rate \(\left(\eta_n\right)\) : This is typically chosen to decrease over time, often set to \(\frac{1}{n}\) or adjusted based on more sophisticated schedules to ensure convergence.
**** Algorithms
***** Batch K-means
\begin{algorithm}
\caption{K-means Clustering Algorithm}
\begin{algorithmic}[1]
    \State \textbf{Initialization:} Select \(K\) random data points from the dataset \(\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\}\) to set initial cluster centers \(\boldsymbol{\mu}_{k}\) for \(k = 1, \ldots, K\)
    \Repeat
        \State \textbf{E-step:} Assign each data point to the nearest cluster center
        \For{each data point \(\mathbf{x}_{n}\), \(n = 1, \ldots, N\)}
            \State \(r_{nk} = \begin{cases} 
              1 & \text{if } k = \arg \min_{j} \|\mathbf{x}_{n} - \boldsymbol{\mu}_{j}\|^2 \\
              0 & \text{otherwise}
            \end{cases}\)
        \EndFor
        \State \textbf{M-step:} Update the cluster centers based on the assignments
        \For{\(k = 1, \ldots, K\)}
            \State \(\boldsymbol{\mu}_{k} = \frac{\sum_{n} r_{nk} \mathbf{x}_{n}}{\sum_{n} r_{nk}}\)
        \EndFor
        \State Check for convergence (no change in \(\boldsymbol{\mu}_{k}\) or maximum iterations reached)
    \Until{convergence criterion is met}
\end{algorithmic}
\end{algorithm}
***** Sequential K-means
\begin{algorithm}
\caption{Sequential K-means Algorithm (Online K-means)}
\begin{algorithmic}[1]
    \State \textbf{Initialization:} Initialize cluster centers \(\boldsymbol{\mu}_{k}\) for \(k = 1, \ldots, K\) (possibly using a small initial set of data points or randomly)
    \State Initialize a counter \(n = 0\)
    \While{data points \(\mathbf{x}_n\) are received}
        \State \(n \leftarrow n + 1\)
        \State Find the nearest cluster center \(\boldsymbol{\mu}_k\) to \(\mathbf{x}_n\)
        \[
        k = \arg \min_{j} \|\mathbf{x}_{n} - \boldsymbol{\mu}_{j}\|^2
        \]
        \State Update the learning rate \(\eta_n\) (e.g., \(\eta_n = \frac{1}{n}\) or another decreasing function)
        \State Update the cluster center \(\boldsymbol{\mu}_k\) using the learning rate \(\eta_n\)
        \[
        \boldsymbol{\mu}_{k}^{\text{new}} = \boldsymbol{\mu}_{k}^{\text{old}} + \eta_{n} (\mathbf{x}_{n} - \boldsymbol{\mu}_{k}^{\text{old}})
        \]
        \State Optionally, perform any additional checks or operations (e.g., periodic evaluation of convergence)
    \EndWhile
\end{algorithmic}
\end{algorithm}
**** Limitations
1) A direct implementation of the \(K\)-means algorithm as discussed here can be relatively slow, because in each \(\mathrm{E}\) step it is necessary to compute the Euclidean distance between every prototype vector and every data point. Various schemes have been proposed for speeding up the \(K\)-means algorithm, some of which are based on precomputing a data structure such as a tree such that nearby points are in the same subtree (Ramasubramanian and Paliwal, 1990; Moore, 2000). Other approaches make use of the triangle inequality for distances, thereby avoiding unnecessary distance calculations (Hodgson, 1998; Elkan, 2003).
The \(K\)-means algorithm is illustrated using the Old Faithful data set in Figure 9.1. For the purposes of this example, we have made a linear re-scaling of the data, known as standardizing, such that each of the variables has zero mean and unit standard deviation. For this example, we have chosen \(K=2\), and so in this case, the assignment of each data point to the nearest cluster centre is equivalent to a classification of the data points according to which side they lie of the perpendicular bisector of the two cluster centres. A plot of the cost function \(J\) given by (9.1) for the Old Faithful example is shown in Figure 9.2.
Figure 9.1 Illustration of the \(K\)-means algorithm using the re-scaled Old Faithful data set. (a) Green points denote the data set in a two-dimensional Euclidean space. The initial choices for centres \(\mu_{1}\) and \(\mu_{2}\) are shown by the red and blue crosses, respectively. (b) In the initial E step, each data point is assigned either to the red cluster or to the blue cluster, according to which cluster centre is nearer. This is equivalent to classifying the points according to which side of the perpendicular bisector of the two cluster centres, shown by the magenta line, they lie on. (c) In the subsequent \(\mathrm{M}\) step, each cluster centre is re-computed to be the mean of the points assigned to the corresponding cluster. (d)-(i) show successive \(\mathrm{E}\) and \(\mathrm{M}\) steps through to final convergence of the algorithm.
Figure 9.2 Plot of the cost function \(J\) given by (9.1) after each \(E\) step (blue points) and \(\mathrm{M}\) step (red points) of the \(K\) means algorithm for the example shown in Figure 9.1. The algorithm has converged after the third M step, and the final EM cycle produces no changes in either the assignments or the prototype vectors.
The \(K\)-means algorithm is based on the use of squared Euclidean distance as the measure of dissimilarity between a data point and a prototype vector. Not only does this limit the type of data variables that can be considered (it would be inappropriate for cases where some or all of the variables represent categorical labels for instance), but it can also make the determination of the cluster means nonrobust to outliers. We can generalize the \(K\)-means algorithm by introducing a more general dissimilarity measure \(\mathcal{V}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\) between two vectors \(\mathbf{x}\) and \(\mathbf{x}^{\prime}\) and then minimizing the following distortion measure
\begin{align*}
\widetilde{J}=\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \mathcal{V}\left(\mathbf{x}_{n}, \boldsymbol{\mu}_{k}\right) \tag{9.6}
\end{align*}
which gives the \(K\)-medoids algorithm. The E step again involves, for given cluster prototypes \(\boldsymbol{\mu}_{k}\), assigning each data point to the cluster for which the dissimilarity to the corresponding prototype is smallest. The computational cost of this is \(O(K N)\), as is the case for the standard \(K\)-means algorithm. For a general choice of dissimilarity measure, the \(\mathrm{M}\) step is potentially more complex than for \(K\)-means, and so it is common to restrict each cluster prototype to be equal to one of the data vectors assigned to that cluster, as this allows the algorithm to be implemented for any choice of dissimilarity measure \(\mathcal{V}(\cdot, \cdot)\) so long as it can be readily evaluated. Thus the M step involves, for each cluster \(k\), a discrete search over the \(N_{k}\) points assigned to that cluster, which requires \(O\left(N_{k}^{2}\right)\) evaluations of \(\mathcal{V}(\cdot, \cdot)\).
One notable feature of the \(K\)-means algorithm is that at each iteration, every data point is assigned uniquely to one, and only one, of the clusters. Whereas some data points will be much closer to a particular centre \(\boldsymbol{\mu}_{k}\) than to any other centre, there may be other data points that lie roughly midway between cluster centres. In the latter case, it is not clear that the hard assignment to the nearest cluster is the most appropriate. We shall see in the next section that by adopting a probabilistic approach, we obtain 'soft' assignments of data points to clusters in a way that reflects the level of uncertainty over the most appropriate assignment. This probabilistic formulation brings with it numerous benefits.
*** Mixtures of Bernoulli distributions
So far in this chapter, we have focussed on distributions over continuous variables described by mixtures of Gaussians. As a further example of mixture modelling, and to illustrate the EM algorithm in a different context, we now discuss mixtures of discrete binary variables described by Bernoulli distributions. This model is also known as latent class analysis (Lazarsfeld and Henry, 1968; McLachlan and Peel, 2000). As well as being of practical importance in its own right, our discussion of Bernoulli mixtures will also lay the foundation for a consideration of hidden Markov models over discrete variables.
Consider a set of \(D\) binary variables \(x_{i}\), where \(i=1, \ldots, D\), each of which is governed by a Bernoulli distribution with parameter \(\mu_{i}\), so that
\begin{align*}
p(\mathbf{x} \mid \boldsymbol{\mu})=\prod_{i=1}^{D} \mu_{i}^{x_{i}}\left(1-\mu_{i}\right)^{\left(1-x_{i}\right)} \tag{9.44}
\end{align*}
where \(\mathbf{x}=\left(x_{1}, \ldots, x_{D}\right)^{\mathrm{T}}\) and \(\boldsymbol{\mu}=\left(\mu_{1}, \ldots, \mu_{D}\right)^{\mathrm{T}}\). We see that the individual variables \(x_{i}\) are independent, given \(\boldsymbol{\mu}\). The mean and covariance of this distribution are easily seen to be
\begin{align*}
\mathbb{E}[\mathbf{x}] & =\boldsymbol{\mu}  \tag{9.45}\\
\operatorname{cov}[\mathbf{x}] & =\operatorname{diag}\left\{\mu_{i}\left(1-\mu_{i}\right)\right\}
\tag{9.46}
\end{align*}
Now let us consider a finite mixture of these distributions given by
\begin{align*}
p(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\pi})=\sum_{k=1}^{K} \pi_{k} p\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}\right) \tag{9.47}
\end{align*}
where \(\boldsymbol{\mu}=\left\{\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{K}\right\}, \boldsymbol{\pi}=\left\{\pi_{1}, \ldots, \pi_{K}\right\}\), and
\begin{align*}
p\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}\right)=\prod_{i=1}^{D} \mu_{k i}^{x_{i}}\left(1-\mu_{k i}\right)^{\left(1-x_{i}\right)} \tag{9.48}
\end{align*}
The mean and covariance of this mixture distribution are given by
\begin{align*}
\mathbb{E}[\mathbf{x}] & =\sum_{k=1}^{K} \pi_{k} \boldsymbol{\mu}_{k}  \tag{9.49}\\
\operatorname{cov}[\mathbf{x}] & =\sum_{k=1}^{K} \pi_{k}\left\{\boldsymbol{\Sigma}_{k}+\boldsymbol{\mu}_{k} \boldsymbol{\mu}_{k}^{\mathrm{T}}\right\}-\mathbb{E}[\mathbf{x}] \mathbb{E}[\mathbf{x}]^{\mathrm{T}}
\tag{9.50}
\end{align*}
where \(\boldsymbol{\Sigma}_{k}=\operatorname{diag}\left\{\mu_{k i}\left(1-\mu_{k i}\right)\right\}\). Because the covariance matrix \(\operatorname{cov}[\mathbf{x}]\) is no longer diagonal, the mixture distribution can capture correlations between the variables, unlike a single Bernoulli distribution.
If we are given a data set \(\mathbf{X}=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\right\}\) then the log likelihood function for this model is given by
\begin{align*}
\ln p(\mathbf{X} \mid \boldsymbol{\mu}, \boldsymbol{\pi})=\sum_{n=1}^{N} \ln \left\{\sum_{k=1}^{K} \pi_{k} p\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}\right)\right\} \tag{9.51}
\end{align*}
Again we see the appearance of the summation inside the logarithm, so that the maximum likelihood solution no longer has closed form.
We now derive the EM algorithm for maximizing the likelihood function for the mixture of Bernoulli distributions. To do this, we first introduce an explicit latent variable \(\mathbf{z}\) associated with each instance of \(\mathbf{x}\). As in the case of the Gaussian mixture, \(\mathbf{z}=\left(z_{1}, \ldots, z_{K}\right)^{\mathrm{T}}\) is a binary \(K\)-dimensional variable having a single component equal to 1 , with all other components equal to 0 . We can then write the conditional distribution of \(\mathbf{x}\), given the latent variable, as
\begin{align*}
p(\mathbf{x} \mid \mathbf{z}, \boldsymbol{\mu})=\prod_{k=1}^{K} p\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}\right)^{z_{k}} \tag{9.52}
\end{align*}
while the prior distribution for the latent variables is the same as for the mixture of Gaussians model, so that
\begin{align*}
p(\mathbf{z} \mid \boldsymbol{\pi})=\prod_{k=1}^{K} \pi_{k}^{z_{k}} \tag{9.53}
\end{align*}
If we form the product of \(p(\mathbf{x} \mid \mathbf{z}, \boldsymbol{\mu})\) and \(p(\mathbf{z} \mid \boldsymbol{\pi})\) and then marginalize over \(\mathbf{z}\), then we recover (9.47).
In order to derive the EM algorithm, we first write down the complete-data log likelihood function, which is given by
\begin{align*}
\begin{align*}
& \ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\pi})=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k}\left\{\ln \pi_{k}\right. \\
& \left.\quad+\sum_{i=1}^{D}\left[x_{n i} \ln \mu_{k i}+\left(1-x_{n i}\right) \ln \left(1-\mu_{k i}\right)\right]\right\}
\end{align*} \tag{9.54}
\end{align*}
where \(\mathbf{X}=\left\{\mathbf{x}_{n}\right\}\) and \(\mathbf{Z}=\left\{\mathbf{z}_{n}\right\}\). Next we take the expectation of the complete-data \(\log\) likelihood with respect to the posterior distribution of the latent variables to give
\begin{align*}
\begin{align*}
& \mathbb{E}_{\mathbf{Z}} {[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\pi})]=\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left\{\ln \pi_{k}\right.} \\
&\left.\quad+\sum_{i=1}^{D}\left[x_{n i} \ln \mu_{k i}+\left(1-x_{n i}\right) \ln \left(1-\mu_{k i}\right)\right]\right\}
\end{align*} \tag{9.55}
\end{align*}
where \(\gamma\left(z_{n k}\right)=\mathbb{E}\left[z_{n k}\right]\) is the posterior probability, or responsibility, of component \(k\) given data point \(\mathbf{x}_{n}\). In the \(\mathrm{E}\) step, these responsibilities are evaluated using Bayes' theorem, which takes the form
\begin{align*}
\begin{align*}
\gamma\left(z_{n k}\right)=\mathbb{E}\left[z_{n k}\right] & =\frac{\sum_{z_{n k}} z_{n k}\left[\pi_{k} p\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}\right)\right]^{z_{n k}}}{\sum_{z_{n j}}\left[\pi_{j} p\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{j}\right)\right]^{z_{n j}}} \\
& =\frac{\pi_{k} p\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} p\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{j}\right)}
\end{align*} \tag{9.56}
\end{align*}
If we consider the sum over \(n\) in (9.55), we see that the responsibilities enter only through two terms, which can be written as
\begin{align*}
\begin{align*}
N_{k} & =\sum_{n=1}^{N} \gamma\left(z_{n k}\right)  \tag{9.57}\\
\overline{\mathbf{x}}_{k} & =\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right) \mathbf{x}_{n}
\end{align*} \tag{9.58}
\end{align*}
where \(N_{k}\) is the effective number of data points associated with component \(k\). In the M step, we maximize the expected complete-data log likelihood with respect to the parameters \(\boldsymbol{\mu}_{k}\) and \(\boldsymbol{\pi}\). If we set the derivative of (9.55) with respect to \(\boldsymbol{\mu}_{k}\) equal to zero and rearrange the terms, we obtain
\begin{align*}
\boldsymbol{\mu}_{k}=\overline{\mathbf{x}}_{k} \tag{9.59}
\end{align*}
We see that this sets the mean of component \(k\) equal to a weighted mean of the data, with weighting coefficients given by the responsibilities that component \(k\) takes for data points. For the maximization with respect to \(\pi_{k}\), we need to introduce a Lagrange multiplier to enforce the constraint \(\sum_{k} \pi_{k}=1\). Following analogous steps to those used for the mixture of Gaussians, we then obtain
\begin{align*}
\pi_{k}=\frac{N_{k}}{N} \tag{9.60}
\end{align*}
which represents the intuitively reasonable result that the mixing coefficient for component \(k\) is given by the effective fraction of points in the data set explained by that component.
Note that in contrast to the mixture of Gaussians, there are no singularities in which the likelihood function goes to infinity. This can be seen by noting that the likelihood function is bounded above because \(0 \leqslant p\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}\right) \leqslant 1\). There exist singularities at which the likelihood function goes to zero, but these will not be found by EM provided it is not initialized to a pathological starting point, because the EM algorithm always increases the value of the likelihood function, until a local maximum is found. We illustrate the Bernoulli mixture model in Figure 9.10 by using it to model handwritten digits. Here the digit images have been turned into binary vectors by setting all elements whose values exceed 0.5 to 1 and setting the remaining elements to 0 . We now fit a data set of \(N=600\) such digits, comprising the digits ' 2 ',' 3 ', and' 4 ', with a mixture of \(K=3\) Bernoulli distributions by running 10 iterations of the EM algorithm. The mixing coefficients were initialized to \(\pi_{k}=1 / K\), and the parameters \(\mu_{k j}\) were set to random values chosen uniformly in the range \((0.25,0.75)\) and then normalized to satisfy the constraint that \(\sum_{j} \mu_{k j}=1\). We see that a mixture of 3 Bernoulli distributions is able to find the three clusters in the data set corresponding to the different digits.
The conjugate prior for the parameters of a Bernoulli distribution is given by the beta distribution, and we have seen that a beta prior is equivalent to introducing additional effective observations of \(\mathbf{x}\). We can similarly introduce priors into the Bernoulli mixture model, and use EM to maximize the posterior probability distributions.
Figure 9.10 Illustration of the Bernoulli mixture model in which the top row shows examples from the digits data set after converting the pixel values from grey scale to binary using a threshold of 0.5 . On the bottom row the first three images show the parameters \(\mu_{k i}\) for each of the three components in the mixture model. As a comparison, we also fit the same data set using a single multivariate Bernoulli distribution, again using maximum likelihood. This amounts to simply averaging the counts in each pixel and is shown by the right-most image on the bottom row.
It is straightforward to extend the analysis of Bernoulli mixtures to the case of multinomial binary variables having \(M>2\) states by making use of the discrete distribution (2.26). Again, we can introduce Dirichlet priors over the model parameters if desired.
* PH 354: Computational Physics
** Machine Representation, Precision, and Errors
:LOGBOOK:
CLOCK: [2024-06-07 Fri 18:02]--[2024-06-07 Fri 22:03] =>  4:01
CLOCK: [2024-06-07 Fri 14:09]--[2024-06-07 Fri 15:43] =>  1:34
CLOCK: [2024-06-07 Fri 12:41]--[2024-06-07 Fri 13:29] =>  0:48
CLOCK: [2024-06-07 Fri 12:25]--[2024-06-07 Fri 12:38] =>  0:13
:END:
*** Machine representation
A computer represents numbers in a binary form. Hence, every computer has a limit how small/large a number can be.
#+NAME: Word
#+begin_definition latex
In computing, a word is the natural unit of data used by a particular processor design. A word is a fixed-sized datum handled as a unit by the instruction set or the hardware of the processor.
#+end_definition
#+NAME: Word length
#+begin_definition latex
The word length is the size of a word (in bits or bytes).
#+end_definition
+ The majority of the registers in a processor are usually word-sized.
+ The largest datum that can be transferred to and from the working memory in a single operation is a word in many (not all) architectures.
+ The largest possible address size, used to designate a location in memory, is typically a word length.
+ Most common architecture have a word length of 4 bytes (32 bits) or 8 bytes (64 bits).
**** Integer representation
- Integers are represented exactly on a computer.
- If using /fixed-precision arithmetic/, the range of a integer depends /word length/ of the processor and the /type/ of the integer as defined by the programming language:
  + 32 bit signed :: The range is from \( - 2^{31} + 1 \) to \( 2^{31} - 1 \).
  + 32 bit unsigned :: The range is from \( - 2^{32} + 1 \) to \( 2^{32} - 1 \).
  + 64 bit signed :: The range is from \( - 2^{63} + 1 \) to \( 2^{63} - 1 \).
  + 64 bit unsigned :: The range is from \( - 2^{64} + 1 \) to \( 2^{64} - 1 \).
- If using /arbitrary-precision arithmetic/, the range of an integer depends only the memory of the host system. Arbitrary-precision arithmetic is available out of the box in languages like Lisp, Python, Perl, Haskell, Ruby and Raku.
**** Floating-point representation
+ To recover a number from its floating-point representation use the formula
  \[
  \frac{s}{b^{(p-1)}} \times b^{e}
  \]
where \( s \) is the /significand/, \( p \) is the /precision/ (the number of digits in the significand), \( b \) is the /base/, and \( e \) is the /exponent/.
***** Single precision
#+begin_src latex :file ~/.local/images/binary32.png :results file graphics
\begin{tikzpicture}[scale=0.5, transform shape]
    % Define font sizes
    \newcommand{\largefont}{\fontsize{20}{22}\selectfont}
    % Draw Sign bit
    \draw[fill=gray!20] (2,0) rectangle (2.5,0.5);
    \node at (2.25, 1.5) {\largefont Sign};
    % Draw Exponent bits
    \foreach \i in {1,...,8}
        \draw[fill=green!20] (5.0 + \i/2,0) rectangle (5.0 + \i/2+0.5,0.5);
    \node at (7.5, 1.5) {\largefont Exponent};
    % Draw Mantissa bits
    \foreach \i in {9,...,31}
        \draw[fill=red!20] (8.0 + \i/2,0) rectangle (8.0 + \i/2+0.5,0.5);
    \node at (18.0, 1.5) {\largefont Significand};
    % Label bit sections
    \node[below] at (2.25, -0.5) {\largefont 1 bit};
    \node[below] at (7.5, -0.5) {\largefont 8 bits};
    \node[below] at (18.0, -0.5) {\largefont 23 bits};
\end{tikzpicture}
#+end_src
#+RESULTS:
[[file:~/.local/images/binary32.png]]
+ Range of exponent :: \([-127,\,127]\) (\(2^{127} \sim 10^{+38}\))
+ Single precision :: \(6-7\) decimal places (\(1 / 2^{23} \sim 10^{-7}\))
+ Range max :: \(\pm 3.4 \times 10^{38}\).
+ Range min :: \(\pm 1.4 \times 10^{-45}\).
#+begin_src latex :file ~/.local/images/binary32-overflow-underflow.png :results file graphics
\begin{tikzpicture}
    % Draw the number line
    \draw[thick] (-8,0) -- (8,0);
    % Overflow and Underflow regions
    \fill[red!20] (-8, -0.5) rectangle (-5.5, 0.5);
    \fill[red!20] (5.5, -0.5) rectangle (8, 0.5);
    \fill[red!20] (-1, 0.5) rectangle (1, 1.5);
    % Overflow and Underflow labels
    \node[text=black] at (-6.75, 0) {\large\textbf{Overflow}};
    \node[text=black] at (6.75, 0) {\large\textbf{Overflow}};
    \node[text=black] at (0, 1.0) {\large\textbf{Underflow}};
    % Zero mark
    \draw[thick] (0,-0.2) -- (0,0.2);
    \node[below] at (0, -0.5) {\large 0};
    % Exponent labels in green boxes
    \node[fill=green!20, inner sep=2pt, text=black] at (-5.5, -1) {\large \(-10^{38}\)};
    \node[fill=green!20, inner sep=2pt, text=black] at (-1, -1) {\large \(-10^{-45}\)};
    \node[fill=green!20, inner sep=2pt, text=black] at (1, -1) {\large \(10^{-45}\)};
    \node[fill=green!20, inner sep=2pt, text=black] at (5.5, -1) {\large \(10^{38}\)};
    % Draw lines to separate regions
    \draw[thick] (-5.5, -0.5) -- (-5.5, 0.5);
    \draw[thick] (5.5, -0.5) -- (5.5, 0.5);
    \draw[thick] (-1, -0.5) -- (-1, 0.5);
    \draw[thick] (1, -0.5) -- (1, 0.5);
\end{tikzpicture}
#+end_src
#+RESULTS:
[[file:~/.local/images/binary32-overflow-underflow.png]]
+ Example: \(152,853.5047\)
  + Significand :: 1,528,535,047
  + Precision :: 10
  + Base :: 10
  + Exponent :: 5
  + In scientific notation: \(1.528535047 \times 10^{5}\).
+ The numbers that can be represented are *not* /uniformly spaced/; the difference between two consecutive numbers that can be represented varies with their exponent.
#+ATTR_HTML: :width 800px
[[file:~/.local/images/non-compactness-fp-representation.png]]
#+CAPTION: Single-precision floating-point numbers on a number line: the green lines mark values that can be represented.
#+ATTR_HTML: :width 800px
[[file:~/.local/images/non-compactness-fp-representation-2.png]]
#+CAPTION: Augmented version above showing both signs of representable values
****** Example: Bohr's radius
+ In it common to encounter problems in which naive application of single precision floating-point arithmetic is inadequate. Consider the formula for Bohr's radius
\begin{align*}
a_0=\frac{4 \pi \epsilon_0 \hbar^2}{m_e e^2}
\end{align*}
where
\begin{align*}
\begin{gathered}
\epsilon_0=8.85 \times 10^{-12} \mathrm{C}^2 / \mathrm{N} / \mathrm{m}^2 \\
\hbar=6.63 \times 10^{-34} / 2 \pi \mathrm{J} \mathrm{s} \\
m_e=9.11 \times 10^{-31} \mathrm{Kg} \\
e=1.60 \times 10^{-19} \mathrm{C}
\end{gathered}
\end{align*}
+ Numerator is: \(1.24 \times 10^{-78}\) and Denominator is: \(2.33 \times 10^{-68}\).
+ To solve the problem of representation, we can:
  + Restructure the equation,
  + Change units - work in atomic units where all these quantites are \(\mathcal{O}(1)\),
  + Increase precision.
***** Double precision
#+begin_src latex :file ~/.local/images/binary64.png :results file graphics
\begin{tikzpicture}[scale=0.5, transform shape]
    % Define font sizes
    \newcommand{\largefont}{\fontsize{20}{22}\selectfont}
    % Draw Sign bit
    \draw[fill=gray!20] (2,0) rectangle (2.5,0.5);
    \node at (2.25, 1.5) {\largefont Sign};
    % Draw Exponent bits
    \foreach \i in {1,...,11}
        \draw[fill=green!20] (5.0 + \i/2,0) rectangle (5.0 + \i/2+0.5,0.5);
    \node at (8.0, 1.5) {\largefont Exponent};
    % Draw Mantissa bits
    \foreach \i in {9,...,52}
        \draw[fill=red!20] (8.0 + \i/2,0) rectangle (8.0 + \i/2+0.5,0.5);
    \node at (25.0, 1.5) {\largefont Significand};
    % Label bit sections
    \node[below] at (2.25, -0.5) {\largefont 1 bit};
    \node[below] at (8.0, -0.5) {\largefont 11 bits};
    \node[below] at (25.0, -0.5) {\largefont 52 bits};
\end{tikzpicture}
#+end_src
#+RESULTS:
[[file:~/.local/images/binary64.png]]
+ Range of exponent :: \([-1023,\,1023]\) (\(2^{1023} \sim 10^{+308}\))
+ Double precision :: \(15-16\) decimal places (\(1 / 2^{52} \sim 1.2 \times 10^{-15}\))
+ Range max :: \(\pm 1.78 \times 10^{308}\)
+ Range min :: \(\pm 4.94 \times 10^{-324}\)
#+begin_src latex :file ~/.local/images/binary64-overflow-underflow.png :results file graphics
\begin{tikzpicture}
    % Draw the number line
    \draw[thick] (-8,0) -- (8,0);
    % Overflow and Underflow regions
    \fill[red!20] (-8, -0.5) rectangle (-5.5, 0.5);
    \fill[red!20] (5.5, -0.5) rectangle (8, 0.5);
    \fill[red!20] (-1, 0.5) rectangle (1, 1.5);
    % Overflow and Underflow labels
    \node[text=black] at (-6.75, 0) {\large\textbf{Overflow}};
    \node[text=black] at (6.75, 0) {\large\textbf{Overflow}};
    \node[text=black] at (0, 1.0) {\large\textbf{Underflow}};
    % Zero mark
    \draw[thick] (0,-0.2) -- (0,0.2);
    \node[below] at (0, -0.5) {\large 0};
    % Exponent labels in green boxes
    \node[fill=green!20, inner sep=2pt, text=black] at (-5.5, -1) {\large \(-10^{308}\)};
    \node[fill=green!20, inner sep=2pt, text=black] at (-1, -1) {\large \(-10^{-324}\)};
    \node[fill=green!20, inner sep=2pt, text=black] at (1, -1) {\large \(10^{-324}\)};
    \node[fill=green!20, inner sep=2pt, text=black] at (5.5, -1) {\large \(10^{308}\)};
    % Draw lines to separate regions
    \draw[thick] (-5.5, -0.5) -- (-5.5, 0.5);
    \draw[thick] (5.5, -0.5) -- (5.5, 0.5);
    \draw[thick] (-1, -0.5) -- (-1, 0.5);
    \draw[thick] (1, -0.5) -- (1, 0.5);
\end{tikzpicture}
#+end_src
#+RESULTS:
[[file:~/.local/images/binary64-overflow-underflow.png]]
+ In python, negative overflow set to -inf, positive to +inf, and underflow is set to 0.
*** Machine precision
#+NAME: Machine precision
#+begin_definition latex
The machine epsilon, denoted \(\epsilon_{\text {mach }}\), is the smallest number \(\epsilon\) such that \(f l(1+\epsilon)>1\). Thus, \(f l(1+\delta)=f l(1)=1\) whenever \(|\delta|<\epsilon_{\text {mach }}\).
It is the smallest difference between two numbers that the computer recognizes.
#+end_definition
#+NAME: Machine epsilon
#+begin_definition latex
The machine epsilon, denoted \(\epsilon_{\text {mach}}\), is the maximum possible absolute relative error in representing a nonzero real number \(x\) in a floating point number system.
\begin{align*}
\epsilon_{\text {mach }}=\max _x \frac{|x-f l(x)|}{|x|}
\end{align*}
#+end_definition
+ Machine epsilon can be used to measure the level of round-off error in the floating-point number system.
#+begin_src python :results output
import numpy as np
def machineEpsilon(func=float):
    machine_epsilon = func(1)
    while func(1) + func(machine_epsilon) != func(1):
        machine_epsilon_last = machine_epsilon
        machine_epsilon = func(machine_epsilon) / func(2)
    return machine_epsilon_last
# Calculate machine epsilon for different float types
float_eps = machineEpsilon(float)
np_float16_eps = machineEpsilon(np.float16)
np_float32_eps = machineEpsilon(np.float32)
np_float64_eps = machineEpsilon(np.float64)
np_longdouble_eps = machineEpsilon(np.longdouble)
# Print the results with appropriate precision
print(f"float: {float_eps:.16g}")
print(f"np.float16: {np_float16_eps:.4g}")
print(f"np.float32: {np_float32_eps:.7g}")
print(f"np.float64: {np_float64_eps:.16g}")
print(f"np.longdouble: {np_longdouble_eps:.19g}")
#+end_src
#+RESULTS:
: float: 2.220446049250313e-16
: np.float16: 0.0009766
: np.float32: 1.192093e-07
: np.float64: 2.220446049250313e-16
: np.longdouble: 1.084202172485504434e-19
*** Errors
**** Round-off errors
#+NAME: Round-off error
#+begin_definition latex
The difference between the true value of the number and its value on the computer is called round-off error.
#+end_definition
+ Any floating-point number with significant figures greater than the precision of its representation is subject to round-off errors.
***** Round-off error on a number behaves similarly to measurement error in a laboratory experiment
#+NAME: Error constant
#+begin_definition latex
Assume that the error is a uniformly distributed random number with standard deviation \(\sigma=C x\), where \( C \) is a constant that depends on the floating-point representation under use.
#+end_definition
+ Typically, \( C \simeq b^{-p} \) where \( b \) is the base and \( p \) is the precision. Using base-10, \(C \simeq 10^{-16}\) for double precision floating-point representation.
+ When quoting the error on a calculation we typically give the value of the standard deviation \(\sigma\). (We can't give the value of the error \(\epsilon\) itself, since we don't know it-if we did, then we could calculate \(x+\epsilon\) and recover the exact value for the quantity of interest, so there would in effect be no error in the calculation at all.)
+ In many ways the rounding error on a number behaves similarly to measurement error in a laboratory experiment, and the rules for combining errors are the same.
****** Combining rounding error (addition and subtraction)
Let \(x_1\) and \(x_2\) be random variables with standard deviations \(\sigma_1\) and \(\sigma_2\) respectively. The variance \(\sigma^2\) of the random variable \( x_1 + x_2 \) or \( x_1 - x_2 \) is given by
\begin{align*}
\sigma^2=\sigma_1^2+\sigma_2^2 \Longrightarrow \sigma=\sqrt{\sigma_1^2+\sigma_2^2}
\end{align*}
Since \(\sigma_1=C x_1\) and \(\sigma_2=C x_2\)
\begin{align*}
\sigma=\sqrt{C^2 x_1^2+C^2 x_2^2}=C \sqrt{x_1^2+x_2^2}.
\end{align*}
The result trivially generalizes to the sum of \(N\) numbers \(x_1 \ldots x_N\) with errors having standard deviation \(\sigma_i=C x_i\):
\begin{align*}
\sigma^2=\sum_{i=1}^N \sigma_i^2=\sum_{i=1}^N C^2 x_i^2=C^2 N \overline{x^2}
\end{align*}
where \(\overline{x^2}\) is the mean-square value of \(x\). Thus the standard deviation on the final result is
\begin{align*}
\sigma=C \sqrt{N \overline{x^2}}.
\end{align*}
The standard deviation behaves as \( \sigma \sim \sqrt{N} \).
The relative error on \(\sum_i x_i\) is given by
\begin{align*}
\frac{\sigma}{\sum_i x_i}=\frac{C \sqrt{N \overline{x^2}}}{N \bar{x}}=\frac{C}{\sqrt{N}} \frac{\sqrt{\overline{x^2}}}{\bar{x}},
\end{align*}
where \(\bar{x}=N^{-1} \sum_i x_i\) is the mean value of \(x\).
The relative error behaves as \( \sigma \sim \frac{1}{\sqrt{N}} \).
****** Combining rounding error (multiplication and division)
Let \(x_1\) and \(x_2\) be random variables with standard deviations \(\sigma_1\) and \(\sigma_2\) respectively. Suppose that \( x_2 \neq 0 \). The variance \(\sigma^2\) of the random variable \( x_1 \cdot x_2 \) or \( x_1 / x_2 \) is given by
\begin{align*}
\frac{\sigma^2}{x^2}=\frac{\sigma_1^2}{x_1^2}+\frac{\sigma_2^2}{x_2^2} .
\end{align*}
Since \(\sigma_1=C x_1\) and \(\sigma_2=C x_2\)
\begin{align*}
\sigma= \sqrt{x^2 \, (C^2 + C^2)} = \sqrt{2} \, C \, x.
\end{align*}
***** Round-off error leads to loss of significant digits
+ If repeated arithmetic operations are performed on two numbers represented by double precision floating-points and if a number of these operations introduce rounding errors, then there is an erosion of significant figures, the least significant digits being eroded first.
+ The loss of significant digits can occur in so many ways that it defies useful classification and lack systematic cures!
+ The average accumulated multiplication error after \(\mathrm{N}\) multiplications is \(\sqrt{N} \epsilon_{\text{mach}}\), where \(  \)\( \epsilon_{\text{mach}} \) is the machine epsilon.
***** A tiny round-off error can grow rapidly if the algorithm being used is not numerically stable
+ Sometimes the problem is not round-off errors but numerical stability of the algorithm. Even tiny round-off errors grow rapidly if algorithm is not numerically stable.
****** Example 1
Consider
\begin{align*}
& x=1000000000000000.0 \\
& y=1000000000000001.2345678901234
\end{align*}
The true difference \( y-x \) is \(1.2345678901234\). Since the number of significant digits in \( y \) is \(29 \) but the precision of double precision floating-point representation is \( 16 \), the computer rounds \( y \) so that
\begin{align*}
& x=1000000000000000.0 \\
& y=1000000000000001.2
\end{align*}
The value of \( y - x \) on the computer is thus \(y-x=1.2\). 
So instead of 16 significant figures we only have 2 significant figures.
****** Example 2
+ If the difference between two numbers is very small, comparable with the error on the numbers, i.e., with the accuracy of the computer, then the fractional error can become large, leading to an erosion of significant figures. Here is an example showing a dramatic erosion. Consider the numbers
\begin{align*}
x=1, \quad y=1+10^{-14} \sqrt{2}.
\end{align*}
Trivially we see that
\begin{align*}
10^{14} \, (y-x)=\sqrt{2}.
\end{align*}
#+begin_src python :results output
from math import sqrt
x = 1.0
y = 1.0 + (1e-14)*sqrt(2)
print((1e14)*(y-x))
print(sqrt(2))
#+end_src
#+RESULTS:
: 1.4210854715202004
: 1.4142135623730951
+ Only the first digit to the right of the decimal point is significant. The rest is garbage.
****** Example 3
Here's an example that illustrates how numerical instability can introduce round-off error.
Suppose we want to calculate the series \(a_n=\phi^n n=0,1,2 \ldots\) where \(\phi\) is the golden ratio:
\begin{align*}
\phi=\frac{\sqrt{5}-1}{2}
\end{align*}
We will compare two distinct methods:
+ Method 1
\begin{align*}
a_n = a_{n-1} \cdot \phi, \quad a_0 = 1.
\end{align*}
  + Requires only multiplication.
  + If the initial \(\phi=\phi_c+\epsilon_m\), the error in \(\phi^n\) is \(\sqrt{n} \epsilon_m\).
+ Method 2
\begin{align*}
a_n = a_{n-2} - a_{n-1}, \quad a_0 = 1, \, a_1 = \phi.
\end{align*}
  + Relies on \(\phi\) being the root of the qudratic equation: \(\phi^2+\phi-1=0\).
  + If you iterate the above equation, you can get the algorithm. If you simplify each term, you see Fibonacci numbers: \(f_n\).
- Then, \(\phi^n=(-1)^{n-1} f_n \phi+(-1)^n f_{n-1}\).
- Now: \(\phi^n=(-1)^{n-1} f_n\left(\phi_c+\epsilon_m\right)+(-1)^n f_{n-1}\)
- Error in \(\phi^n\) is \(f_n \epsilon_m\).
- When \(\phi_c^n \sim f_n \epsilon_m\), the algorithm fails!
Thus Method 1 is stable, while Method 2 is not.
**** Truncation errors
#+NAME: Truncation error
#+begin_definition latex
An error caused by approximating a mathematical process.
#+end_definition
+ Truncation error is better controlled, e.g., by choosing a more accurate method, than round off error which is set by the precision of the floating-point representation being used.
***** Infinite series
A summation series for \(e^x\) is given by an infinite series such as
\begin{align*}
e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+\cdots
\end{align*}
In reality, we can only use a finite number of these terms as it would take an infinite amount of computational time to make use of all of them. So let's suppose we use only three terms of the series, then
\begin{align*}
e^x \approx 1+x+\frac{x^2}{2!}
\end{align*}
In this case, the truncation error is \(\frac{x^3}{3!}+\frac{x^4}{4!}+\cdots\)
****** Example
*Given the following infinite series, find the truncation error for* \(x=0.75\) *if only the first three terms of the series are used.*
\begin{align*}
S=1+x+x^2+x^3+\cdots, \quad|x|<1
\end{align*}
Using only first three terms of the series gives
\begin{align*}
S_3 & =(1+x+x^2)_{x=0.75} \\
& =1+0.75+(0.75)^2 \\
& =2.3125
\end{align*}
The sum of an infinite geometrical series
\begin{align*}
S=a+a r+a r^2+a r^3+\cdots, r<1
\end{align*}
is given by
\begin{align*}
S=\frac{a}{1-r}
\end{align*}
For our series, \(a=1\) and \(r=0.75\), to give
\begin{align*}
S=\frac{1}{1-0.75}=4
\end{align*}
The truncation error hence is
\begin{align*}
\mathrm{TE}=4-2.3125=1.6875
\end{align*}
***** Differentiation
+ Consider the approximation
\begin{align*}
f^{\prime}(x)=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h} \approx \frac{f(x+h)-f(x)}{h} \quad h > 0
\end{align*}
+ The error caused by choosing \(h\) to be finite is a truncation error in the mathematical process of differentiation.
****** Example
*Find the truncation in calculating the first derivative of* \(f(x)=5 x^3\) at \(x=7\) *using a step size of* \(h=0.25\).
The first derivative of \(f(x)=5 x^3\) is
\begin{align*}
f^{\prime}(x) & =15 x^2,
\end{align*}
and at \( x = 7 \),
\begin{align*}
f^{\prime}(7) & =735.
\end{align*}
The approximate value is given by
\begin{align*}
f^{\prime}(7)=\frac{f(7+0.25)-f(7)}{0.25}=761.5625
\end{align*}
The truncation error is hence
\begin{align*}
\mathrm{TE}=735-761.5625=-26.5625
\end{align*}
***** Integration
+ Consider the approximation
\begin{align*}
\int_a^b f(x) d x= \lim_{N \to \infty} \sum_{i=1}^N f\left(x_i^*\right) \Delta x_i \approx \sum_{i=1}^N f\left(x_i^*\right) \Delta x_i, \quad N < \infty
\end{align*}
where \(\Delta x_i=x_i-x_{i-1} = (b-a)/ N\) and \(x_i^* \in\left[x_{i-1}, \, x_i\right]\).
+ The error caused by choosing \(N\) to be finite is a truncation error in the mathematical process of integration.
+ Consider the approximation
\begin{align*}
\int_0^{\infty} f(x) d x = \lim_{L \to \infty} \int_0^{L} f(x) d x  \approx \int_0^L f(x) d x, \quad L < \infty
\end{align*}
+ The error caused by choosing \(L\) to be finite is a truncation error in the mathematical process of integration.
+ In some cases, a change of variables can help avoid this kind of truncation error provided it does not introduce any singularities.
+ In some cases the "tails" can be evaluated analytically which also helps avoid truncation errors of this kind (see Example 2).
+ Note that this approximation (whether a change of variables or analytic evaluation of the tails eliminates the truncation error or not) still requires the evaluation of \( \int_0^L f(x) d x \) which will introduce truncation errors due the approximation
\begin{align*}
\int_a^b f(x) d x= \lim_{N \to \infty} \sum_{i=1}^N f\left(x_i^*\right) \Delta x_i \approx \sum_{i=1}^N f\left(x_i^*\right) \Delta x_i, \quad N < \infty.
\end{align*}
****** Example 1
*For the integral*
\begin{align*}
\int_3^9 x^2 d x
\end{align*}
*find the truncation error if a two-segment left-hand Riemann sum is used with equal width of segments.*
We have the exact value as
\begin{align*}
\int_3^9 x^2 d x & =\left[\frac{x^3}{3}\right]_3^9 =\left[\frac{9^3-3^3}{3}\right] =234.
\end{align*}
Using two rectangles of equal width to approximate the area under the curve, the approximate value of the integral
\begin{align*}
\int_3^9 x^2 d x &= (x^2)\vert_{x=3} (6-3) + (x^2)\vert_{x=6} (9-6) \\
& =(3^2) 3 + (6^2) 3 \\
& =27+108 =135.
\end{align*}
The truncation error is hence
\begin{align*}
\mathrm{TE}=234 - 135 = 99.
\end{align*}
****** Example 2
 *For the integral*
\begin{align*}
\int_0^{\infty} \frac{\sqrt{x}}{x^2+1}=\int_0^L \frac{\sqrt{x}}{x^2+1}+\int_L^{\infty} \frac{\sqrt{x}}{x^2+1} \approx \int_0^L \frac{\sqrt{x}}{x^2+1}
\end{align*}
*show that truncation error introduced by dropping* \( \int_L^{\infty} \frac{\sqrt{x}}{x^2+1} \) *can be made to vanish for* \( L > 1 \).
For \(L>1\) we have
\begin{align*}
\int_L^{\infty} \frac{\sqrt{x}}{x^2+1} \approx \int_L^{\infty} \frac{1}{x^{\frac{3}{2}}}=\frac{2}{\sqrt{L}},
\end{align*}
so we can write
\begin{align*}
\int_0^{\infty} \frac{\sqrt{x}}{x^2+1}=\int_0^L \frac{\sqrt{x}}{x^2+1}+\int_L^{\infty} \frac{\sqrt{x}}{x^2+1} = \int_0^L \frac{\sqrt{x}}{x^2+1} + \frac{2}{\sqrt{L}}.
\end{align*}
** Roots of Equations
:LOGBOOK:
CLOCK: [2024-06-24 Mon 01:00]--[2024-06-24 Mon 03:53] =>  2:53
CLOCK: [2024-06-23 Sun 23:43]--[2024-06-24 Mon 00:11] =>  0:28
CLOCK: [2024-06-23 Sun 20:06]--[2024-06-23 Sun 23:26] =>  3:20
CLOCK: [2024-06-23 Sun 17:52]--[2024-06-23 Sun 19:44] =>  1:52
CLOCK: [2024-06-07 Fri 22:09]--[2024-06-07 Fri 22:32] =>  0:23
:END:
*** Introduction
#+begin_problem
Given a continuous non-linear function \(f(x)\), find the value \(x=c\) such that \(f(c)=0\).
#+end_problem
#+NAME: Roots of non-linear functions
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The non-linear equation \(f(x)=0\) may be an algebraic equation (roots of polynomials) or a transcendental equation. It may have
1) no roots at all,
2) complex root(s) but no real roots,
3) a single real root, two or more simple real roots,
4) two or more multiple roots,
5) some combination of simple root(s) and multiple root(s), etc.
#+end_remark
#+NAME: General approach
#+ATTR_LATEX: :environment remark
#+begin_remark latex
In general, non-linear equations are solved as follows:
1) Bound the root to an interval \( [a,~b] \) so that \( f(a) \cdot f(b) < 0 \).
2) Start with an initial guess - an approximate root - \( c \in [a,~b] \).
3) Set off an iterative process that updates \( a \), \( b \), and \( c \) to refine the solution \( c \) to a specified tolerance \( \epsilon \).
#+end_remark
#+NAME: Bounding the solution
#+ATTR_LATEX: :environment remark
#+begin_remark latex
To find a bound for the solution is to find an interval \( [a,~b] \) such that
1) \( f \) is continuous in this interval,
2) \( f(a) \cdot f(b) < 0 \).
Some guidelines:
1) Graph the functions to get an idea of the solution and get an approximate solution. No seriously, graph it. In case of multiple solutions it also ensures that you have the solution that you want.
2) Do an incremental search where you start with different guesses and search their neighborhoods.
3) Use your past experience with the problem or a similar one.
4) Obtain a simplified approximate model and try any of the above steps.
#+end_remark
#+NAME: Convergence
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The iteration must be stopped at some point so it one must specify a tolerance \( \epsilon \) with a value that depends on whether one wishes to use the absolute error \( \lvert x_{i+1} - x_{i} \rvert \) or the relative error \( \lvert (x_{i+1} - x_{i})/x_{i+1} \rvert \) to the tolerance. Here \( i \) indexes the steps in the iteration. When \( x_{i+1} \) is such that one of the value of the chosen kind of error falls below the specified tolerance \( \epsilon \), the iteration has converged and \( x^{\ast} = x_{i+1} \) is declared the root.
#+end_remark
#+NAME: Approximate root
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The better the initial guess \( c \), the better our chances of converging to the solution and fewer the number of iterations required to converge to the solution. Therefore, it is crucial to obtain a good approximate root.
#+end_remark
*** Closed-domain method
Closed-domain method have the following properties:
1) starts by bounding the solution in an interval \( [a,~b] \),
2) imposes the following restriction on the approximate root \( c \): at any iteration, \( c \in [a,~b] \),
3) can be used to find repeated roots of with odd multiplicity of a polynomial function because the polynomial does change sign at such roots,
4) convergence is guaranteed provided there is a single root in \( [a,~b] \),
5) *cannot* be used when the interval has more than one roots,
6) *cannot* be used to find complex roots,
7) *cannot* be used to find repeated roots of with even multiplicity of a polynomial function because the polynomial does *not* change sign at such roots,
8) does not generalize to multiple variables.
**** Bisection method
#+NAME: Bisection method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Let the function \( f \) be continuous over the interval \( [a,~b] \). Suppose \( f(a) \cdot f(b) < 0 \). We must have \( x^{\ast} \in [a,~b] \) such that \( f(x^{\ast}) = 0 \). To find \( x^{\ast} \) up to a tolerance \( \epsilon \), start by defining \( c = (a+b) / 2 \), and start an iteration that
1) modifies \( a \) or \( b \) as
\[
f(a) \cdot f(c) \begin{cases}
<0 & \quad b \leftarrow c \\
> 0 & \quad a \leftarrow c.
\end{cases}
\]
2) recalculates \( c = (a+b)/2 \) using the new \( a \) and \( b \).
The iteration stops when any of the following two conditions are met:
1) \( c \) is such that \( f(a) \cdot f(c) = 0 \). The root is \( x^{\ast} = c \),
2) \( a \) and \( b \) are such that \( \lvert b - a \rvert < \epsilon \). The root is \( x^{\ast} = (a+b)/2 \).
After \( n \) iterations, interval \( [a,~b] \) has a size smaller by a factor of \( 2^{-n} \) relative to the original size.
#+end_proposition
#+NAME: Bisection method
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Like any closed-domain method, convergence of the bisection method iteration is guaranteed. However, the method does not use any information about the function's behavior. Relative to methods that do use information about the function's behavior, the convergence of bisection method is slow.
#+end_remark
#+NAME: Bisection search
#+begin_src python :results output
from math import fabs, cos
import numpy as np

def bisect(a, b, eps, func):
    if (func(a) * func(b)) > 0:
        return 9999999, 0  # No root found, return large number and 0 iterations
    
    xl = a
    xr = b
    iterations = 0  # Initialize iteration counter

    while (abs(xl - xr) > eps):
        xm = (xl + xr) / 2.0
        if func(xl) * func(xm) < 0:
            xr = xm
        else:
            xl = xm
        iterations += 1  # Increment iteration counter each loop

    return xm, iterations

def fx(x):
    return x - cos(x)

root, iter_count = bisect(0., 1.0, 1.0e-6, fx)
print(f"Root: {np.around(root, 6)}, Iterations: {iter_count}")
#+end_src

#+RESULTS: Bisection search
: Root: 0.739085, Iterations: 20

**** False position method
#+NAME: False Position Method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Let the function \( f \) be continuous over the interval \( [a,~b] \). Suppose \( f(a) \cdot f(b) < 0 \). We must have \( x^{\ast} \in [a,~b] \) such that \( f(x^{\ast}) = 0 \). To find \( x^{\ast} \) up to a tolerance \( \epsilon \), start by defining \( c = (b \cdot f(a) + a \cdot f(b)) / (f(b) - f(a)) \), and start an iteration that
1) modifies \( a \) or \( b \) as
\[
f(a) \cdot f(c) \begin{cases}
<0 & \quad b \leftarrow c \\
> 0 & \quad a \leftarrow c.
\end{cases}
\]
2) recalculates \( c = (b \cdot f(a) + a \cdot f(b)) / (f(b) - f(a)) \) using the new \( a \) and \( b \).
The iteration stops when any of the following two conditions are met:
1) \( c \) is such that \( f(a) \cdot f(c) = 0 \). The root is \( x^{\ast} = c \),
2) \( a \) and \( b \) are such that \( \lvert b - a \rvert < \epsilon \). The root is \( x^{\ast} = (a+b)/2 \).
#+end_proposition
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Like any closed-domain method, convergence of the false position method iteration is guaranteed. Furthermore it uses the information about the function's behavior by approximating it with a linear function \( g \) with \( g(a) = f(a) \) \( g(b) = f(b) \) and then defining \( c \) to be the root of \( g \) and therefore \( f \) (obviously the root \( c \) of the linear function \( g \) is not the actual root of the nonlinear function \( f \), thus the name /false position/). It is straightforward to obtain an expression for \( c \):
The slope of the linear function \( \mathrm{D}_{x} g(x) \) is given by:
\[
\mathrm{D}_{x} g(x) = \frac{f(b) - f(a)}{b - a}.
\]
It is also given by:
\[
\mathrm{D}_{x} g(x) = \frac{f(b) - f(c)}{b - c}.
\]
We therefore have
\[
\frac{f(b) - f(a)}{b - a} = \frac{f(b) - f(c)}{b - c}.
\]
Because \( g \) approximates \( f \), we must have \( f(c) = 0 \). Substituting \( f(c) = 0 \) in the above equation and solving for \( c \) we get
\[
c = b - f(b) \frac{b - a}{f(b) - f(a)} = \frac{a f(b) - b f(a)}{f(b) - f(a)}.
\]
The convergence of the false position method is typically faster compared to the bisection method. This is because it uses information about the function in the iterative refinement of the latest approximate root.
#+end_remark
#+NAME: False-position method
#+begin_src python :results output
import numpy as np
from math import fabs, cos

def falseposition(a, b, eps, func):
    if (func(a) * func(b)) > 0:
        return 9999999, 0  # No root found, return large number and 0 iterations
    
    xl = a
    xr = b
    xm = a
    iterations = 0  # Initialize iteration counter

    while (abs(func(xm)) > eps):
        xm = (xl*func(xr) - xr*func(xl)) / (func(xr) - func(xl))
        if func(xl) * func(xm) < 0:
            xr = xm
        else:
            xl = xm
        iterations += 1  # Increment iteration counter each loop

    return xm, iterations

def fx(x):
    return x - cos(x)

root, iter_count = falseposition(0.0, 1.0, 1.0e-6, fx)
print(f"Root: {np.around(root, 6)}, Iterations: {iter_count}")
#+end_src

#+RESULTS: False-position method
: Root: 0.739085, Iterations: 5

*** Open-domain method
Open-domain method have the following properties:
1) starts by bounding the solution in an interval \( [a,~b] \),
2) does *not* impose the following restriction on the approximate root \( c \): at any iteration, \( c \in [a,~b] \).
3) Convergence is *not* guaranteed in general,
4) can be used when the interval has more than one roots,
5) can be used to find complex roots,
6) can be used to find repeated roots of with both odd and even multiplicity of a polynomial function because it does not assume that the polynomial changes its sign sign at such roots,
7) does not generalize to multiple variables,
8) generalizes to multiple variables.
#+NAME: Open-domain methods
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The second properties just means that \( c \notin [a,~b] \) is possible at some stage of the iteration. Clearly, convergence is not guaranteed in general.
#+end_remark
**** Relaxation method
***** Single variable
#+NAME: Relaxation method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Let the function \( g \) be continuous over the interval \( \mathbb{R} \). We want to find  have \( x^{\ast} \in \mathbb{R} \) such that \( g(x^{\ast}) = 0 \). Further suppose that for any \( x \), \( g \) admits a decomposition into the form \( x = f(x) \) where \( f \) is another continuous function over \( \mathbb{R} \). To find \( x^{\ast} \) up to a tolerance \( \epsilon \), start with an approximate root \( x_{0} \) and start an iteration with the following recurrence relation
\[
x_{i+1} = f(x_{i})
\]
The iteration can be stopped when \( x_{i+1} \) and \( x_{i} \) are such that \( \lvert x_{i+1} - x_{i} \rvert < \epsilon \) with the root being \( x^{\ast} = (x_{i+1} + x_{i})/2 \), or a more principled error analysis can inform the stopping criteria.
#+end_proposition
#+NAME: Convergence
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Assume we have \( g(x) \) admits the decomposition \(x=f(x)\) that has a solution at \(x=x^{\ast}\) and focus on a region when \(x\) is close to \(x^{\ast}\). Performing a Taylor expansion, the value \(x^{\prime}\) after an iteration of the method is given in terms of the previous value \(x\) by
\[
x^{\prime}=f(x)=f\left(x^{\ast}\right)+\left(x-x^{\ast}\right) \mathrm{D}_{x} f\left(x^{\ast}\right)+\ldots
\]
But by definition, \(x^{\ast}\) is a solution of the original equation, meaning that \(x^{\ast}=\) \(f\left(x^{\ast}\right)\), so
\[
x^{\prime}-x^{\ast}=\left(x-x^{\ast}\right) \mathrm{D}_{x} f\left(x^{\ast}\right) + \mathcal{O}(x^{2}),
\]
Neglecting \( \mathcal{O} (x^{2}) \) we have
\[
x^{\ast} - x^{\prime} \approx \left(x^{\ast}- x\right) \mathrm{D}_{x} f\left(x^{\ast}\right),
\]
or
\[
\epsilon^{\prime} \approx \epsilon \cdot \mathrm{D}_{x} f\left(x^{\ast}\right).
\]
For convergence we must have \( \lvert \mathrm{D}_{x} f(x^{\ast}) \rvert < 1 \). Suppose that \(\left|\mathrm{D}_{x} f\left(x^{\ast}\right)\right|>1\). If \( f \) is invertible then \( g(x) \) also admits the decomposition \( x = f^{-1} (x) \) for all \( x \). Let \(u=f^{-1}(x)\), so that \( f(u) = x \).
\[
\mathrm{D}_{x} f(u) = \mathrm{D}_{u} x 
\]
When \(x=x^{\ast}\) we have \(u=f^{-1}\left(x^{\ast}\right)=x^{\ast}\) so that \( \mathrm{D}_{x} u = \mathrm{D}_{x} f^{-1} (x^{\ast}) \). But \( (\mathrm{D}_{x} u) (\mathrm{D}_{u} x) = 1 \) so we have \( \mathrm{D}_{x} u = (\mathrm{D}_{u} x)^{-1} \). It follows that
\[
\mathrm{D}_{x} f^{-1} (x^{\ast}) = \frac{1}{\mathrm{D}_{x} f(x^{\ast})}.
\]
Thus the derivative of \(f^{-1}\) at \(x^{\ast}\) is simply the reciprocal of the derivative of \(f\). That means that if the derivative of \(f\) is greater than one then the derivative of \(f^{-1}\) must be less than one. Hence, if the relaxation method fails to converge for \(x=f(x)\) it will succeed for the equivalent form \(x=f^{-1}(x)\). Therefore if \( g \) is such that it admits the decomposition \( x = f(x) \) for all \( x \) where \( f \) is an invertible function, then a appropriately executed fixed point iteration is guaranteed to converge.
#+end_remark
#+NAME: Rate of convergence
#+ATTR_LATEX: :environment remark
#+begin_remark latex
We saw that
\[
\epsilon^{\prime} \approx \epsilon \cdot \mathrm{D}_{x} f\left(x^{\ast}\right)
\]
where \(\epsilon\) is the error on our current estimate of the solution to the equation and \(\epsilon^{\prime}\) is the error on the next estimate.
We thus have
\[
x^{\ast} = x+\epsilon = x^{\prime}+\epsilon^{\prime} = x+\frac{\epsilon^{\prime}}{\mathrm{D}_{x} f\left(x^{\ast}\right)}.
\]
Solving
\[
x^{\prime}+\epsilon^{\prime} = x+\frac{\epsilon^{\prime}}{\mathrm{D}_{x} f\left(x^{\ast}\right)}
\]
for  \(\epsilon^{\prime}\) we have
\begin{align*}
\epsilon^{\prime}=\frac{x-x^{\prime}}{1-1 / \mathrm{D}_{x} f\left(x^{\ast}\right)} \simeq \frac{x-x^{\prime}}{1-1 / \mathrm{D}_{x} f(x)},
\end{align*}
where we have made use of the fact that \(x\) is close to \(x^{\ast}\), so that \(\mathrm{D}_{x} f(x) \simeq \mathrm{D}_{x} f\left(x^{\ast}\right)\). If we know the form of the function \(f(x)\) at \( x \) then we can calculate its derivative and then use this formula to estimate the error \(\epsilon^{\prime}\) on the new value \(x^{\prime}\) for the solution at each step. Then, for instance, we can simply repeat the iteration until the magnitude of this estimated error falls below some target value, ensuring that we get an answer that is as accurate as we want, without wasting any time on additional iterations.
#+end_remark
#+NAME: Rate of convergence
#+begin_remark latex
Sometimes evaluation of \( \mathrm{D}_{x} f(x) \) for \( x \) is impossible/expensive. In such situations we must approximate the derivative. Suppose we have three successive estimates of \(x\), which we denote \(x, x^{\prime}\), and \(x^{\prime \prime}\). We would like to calculate the error on the most recent estimate \(x^{\prime \prime}\), which by Eq. (6.83) is
\begin{align*}
\epsilon^{\prime \prime}=\frac{x^{\prime}-x^{\prime \prime}}{1-1 / \mathrm{D}_{x} f\left(x^{\ast}\right)} \simeq \frac{x^{\prime}-x^{\prime \prime}}{1-1 / \mathrm{D}_{x} f(x)} .
\end{align*}
Now we approximate \(\mathrm{D}_{x} f(x)\) as
\begin{align*}
\mathrm{D}_{x} f(x) \simeq \frac{f(x)-f\left(x^{\prime}\right)}{x-x^{\prime}} .
\end{align*}
But by definition \(x^{\prime}=f(x)\) and \(x^{\prime \prime}=f\left(x^{\prime}\right)\), so
\begin{align*}
\mathrm{D}_{x} f(x) \simeq \frac{x^{\prime}-x^{\prime \prime}}{x-x^{\prime}} .
\end{align*}
Substituting into Eq. (6.84), we then find that the error on the third and most recent of our estimates of the solution is given approximately by
\begin{align*}
\epsilon^{\prime \prime} \simeq \frac{x^{\prime}-x^{\prime \prime}}{1-\left(x-x^{\prime}\right) /\left(x^{\prime}-x^{\prime \prime}\right)}=\frac{\left(x^{\prime}-x^{\prime \prime}\right)^2}{2 x^{\prime}-x-x^{\prime \prime}} .
\end{align*}
Thus, if we keep track of three successive estimates of \(x\) at each stage of the calculation we can estimate the error even when we cannot calculate a derivative of \(f(x)\) directly.
#+end_remark
#+NAME: Relaxation method
#+begin_src python :results output
from math import fabs, cos, sin
import numpy as np

def relax(a, eps, func):
    maxiter = 100
    xi = a
    i = 0
    while (abs(func(xi) - xi) > eps and i < maxiter):
        i = i + 1
        xin = func(xi)
        xi = xin
    return xi, i

def fx(x):
    return cos(x)

root, iter_count = relax(1., 1.0e-6, fx)
print(f"Root: {np.around(root.real, 6)}, Iterations: {iter_count}")
#+end_src

#+RESULTS: Relaxation method
: Root: 0.739085, Iterations: 33

***** Multiple variables
For any number \(N\) of simultaneous equations in \(N\) unknown variables \(x_1,~x_2,~\ldots\) if a the following decomposition is admissible
\begin{align*}
\begin{gathered}
x_1=f_1\left(x_1, \ldots, x_N\right), \\
\vdots \\
x_N=f_N\left(x_1, \ldots, x_N\right),
\end{gathered}
\end{align*}
then we can choose starting values for all the variables and apply the equations repeatedly to find a solution. As with the relaxation method for a single variable the method is not guaranteed to converge to a solution. Depending on the exact form of the equations it will sometimes converge and sometimes not. If it does not converge then it may be possible to rearrange the equations into a different form that will converge, as described for the one-variable case.
**** Over-relaxation method
#+NAME: Over-Relaxation method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Let the function \( g \) be continuous over the interval \( \mathbb{R} \). We want to find  have \( x^{\ast} \in \mathbb{R} \) such that \( g(x^{\ast}) = 0 \). Further suppose that for any \( x \), \( g \) admits a decomposition into the form \( x = f(x) \) where \( f \) is another continuous function over \( \mathbb{R} \). To find \( x^{\ast} \) up to a tolerance \( \epsilon \), start with an approximate root \( x_{0} \) and start an iteration with the following recurrence relation
\[
x_{i+1} = (1+\omega) f(x_{i})-\omega x_{i}
\]
where \( \omega \) is found by trial and starting from \( 0 \) (\( \omega = 0 \) degenerates to a regular relaxation). The iteration can be stopped when \( x_{i+1} \) and \( x_{i} \) are such that \( \lvert x_{i+1} - x_{i} \rvert < \epsilon \) with the root being \( x^{\ast} = (x_{i+1} + x_{i})/2 \), or a more principled error analysis can inform the stopping criteria.
#+end_proposition
#+NAME: Over-relaxation method
#+ATTR_LATEX: :environment remark
#+begin_remark latex
We can rewrite the equation \(x^{\prime}=f(x)\) in the form \(x^{\prime}=x+\Delta x\), where
\[
\Delta x=x^{\prime}-x=f(x)-x.
\]
The overrelaxation method involves iteration of the modified equation
\[
x^{\prime}=x+(1+\omega) \Delta x = x+(1+\omega) (f(x) - x) = (1+\omega) f(x)-\omega x.
\]
If the parameter \(\omega\) is zero, then this is the same as the ordinary relaxation method, but for \(\omega>0\) the method takes the amount \(\Delta x\) by which the value of \(x\) would have been changed and changes by a little more.
For the method to work the value of \(\omega\) must be chosen correctly, although there is some wiggle room - there is an optimal value, but other values close to it will typically also give good results. Unfortunately, there is no general theory that tells us what the optimal value is. Usually it is found by trial and error.
#+end_remark
#+NAME: Convergence
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The overrelaxation method modifies the update step to:
\[
x' = (1 + \omega)f(x) - \omega x.
\]
We assume \(x\) is close to \(x^*\), so we perform a Taylor expansion of \(f(x)\) around \(x^*\)
\[
f(x) \approx f(x^*) + (x - x^*) \mathrm{D}_{x} f(x^*) = x^* + (x - x^*) \mathrm{D}_{x} f(x^*),
\]
where we have used \(f(x^*) = x^*\). Substituting this into the over-relaxation formula:
\[
x' \approx (1 + \omega) \left[ x^* + (x - x^*) \cdot \mathrm{D}_{x} f(x^*) \right] - \omega x,
\]
which has a rewrite
\[
x' \approx x^* + \omega x^* + (1 + \omega)(x - x^*) \mathrm{D}_{x} f(x^*) - \omega x,
\]
or
\[
x' - x^* \approx (1 + \omega)(x - x^*) \mathrm{D}_{x} f(x^*) - \omega (x - x^*),
\]
or
\[
x' - x^* \approx (x - x^*) \left[(1 + \omega) \mathrm{D}_{x} f(x^*) - \omega \right].
\]
Since \(\epsilon \equiv x - x^*\) and \(\epsilon' \equiv x' - x^*\), we have
\[
\epsilon' \approx \epsilon \left[(1 + \omega) \mathrm{D}_{x} f(x^*) - \omega \right].
\]
The over-relaxation iteration converges when \( \lvert \left[(1 + \omega) \mathrm{D}_{x} f(x^*) - \omega \right] \rvert < 1 \).
#+end_remark
#+NAME: Rate of convergence
#+ATTR_LATEX: :environment remark
#+begin_remark latex
We saw that
\[
\epsilon' \approx \epsilon \left[(1 + \omega) \cdot \mathrm{D}_{x} f(x^*) - \omega \right],
\]
and that over-relaxation iteration converges when \( \lvert \left[(1 + \omega) \mathrm{D}_{x} f(x^*) - \omega \right] \rvert < 1 \).
By analogy with the relaxation method where we obtained
\[
\epsilon^{\prime} \approx \epsilon \cdot \mathrm{D}_{x} f\left(x^{\ast}\right),
\]
and
\[
\epsilon^{\prime \prime}=\frac{x^{\prime}-x^{\prime \prime}}{1-1 / \mathrm{D}_{x} f\left(x^{\ast}\right)} \simeq \frac{x^{\prime}-x^{\prime \prime}}{1-1 / \mathrm{D}_{x} f(x)},
\]
we have for the over-relaxation method
\[
\epsilon^{\prime \prime} \simeq \frac{x^{\prime}-x^{\prime \prime}}{1-1 /\left[(1+\omega) \mathrm{D}_{x} f(x)-\omega\right]},
\]
where the symbols are to be interpreted similarly.
#+end_remark

#+NAME: Over-relaxation method
#+begin_src python :results output
from math import fabs, cos, sin
import numpy as np

def overrelax(a, eps, func, omega):
    maxiter = 100
    xi = a
    i = 0
    while (abs(func(xi) - xi) > eps and i < maxiter):
        i = i + 1
        xin = omega * func(xi) + (1-omega) * xi
        xi = xin
    return xi, i

def fx(x):
    return cos(x)

root, iter_count = overrelax(1., 1.0e-6, fx, 0.54)
print(f"Root: {np.around(root.real, 6)}, Iterations: {iter_count}")
#+end_src

#+RESULTS: Over-relaxation method
: Root: 0.739085, Iterations: 6

**** Newton-Raphson method
This method truly shines in polishing roots obtained using other methods.
***** One variable
#+NAME: Newton-Raphson method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Let the function \( f \) be continuous over the interval \( \mathbb{R} \). We want to find  have \( x^{\ast} \in \mathbb{R} \) such that \( f(x^{\ast}) = 0 \). To find \( x^{\ast} \) up to a tolerance \( \epsilon \), start with an approximate root \( x_{0} \) and start an iteration with the following recurrence relation
\[
x_{i+1} = x_{i} - \frac{f(x_{i})}{\mathrm{D}_{x} f(x_{i})}
\]
The iteration stops when \( x_{i+1} \) and \( x_{i} \) are such that \( \lvert x_{i+1} - x_{i} \rvert < \epsilon \). The root is \( x^{\ast} = (x_{i+1} + x_{i})/2 \).
#+end_proposition
#+NAME: Newton-Raphson method
#+ATTR_LATEX: :environment remark
#+begin_remark latex
A continuous function \(f(x)\) can be expanded around a point close to the true root \(x^{\ast}\) is terms of a Taylor's series:
\[
f (x^{\ast}) = f(x) + (x^{\ast}-x) \mathrm{D}_{x} f(x) + \frac{(x^{\ast}-x)^2}{2!} \mathrm{D}_{x}^{2} f(x)+\ldots
\]
By definition, the root of \( f \) is obtained by solving \(f(x^{\ast})=0\) for \(  x^{\ast}\) so
\[
0 = f\left(x\right)+\left(x^{\ast}-x\right) \mathrm{D}_{x} f(x)+\frac{\left(x^{\ast}-x_0\right)^2}{2!} \mathrm{D}_{x}^{2} f(x)+\ldots
\]
Dividing both sides by \( \mathrm{D}_{x} f(x) \) and rearranging yields
\[
x^{\ast} = \bigg[x - \frac{f(x)}{\mathrm{D}_{x} f(x)}\bigg] - (x^{\ast}-x)^2 \bigg(\frac{\mathrm{D}_{x}^{2} f(x)}{2 ~ \mathrm{D}_{x} f(x)}\bigg) + \mathcal{O}(x^{3})
\]
Neglecting \( \mathcal{O} (x^{2}) \) terms,
\[
x^{\ast} = x-\frac{f\left(x_0\right)}{\mathrm{D}_{x} f\left(x_0\right)}.
\]
Turning the above into a recurrence relation we obtain the Newton-Raphson iteration
\[
x_{i+1} = x_{i} - \frac{f(x_{i})}{\mathrm{D}_{x} f(x_{i})}.
\]
#+end_remark
#+NAME: Derivative of f(x)
#+ATTR_LATEX: :environment remark
#+begin_remark latex
This method uses the derivatives \(\mathrm{D}_{x} f(x)\) of the function \(f(x)\) to accelerate convergence for solving \(f(x)=0\). The immediate takeaway of this is that for any \( x \), you must have a way of evaluating \( \mathrm{D}_{x} f(x) \). Besides being an additional computational expense, in some cases, it might not be possible to calculate the derivative easily. Furthermore, if the derivative becomes small - the method may end up not converging - as the next step might be far away from the root.
#+end_remark
#+NAME: Convergence
#+ATTR_LATEX: :environment remark
#+begin_remark latex
We obtained
\[
x^{\ast} = \bigg[x - \frac{f(x)}{\mathrm{D}_{x} f(x)}\bigg] - (x^{\ast}-x)^2 \bigg(\frac{\mathrm{D}_{x}^{2} f(x)}{2 ~ \mathrm{D}_{x} f(x)}\bigg)
\]
But \(\epsilon \equiv x^{\ast } = x - f(x) / \mathrm{D}_{x} f(x) \) is our new estimate, say \( x^{\prime} \) so
\[
x^{\ast} = x^{\prime} - (x^{\ast}-x)^2 \bigg(\frac{\mathrm{D}_{x}^{2} f(x)}{2 ~ \mathrm{D}_{x} f(x)}\bigg)
\]
Let \(\epsilon \equiv x^{\ast}-x\) and \( \epsilon^{\prime} \equiv x^{\ast} - x^{\prime} \) be the errors in approximating \( x^{\ast} \) with \( x \) and \( x^{\prime} \). We then have
\[
\epsilon^{\prime} =  \epsilon^{2} \bigg(- \frac{\mathrm{D}_{x}^{2} f (x)}{2 ~ \mathrm{D}_{x} f (x)} \bigg).
\]
Thus, the rate of convergence of the Newton-Raphson iteration is quadratic.
#+end_remark
#+NAME: Local convergence
#+ATTR_LATEX: :environment remark
#+begin_remark latex
We obtained
\[
x^{\ast} = \bigg[x - \frac{f(x)}{\mathrm{D}_{x} f(x)}\bigg] - (x^{\ast}-x)^2 \bigg(\frac{\mathrm{D}_{x}^{2} f(x)}{2 ~ \mathrm{D}_{x} f(x)}\bigg) + \mathcal{O}(x^{3})
\]
The Newton-Raphson iteration neglects \( \mathcal{O}(x^{2}) \) terms in the Taylor series for \( f(x) \) close to the root \( x^{\ast} \). Therefore, the initial approximation must be sufficiently close to the root for the iteration to converge.
Consider the previous result
\[
\epsilon^{\prime} =  \epsilon^{2} \bigg(- \frac{\mathrm{D}_{x}^{2} f (x)}{2 ~ \mathrm{D}_{x} f (x)} \bigg).
\]
We must \( \epsilon^{\prime} \leq \epsilon \) so that
\[
\epsilon^{2} \bigg(- \frac{\mathrm{D}_{x}^{2} f (x)}{2 ~ \mathrm{D}_{x} f (x)} \bigg) \leq \epsilon
\]
from which we obtain
\[
\epsilon \leq \bigg(-\frac{2 ~ \mathrm{D}_{x} f (x)}{\mathrm{D}_{x}^{2} f (x)} \bigg)
\].
In the beginning \( \epsilon = \lvert x^{\ast} - x_{0} \rvert \). Therefore, when
\[
\frac{\lvert x^{\ast} - x_{0} \rvert}{2} \leq \bigg(-\frac{\mathrm{D}_{x} f (x_{0})}{\mathrm{D}_{x}^{2} f (x_{0})} \bigg)
\]
is satisfied, convergence is guaranteed. Thus, Newton-Rhapson method has excellent local convergence properties, but global convergence properties can be quite poor - due to neglect of higher order terms in the Taylor series expansion.
#+end_remark
#+NAME: Flat regions
#+ATTR_LATEX: :environment remark
#+begin_remark latex
If the roots happens to be in a region where the function \( f \) is relatively flat (most close by \( x \) have both \( f(x) \to 0 \) and \(\mathrm{D}_{x} f(x) \rightarrow 0\)), convergence can be quite slow.
#+end_remark
#+NAME: Local minima
#+ATTR_LATEX: :environment remark
#+begin_remark latex
If at any stage, the iteration finds itself close to a local extremum (an \( x \) with \( \mathrm{D}_{x} f(x)  = 0\) but \( \lvert f(x) \rvert \gg 0 \) is close), then \(\frac{f\left(x\right)}{\mathrm{D}_{x} f\left(x\right)}\) can become arbitrarily large, making \( x_{i+1} - x_{i} \) arbitrarily large.
#+end_remark
#+NAME: Assymetric functions
#+ATTR_LATEX: :environment remark
#+begin_remark latex
If we have a function \( f \) that satisfies \( f(a+x)=-f(a-x) \) or even \( f(a+x) \approx -f(a-x) \) at one or more values of \( x \) for some arbitrary constant \( a \), then the iteration can get stuck in a loop where \(x_{i} \to x_{i+1} \) and \( x_{i+1} \to x_{i} \).
#+end_remark
#+NAME: Newton's method
#+begin_src python :results output
from math import fabs, cos, sin
import numpy as np

def newton(a, eps, func, funcp):
    maxiter = 100
    xi = a
    i = 0
    while (abs(func(xi)) > eps and abs(funcp(xi)) > 1e-8 and i < maxiter):
        i = i + 1
        xip = xi - func(xi) / funcp(xi)
        xi = xip
    return xi, i

def fx(x):
    return x - cos(x)

def fpx(x):
    return 1 + sin(x)

root, iter_count = newton(1.0, 1.0e-6, fx, fpx)
print(f"Root: {np.around(root, 6)}, Iterations: {iter_count}")
#+end_src

#+RESULTS: Newton's method
: Root: 0.739085, Iterations: 3

***** Two variables
#+NAME: Newton-Raphson method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Let the functions \( \mathbf{f} \) be continuous over the domain \( \mathbb{R}^{n} \) with a range \( \mathbb{R}^{n} \). Suppose that the Jacobian \( \mathbf{J} \) is invertible for all \( \mathbf{x} \in \mathbb{R}^{n} \). We want to find  have \( \mathbf{x}^{\ast} \in \mathbb{R}^{n} \) such that \( \mathbf{f}(\mathbf{x}^{\ast}) = \mathbf{0} \). To find \( \mathbf{x}^{\ast} \) up to a tolerance \( \epsilon \), start with an approximate root \( \mathbf{x}_{0} \) and start an iteration with the following recurrence relation
\[
\mathbf{x}_{i+1}=\mathbf{x}_i-\mathbf{J}^{-1}\left(\mathbf{x}_{i}\right) \cdot \mathbf{f}\left(\mathbf{x}_i\right).
\]
The iteration stops when \( \mathbf{x}_{i+1} \) and \( \mathbf{x}_{i} \) are such that \( \lvert \mathbf{x}_{i+1} - \mathbf{x}_{i} \rvert < \epsilon \). The root is \( \mathbf{x}^{\ast} = (\mathbf{x}_{i+1} + \mathbf{x}_{i})/2 \).
#+end_proposition
#+NAME: Newton-Raphson method
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Starting from true root \( \mathbf{x}^{\ast} \) and an approximate root \( \mathbf{x} \) close to the true root we have the Taylor series to first order
\[
\mathbf{f}(\mathbf{x}^{\ast}) = 0 = \mathbf{f}\left(\mathbf{x}\right)+\mathbf{J}\left(\mathbf{x}\right) \cdot\left(\mathbf{x}^{\ast}-\mathbf{x}\right),
\]
Multiplying throughout with \( \mathbf{J}^{-1} (\mathbf{x}) \) we get
\[
\mathbf{x}^{\ast} =\mathbf{x} + \mathbf{J}^{-1} (\mathbf{x}) \mathbf{f}\left(\mathbf{x}\right),
\]
which when turned into a recurrence yields
\[
\mathbf{x}_{i+1}=\mathbf{x}_i-\mathbf{J}^{-1}\left(\mathbf{x}_{i}\right) \cdot \mathbf{f}\left(\mathbf{x}_i\right).
\]
#+end_remark
#+NAME: Two variables
#+ATTR_LATEX: :environment remark
#+begin_remark latex
We are given two equations:
\begin{align*}
& f(x, y)=0 \\
& g(x, y)=0.
\end{align*}
We have to find the solution \(\left(x^*, y^*\right)\) such that:
\begin{align*}
& f\left(x^*, y^*\right)=0 \\
& g\left(x^*, y^*\right)=0.
\end{align*}
Taylor expanding about \(\left(x^*, y^*\right)\) :
\begin{align*}
& f(x, y)=f\left(x^*, y^*\right)+\left(x-x^*\right) f_x^{\prime}+\left(y-y^*\right) f_y^{\prime}+\ldots \\
& g(x, y)=g\left(x^*, y^*\right)+\left(x-x^*\right) g_x^{\prime}+\left(y-y^*\right) g_y^{\prime}+\ldots.
\end{align*}
Keeping only first-order terms and given \(f\left(x^*, y^*\right)=0\) and \(g\left(x^*, y^*\right)=0\), one has a system of linear equations for \(x^*\) and \(y^*\) such that:
\begin{align*}
& x^*=x+\frac{f_y^{\prime} g(x, y)-g_y^{\prime} f(x, y)}{f_x^{\prime} g_y^{\prime}-f_y^{\prime} g_x^{\prime}} \\
& y^*=y+\frac{g_x^{\prime} f(x, y)-f_x^{\prime} g(x, y)}{f_x^{\prime} g_y^{\prime}-f_y^{\prime} g_x^{\prime}}.
\end{align*}
#+end_remark
#+NAME: Newton's method (two variables)
#+begin_src python :results output
from math import fabs, cos, sin
import numpy as np

def newton2(a, b, eps, func1, func1p, func2, func2p):
    maxiter = 1000
    xi = a
    yi = b
    i = 0
    while (abs(func1(xi, yi)) > eps and abs(func2(xi, yi)) > eps) and i < maxiter:
        i = i + 1
        f1 = func1(xi, yi)
        f2 = func2(xi, yi)
        f1x, f1y = func1p(xi, yi)
        f2x, f2y = func2p(xi, yi)
        den = f1x*f2y - f1y*f2x
        xip = xi + (f1y*f2 - f2y*f1) / den
        yip = yi + (f2x*f1 - f1x*f2) / den
        xi = xip
        yi = yip
    return xi, yi, i

def f(x, y):
    return y * y * (1. - x) - x * x * x

def fp(x, y):
    return -y * y - 3. * x * x, 2. * y * (1. - x)

def g(x, y):
    return y*y + x*x - 1.

def gp(x, y):
    return 2.*x, 2.*y

xroot, yroot, iter_count = newton2(1., 1., 1.0e-6, f, fp, g, gp)
print(f"x root: {np.around(xroot, 6)}, y root: {np.around(yroot, 6)}, Iterations: {iter_count}")
#+end_src

#+RESULTS: Newton's method (two variables)
: x root: 0.618034, y root: 0.786151, Iterations: 4

**** Method of secants
#+NAME: Method of secants
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Let the function \( f \) be continuous over the interval \( \mathbb{R} \). We want to find  have \( x^{\ast} \in \mathbb{R} \) such that \( f(x^{\ast}) = 0 \). To find \( x^{\ast} \) up to a tolerance \( \epsilon \), start with a couple of approximate roots \( x_{0} \) and \( x_{-1} \). Then start an iteration with the following recurrence relation
\[
x_{i+1} = \frac{x_{i-1} \cdot f(x_{i}) - x_{i} \cdot f(x_{i-1})}{f(x_{i}) - f(x_{i-1})}
\]
The iteration stops when \( x_{i+1} \) and \( x_{i} \) are such that \( \lvert x_{i+1} - x_{i} \rvert < \epsilon\), \(  \lvert x_{i} - x_{i-1} \rvert < \epsilon \), and \( \lvert x_{i+1} - x_{i-1} \rvert < \epsilon \). The root is
\[ x^{\ast} = \frac{1}{2} \bigg[\frac{x_{i+1} + x_{i}}{2} + \frac{x_{i} + x_{i-1}}{2} \bigg] \].
#+end_proposition
#+NAME: Method of secants
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Consider the Newton-Raphson iteration
\[
x_{i+1} = x_{i} - \frac{f(x_{i})}{\mathrm{D}_{x} f(x_{i})}.
\]
Suppose that evaluation of \( \mathrm{D} f \) is either impossible or expensive. The secant method approximates \( f \) locally with a linear function \( g \), which is the secant to \( f \) and the root of \( g \) in this local region is taken as an improved approximation to the root of \( f \). In other words
\[
\mathrm{D}_{x} f(x_{i}) \approx \mathrm{D}_{x} g(x_{i}) = \frac{f(x_{i}) - f(x_{i-1})}{x_{i} - x_{i-1}}
\]
Substituting into the Newton-Raphson iteration and simplifying we obtain the secant method iteration
\[
x_{i+1} = \frac{x_{i-1} \cdot f(x_{i}) - x_{i} \cdot f(x_{i-1})}{f(x_{i}) - f(x_{i-1})}
\]
#+end_remark
#+NAME: Similarity to false position method
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The secant method iteration
\[
x_{i+1} = \frac{x_{i-1} \cdot f(x_{i}) - x_{i} \cdot f(x_{i-1})}{f(x_{i}) - f(x_{i-1})}
\]
under \( x_{i-1} \to a \), \( x_{i} \to b \), \( x_{i+1} \to c \) becomes the false position method iteration
\[
c = \frac{ a \cdot f(b) - b \cdot f(a)}{f(b) - f(a)}.
\] 
#+end_remark

#+NAME: Secant method
#+begin_src python :results output
from math import fabs, cos
import numpy as np

def secant(a, b, eps, func):
    maxiter = 100
    xi = a
    xim = b
    i = 0
    while (abs(func(xi)) > eps and abs(func(xi) - func(xim)) > eps and i < maxiter):
        i = i + 1
        xip = (xim*func(xi) - xi*func(xim)) / (func(xi) - func(xim))
        xim = xi
        xi = xip
    return xi, i

def fx(x):
    return x - cos(x)

root, iter_count = secant(0., 1., 1.0e-6, fx)
print(f"Root: {np.around(root, 6)}, Iterations: {iter_count}")
#+end_src

#+RESULTS: Secant method
: Root: 0.739085, Iterations: 4

**** Muller's method
#+NAME: Muller's method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Let the function \( f \) be continuous over the interval \( \mathbb{R} \). We want to find  have \( x^{\ast} \in \mathbb{R} \) such that \( f(x^{\ast}) = 0 \). To find \( x^{\ast} \) up to a tolerance \( \epsilon \), start with a three approximate roots \( x_{0} \), \( x_{-1} \), and \( x_{-2} \) . Then start an iteration with the following recurrence relation
\[
x_{i+1}=x_{i}-\frac{2 f\left(x_{i}\right)}{w \pm \sqrt{w^2-4 f\left(x_{i}\right) f\left[x_{i}, x_{i-1}, x_{i-2}\right]}} .
\]
where
\[
w=f\left[x_{i}, x_{i-1}\right]+f\left[x_{i}, x_{i-2}\right]-f\left[x_{i-1}, x_{i-2}\right].
\]
\( [\cdot,~\cdot] \) denotes Newton's divided differences. The sign in \( w^2-4 f\left(x_{i}\right) f\left[x_{i}, x_{i-1}, x_{i-2}\right]} \) should be chosen such that the denominator is as large as possible in magnitude.
The iteration stops when \( x_{i+1} \), \( x_{i} \), \( x_{i-1} \), and \( x_{i-2} \) are such that \( \lvert x_{i+1} - x_{i} \rvert < \epsilon\), \(  \lvert x_{i} - x_{i-1} \rvert < \epsilon \), \( \lvert x_{i-1} - x_{i-2} \rvert < \epsilon\), and \( \lvert x_{i+1} - x_{i-2} \rvert < \epsilon \). The root is
\[
x^{\ast} = \frac{1}{3} \bigg[\frac{x_{i+1} + x_{i}}{2} + \frac{x_{i} + x_{i-1}}{2} + \frac{x_{i-1} + x_{i-2}}{2}\bigg]
\].
#+end_proposition
#+NAME: Muller's method
#+ATTR_LATEX: :environment remark
#+begin_remark latex
A parabola is constructed which goes through the three points \(\left(x_{i}, f\left(x_{i}\right)\right),\left(x_{i-1}, f\left(x_{i-1}\right)\right)\), and \(\left(x_{i-2}, f\left(x_{i-2}\right)\right)\). When written in the Newton form, \(y_{i+1}(x)\) is
\[
f\left(x_{i}\right)+\left(x-x_{i}\right) f\left[x_{i}, x_{i-1}\right]+\left(x-x_{i}\right)\left(x-x_{i-1}\right) f\left[x_{i}, x_{i-1}, x_{i-2}\right],
\]
where \(f\left[x_{i}, x_{i-1}\right]\) and \(f\left[x_{i}, x_{i-1}, x_{i-2}\right]\) denote divided differences. This can be rewritten as
\[
f\left(x_{i}\right)+w\left(x-x_{i}\right)+f\left[x_{i}, x_{i-1}, x_{i-2}\right]\left(x-x_{i}\right)^2
\]\[
f\left(x_{i}\right)+w\left(x-x_{i}\right)+f\left[x_{i}, x_{i-1}, x_{i-2}\right]\left(x-x_{i}\right)^2
\]
where
\[
w=f\left[x_{i}, x_{i-1}\right]+f\left[x_{i}, x_{i-2}\right]-f\left[x_{i-1}, x_{i-2}\right] .
\]
The next iterate \(x_{i+1}\) is now given as the solution closest to \(x_{i}\) of the quadratic equation
\[
f\left(x_{i}\right)+w\left(x-x_{i}\right)+f\left[x_{i}, x_{i-1}, x_{i-2}\right]\left(x-x_{i}\right)^2 = 0.
\]
This yields the recurrence Muller method recurrence
\[
x_{i+1}=x_{i}-\frac{2 f\left(x_{i}\right)}{w \pm \sqrt{w^2-4 f\left(x_{i}\right) f\left[x_{i}, x_{i-1}, x_{i-2}\right]}} .
\]
In this formula, the sign should be chosen such that the denominator is as large as possible in magnitude. We do not use the standard formula for solving quadratic equations because that may lead to loss of significance.
#+end_remark
#+NAME: Relation to the secant method
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The Muller's method is a variant of method of secants. The non linear function \(f\) is aproximated locally by the quadratic function \(g\), and the root of \(g\) is taken as an improved approximation to the root of \(f\). The only difference between Muller's method and method of secants is that the \(g\) is quadratic function in Muller's method and linear function in secant method!
#+end_remark
#+NAME: Complex roots
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Note that \(x_{i+1}\) can be complex, even if the previous iterates were all real. This is in contrast with other root-finding algorithms like the Newton-Rhapson method or secant method, whose iterates will remain real if one starts with real numbers. Having complex iterates can be an advantage (if one is looking for complex roots) or a disadvantage (if it is known that all roots are real), depending on the problem.
#+end_remark
#+NAME: Muller's method
#+begin_src python :results output
import numpy as np
import cmath  # For handling complex numbers in case the roots are complex

def muller(x0, x1, x2, eps, func):
    maxiter = 100
    xi0 = x0
    xi1 = x1
    xi2 = x2
    h1 = xi1 - xi0
    h2 = xi2 - xi1
    d1 = (func(xi1) - func(xi0)) / h1
    d2 = (func(xi2) - func(xi1)) / h2
    d = (d2 - d1) / (h2 + h1)
    i = 0

    while (abs(func(xi2)) > eps) and (i < maxiter):
        b = d2 + h2 * d
        D = cmath.sqrt(b**2 - 4 * func(xi2) * d)
        if abs(b - D) < abs(b + D):
            E = b + D
        else:
            E = b - D

        h = -2 * func(xi2) / E
        xi = xi2 + h
        if abs(h) < eps:
            break

        xi0 = xi1
        xi1 = xi2
        xi2 = xi
        h1 = xi1 - xi0
        h2 = xi2 - xi1
        d1 = (func(xi1) - func(xi0)) / h1
        d2 = (func(xi2) - func(xi1)) / h2
        d = (d2 - d1) / (h2 + h1)
        
        i += 1

    return xi, i

def fx(x):
    return x - cmath.cos(x)  # Use cmath's cos function to handle complex numbers

root, iter_count = muller(1., 1.5, 2., 1.0e-6, fx)
print(f"Root: {np.around(root.real, 6)}, Iterations: {iter_count}")
#+end_src

#+RESULTS: Muller's method
: Root: 0.739085, Iterations: 4

**** Brute force method
#+NAME: Brute force method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Let the function \( f \) be continuous over the interval \( [a,~b] \) with multiple roots inside the interval. The brute force method splits the interval \([a,~b]\) into smaller intervals with some step size (say \(h\)) and then applies any of the root finding method discussed above in each of the sub-intervals.
#+end_proposition
#+NAME: Step size
#+ATTR_LATEX: :environment remark
#+begin_remark latex
If the step size is too large - one may miss multiple roots. Choosing too small a step size will result in too many computations. A graphical analysis is very helpful in deciding the step size \(h\). It may be good to evaluate roots with \(h\) and then with \(h / 10\) to confirm that the number of roots remains unchanged.
#+end_remark
*** Pitfalls
#+NAME: Pitfalls of Root Finding
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Root finding methods can encounter several pitfalls, including:
Lack of good initial guess: A poor initial guess can lead to slow convergence or failure to find a root.
Convergence to the wrong root: The method may converge to an unintended root.
Closely spaced roots, multiple roots, inflection points: These can complicate the root-finding process.
Additionally, consider the following challenges:
Complex roots: Real methods may struggle with equations that have complex roots.
Ill-conditioning of the nonlinear equations: Sensitive equations can lead to numerical instability.
Regarding slow convergence:
- Methods generally work well for simple, smoothly varying problems with a good initial guess.
- Efficiency is crucial when solving problems repeatedly.
- Anticipate the presence of complex roots.
- There is a trade-off between your time and the computer's time.
- Choosing the right method can be difficult; a method that works for one equation may fail for another.
- Visualizing the data can be highly beneficial, aiding in the selection of appropriate methods and understanding the behavior of the function.
Ensuring a robust approach to root finding involves careful consideration of these factors.
#+end_remark
** Fourier Methods
:LOGBOOK:
CLOCK: [2024-06-23 Sun 13:42]--[2024-06-23 Sun 17:52] =>  4:10
CLOCK: [2024-06-22 Sat 22:01]--[2024-06-22 Sat 22:41] =>  0:40
CLOCK: [2024-06-13 Thu 21:02]--[2024-06-13 Thu 22:22] =>  1:20
:END:
*** Fourier series
#+NAME: Fourier cosine series
#+begin_definition latex
Let \(f: [0,\,L] \rightarrow \mathbb{R}\) be periodic function that is bounded in the closed interval \( [0,\,L] \) and has at most a finite number of discontinuities. Suppose that \( f \) is an even function of \( x \) in the interval \( [0,\, L] \). The Fourier cosine series is defined as
\[
\sum_{k=0}^{N} A_k \cos \left(\frac{2 \pi k x}{L}\right), \quad A_k = \frac{2}{L} \int_{0}^{L} f(x) \cos \bigg(\frac{2 \pi k x}{L} \bigg) \mathrm{d}x.
\]
#+end_definition
#+NAME: Fourier sine series
#+begin_definition latex
Let \(f: [0,\,L] \rightarrow \mathbb{R}\) be periodic function that is bounded in the closed interval \( [0,\,L] \) and has at most a finite number of discontinuities. Suppose that \( f \) is an odd function of \( x \) in the interval \( [0,\, L] \). The Fourier sine series is defined as
\[
\sum_{k=1}^{N} A_k \sin \left(\frac{2 \pi k x}{L}\right), \quad A_k = \frac{2}{L} \int_{0}^{L} f(x) \sin \bigg(\frac{2 \pi k x}{L} \bigg) \mathrm{d}x.
\]
#+end_definition
#+NAME: Fourier series
#+begin_definition latex
Let \(f: [0,\,L] \rightarrow \mathbb{R}\) be a periodic function (odd, even, or neither) that is bounded in the closed interval \( [0,\,L] \) and has at most a finite number of discontinuities. The Fourier series is defined as
\[
\sum_{k=0}^{N} A_k \cos \left(\frac{2 \pi k x}{L}\right) + \sum_{k=1}^{N} B_k \sin \left(\frac{2 \pi k x}{L}\right), \quad A_k = \frac{2}{L} \int_{0}^{L} f(x) \cos \bigg(\frac{2 \pi k x}{L} \bigg) \mathrm{d}x, \quad B_k = \frac{2}{L} \int_{0}^{L} f(x) \sin \bigg(\frac{2 \pi k x}{L} \bigg) \mathrm{d}x.
\]
#+end_definition
#+NAME: Fourier series (exponential form)
#+begin_definition latex
Let \(f: [0,\,L] \rightarrow \mathbb{R}\) be periodic function (odd, even, or neither) that is bounded in the closed interval \( [0,\,L] \) and has at most a finite number of discontinuities. The Fourier series is defined as
\[
\sum_{k=-N}^{N} C_k \exp \left(i \, \frac{2 \pi k x}{L}\right), \quad C_k = \frac{1}{L} \int_{0}^{L} f(x) \exp \bigg(- i \, \frac{2 \pi k x}{L} \bigg) \mathrm{d} x.
\]
#+end_definition
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The exponential form can be obtained from the Fourier series
\begin{align*}
\sum_{k=0}^{N} A_k \cos \left(\frac{2 \pi k x}{L}\right)+\sum_{k=1}^{N} B_k \sin \left(\frac{2 \pi k x}{L}\right)
\end{align*}
by substituting \(\cos \theta=[\exp (-\mathrm{i} \theta) +\exp(\mathrm{i} \theta)]/2\) and \(\sin \theta= i \, [\exp (-\mathrm{i} \theta) - \exp(\mathrm{i} \theta)]/2\).
\begin{align*}
& \frac{1}{2} \sum_{k=0}^{N} \alpha_k\left[\exp \left(-\mathrm{i} \frac{2 \pi k x}{L}\right)+\exp \left(\mathrm{i} \frac{2 \pi k x}{L}\right)\right] \\
&+ \frac{\mathrm{i}}{2} \sum_{k=1}^{N} \beta_k\left[\exp \left(-\mathrm{i} \frac{2 \pi k x}{L}\right)-\exp \left(\mathrm{i} \frac{2 \pi k x}{L}\right)\right].
\end{align*}
Collecting terms we get the exponential form of the Fourier series
\begin{align*}
\sum_{k=-N}^{N} C_k \exp \left(\mathrm{i} \frac{2 \pi k x}{L}\right),
\end{align*}
where the sum now runs from \(-N\) to \(+N\) and
\begin{align*}
C_k= 
\begin{cases}
\frac{1}{2}\left(A_{-k}+\mathrm{i} B_{-k}\right) & \text { if } k<0, \\
A_0 & \text { if } k=0, \\
\frac{1}{2}\left(A_k-\mathrm{i} B_k\right) & \text { if } k>0.
\end{cases}
\end{align*}
#+end_remark
#+NAME: Fourier's theorem
#+begin_theorem latex
Let \(f: [0,\,L] \rightarrow \mathbb{R}\) be function (odd, even, or neither) that is bounded in the closed interval \( [0,\,L] \) and has at most a finite number of discontinuities. We have
\[
f(x) = \lim_{N \to \infty} \bigg[ \sum_{k=-N}^{N} C_k \exp \left(i \, \frac{2 \pi k x}{L}\right) \bigg] = \sum_{k=-\infty}^{\infty} C_k \exp \left(i \, \frac{2 \pi k x}{L}\right), \quad C_k = \frac{1}{L} \int_{0}^{L} f(x) \exp \bigg(- i \, \frac{2 \pi k x}{L} \bigg) \mathrm{d} x.
\]
#+end_theorem
#+NAME: Fourier's theorem
#+ATTR_LATEX: :environment proof
#+begin_proof latex
We can obtain the expression for \( C_k \) without recourse to the sine and cosine Fourier series by starting with the identity
\begin{align*}
\int_0^L f(x) \exp \left(-\mathrm{i} \frac{2 \pi k x}{L}\right) \mathrm{d} x=\sum_{k^{\prime}=-\infty}^{\infty} C_{k^{\prime}} \int_0^L \exp \left(\mathrm{i} \frac{2 \pi\left(k^{\prime}-k\right) x}{L}\right) \mathrm{d} x,
\end{align*}
where we have used the result from the Fourier theorem. For \(k^{\prime} \neq k\)
\begin{align*}
\begin{aligned}
\int_0^L \exp \left(\mathrm{i} \frac{2 \pi\left(k^{\prime}-k\right) x}{L}\right) \mathrm{d} x & =\frac{L}{\mathrm{i} 2 \pi\left(k^{\prime}-k\right)}\left[\exp \left(\mathrm{i} \frac{2 \pi\left(k^{\prime}-k\right) x}{L}\right)\right]_0^L \\
& =\frac{L}{\mathrm{i} 2 \pi (k^{\prime}-k)} [\exp (\mathrm{i} 2 \pi (k^{\prime}-k)) -1 ] \\
& =0
\end{aligned}
\end{align*}
since \(\exp (\mathrm{i} \, 2 \pi n ) = 1\) for any integer \(n = k^{\prime} - k\). For \(k^{\prime} = k\)
\begin{align*}
\int_0^L f(x) \exp \left(-\mathrm{i} \frac{2 \pi k x}{L}\right) \mathrm{d} x=L C_k,
\end{align*}
which yields
\begin{align*}
C_k=\frac{1}{L} \int_0^L f(x) \exp \left(-\mathrm{i} \frac{2 \pi k x}{L}\right) \mathrm{d} x .
\end{align*}
#+end_proof
*** The discrete Fourier transform (DFT)
#+NAME: Non-integrable Fourier coefficients
#+ATTR_LATEX: :environment remark
#+begin_remark latex
For some functions \( f(x) \), the integral to compute the Fourier coefficients \(C_k\) can be solved analytically. However, this is often impractical due to the complexity of \( f(x) \) or its absence in analytic form. In such cases, numerical methods are used to compute the Fourier coefficients.
#+end_remark
#+NAME: Fourier coefficients
#+begin_proposition latex
Let \(f: [0,\,L] \rightarrow \mathbb{R}\) be a periodic function that is bounded in the closed interval \( [0,\,L] \) and has at most a finite number of discontinuities. Let \( x_{n} \equiv (n/N) ~ L \) denote \( N \) equally spaced points in the interval \( [0,~L] \). The Fourier coefficient \( C_{k} \) is given by
\[
C_k = \frac{1}{N} \sum_{n=0}^{N-1} f(x_n) \exp \left(-\mathrm{i} \frac{2 \pi k x_n}{L} \right).
\]
#+end_proposition
#+NAME: Fourier coefficients
#+ATTR_LATEX: :environment proof
#+begin_proof latex
For some functions \( f(x) \), the integral to compute the Fourier coefficients \(C_k\) can be solved analytically. However, this is often impractical due to the complexity of \( f(x) \) or its absence in analytic form. In such cases, numerical methods are used to compute the Fourier coefficients. Using the trapezoidal rule with \( N \) slices of width \( h = L / N \), we get:

\[ C_k = \frac{1}{L} \frac{L}{N} \left[\frac{1}{2} f(0) + \frac{1}{2} f(L) + \sum_{n=1}^{N-1} f(x_n) \exp \left(-\mathrm{i} \frac{2 \pi k x_n}{L}\right) \right], \]

where the sample points \( x_n \) are given by:

\[ x_n = \frac{n}{N} L. \]

Given \( f(x) \) is periodic (\( f(L) = f(0) \)), this simplifies to:

\[ C_k = \frac{1}{N} \sum_{n=0}^{N-1} f(x_n) \exp \left(-\mathrm{i} \frac{2 \pi k x_n}{L} \right). \]

This formula is computationally convenient as it only requires the values of \( f(x) \) at equally spaced sample points \( x_n \).
#+end_proof
#+ATTR_LATEX: :environment remark
#+begin_remark latex
This formula is computationally convenient as it only requires the values of \( f(x) \) at equally spaced sample points \( x_n \).
#+end_remark
#+NAME: Discrete Fourier transform (DFT)
#+begin_proposition latex
Let \(y_{0},\ldots,y_{n},\ldots,y_{N-1}\) be samples. The discrete Fourier transform of (DFT) of these samples \( y_{n} \) yields the fourier coefficient \( c_{k} \)
\[
c_k = \sum_{n=0}^{N-1} y_n \exp \left(-\mathrm{i} \frac{2 \pi k n}{L} \right).
\]
#+end_proposition
#+NAME: Discrete Fourier transform (DFT)
#+ATTR_LATEX: :environment proof
#+begin_proof latex
This result follows immeditely on defining \( y_n = f(x_n) \), leading to:
\[ C_k = \frac{1}{N} \sum_{n=0}^{N-1} y_n \exp \left(-\mathrm{i} \frac{2 \pi k n}{N} \right). \]
The previous form required the values of \( f(x) \) at equally spaced sample points \( x_n \). This form even omits the need for knowing the positions \( x_n \) or the interval width \( L \). The sum
\[ c_k = \sum_{n=0}^{N-1} y_n \exp \left(-\mathrm{i} \frac{2 \pi k n}{N} \right) \]
is known as the discrete Fourier transform (DFT) of the samples \( y_n \) by convention. We have \( C_k = c_k / N \). While \( C_k \) represents the true Fourier coefficients, \( c_k \) is conventionally used in discussions of the DFT. Throughout the chapter, \( c_k \) will be referred to as "Fourier coefficients," though technically they differ by the factor \( 1 / N \).
#+end_proof
#+NAME: Inverse DFT
#+begin_proposition latex
Let \(c_{0},\ldots,c_{k},\ldots,c_{N-1}\) be Fourier coefficients obtained via DFT. The inverse Fourier transform of of these coefficients yields the original samples \( y_{n} \)
\[
y_n=\frac{1}{N} \sum_{k=0}^{N-1} c_k \exp \left(\mathrm{i} \frac{2 \pi k n}{N}\right)
\]
#+end_proposition
#+NAME: Inverse DFT
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Recall the standard geometric series:
\[
\sum_{k=0}^{N-1} a^k = \frac{1 - a^N}{1 - a}
\]
Set \(a = \exp(\mathrm{i} 2 \pi m / N)\) with \(m\) integer to get
\[
\sum_{k=0}^{N-1} \exp(\mathrm{i} 2 \pi k m / N) = \frac{1 - \exp(\mathrm{i} 2 \pi m)}{1 - \exp(\mathrm{i} 2 \pi m / N)}.
\]
Since \(\exp(\mathrm{i} 2 \pi m) = 1\) for all integers \(m\), the numerator vanishes, resulting in zero. The exception is when \(m = 0\) or a multiple of \(N\), where the denominator also vanishes. In this case, the sum is straightforward:
\[
\sum_{k=0}^{N-1} 1 = N.
\]
Thus,
\[
\sum_{k=0}^{N-1} \exp(\mathrm{i} 2 \pi k m / N) = 
\begin{cases} 
N & \text{if } m \text{ is zero or a multiple of } N, \\ 
0 & \text{otherwise.} 
\end{cases}
\]

Consider the discrete Fourier transform:
\[
\sum_{k=0}^{N-1} c_k \exp\left(\mathrm{i} \frac{2 \pi k n}{N}\right) = \sum_{k=0}^{N-1} \sum_{n^{\prime}=0}^{N-1} y_{n^{\prime}} \exp\left(-\mathrm{i} \frac{2 \pi k n^{\prime}}{N}\right) \exp\left(\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
Reordering the summation:
\[
= \sum_{n^{\prime}=0}^{N-1} y_{n^{\prime}} \sum_{k=0}^{N-1} \exp\left(\mathrm{i} \frac{2 \pi k (n - n^{\prime})}{N}\right).
\]
Since \(0 \leq n, n' < N\), \(n - n'\) cannot be a nonzero multiple of \(N\), but can be zero if \(n = n'\). Therefore, the sum is \(N\) when \(n = n'\) and zero otherwise. This simplifies to:
\[
\sum_{k=0}^{N-1} c_k \exp\left(\mathrm{i} \frac{2 \pi k n}{N}\right) = N y_n,
\]
or equivalently,
\[
y_n = \frac{1}{N} \sum_{k=0}^{N-1} c_k \exp\left(\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
#+end_proof
#+NAME: Inverse DFT
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The inverse DFT is the counterpart to the forward DFT. It tells us that, given the coefficients \(c_k\) we get from the DFT, we can recover the values of the samples \(y_n\) that they came from exactly (except for rounding error). The Fourier coefficients exact in the sense that we can completely recover the original samples from them.
#+end_remark
#+NAME: DFT
#+ATTR_LATEX: :environment remark
#+begin_remark latex
It is important to appreciate that, unlike the original Fourier series, the DFT only provides the sample values \(y_n = f(x_n)\). It tells us nothing about \(f(x)\) between the sample points. This is because the computation of the \(c_k\) in the DFT uses only the values at the sample points, so the \(c_k\) cannot contain any information about values between them. Any two functions that have the same values at the sample points will have the same DFT, regardless of their behavior between the points.
For instance, two functions can have identical DFTs even if they differ significantly between the second and third sample points. However, if a function is reasonably smooth without significant excursions between samples, knowing the values at the sample points can still give a good approximation of the function's general shape. In many cases, we are interested in a function represented as a set of samples rather than as a continuous function. For such data, the DFT is an excellent tool.
#+end_remark
#+NAME: DFT for real-valued functions
#+ATTR_LATEX: :environment remark
#+begin_remark latex
All of the results above apply whether \(f(x)\) is a real function or a complex one-the DFT works equally well for either. In most practical situations, however, we're interested in real functions, in which case there are some further simplifications we can do. On the other hand, if the \(y_n\) are complex then we need to calculate all \(N\) Fourier coefficients.
#+end_remark
#+NAME: DFT for real-valued functions
#+begin_corollary latex
If the samples \(y_{n}\) are real, then the Fourier coefficients satisfy:
\[
c_{N-r} = c_r^* \quad \text{for} \quad 1 \leq r < \frac{1}{2} N.
\]
Thus, we only need to calculate the coefficients \(c_{k}\) for \(0 \leq k \leq \frac{1}{2} N\). If \(N\) is even, this requires calculating \(\frac{1}{2} N + 1\) coefficients; if \(N\) is odd, \(\frac{1}{2}(N + 1)\) coefficients.
#+end_corollary
#+NAME: DFT for real-valued functions
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Suppose all \(y_n\) are real and consider \(c_k\) for some \(k\) that is less than \(N\) but greater than \(\frac{1}{2} N\), which we write as \(k = N - r\) with \(1 \leq r < \frac{1}{2} N\). Then,
\[
c_{N-r} = \sum_{n=0}^{N-1} y_n \exp\left(-\mathrm{i} \frac{2 \pi (N-r) n}{N}\right) = \sum_{n=0}^{N-1} y_n \exp(-\mathrm{i} 2 \pi n) \exp\left(\mathrm{i} \frac{2 \pi r n}{N}\right).
\]
Since \(\exp(-\mathrm{i} 2 \pi n) = 1\) for all integer \(n\) and \(y_n\) are real, we have:
\[
c_{N-r} = \sum_{n=0}^{N-1} y_n \exp\left(\mathrm{i} \frac{2 \pi r n}{N}\right) = c_r^*.
\]
Thus, \(c_{N-1} = c_1^*\), \(c_{N-2} = c_2^*\), and so forth. Therefore, when calculating the Fourier transform of a real function, we only need the coefficients \(c_k\) for \(0 \leq k \leq \frac{1}{2} N\). The other half are just the complex conjugates of the first half. If \(N\) is even, we calculate \(\frac{1}{2} N + 1\) coefficients; if \(N\) is odd, \(\frac{1}{2}(N + 1)\) coefficients.

For both even and odd values of \(N\), this expression can be written in Python as "N // 2 + 1", where "//" is the integer division operator. If the \(y_n\) are complex, then this simplification does not apply, and all \(N\) Fourier coefficients must be calculated.
#+end_proof
#+NAME: DFT
#+begin_src python :results output
import numpy as np
from numpy import exp, pi, sin

def dft(y):
    N = len(y)
    c = np.zeros(N, dtype=complex)
    # Compute DFT up to the Nyquist frequency for efficiency
    for k in range(N // 2 + 1):
        for n in range(N):
            c[k] += y[n] * exp(-2j * pi * k * n / N)
    
    # If the input data is real, use the symmetry property to fill in the rest
    if np.isrealobj(y):
        for k in range(1, N // 2):
            c[N - k] = np.conj(c[k])
    else:
        # Complete the computation for the remaining coefficients if input is complex
        for k in range(N // 2 + 1, N):
            for n in range(N):
                c[k] += y[n] * exp(-2j * pi * k * n / N)
    
    return c

def idft(c):
    N = len(c)
    y = np.zeros(N, dtype=complex)
    # Reconstruct the full series using symmetry and complete coefficients
    for n in range(N):
        for k in range(N // 2 + 1):
            angle = 2j * pi * k * n / N
            y[n] += c[k] * exp(angle)
            if k != 0 and k != N // 2:  # Add the symmetric counterpart
                y[n] += np.conj(c[k]) * exp(-angle)
        y[n] /= N
    return y

# Parameters
N = 10
a = 2
x = np.linspace(0, 2 * pi, N, endpoint=False)
y = sin(a * x)

# DFT and IDFT
c = dft(y)
y_reconstructed = idft(c)

# Printing results
print("Original samples (y):")
print(np.round(y, decimals=2))
print("\nFourier coefficients (c):")
print(c)
print("\nReconstructed samples (y_reconstructed):")
print(np.round(y_reconstructed.real, decimals=2))  # We take the real part since the input is real
#+end_src
#+RESULTS: DFT
#+begin_example
Original samples (y):
[ 0.    0.95  0.59 -0.59 -0.95 -0.    0.95  0.59 -0.59 -0.95]

Fourier coefficients (c):
[2.22044605e-16+0.00000000e+00j 0.00000000e+00+0.00000000e+00j
 2.77555756e-16-5.00000000e+00j 5.55111512e-17+2.22044605e-16j
 1.11022302e-16+3.33066907e-16j 4.44089210e-16+4.44878991e-16j
 1.11022302e-16-3.33066907e-16j 5.55111512e-17-2.22044605e-16j
 2.77555756e-16+5.00000000e+00j 0.00000000e+00-0.00000000e+00j]

Reconstructed samples (y_reconstructed):
[ 0.    0.95  0.59 -0.59 -0.95 -0.    0.95  0.59 -0.59 -0.95]
#+end_example
#+NAME: Positions of the samples (DFT)
#+begin_corollary latex
Let \(x_n = \frac{n}{N} L\) for \(n = 0, 1, \ldots, N-1\) be sample points with corresponding values \(y_n = f(x_n)\), and let \(c_k\) for \(k = 0, 1, \ldots, N-1\) be the Fourier coefficients obtained via the DFT of \(y_n\):
\[
c_k = \sum_{n=0}^{N-1} y_n \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
Suppose we shift the sample points to \(x_n^{\prime} = x_n + \Delta = \frac{n}{N} L + \Delta\), with corresponding values \(y_n^{\prime} = f(x_n^{\prime})\). Let \(c_k^{\prime}\) for \(k = 0, 1, \ldots, N-1\) be the Fourier coefficients obtained via the DFT of \(y_n^{\prime}\):
\[
c_k^{\prime} = \sum_{n=0}^{N-1} y_n^{\prime} \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
Then the relationship between \(c_k\) and \(c_k^{\prime}\) is given by:
\[
c_k^{\prime} = \exp\left(\mathrm{i} \frac{2 \pi k \Delta}{L}\right) c_k.
\]
#+end_corollary
#+NAME: Positions of the samples (DFT) proof
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Consider the shifted sample points \(x_n^{\prime} = x_n + \Delta = \frac{n}{N} L + \Delta\) with corresponding values \(y_n^{\prime} = f(x_n^{\prime})\). The discrete Fourier transform for these shifted samples is:
\[
c_k^{\prime} = \sum_{n=0}^{N-1} y_n^{\prime} \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
\]

To relate \(c_k\) and \(c_k^{\prime}\), we start by considering the DFT of the shifted samples:
\[
c_k = \sum_{n=0}^{N-1} f(x_n + \Delta) \exp\left(-\mathrm{i} \frac{2 \pi k (x_n + \Delta)}{L}\right).
\]
This expression can be rewritten as:
\[
c_k = \exp\left(-\mathrm{i} \frac{2 \pi k \Delta}{L}\right) \sum_{n=0}^{N-1} f(x_n^{\prime}) \exp\left(-\mathrm{i} \frac{2 \pi k x_n}{L}\right).
\]
Letting \(y_n^{\prime} = f(x_n^{\prime})\), we obtain:
\[
c_k = \exp\left(-\mathrm{i} \frac{2 \pi k \Delta}{L}\right) \sum_{n=0}^{N-1} y_n^{\prime} \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
This is the same as the original DFT with an additional phase factor. Defining a new coefficient \(c_k^{\prime} = \exp\left(\mathrm{i} \frac{2 \pi k \Delta}{L}\right) c_k\), we get:
\[
c_k^{\prime} = \sum_{n=0}^{N-1} y_n^{\prime} \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
Thus, the DFT of the shifted samples yields the Fourier coefficients \(c_k^{\prime}\), which relate to the original coefficients \(c_k\) by the phase factor \(\exp\left(\mathrm{i} \frac{2 \pi k \Delta}{L}\right)\).
#+end_proof
#+NAME: Type-I and Type-II DFT
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The discrete Fourier transform (DFT) can be categorized into Type-I and Type-II based on the sample point shift \(\Delta\).
Type-I DFT corresponds to \(\Delta = 0\), where the sample points are \(x_n = \frac{n}{N} L\) and the Fourier coefficients are:
  \[
  c_k = \sum_{n=0}^{N-1} y_n \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
  \]
Type-II DFT corresponds to \(\Delta = \frac{1}{2}\), where the sample points are \(x_n^{\prime} = x_n + \frac{1}{2} = \frac{n}{N} L + \frac{1}{2}\) and the Fourier coefficients are:
  \[
  c_k^{\prime} = \sum_{n=0}^{N-1} y_n^{\prime} \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
  \]
Using the relationship between \(c_k\) and \(c_k^{\prime}\), we substitute \(\Delta = \frac{1}{2}\) into the previous corollary to show that Type-II DFT coefficients can be obtained from Type-I DFT coefficients:
\[
c_k^{\prime} = \exp\left(\mathrm{i} \frac{2 \pi k \cdot \frac{1}{2}}{L}\right) c_k = \exp\left(\mathrm{i} \frac{\pi k}{L}\right) c_k.
\]
Thus, the Type-II DFT coefficients \(c_k^{\prime}\) are related to the Type-I DFT coefficients \(c_k\) by the phase factor \(\exp\left(\mathrm{i} \frac{\pi k}{L}\right)\).
#+end_remark
#+NAME: Discrete Fourier transform (two-dimensional)
#+begin_proposition latex
Let \(y_{mn}\) be samples on an \(M \times N\) grid. The two-dimensional discrete Fourier transform (DFT) of these samples yields the Fourier coefficients \(c_{kl}\):
\[
c_{kl} = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} y_{mn} \exp\left[-\mathrm{i} 2 \pi \left(\frac{k m}{M} + \frac{l n}{N}\right)\right].
\]
#+end_proposition
#+NAME: Discrete Fourier transform (two-dimensional)
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Functions of two variables \(f(x, y)\) can be Fourier transformed using a two-dimensional Fourier transform, which means transforming with respect to one variable and then with respect to the other. 

Suppose we have an \(M \times N\) grid of samples \(y_{mn}\). To carry out the two-dimensional Fourier transform, we first perform a Fourier transform on each of the \(M\) rows:
\[
c_{ml}^{\prime} = \sum_{n=0}^{N-1} y_{mn} \exp\left(-\mathrm{i} \frac{2 \pi l n}{N}\right).
\]
For each row \(m\), we obtain \(N\) coefficients, one for each \(l\). Next, we take the \(l\)-th coefficient in each of the \(M\) rows and Fourier transform these \(M\) values:
\[
c_{kl} = \sum_{m=0}^{M-1} c_{ml}^{\prime} \exp\left(-\mathrm{i} \frac{2 \pi k m}{M}\right).
\]
Substituting \(c_{ml}^{\prime}\) into this expression, we get a single expression for the complete Fourier transform in two dimensions:
\[
c_{kl} = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} y_{mn} \exp\left[-\mathrm{i} 2 \pi \left(\frac{k m}{M} + \frac{l n}{N}\right)\right].
\]
#+end_proof
#+NAME: Inverse DFT (two-dimensional)
#+begin_proposition latex
Let \(c_{kl}\) be the Fourier coefficients obtained via the two-dimensional DFT. The inverse Fourier transform of these coefficients yields the original samples \(y_{mn}\):
\[
y_{mn} = \frac{1}{MN} \sum_{k=0}^{M-1} \sum_{l=0}^{N-1} c_{kl} \exp\left[\mathrm{i} 2 \pi \left(\frac{k m}{M} + \frac{l n}{N}\right)\right].
\]
#+end_proposition
#+NAME: Inverse DFT (two-dimensional)
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Consider the two-dimensional discrete Fourier transform:
\[
c_{kl} = \sum_{m=0}^{M-1} \sum_{n=0}^{N-1} y_{mn} \exp\left[-\mathrm{i} 2 \pi \left(\frac{k m}{M} + \frac{l n}{N}\right)\right].
\]
We aim to recover \(y_{mn}\) from \(c_{kl}\). Start with the inverse DFT definition:
\[
y_{mn} = \frac{1}{MN} \sum_{k=0}^{M-1} \sum_{l=0}^{N-1} c_{kl} \exp\left[\mathrm{i} 2 \pi \left(\frac{k m}{M} + \frac{l n}{N}\right)\right].
\]

Substituting the expression for \(c_{kl}\):
\[
y_{mn} = \frac{1}{MN} \sum_{k=0}^{M-1} \sum_{l=0}^{N-1} \left( \sum_{m'=0}^{M-1} \sum_{n'=0}^{N-1} y_{m'n'} \exp\left[-\mathrm{i} 2 \pi \left(\frac{k m'}{M} + \frac{l n'}{N}\right)\right] \right) \exp\left[\mathrm{i} 2 \pi \left(\frac{k m}{M} + \frac{l n}{N}\right)\right].
\]

Reordering the sums and recognizing the orthogonality of the exponential terms, we simplify to:
\[
y_{mn} = \sum_{m'=0}^{M-1} \sum_{n'=0}^{N-1} y_{m'n'} \left( \frac{1}{MN} \sum_{k=0}^{M-1} \exp\left[\mathrm{i} 2 \pi \frac{k (m - m')}{M}\right] \sum_{l=0}^{N-1} \exp\left[\mathrm{i} 2 \pi \frac{l (n - n')}{N}\right] \right).
\]

The sums over \(k\) and \(l\) evaluate to \(MN\) when \(m = m'\) and \(n = n'\), and zero otherwise, yielding:
\[
y_{mn} = y_{mn}.
\]

Thus, the inverse transform is:
\[
y_{mn} = \frac{1}{MN} \sum_{k=0}^{M-1} \sum_{l=0}^{N-1} c_{kl} \exp\left[\mathrm{i} 2 \pi \left(\frac{k m}{M} + \frac{l n}{N}\right)\right].
\]
#+end_proof
#+NAME: 2D DFT for real-valued functions
#+ATTR_LATEX: :environment remark
#+begin_remark latex
If the samples \(y_{mn}\) are realas they almost always arethen there is a further point to notice. When we perform the first set of Fourier transforms, Eq. (7.25), we transform a row of \(N\) real numbers for each value of \(l\), resulting in \(\frac{1}{2} N + 1\) independent Fourier coefficients (if \(N\) is even) or \(\frac{1}{2}(N + 1)\) (if \(N\) is odd), with the remaining coefficients being the complex conjugates. When performing the second set of transforms in Eq. (7.26), we transform \(M\) complex numbers, necessitating the calculation of all \(M\) Fourier coefficients. Thus, the 2D Fourier transform of an \(M \times N\) grid of real numbers results in a grid of complex numbers with \(M \times (\frac{1}{2} N + 1)\) independent coefficients if \(N\) is even, or \(M \times \frac{1}{2}(N + 1)\) if \(N\) is odd.
#+end_remark
#+NAME: 2D DFT for real-valued functions
#+begin_corollary latex
If the samples \(y_{mn}\) are real, then the two-dimensional Fourier transform yields a grid of complex numbers with \(M \times (\frac{1}{2} N + 1)\) independent coefficients if \(N\) is even, or \(M \times \frac{1}{2}(N + 1)\) if \(N\) is odd.
#+end_corollary
#+NAME: 2D DFT for real-valued functions
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Suppose the samples \(y_{mn}\) are real. When we perform the first set of Fourier transforms, we apply Eq. (7.25):
\[
c_{ml}^{\prime} = \sum_{n=0}^{N-1} y_{mn} \exp\left(-\mathrm{i} \frac{2 \pi l n}{N}\right).
\]
Since \(y_{mn}\) are real, each row's transform results in \(\frac{1}{2} N + 1\) independent Fourier coefficients if \(N\) is even, or \(\frac{1}{2}(N + 1)\) if \(N\) is odd. The remaining coefficients are complex conjugates.

For the second set of transforms in Eq. (7.26), we transform \(M\) complex numbers:
\[
c_{kl} = \sum_{m=0}^{M-1} c_{ml}^{\prime} \exp\left(-\mathrm{i} \frac{2 \pi k m}{M}\right).
\]
Since these values are complex, all \(M\) Fourier coefficients must be calculated. Therefore, the two-dimensional Fourier transform of an \(M \times N\) grid of real numbers results in \(M \times (\frac{1}{2} N + 1)\) independent coefficients if \(N\) is even, or \(M \times \frac{1}{2}(N + 1)\) if \(N\) is odd.
#+end_proof
#+NAME: Applications of 2D DFT
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Two-dimensional transforms are used, for example, in image processing, and are widely employed in astronomy to analyze photographs of the sky and reveal features that are otherwise hard to make out. They are also used in the confocal microscope and the electron microscope, two instruments that find use in many branches of science.
#+end_remark
#+NAME: Physical interpretation
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The Fourier transform decomposes a function into a set of real or complex sinusoidal waves, each with a distinct frequency. In the expression
\[
y_n = \frac{1}{N} \sum_{k=0}^{N-1} c_k \exp\left(\mathrm{i} \frac{2 \pi k n}{N}\right),
\]
each term represents a wave with its own frequency. For spatial functions, these are spatial frequencies; for temporal functions, they are temporal frequencies like musical notes. The Fourier transform indicates how a function can be expressed as a sum of waves of given frequencies, with the coefficients \(c_k\) specifying the amplitude of each frequency.

Physically, the Fourier transform reveals the frequency breakdown of a signal. This is analogous to the "signal analyzers" in home stereo systems, which graphically display the frequencies present in music by performing a Fourier transform and showing the result.
#+end_remark
#+begin_src python :noeval
from numpy import loadtxt, abs, exp, pi, zeros
import matplotlib.pyplot as plt
import numpy as np

def dft(y):
    N = len(y)
    c = zeros(N, dtype=complex)
    for k in range(N // 2 + 1):
        for n in range(N):
            c[k] += y[n] * exp(-2j * pi * k * n / N)
    if np.isrealobj(y):
        for k in range(1, N // 2):
            c[N - k] = np.conj(c[k])
    else:
        for k in range(N // 2 + 1, N):
            for n in range(N):
                c[k] += y[n] * exp(-2j * pi * k * n / N)
    return c

# Load the data from a text file
y = loadtxt("/home/b/library/books/physics/newman-comp-phy/resources/exercises/chapter7/pitch.txt", float)

# Compute the DFT of the data
c = dft(y)

# Create a figure with two subplots
plt.figure(figsize=(5, 4))  # Smaller width and appropriate height for two subplots

# Plot the original data in the first subplot
plt.subplot(2, 1, 1)  # Two rows, one column, first subplot
plt.plot(y, color='black', linewidth=0.5)  # Adjusted line width and color
plt.title("Original Pitch Data")
plt.tight_layout()

# Plot the magnitude of the Fourier coefficients in the second subplot
plt.subplot(2, 1, 2)  # Two rows, one column, second subplot
plt.plot(abs(c), color='black', linewidth=0.5)  # Adjusted line width and color
plt.xlim(0, 500)
plt.title("Magnitude of Fourier Coefficients")
plt.tight_layout()

# Save the combined figure
plt.savefig('/home/b/.local/images/pitch.png')
plt.close()
#+end_src
#+HTML_ATTR: :width 30px
#+CAPTION: An example signal is shown in the top panel. It has an overall wavelike shape with a well-defined frequency, but also contains some noise, or randomness. The DFT of the signal shown in the bottom panel.
[[file:~/.local/images/pitch.png]]
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Consider the figure above. The signal consists of a primary wave with a well-defined frequency and additional noise, seen as smaller wiggles. Listening to this signal would produce a constant note with background hiss. The Fourier coefficients are complex, and their absolute values measure the amplitude of each wave in the Fourier series.
The bottom panel shows the magnitude of these coefficients. The horizontal axis measures \(k\), proportional to the frequency, and the vertical axis shows \(\left|c_{k}\right|\), the magnitude of the coefficients. The largest spike corresponds to the main wave's frequency in the top panel. The smaller spikes are harmonics, indicating the original wave is not a pure sine wave. The jagged line represents noise, appearing as a uniform random background due to "white noise," which contains equal amounts of all frequencies.
Fourier transforms are useful in physics for understanding measurements or signals by breaking them down into component frequencies, providing a "spectrum analyzer" view of the data.
#+end_remark
#+NAME: Discrete cosine transform (DCT)
#+begin_proposition latex
Let \(y_{0}, \ldots, y_{n}, \ldots, y_{N-1}\) be symmetric samples such that \(y_n = y_{N-n}\). The discrete cosine transform (DCT) of these samples \(y_n\) yields the Fourier coefficient \(c_k\):
\[
c_k = y_0 + y_{N/2} \cos\left(\frac{2 \pi k (N/2)}{N}\right) + 2 \sum_{n=1}^{\frac{1}{2} N-1} y_n \cos\left(\frac{2 \pi k n}{N}\right).
\]
#+end_proposition
#+NAME: Discrete cosine transform (DCT)
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Given symmetric samples \(y_n = y_{N-n}\), we start with the Discrete Fourier Transform (DFT) for symmetric samples:
\[
c_k = \sum_{n=0}^{N-1} y_n \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
Splitting the sum into two parts:
\[
c_k = \sum_{n=0}^{\frac{1}{2} N} y_n \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right) + \sum_{n=\frac{1}{2} N+1}^{N-1} y_{N-n} \exp\left(\mathrm{i} \frac{2 \pi k (N-n)}{N}\right).
\]
Using the change of variables \(N-n \rightarrow n\) in the second sum, we get:
\[
c_k = \sum_{n=0}^{\frac{1}{2} N} y_n \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right) + \sum_{n=1}^{\frac{1}{2} N-1} y_n \exp\left(\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
Combining terms and using \(\cos \theta = \frac{1}{2} (\exp(\mathrm{i} \theta) + \exp(-\mathrm{i} \theta))\), we have:
\[
c_k = y_0 + y_{N/2} \cos\left(\frac{2 \pi k (N/2)}{N}\right) + 2 \sum_{n=1}^{\frac{1}{2} N-1} y_n \cos\left(\frac{2 \pi k n}{N}\right).
\]
#+end_proof
#+NAME: Inverse discrete cosine transform (IDCT)
#+begin_proposition latex
Let \(c_{0}, \ldots, c_{k}, \ldots, c_{N-1}\) be Fourier coefficients obtained via DCT. The inverse discrete cosine transform (IDCT) of these coefficients yields the original samples \(y_{n}\):
\[
y_n = \frac{1}{N}\left[c_0 + c_{N/2} \cos\left(\frac{2 \pi n (N/2)}{N}\right) + 2 \sum_{k=1}^{\frac{1}{2} N-1} c_k \cos\left(\frac{2 \pi k n}{N}\right)\right].
\]
#+end_proposition
#+NAME: Inverse discrete cosine transform (IDCT)
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Given the DCT coefficients, we aim to recover the original samples \(y_n\). Using the definition:
\[
y_n = \frac{1}{N} \sum_{k=0}^{N-1} c_k \exp\left(\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
For real samples and coefficients:
\[
y_n = \frac{1}{N}\left[\sum_{k=0}^{\frac{1}{2} N} c_k \exp\left(\mathrm{i} \frac{2 \pi k n}{N}\right) + \sum_{k=\frac{1}{2} N+1}^{N-1} c_{N-k} \exp\left(-\mathrm{i} \frac{2 \pi (N-k) n}{N}\right)\right].
\]
Using symmetry:
\[
y_n = \frac{1}{N}\left[c_0 + c_{N/2} \cos\left(\frac{2 \pi n (N/2)}{N}\right) + 2 \sum_{k=1}^{\frac{1}{2} N-1} c_k \cos\left(\frac{2 \pi k n}{N}\right)\right].
\]
Thus, the IDCT is:
\[
y_n = \frac{1}{N}\left[c_0 + c_{N/2} \cos\left(\frac{2 \pi n (N/2)}{N}\right) + 2 \sum_{k=1}^{\frac{1}{2} N-1} c_k \cos\left(\frac{2 \pi k n}{N}\right)\right].
\]
#+end_proof
#+NAME: Symmetrizing a function for DCT
#+begin_proposition latex
Any non-symmetric function on a finite interval can be symmetrized for the purposes of a Discrete Cosine Transform (DCT) by adding a mirror image of itself, thereby creating a symmetric periodic function.
#+end_proposition
#+NAME: Symmetrizing a function for DCT
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Consider the cosine series
\[
\sum_{k=0}^{N} A_k \cos \left(\frac{2 \pi k x}{L}\right), \quad A_k = \frac{2}{L} \int_{0}^{L} f(x) \cos \bigg(\frac{2 \pi k x}{L} \bigg) \mathrm{d}x.
\]
Functions of this form are symmetric about the midpoint of the interval at \(\frac{1}{2} L\), thus the cosine series can only represent symmetric functions. To symmetrize any function, we add a mirror image of itself, creating a symmetric periodic function.
Given a set of samples \(y_{n}\), we mirror these samples to form a symmetric set: \( y_n = y_{N-n} \quad \forall n \). This implies that the total number of samples \(N\) is always even. With symmetric samples, we have:
\[
y_0 = y_N, y_1 = y_{N-1}, y_2 = y_{N-2}, \ldots
\]
The DCT can be derived from the Discrete Fourier Transform (DFT) for symmetric samples:
\[
c_k = \sum_{n=0}^{N-1} y_n \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
Splitting the sum into two parts:
\[
c_k = \sum_{n=0}^{\frac{1}{2} N} y_n \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right) + \sum_{n=\frac{1}{2} N+1}^{N-1} y_{N-n} \exp\left(\mathrm{i} \frac{2 \pi k (N-n)}{N}\right).
\]
Using the change of variables \(N-n \rightarrow n\) in the second sum, we get:
\[
c_k = \sum_{n=0}^{\frac{1}{2} N} y_n \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right) + \sum_{n=1}^{\frac{1}{2} N-1} y_n \exp\left(\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
Combining terms and using \(\cos \theta = \frac{1}{2} (\exp(\mathrm{i} \theta) + \exp(-\mathrm{i} \theta))\), we have:
\[
c_k = y_0 + y_{N/2} \cos\left(\frac{2 \pi k (N/2)}{N}\right) + 2 \sum_{n=1}^{\frac{1}{2} N-1} y_n \cos\left(\frac{2 \pi k n}{N}\right).
\]
Thus, any non-symmetric function can be transformed into a symmetric periodic function, allowing the use of DCT.
#+end_proof
#+NAME: Cosine transform and non-periodic Data
#+ATTR_LATEX: :environment remark
#+begin_remark latex
A nice feature of the cosine transform is that, unlike the Discrete Fourier Transform (DFT), it does not assume the samples themselves are periodic. The function being transformed is first mirrored, so the first and last of the original samples are not required to take the same value. In the DFT, any function, periodic or not, can be made periodic by repeating it endlessly, making the last sample the same as the first. However, this process can create substantial discontinuities in the function, which can cause problems for DFTs. The discrete cosine transform, by contrast, does not suffer from these problems and is often preferable for data that are not inherently periodic.
#+end_remark
#+NAME: DCT properties and symmetry
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Note how the indices of both samples and Fourier coefficients run from 0 to \(\frac{1}{2} N\), so there are \(\frac{1}{2} N + 1\) of each in total. The DCT takes \(\frac{1}{2} N + 1\) real numbers and transforms them into \(\frac{1}{2} N + 1\) real coefficients, and the inverse DCT does the reverse. The forward and inverse transforms are actually the same mathematical expression, except for the leading factor of \(\frac{1}{N}\):
\[
y_n = \frac{1}{N}\left[c_0 + c_{N/2} \cos\left(\frac{2 \pi n (N/2)}{N}\right) + 2 \sum_{k=1}^{\frac{1}{2} N-1} c_k \cos\left(\frac{2 \pi k n}{N}\right)\right].
\]
Thus, this transform is sometimes said to be its own inverse (except for the factor \(\frac{1}{N}\)).
#+end_remark
#+NAME: Type-II discrete cosine transform (Type-II DCT)
#+begin_proposition latex
Let \(y_{0}, \ldots, y_{n}, \ldots, y_{N-1}\) be symmetric samples such that \(y_n = y_{N-1-n}\). The Type-II discrete cosine transform (DCT) of these samples \(y_n\) yields the Fourier coefficient \(a_k\):
\[
a_k = \sum_{n=0}^{N-1} y_n \cos\bigg(\frac{\pi k}{N} (n + 1/2)\bigg).
\]
The inverse transform is:
\[
y_n = \frac{1}{N}\bigg[a_0 + 2 \sum_{k=1}^{N-1} a_k \cos\bigg(\frac{\pi k}{N} (n + 1/2)\bigg)\bigg].
\]
#+end_proposition
#+NAME: Type-II discrete cosine transform (Type-II DCT)
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Consider the case where the sample points are in the middle of the sample intervals, and symmetry about the midpoint implies \(y_n = y_{N-1-n}\). Using the DFT, we have:
\[
c_k = \sum_{n=0}^{1/2 N-1} y_n \exp\bigg(-\mathrm{i} \frac{2 \pi k n}{N}\bigg) + \sum_{n=1/2 N}^{N-1} y_n \exp\bigg(-\mathrm{i} \frac{2 \pi k n}{N}\bigg).
\]
This can be rewritten as:
\[
c_k = \exp\bigg(\mathrm{i} \frac{\pi k}{N}\bigg) \bigg[\sum_{n=0}^{1/2 N-1} y_n \exp\bigg(-\mathrm{i} \frac{2 \pi k}{N} (n + 1/2)\bigg) + \sum_{n=0}^{1/2 N-1} y_{N-1-n} \exp\bigg(\mathrm{i} \frac{2 \pi k}{N} (n + 1/2)\bigg)\bigg].
\]
Simplifying and combining terms:
\begin{align*}
c_k &= \exp\bigg(\mathrm{i} \frac{\pi k}{N}\bigg) \bigg[\sum_{n=0}^{1/2 N-1} y_n \exp\bigg(-\mathrm{i} \frac{2 \pi k}{N} (n + 1/2)\bigg) + \sum_{n=0}^{1/2 N-1} y_n \exp\bigg(\mathrm{i} \frac{2 \pi k}{N} (n + 1/2)\bigg)\bigg] \\
&=2 \exp \left(\mathrm{i} \frac{\pi k}{N}\right) ~ \sum_{n=0}^{\frac{1}{2} N-1} \cos \left(\frac{2 \pi k\left(n+1/2\right)}{N}\right).
\end{align*}
Using \(\cos \theta = 1/2 (\exp(\mathrm{i} \theta) + \exp(-\mathrm{i} \theta))\), we define:
\[
a_k = 2 \sum_{n=0}^{1/2 N-1} y_n \cos\bigg(\frac{2 \pi k}{N} (n + 1/2)\bigg).
\]
Rewriting for conventional notation, we redefine \( 1/2 N \rightarrow N \) and \( a_k \rightarrow 2 a_k \):
\[
a_k = \sum_{n=0}^{N-1} y_n \cos\bigg(\frac{\pi k}{N} (n + 1/2)\bigg).
\]
The inverse transform is:
\[
y_n = \frac{1}{N}\bigg[a_0 + 2 \sum_{k=1}^{N-1} a_k \cos\bigg(\frac{\pi k}{N} (n + 1/2)\bigg)\bigg].
\]
Thus, the Type-II DCT transforms \(N\) inputs \(y_n\) into \(N\) outputs \(a_k\).
#+end_proof
#+NAME: Type-II DCT
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The Type-II Discrete Cosine Transform (DCT) is often preferred for its elegance and common usage. In this transform, symmetry about the midpoint implies \(y_n = y_{N-1-n}\), and the sample points are in the middle of the intervals. This version of the DCT transforms \(1/2 N\) inputs \(y_n\) into \(1/2 N\) outputs \(a_k\). To simplify notation, it is common to redefine \(1/2 N \rightarrow N\) and \(a_k \rightarrow 2 a_k\), resulting in:
\[
a_k = \sum_{n=0}^{N-1} y_n \cos \bigg(\frac{\pi k (n + 1/2)}{N}\bigg),
\]
\[
y_n = \frac{1}{N}\bigg[a_0 + 2 \sum_{k=1}^{N-1} a_k \cos \bigg(\frac{\pi k (n + 1/2)}{N}\bigg)\bigg].
\]
This form is sometimes referred to as "the" discrete cosine transform due to its prevalence.
#+end_remark
#+NAME: Discrete sine transform
#+ATTR_LATEX: :environment remark
#+begin_remark latex
One can also calculate discrete sine transforms, as in Eq. (7.2). Sine transforms are used less often than cosine transforms because they force the function \(f(x)\) to be zero at either end of its range. This limitation reduces their applicability in real-world applications, where relatively few functions exhibit this behavior. However, the sine transform finds use in physics for representing functions with boundary conditions that force them to be zero at the ends of an interval. Examples include the displacement of a vibrating string clamped at both ends or quantum wavefunctions in a closed box.
#+end_remark
#+NAME: DCT and the music and television industry
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Without the discrete cosine transform, we would not have a music and television industry - streaming music services, music downloads, internet radio, digital TV, etc. The following 4 remarks show why.
#+end_remark
#+NAME: DCT in Image Compression (JPEG)
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Discrete cosine transforms (DCTs) form the basis of the JPEG image format. Digital images are grids of pixels, each represented by a number. Storing these numbers directly creates large files. JPEG reduces file size by dividing an image into blocks, performing 2D Type-II DCTs on these blocks, and discarding small coefficients. The remaining coefficients are stored, and the image is reconstructed using the inverse DCT when viewed. This technique significantly reduces file size but can introduce compression artifacts due to discarded data.
#+end_remark
#+NAME: DCT in Video Compression (MPEG)
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The MPEG format for compressing video uses a variant of the DCT technique. Video frames are divided into blocks, transformed using DCT, and small coefficients are discarded. This reduces the storage and transmission size of video files. While this compression can lead to minor visual artifacts, it enables efficient storage and transmission of television broadcasts, DVDs, and Internet video.
#+end_remark
#+NAME: DCT in Audio Compression (MP3)
#+ATTR_LATEX: :environment remark
#+begin_remark latex
MP3 audio compression also uses DCT. Audio signals are digitized into samples, divided into blocks, and transformed using DCT. Some Fourier components are discarded to save space, guided by what the human ear can and cannot hear. For instance, high-frequency sounds masked by loud low-frequency sounds are discarded. This method allows efficient storage and transmission of audio recordings, crucial for streaming services, music downloads, and digital radio.
#+end_remark
#+NAME: Importance of DCT in Digital Media
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The digital media economy relies heavily on DCT for compressing images, video, and audio. Technologies like JPEG, MPEG, and MP3 use DCT to reduce file sizes, enabling efficient storage and transmission. This compression makes modern digital media services possible, from streaming music to digital TV.
#+end_remark
*** The fast Fourier transform (FFT)
#+NAME: Computability rule
#+ATTR_LATEX: :environment remark
#+begin_remark latex
A useful rule of thumb is: the largest number of operations you can do in a computer program is about a billion if you want it to run in a reasonable amount of time.
#+end_remark
#+NAME: Motivating the fast Fourier transform
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The discrete Fourier transform (DFT) is defined by:
\[
c_{k}=\sum_{n=0}^{N-1} y_{n} \exp \left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
To compute this for each of the \(\frac{1}{2} N+1\) distinct coefficients \(c_{k}\), we need \(N\left(\frac{1}{2} N+1\right)\) terms, resulting in a little over \(\frac{1}{2} N^{2}\) arithmetic operations. Given the computability rule, setting \(\frac{1}{2} N^{2}=10^{9}\) and solving for \(N\), the largest feasible set of samples for timely DFT calculation has about \(N \simeq 45000\) samples. We are treating the evaluation of each term in the sum as a single operation, which is not really correcteach term involves several multiplication operations and the calculation of an exponential. However, the billion-operation rule of thumb is only an approximation anyway, so a rough estimate of the number of operations is good enough.
In any case, \(N \simeq 45000\) samples is often insufficient for practical applications, such as representing one second of audio signals. To handle larger datasets efficiently, we use a technique called the fast Fourier transform (FFT), discovered by Carl Friedrich Gauss in 1805, which significantly speeds up the calculation.
#+end_remark
#+ATTR_LATEX: :environment remark
#+begin_remark latex
In much of the computational physics literature you will see the fast Fourier transform attributed not to Gauss but to the computer scientists James Cooley and John Tukey, who published a paper describing it in 1965 [Cooley, J. W. and Tukey, J. W., An algorithm for the machine calculation of complex Fourier series, Mathematics of Computation 19, 297-301 (1965)]. Although Cooley and Tukey's paper was influential in popularizing the method, however, it was, unbeknownst to its authors, not the first description of the FFT. Only later did they learn that they'd been scooped by Gauss 160 years earlier.
#+end_remark
#+NAME: Gauss's contributions to computational physics
#+ATTR_LATEX: :environment remark
#+begin_remark latex
What with Gaussian quadrature, Gaussian elimination, the Gauss-Newton method, and now the fast Fourier transform, Gauss seems to have discovered most of computational physics more than a century before the computer was invented.
#+end_remark
#+NAME: Danielson-Lanczos lemma
#+begin_lemma latex
The discrete Fourier transform (DFT) of a sequence of length \( N \) can be decomposed into two smaller DFTs of sequences of length \( N/2 \). This is expressed as follows:
\[
X_k = E_k + e^{-2\pi i k / N} O_k, \quad X_{k+N/2} = E_k - e^{-2\pi i k / N} O_k
\]
for \( k = 0, 1, \ldots, N/2 - 1 \), where \( X_k \) is the DFT of the original sequence, \( E_k \) is the DFT of the even-indexed subsequence, and \( O_k \) is the DFT of the odd-indexed subsequence.
#+end_lemma
#+NAME: Danielson-Lanczos lemma
#+begin_proof latex
To prove the lemma, consider a sequence \( x_n \) of length \( N \) and its DFT defined as:
\[
X_k = \sum_{n=0}^{N-1} x_n e^{-2\pi i k n / N}
\]
Now, split the sequence into even \( x_{2m} \) and odd \( x_{2m+1} \) indexed parts:
\[
X_k = \sum_{m=0}^{N/2-1} x_{2m} e^{-2\pi i k (2m) / N} + \sum_{m=0}^{N/2-1} x_{2m+1} e^{-2\pi i k (2m+1) / N}
\]
This can be rewritten using \( N' = N/2 \):
\[
X_k = \sum_{m=0}^{N'-1} x_{2m} e^{-2\pi i k m / N'} + e^{-2\pi i k / N} \sum_{m=0}^{N'-1} x_{2m+1} e^{-2\pi i k m / N'}
\]
Define \( E_k = \sum_{m=0}^{N'-1} x_{2m} e^{-2\pi i k m / N'} \) and \( O_k = \sum_{m=0}^{N'-1} x_{2m+1} e^{-2\pi i k m / N'} \), which are the DFTs of the even and odd parts, respectively. Thus:
\[
X_k = E_k + e^{-2\pi i k / N} O_k
\]
Using the periodicity of the DFT, \( X_{k+N'} \) for \( k < N' \) becomes:
\[
X_{k+N'} = \sum_{m=0}^{N'-1} x_{2m} e^{-2\pi i (k+N') m / N'} + e^{-2\pi i (k+N') / N} \sum_{m=0}^{N'-1} x_{2m+1} e^{-2\pi i (k+N') m / N'}
\]
The exponential terms simplify as \( e^{-2\pi i (k+N') m / N'} = e^{-2\pi i k m / N'} \) and \( e^{-2\pi i (k+N') / N} = e^{-2\pi i k / N} e^{-\pi i} = -e^{-2\pi i k / N} \), leading to:
\[
X_{k+N'} = E_k - e^{-2\pi i k / N} O_k
\]
This completes the proof, showing how \( X_k \) and \( X_{k+N/2} \) are constructed from \( E_k \) and \( O_k \) using the Fourier transform properties.
#+end_proof
#+NAME: Fast Fourier Transform (FFT)
#+begin_proposition latex
Let \(N = 2^m\) where \(m\) is an integer. The discrete Fourier transform of a function with \(N\) samples \(y_0, y_1, \ldots, y_{N-1}\) can be computed efficiently by recursively dividing the DFT into smaller DFTs, taking advantage of even and odd indexed terms. The DFT is defined as:
\[
c_k = \sum_{n=0}^{N-1} y_n \exp\left(-\mathrm{i} \frac{2 \pi k n}{N}\right).
\]
For \(k = 0, 1, \ldots, N-1\), we compute \(c_k\) by splitting the DFT into sums of even and odd indexed terms:
\[
E_k = \sum_{r=0}^{\frac{1}{2} N-1} y_{2r} \exp\left(-\mathrm{i} \frac{2 \pi k r}{\frac{1}{2} N}\right), \quad O_k = \sum_{r=0}^{\frac{1}{2} N-1} y_{2r+1} \exp\left(-\mathrm{i} \frac{2 \pi k r}{\frac{1}{2} N}\right).
\]
so that the Fourier coefficient \(c_k\) is
\[
c_k = E_k + \exp\left(-\mathrm{i} \frac{2 \pi k}{N}\right) O_k.
\]
Since \( E_{k} \) and \( O_{k} \) are just DFTs of a smaller subset of the \(y_0, y_1, \ldots, y_{N-1}\), they admit similar splits into DFTs of even smaller subsets. This divide and conquer procedure of computing DFTs is called the fast Fourier transform (FFT). The FFT process starts at the last level and works backwards, combining results until the full DFT is reconstructed.
#+end_proposition
#+NAME: Fast Fourier Transform (FFT)
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Consider the sum in the DFT equation and divide the terms into two equally sized groups, which we can do when \(N\) is a power of two. The even terms are where \(n = 2r\) and the odd terms are where \(n = 2r + 1\).

For the even terms:
\[
E_k = \sum_{r=0}^{\frac{1}{2} N-1} y_{2r} \exp\left(-\mathrm{i} \frac{2 \pi k (2r)}{N}\right) = \sum_{r=0}^{\frac{1}{2} N-1} y_{2r} \exp\left(-\mathrm{i} \frac{2 \pi k r}{\frac{1}{2} N}\right),
\]
which is a Fourier transform with \(\frac{1}{2} N\) samples.

For the odd terms:
\[
\sum_{r=0}^{\frac{1}{2} N-1} y_{2r+1} \exp\left(-\mathrm{i} \frac{2 \pi k (2r+1)}{N}\right) = \exp\left(-\mathrm{i} \frac{2 \pi k}{N}\right) \sum_{r=0}^{\frac{1}{2} N-1} y_{2r+1} \exp\left(-\mathrm{i} \frac{2 \pi k r}{\frac{1}{2} N}\right) = \exp\left(-\mathrm{i} \frac{2 \pi k}{N}\right) O_k,
\]
where \(O_k\) is another Fourier transform with \(\frac{1}{2} N\) samples.

Thus, the complete Fourier coefficient \(c_k\) is:
\[
c_k = E_k + \exp\left(-\mathrm{i} \frac{2 \pi k}{N}\right) O_k.
\]

In other words, \(c_k\) is the sum of two smaller DFTs plus a twiddle factor \(\exp\left(-\mathrm{i} \frac{2 \pi k}{N}\right)\).

To compute these smaller DFTs, we repeat the process by splitting each into their even and odd terms. Because \(N\) is a power of two, we can keep dividing until each transform is just a single sample. The Fourier transform of a single sample \(y_0\) is:
\[
c_0 = y_0.
\]

Stages of decomposition:

At stage \(m\), there are \(2^m\) sets, each with \(N/2^m\) samples. The DFT of the \(j\)th set is:
\[
\sum_{r=0}^{N / 2^m - 1} y_{2^m r + j} \exp \left(-\mathrm{i} \frac{2 \pi k (2^m r + j)}{N}\right) = \exp\left(-\mathrm{i} \frac{2 \pi k j}{N}\right) \sum_{r=0}^{N / 2^m - 1} y_{2^m r + j} \exp \left(-\mathrm{i} \frac{2 \pi k r}{N / 2^m}\right) = \exp\left(-\mathrm{i} \frac{2 \pi k j}{N}\right) E_k^{(m, j)},
\]
where \(E_k^{(m, j)}\) is the DFT of the set with \(N / 2^m\) samples.

Recursive formula:

Using the recursive relationship:
\[
E_k^{(m, j)} = E_k^{(m+1, j)} + \exp\left(-\mathrm{i} \frac{2 \pi 2^m k}{N}\right) E_k^{(m+1, j + 2^m)},
\]
we can compute the full DFT by starting at the last level \(m = \log_2 N\) and working backwards to \(m = 0\). For \(m = 0\):
\[
E_k^{(0, 0)} = \sum_{r=0}^{N-1} y_r \exp\left(-\mathrm{i} \frac{2 \pi k r}{N}\right) = c_k,
\]
yielding the final Fourier coefficients \(c_k\).

Handling coefficient ranges:

For any \(m\), we need to evaluate \(E_k^{(m, j)}\) for \(k = 0 \ldots N / 2^m - 1\). Coefficients beyond \(N / 2^{m+1} - 1\) repeat:
\[
E_{N / 2^{m+1} + s}^{(m+1, j)} = \exp(-\mathrm{i} 2 \pi r) \sum_{r=0}^{N / 2^{m+1} - 1} y_{2^{m+1} r + j} \exp \left(-\mathrm{i} \frac{2 \pi s r}{N / 2^{m+1}}\right) = E_s^{(m+1, j)}.
\]
Thus, evaluating:
\[
E_k^{(m, j)} = E_k^{(m+1, j)} + \exp\left(-\mathrm{i} \frac{2 \pi 2^m k}{N}\right) E_k^{(m+1, j + 2^m)}
\]
for all \(k\) is straightforward.
#+end_proof
#+NAME: FFT using Numpy
#+begin_remark latex
In Python, fast Fourier transforms are provided by the module =numpy.fft=.
1) =rfft= and =irfft= for the FFT and the inverse FFT respectively of real samples
2) =fft= and =ifft= for the FFT and inverse FFT of complex samples
3) =rfft2= and =irfft2= for the 2D FFT and the inverse 2D FFT respectively of real samples
4) =fft2= and =ifft2= for the 2D FFT and the inverse 2D FFT respectively of complex samples
All of these work for any number of samples, not necessarily powers of \( 2 \).
The 1D Fourier transforms take a one-dimensional array as input and return a one-dimensional array. When the samples are real and the length of the array \( N \) is even, only the first \(N/2+1\) coefficients are independent and the rest are their complex conjugates. =rfft= returns only the first \(N/2+1\) coefficients. Likewise, the =irfft= function takes as input an array that has \(N/2+1\) complex elements and creates an array with \(N\) real elements.
The 2D Fourier transforms take a two-dimensional array as input and return a two-dimensional array. When the samples are real and the length of each row \( N \) is even, only the first \( N/2 + 1 \) coefficients are independent and rest are their complex conjugates. =rfft2= returns only the first \( N/2 + 1 \) coefficients, so if there are \( M \) rows so that the input array is an \(M \times N\) grid of real numbers, the output is a grid of complex numbers with dimensions \(M \times\left(N/2+1\right)\). Likewise, the =irfft2= function takes as input a grid of complex numbers with dimensions \(M \times\left(N/2+1\right)\) and returns an \( M \times N \) grid of real numbers.
#+end_remark
#+NAME: Custom FFT (N = 2^m)
#+begin_src python :results output
import numpy as np
from numpy.fft import rfft, irfft, ifft
import timeit

def fft(y):
    N = len(y)
    if N & (N - 1) != 0:
        raise ValueError("Length of the input vector must be a power of 2")

    y = np.array(y, dtype=complex)  # Ensure y is complex for handling twiddle factors

    # Bit-reversed permutation
    j = 0
    for i in range(1, N):
        bit = N >> 1
        while j >= bit:
            j -= bit
            bit >>= 1
        j += bit
        if i < j:
            y[i], y[j] = y[j], y[i]

    # Danielson-Lanczos lemma
    step = 2
    while step <= N:
        halfstep = step // 2
        w_phase_step = -2j * np.pi / step
        for i in range(0, N, step):
            w_phase = 1
            for k in range(halfstep):
                u = y[i + k]
                t = w_phase * y[i + k + halfstep]
                y[i + k] = u + t
                y[i + k + halfstep] = u - t
                w_phase *= np.exp(w_phase_step)  # Apply twiddle factor progressively
        step *= 2

    return y[:N//2 + 1]  # Return first N/2 + 1 elements to match rfft

# Test the FFTs
y = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7], dtype=float)
c_fft = fft(y)
c_rfft = rfft(y)
y_fft = irfft(c_fft, n=len(y))
y_rfft = irfft(c_rfft, n=len(y))
# Printing results
print("Fourier coefficients from custom FFT:")
print(c_fft)
print("\nFourier coefficients from numpy's rfft:")
print(c_rfft)
print("\nRecovered sequence from custom FFT (using ifft):")
print(np.round(y_fft, decimals=1))
print("\nRecovered sequence from numpy's rfft (using irfft):")
print(np.round(y_rfft, decimals=1))

# Generate a random array of length 2**10, a typical size used in FFTs
test_array = np.random.random(1024)

# Time the custom FFT
custom_fft_time = timeit.timeit('fft(test_array)', globals=globals(), number=100)

# Time NumPy's rfft
numpy_rfft_time = timeit.timeit('rfft(test_array)', globals=globals(), number=100)

print(f"\nCustom FFT function time: {custom_fft_time} seconds")
print(f"\nNumPy rFFT function time: {numpy_rfft_time} seconds")
#+end_src
#+RESULTS: Custom FFT (N = 2^m)
#+begin_example
Fourier coefficients from custom FFT:
[ 2.8+0.j         -0.4+0.96568542j -0.4+0.4j        -0.4+0.16568542j
 -0.4+0.j        ]

Fourier coefficients from numpy's rfft:
[ 2.8+0.j         -0.4+0.96568542j -0.4+0.4j        -0.4+0.16568542j
 -0.4+0.j        ]

Recovered sequence from custom FFT (using ifft):
[-0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7]

Recovered sequence from numpy's rfft (using irfft):
[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7]

Custom FFT function time: 0.651052994071506 seconds

NumPy rFFT function time: 0.0005206390051171184 seconds
#+end_example
#+NAME: Fast cosine transform
#+ATTR_LATEX: :environment remark
#+begin_remark latex
The module =numpy.fft= does not provide functions for fast cosine and sine transforms, but it is not hard to create functions that do the job. A discrete cosine transform is nothing more than an ordinary discrete Fourier transform performed on a set of samples that are symmetric around the middle of the transform interval. The most common type of discrete cosine transform, often just called "the" discrete cosine transform, corresponds to samples that satisfy \(y_{2N-1-n}=y_{n}\) for \(n=0 \ldots N-1\). Given \(N\) samples, we can calculate the discrete cosine transform by "mirroring" those samples to create a symmetric array of twice the size, performing an ordinary discrete Fourier transform on the result, and discarding the imaginary parts of the coefficients (which should be zero anyway).
#+end_remark
#+NAME: Fast cosine transform
#+begin_src python
from numpy.fft import rfft
from numpy import empty,arange, exp,real,pi
def dct(y):
    N = len(y)
    y2 = empty(2*N,float)
    for n in range(N):
        y2 [n] = y[n]
        y2[2*N-1-n] = y[n]
    c = rfft(y2)
    phi = exp(-1j*pi*arange(N)/(2*N))
    return real(phi*c[:N])
#+end_src
#+NAME: Fast cosine transform
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Note the use of the multiplier phi, which accounts for the leading phase factor in the Type-II DCT
\[
c_{k} = 2 \exp \left(\mathrm{i} \frac{\pi k}{N}\right) ~ \sum_{n=0}^{\frac{1}{2} N-1} \cos \left(\frac{2 \pi k\left(n+1/2\right)}{N}\right).
\]
#+end_remark
** Numerical Linear Algebra
:LOGBOOK:
CLOCK: [2024-06-24 Mon 04:32]--[2024-06-24 Mon 05:39] =>  1:07
CLOCK: [2024-06-09 Sun 03:06]--[2024-06-09 Sun 05:07] =>  2:01
CLOCK: [2024-06-08 Sat 13:51]--[2024-06-08 Sat 16:58] =>  3:07
CLOCK: [2024-06-08 Sat 11:22]--[2024-06-08 Sat 12:47] =>  1:25
:END:
*** Basics
#+NAME: Vector Space
#+ATTR_LATEX: :environment definition
#+BEGIN_definition
Let \(\mathbb{V} = \left \lbrace \mathbf{1}, \thinspace \mathbf{2}, \thinspace \dotso, \thinspace \mathbf{v}, \thinspace \mathbf{w}, \dotso \right \rbrace\) be a set of elements. Let \(\mathbb{F}\) be a /field/ over scalars, \(B_{1}\) and \(B_{2}\) be two binary operations defined on the elements of \(\mathbb{V}\) such that:

\[B_{1}: \mathbb{V} \times \mathbb{V} \to \mathbb{V}\]
\[B_{2}: \mathbb{F} \times \mathbb{V} \to \mathbb{V}\]

\(\mathbb{V}\) called a /vector space/, and the elements called /vectors/ if: 

+ \(B_{2}\) is /associative/, /distributive in scalars/, /distributive in vectors/,
+ \(B_{1}\) is /associative/ and /commutative/
+ There is a /null vector/ \(\mathbf{0}\) in \(\mathbb{V}\) that satisfies

  \[ \mathbf{v} + \mathbf{0} = \mathbf{v},\]
+ For every vector \(\mathbf{v}\), there exists an /inverse under addition/ \(\mathbf{-v}\) such that

  \[\mathbf{v} + \mathbf{-v} = \mathbf{0}\].
#+END_definition
#+NAME: Field
#+ATTR_LATEX: :environment definition
#+begin_definition latex
The numbers \(a\), \(b\), \(\dotso\), are called the /field/ over which the vector space is defined.
#+end_definition
#+NAME: Linear Independence
#+ATTR_LATEX: :environment definition
#+begin_definition latex
A set of vectors is called linearly independent /iff/:
\begin{align*}
\sum_{i=1}^{n} a_{i} \mathbf{e} = 0 \Longrightarrow a_{i} = 0 \quad \text{for all} \quad i.
\end{align*}
#+end_definition
#+NAME: Dimensionality
#+ATTR_LATEX: :environment definition
#+begin_definition latex
A vector space has dimensions \(n\) if it can accommodate a maximum of \(n\) linearly independent vectors. When such a vector space is defined over the field of real numbers it is denoted by \(\mathbb{V}^n(R)\). When it is defined over the field of complex numbers it is denoted by \(\mathbb{V}^{n}(C)\).
#+end_definition
#+NAME: Span
#+ATTR_LATEX: :environment theorem
#+begin_theorem latex
Any \(\mathbf{v} \in V^n\) may be written as a linear combination of \(n\) linearly independent vectors \(\{\mathbf{1}, \ldots, \mathbf{n}\}\).
#+end_theorem
#+NAME: Basis
#+ATTR_LATEX: :environment definition
#+begin_definition latex
A set of \(n\) linearly independent vectors in an n-dimensional vector space is called a /basis/.

\[\mathbf{v}=\sum_{i=1}^n v_i \mathbf{e}\]

where \(\left \lbrace \mathbf{1}, \mathbf{2}, \dotso, \mathbf{n} \right \rbrace\) form a /basis/.
#+end_definition
#+NAME: Components of a Vector
#+ATTR_LATEX: :environment definition
#+begin_definition latex
The coefficients of expansion \(v_i\) of a vector in terms of a linearly independent basis \(\{\mathbf{e}\}\) are called the components of the vector in that basis.
#+end_definition
#+NAME: Uniqueness of Vector Expansion
#+ATTR_LATEX: :environment theorem
#+begin_theorem latex
The expansion of a vector \(\mathbf{v}\), in a given basis \(\{ \mathbf{e}\}\), with the coefficients \(\left\{v_i\right\}\) is unique.
If \(\mathbf{v}\) and \(\mathbf{w}\) are expanded in the same basis, we have
\begin{align*}
&\mathbf{v}=\sum v_i \mathbf{e}, \quad
\mathbf{w}=\sum \omega_i \mathbf{e}, \quad \mathbf{v} + \mathbf{w} = \sum\left(v_i + w_i\right) \mathbf{e}
\end{align*}
Given a scalar \( a \) and the expansion \( \mathbf{v}=\sum v_i \mathbf{e} \) we have
\begin{align*}
\mathbf{v}=\sum v_i \mathbf{e}, \quad a\mathbf{v}=\sum a v_i \mathbf{e}.
\end{align*}
#+end_theorem
#+NAME: Inner Product
#+ATTR_LATEX: :environment definition
#+begin_definition latex
The inner product between two vectors \(\mathbf{v}\) and \(\mathbf{w}\), denoted as \(\langle \mathbf{v}, \mathbf{w}\rangle\), is a number (complex in general) that has the properties:
(a) \(\langle \mathbf{v}, \mathbf{w}\rangle = \langle \mathbf{w}, \mathbf{v}\rangle^*\) (Skew-symmetry)
(b) \(\langle \mathbf{v}, \mathbf{v}\rangle \geqslant 0,\ \langle \mathbf{v}, \mathbf{v}\rangle = 0 \Longleftrightarrow \mathbf{v} = \mathbf{0}\) (Positive semi-definiteness)
(c) \(\langle \mathbf{v}, a \mathbf{w} + b \mathbf{z} \rangle = a \langle \mathbf{v}, \mathbf{w} \rangle + b \langle \mathbf{v}, \mathbf{z} \rangle\) (Linearity in the second argument)
#+end_definition
#+NAME: Anti-linearity of the Inner Product
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
Skew-symmetry implies /anti-linearity of the inner product in the first argument/.
\(\langle a \mathbf{w} + b \mathbf{z}, \mathbf{v} \rangle = a^* \langle \mathbf{w}, \mathbf{v} \rangle + b^* \langle \mathbf{z}, \mathbf{v} \rangle\)
#+end_corollary
#+NAME: Orthogonality
#+ATTR_LATEX: :environment definition
#+begin_definition
\(\mathbf{v}\) and \(\mathbf{w}\) are called orthogonal (to each other) iff \(\langle \mathbf{v}, \mathbf{w} \rangle = 0\).
#+end_definition
#+NAME: Norm
#+begin_definition latex
Given a vector \(\mathbf{v}\), \(\sqrt{\langle \mathbf{v}, \mathbf{v}\rangle} \equiv \|\mathbf{v}\|\) is called its norm. A normalized vector has unit norm.
#+end_definition
#+NAME: Orthonormal Basis
#+begin_definition latex
A set of normalized basis vectors that are pairwise orthogonal are said to form an orthonormal basis.
#+end_definition
#+NAME: Inner product of Vector expansions
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Given two vectors \(\mathbf{v}\) and \(\mathbf{w}\) expanded in two different bases \(\{\mathbf{e}_i\}\) and \(\{\mathbf{e}_j\}\), their inner product is given as 
\[\langle \mathbf{v}, \mathbf{w} \rangle = \sum_i \sum_j v_i^* w_j \langle \mathbf{e}_i, \mathbf{e}_j \rangle\].
For every vector \(\mathbf{v}\), there exists an adjoint, denoted by \(\mathbf{v}^\dagger\). The adjoint is the vector's transpose-conjugate. Bras and kets reside in distinct vector spaces. Formally, the inner product is a bilinear mapa machine that takes vectors from two distinct vector spaces, the space of the kets and a dual space of bras, and produces a number.
\begin{equation*}
\mathbf{v} \leftrightarrow \left[\begin{array}{c}
v_1 \\
\vdots \\
v_n
\end{array}\right] \leftrightarrow \left[v_1^*, \ldots, v_n^*\right] \leftrightarrow \mathbf{v}^\dagger
\end{equation*}
Expanding two vectors in a common orthonormal basis simplifies the inner product:
\begin{align*}
\langle \mathbf{v}, \mathbf{w} \rangle = \sum_i \sum_j v_i^* w_j \underbrace{ \langle \mathbf{e}_i, \mathbf{e}_j \rangle }_{\equiv \delta_{ij}} &= \sum_i v_i^* w_i \langle \mathbf{e}_i, \mathbf{e}_i \rangle = \sum_i v_i^* w_i.
\end{align*}
Expanding a vector in an orthonormal basis simplifies finding the coefficients:
\begin{align*}
\mathbf{v} = \sum_i v_i \mathbf{e}_i, \quad \langle \mathbf{e}_j, \mathbf{v} \rangle = \sum_i v_i \underbrace{\langle \mathbf{e}_j, \mathbf{e}_i \rangle}_{\equiv \delta_{ji}} = v_j \Longrightarrow \mathbf{v} = \sum_i \mathbf{e}_i \langle \mathbf{e}_i, \mathbf{v} \rangle.
\end{align*}
#+end_remark
#+NAME: Gram-Schmidt
#+begin_theorem latex
Any basis \(\{\mathbf{e}_i\}\) can be converted into an orthonormal basis. An orthonormal basis has the property:

\begin{align*}
\langle \mathbf{e}_i, \mathbf{e}_j \rangle = \delta_{ij} \equiv \begin{cases}1, & i = j \\ 0, & i \neq j\end{cases}.
\end{align*}

Here \(\delta_{ij}\) is the /Kronecker delta/.

Let \(\mathbf{v}_I\), \(\mathbf{v}_{II}\), \(\dotso\) be a linearly independent basis. The orthonormal basis \(\mathbf{e}_1, \mathbf{e}_2, \ldots\) is:

\(\mathbf{e}_1 = \mathbf{v}_I / \|\mathbf{v}_I\|\)

\(\mathbf{e}_2 = \mathbf{v}_{II}^\prime / \|\mathbf{v}_{II}^\prime\|, \quad \mathbf{v}_{II}^\prime = \mathbf{v}_{II} - \mathbf{e}_1 \langle \mathbf{e}_1, \mathbf{v}_{II} \rangle\)

\(\mathbf{e}_3 = \mathbf{v}_{III}^\prime / \|\mathbf{v}_{III}^\prime\|, \quad  \mathbf{v}_{III}^\prime = \mathbf{v}_{III} - \mathbf{e}_2 \langle \mathbf{e}_2, \mathbf{v}_{III} \rangle - \mathbf{e}_1 \langle \mathbf{e}_1, \mathbf{v}_{III} \rangle\)

Linear independence of \(\{ \mathbf{v}_I, \mathbf{v}_{II}, \dotso \}\) guarantees completion of the orthogonalization.
#+end_theorem
#+NAME: Dimensionality
#+begin_theorem latex
The dimensionality of a space equals \(n_{\perp}\), the maximum number of mutually orthogonal vectors in it.
#+end_theorem
#+NAME: Schwarz Inequality
#+begin_theorem latex
\begin{align*}
|\langle \mathbf{v}, \mathbf{w} \rangle| \leqslant \|\mathbf{v}\| \|\mathbf{w}\|.
\end{align*}
#+end_theorem
#+NAME: Triangle Inequality
#+begin_theorem latex
\begin{align*}
\|\mathbf{v} + \mathbf{w}\| \leq \|\mathbf{v}\| + \|\mathbf{w}\|.
\end{align*}
#+end_theorem
#+NAME: Subspace
#+begin_definition latex
Given a vector space \(\mathbb{V}\), a subset of its elements that form a vector space among themselves is called a /subspace/. The \(i\)-th subspace with dimensionality \(n_i\) is denoted as \(\mathbb{V}_i^{n_{i}}\).
#+end_definition
#+NAME: Sum of Vector (sub) Spaces
#+begin_definition latex
Given two subspaces \(\mathbb{V}_i^{n_i}, \mathbb{V}_j^{m_j}\), the sum, denoted as

\(\mathbb{V}_{i}^{n_i} \oplus \mathbb{V}_{j}^{m_j} \equiv \mathbb{V}_{k}^{o_{k}}\)

is the set containing:

(1) all elements of \(\mathbb{V}_i^{n_i}\)

(2) all elements of \(\mathbb{V}_j^{m_j}\)

(3) all possible linear combinations of the above.
#+end_definition
#+NAME: Linear Operator
#+begin_definition latex
A linear operator is a function from one vector space to another; a mapping between vectors.
\(\Omega: \mathbb{V} \rightarrow \mathbb{W}, \quad \Omega \mathbf{v} = \mathbf{w}\).
#+end_definition
#+NAME: Closed Linear Operator
#+ATTR_LATEX: :environment remark
#+begin_remark latex
A linear operator satisfying closure has the following properties:
\(\Omega: \mathbb{V} \rightarrow \mathbb{V}, \quad \Omega \mathbf{v} = \mathbf{v}'\)
+ \(\Omega \alpha \mathbf{v}_i = \alpha \Omega \mathbf{v}_i\)
+ \(\Omega(\alpha \mathbf{v}_i + \beta \mathbf{v}_j) = \alpha \Omega \mathbf{v}_i + \beta \Omega \mathbf{v}_j\)
+ \(\mathbf{v}_i^\dagger \alpha \Omega = \mathbf{v}_i^\dagger \Omega \alpha\)
+ \((\mathbf{v}_i^\dagger \alpha + \mathbf{v}_j^\dagger \beta) \Omega = \alpha \mathbf{v}_i^\dagger \Omega + \beta \mathbf{v}_j^\dagger \Omega\)
+ \(\Omega \mathbf{v} = \Omega \left( \sum_i v_i \mathbf{e}_i \right) = \sum_i v_i \Omega \mathbf{e}_i = \sum_i v_i \mathbf{e}_i'\)
#+end_remark
#+NAME: Commutator
#+begin_definition latex
\([\Omega, \Lambda] \equiv \Omega \Lambda - \Lambda \Omega\) is called the commutator of \(\Omega\) and \(\Lambda\).
#+end_definition
#+NAME: Commutator
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
\begin{align*}
&{[\Omega, \Lambda \theta]=\Lambda[\Omega, \theta]+[\Omega, \Lambda] \theta} \\
\end{align*}
#+end_corollary
#+NAME: Commutator
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
\begin{align*}
&{[\Lambda \Omega, \theta]=\Lambda[\Omega, \theta]+[\Lambda, \theta] \Omega}
\end{align*}
#+end_corollary
#+NAME: Identity Operator
#+begin_definition latex
\(I\) is the identity operator, a machine that returns the same vector it took.
#+end_definition
#+NAME: Inverse of an Operator
#+begin_definition latex
The inverse of an operator \(\Omega\), denoted by \(\Omega^{-1}\), by definition, satisfies:
\begin{align*}
\Omega \Omega^{-1}=\Omega^{-1} \Omega=I.
\end{align*}
#+end_definition
#+NAME: Existence of the Inverse of an Operator
#+ATTR_LATEX: :environment remark
#+begin_remark latex
In general, the existence of the inverse is not guaranteed.
#+end_remark
#+NAME: Inverse of a product of Operators
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
The inverse of a product of operators is given as:
\begin{align*}
(\Omega \Lambda)^{-1}=\Lambda^{-1} \Omega^{-1}.
\end{align*}
#+end_corollary
#+NAME: Matrix Elements of an Operator
#+begin_definition latex
In a given basis \(\{\mathbf{e}_i\}\) for \(\mathbb{V}^n\), an operator is represented as a matrix. The \(ij^{\text{th}}\) element of this matrix is
\begin{align*}
\Omega_{ij} = \langle \mathbf{e}_i, \Omega \mathbf{e}_j \rangle.
\end{align*}
The \(n^2\) numbers, \(\Omega_{ij}\), are the /matrix elements/ of \(\Omega\) in this basis.
#+end_definition
#+NAME: Vector Expansion under the action of an Operator
#+ATTR_LATEX: :environment definition
#+begin_definition latex
\begin{align*}
\mathbf{v} = \sum_i v_i \mathbf{e}_i, \quad \Omega \mathbf{v} = \mathbf{v}', \quad
\mathbf{v}' = \sum_i v_i' \mathbf{e}_i, \quad v_i' = \sum_j \Omega_{ij} v_j
\end{align*}
#+end_definition
#+NAME: Projection Operator
#+begin_definition latex
The projection operator for \(\mathbf{e}_i\) is defined as:

\begin{align*}
\mathbb{P}_i = \mathbf{e}_i \mathbf{e}_i^\dagger, \quad (\mathbb{P}_i)_{kl} = \langle \mathbf{e}_k, \mathbf{e}_i \rangle \langle \mathbf{e}_i, \mathbf{e}_l \rangle = \delta_{ki} \delta_{il}.
\end{align*}
#+end_definition
#+NAME: Completeness Relation
#+begin_definition latex
The identity operator may be expressed as a sum over projection operators. It is called the completeness relation.
\begin{align*}
I = \sum_{i=1}^n \mathbf{e}_i \mathbf{e}_i^\dagger = \sum_{i=1}^n \mathbb{P}_i.
\end{align*}
#+end_definition
#+NAME: Projection Operator
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
Projection operators have the property:
\begin{align*}
\mathbb{P}_i \mathbb{P}_j = \mathbf{e}_i \langle \mathbf{e}_i, \mathbf{e}_j \rangle \mathbf{e}_j^\dagger = \delta_{ij} \mathbb{P}_j.
\end{align*}
#+end_corollary
#+NAME: Action of projection operators on vectors
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
\begin{align*}
\mathbb{P}_i \mathbf{v} = \mathbf{e}_i \left\langle \mathbf{e}_i, \sum_{j=1}^n v_j \mathbf{e}_j \right\rangle = \sum_{j=1}^n v_j \mathbf{e}_i \langle \mathbf{e}_i, \mathbf{e}_j \rangle = \sum_{j=1}^n v_j \mathbf{e}_i \delta_{ij} = v_i \mathbf{e}_i,
\end{align*}
\begin{align*}
\mathbf{v}^\dagger \mathbb{P}_i = \sum_{j=1}^n v_j^* \mathbf{e}_j^\dagger \mathbf{e}_i \mathbf{e}_i^\dagger = \sum_{j=1}^n v_j^* \langle \mathbf{e}_j, \mathbf{e}_i \rangle \mathbf{e}_i^\dagger = \sum_{j=1}^n v_j^* \delta_{ji} \mathbf{e}_i^\dagger = v_i^* \mathbf{e}_i^\dagger.
\end{align*}
#+end_corollary
#+NAME: Outer Product
#+begin_definition latex
Given two vectors of size \(m \times 1\) and \(n \times 1\) respectively
\begin{align*}
\mathbf{u}=\left[\begin{array}{c}
u_1 \\
u_2 \\
\vdots \\
u_m
\end{array}\right], \quad \mathbf{v}=\left[\begin{array}{c}
v_1 \\
v_2 \\
\vdots \\
v_n
\end{array}\right]
\end{align*}
their outer product, denoted \(\mathbf{u} \otimes \mathbf{v}\), is defined as the \(m \times n\) matrix \(\mathbf{A}\) obtained by multiplying each element of \(\mathbf{u}\) by each element of \(\mathbf{v}:\)
\begin{align*}
\mathbf{u} \otimes \mathbf{v}=\mathbf{A}=\left[\begin{array}{cccc}
u_1 v_1 & u_1 v_2 & \ldots & u_1 v_n \\
u_2 v_1 & u_2 v_2 & \ldots & u_2 v_n \\
\vdots & \vdots & \ddots & \vdots \\
u_m v_1 & u_m v_2 & \ldots & u_m v_n
\end{array}\right]
\end{align*}
Or, in index notation:
\begin{align*}
(\mathbf{u} \otimes \mathbf{v})_{i j}=u_i v_j.
\end{align*}
The outer product \(\mathbf{u} \otimes \mathbf{v}\) is equivalent to a matrix multiplication \(\mathbf{u v}^{\mathrm{T}}\), provided that \(\mathbf{u}\) is represented as a \(m \times 1\) column vector and \(\mathbf{v}\) as a \(n \times 1\) column vector (which makes \(\mathbf{v}^{\mathrm{T}}\) a row vector). For instance, if \(m=4\) and \(n=3\), then
\begin{align*}
\mathbf{u} \otimes \mathbf{v}=\mathbf{u} \mathbf{v}^{\top}=\left[\begin{array}{l}
u_1 \\
u_2 \\
u_3 \\
u_4
\end{array}\right]\left[\begin{array}{lll}
v_1 & v_2 & v_3
\end{array}\right]=\left[\begin{array}{lll}
u_1 v_1 & u_1 v_2 & u_1 v_3 \\
u_2 v_1 & u_2 v_2 & u_2 v_3 \\
u_3 v_1 & u_3 v_2 & u_3 v_3 \\
u_4 v_1 & u_4 v_2 & u_4 v_3
\end{array}\right]
\end{align*}
For complex vectors, given the conjugate transpose of \(\mathbf{v}\), denoted \(\mathbf{v}^{\dagger}\) or \(\left(\mathbf{v}^{\top}\right)^*\)
\begin{align*}
\mathbf{u} \otimes \mathbf{v}=\mathbf{u v}^{\dagger}=\mathbf{u}\left(\mathbf{v}^{\top}\right)^*
\end{align*}
#+end_definition
#+NAME: Outer Product
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
If \(\mathbf{u}\) and \(\mathbf{v}\) are vectors of the same dimension bigger than 1 , then \(\operatorname{det}(\mathbf{u} \otimes \mathbf{v})=0\).
#+end_corollary
#+NAME: Inner and Outer Products
#+ATTR_LATEX: :environment theorem
#+begin_theorem latex
Given three vectors of size \(m \times 1\), \(n \times 1\), and \(n \times 1\) respectively
\begin{align*}
\mathbf{u}=\left[\begin{array}{c}
u_1 \\
u_2 \\
\vdots \\
u_m
\end{array}\right], \quad \mathbf{v}=\left[\begin{array}{c}
v_1 \\
v_2 \\
\vdots \\
v_n
\end{array}\right], \quad \mathbf{w}=\left[\begin{array}{c}
w_1 \\
w_2 \\
\vdots \\
w_n
\end{array}\right]
\end{align*}
we have
\[
\langle (\mathbf{u} \otimes \mathbf{v}),~ \mathbf{w} \rangle = \langle \langle \mathbf{v},~ \mathbf{w}\rangle,~ \mathbf{u} \rangle.
\]
#+end_theorem
#+NAME: 
#+ATTR_LATEX: :environment theorem
#+begin_theorem latex
Given three vectors of size \(m \times 1\), \(n \times 1\), and \(m \times 1\) respectively
\begin{align*}
\mathbf{u}=\left[\begin{array}{c}
u_1 \\
u_2 \\
\vdots \\
u_m
\end{array}\right], \quad \mathbf{v}=\left[\begin{array}{c}
v_1 \\
v_2 \\
\vdots \\
v_n
\end{array}\right], \quad \mathbf{x}=\left[\begin{array}{c}
x_1 \\
x_2 \\
\vdots \\
x_m
\end{array}\right]
\end{align*}
we have
\[
\langle \mathbf{x}, ~(\mathbf{u} \otimes \mathbf{v}) \rangle = \langle \langle \mathbf{x},~ \mathbf{u}) \rangle,~ \mathbf{v}^{\mathrm{T}} \rangle
\]
#+end_theorem
#+NAME: Action of the Outer Product on a Vector
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
Multiplication of a vector \(\mathbf{w}\) by the matrix \(\mathbf{u} \otimes \mathbf{v}\) can be written in terms of the inner product, using the relation
\[
(\mathbf{u} \otimes \mathbf{v})~\mathbf{w}=\mathbf{u} \mathbf{v}^{\top} \mathbf{w}.
\]
#+end_corollary
#+NAME: Adjoint of an Operator
#+begin_definition latex
An operator acting on a ket has a corresponding operator that acts on the bras, called its adjoint.
\begin{align*}
\Omega \mathbf{v} = \mathbf{v}', \quad \mathbf{v}'^\dagger = \mathbf{v}^\dagger \Omega^{\dagger}
\end{align*}
\begin{align*}
(\Omega^{\dagger})_{ij} = \langle \mathbf{e}_i, \Omega^{\dagger} \mathbf{e}_j \rangle = \langle \Omega \mathbf{e}_i, \mathbf{e}_j \rangle = \langle \mathbf{e}_j, \Omega \mathbf{e}_i \rangle^* = (\Omega_{ji}^*).
\end{align*}
#+end_definition
#+NAME: Adjoint of a product
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
The adjoint of a product is the product of the adjoints in reverse order:
\begin{align*}
(\Omega \Lambda)^{\dagger} = \Lambda^{\dagger} \Omega^{\dagger}.
\end{align*}
*To take the adjoint of an equation consisting of kets, scalars, and operators, reverse the order of all factors and make the substitutions*
\begin{align*}
\Omega \leftrightarrow \Omega^{\dagger}, \quad \mathbf{v} \leftrightarrow \mathbf{v}^\dagger, \quad \alpha \leftrightarrow \alpha^{\ast}.
\end{align*}
Example: The adjoint of
\begin{align*}
&\alpha_1 \mathbf{v}_1 = \alpha_2 \mathbf{v}_2 + \alpha_3 (\mathbf{v}_3 \otimes \mathbf{v}_4) + \alpha_4 \Omega \Lambda \mathbf{v}_6 \\
\end{align*}
is
\begin{align*}
\mathbf{v}_1^\dagger \alpha_1^{*} = \mathbf{v}_2^\dagger \alpha_2^{*} + (\mathbf{v}_4 \otimes \mathbf{v}_3^\dagger) \alpha_3^* + \mathbf{v}_6^\dagger \Lambda^{\dagger} \Omega^{\dagger} \alpha_4^*.
\end{align*}
#+end_corollary
#+NAME: Hermitian Operator
#+begin_definition latex
An operator \(\Omega\) is Hermitian if \(\Omega^{\dagger}=\Omega\).
#+end_definition
#+NAME: Anti-Hermitian
#+begin_definition latex
An operator \(\Omega\) is anti-Hermitian if \(\Omega^{\dagger} = -\Omega\).
#+end_definition
#+NAME: Hermitian Decomposition
#+begin_theorem latex
An arbitrary operator \(\Omega\) can be expressed as a sum of a Hermitian and anti-Hermitian operator:
\begin{align*}
\Omega=\frac{\Omega+\Omega^{\dagger}}{2}+\frac{\Omega-\Omega^{\dagger}}{2}.
\end{align*}
#+end_theorem
#+NAME: Unitary Operator
#+begin_definition latex
An operator \(U\) is unitary if
\begin{align*}
U U^{\dagger}=I=U^{\dagger} U, \quad U^{-1}=U^{\dagger}, \quad [U, U^{\dagger}]=0.
\end{align*}
#+end_definition
#+NAME: Unitary Operators Preserve the Norm
#+begin_theorem latex
Unitary operators preserve the inner product between the vectors they act on, i.e.,
if
\begin{align*}
U \mathbf{v}_1 = \mathbf{v}_1', \quad \text{and} \quad U \mathbf{v}_2 = \mathbf{v}_2'
\end{align*}
then
\begin{align*}
\langle \mathbf{v}_2', \mathbf{v}_1' \rangle = \langle \mathbf{v}_2, \mathbf{v}_1 \rangle.
\end{align*}
#+end_theorem
#+NAME: Columns (or rows) of a Unitary Matrix are Pairwise Orthonormal
#+begin_theorem latex
The \(n\) columns and \(n\) rows of an \(n \times n\) unitary matrix, when treated as vectors in \(\mathbb{V}^n\), are orthonormal.
#+end_theorem
#+NAME: Hermitian, anti-Hermitian, and unitary operators
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Hermitian, anti-Hermitian, and unitary operators are analogous to pure real, pure imaginary, and complex numbers of unit modulus, respectively.
#+end_remark
#+NAME: Generalized rotation
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Unitary operators are generalizations of rotation operators from \(\mathbb{V}^{3}(R)\) to \(\mathbb{V}^{n}(C)\), preserving the inner product and thus the length of vectors.
#+end_remark
#+NAME: Active Transformation
#+begin_definition latex
\begin{align*}
\mathbf{v} \rightarrow U \mathbf{v}, \quad \Omega \rightarrow \Omega, \quad \langle \mathbf{v}', \Omega \mathbf{v} \rangle \rightarrow \langle \mathbf{v}', U^{\dagger} \Omega U \mathbf{v} \rangle
\end{align*}
#+end_definition
#+NAME: Passive Transformation
#+begin_definition latex
\begin{align*}
\mathbf{v} \rightarrow \mathbf{v}, \quad \Omega \rightarrow U^{\dagger} \Omega U, \quad \langle \mathbf{v}', \Omega \mathbf{v} \rangle \rightarrow \langle \mathbf{v}', U^{\dagger} \Omega U \mathbf{v} \rangle
\end{align*}
#+end_definition
#+NAME: Passive Transformation
#+ATTR_LATEX: :environment remark
#+begin_remark latex
A "passive transformation" is really an "active transformation" done backstage.
#+end_remark
#+NAME: Eigenvalue Equation
#+begin_definition latex
\begin{align*}
\Omega \mathbf{v} = \omega \mathbf{v}.
\end{align*}
#+end_definition
#+NAME: Characteristic Polynomial
#+begin_definition latex
\begin{align*}
P^{n}(\omega) = \sum_{m=0}^n c_{m} \omega^{m}.
\end{align*}
#+end_definition
#+NAME: Characteristic Equation
#+begin_definition latex
\begin{align*}
P^{n}(\omega) = \sum_{m=0}^n c_{m} \omega^{m} = 0.
\end{align*}
#+end_definition
#+NAME: Eigenvalue equation to Characteristic Equation
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
A sequence of steps to go from the eigenvalue equation to the characteristic equation:
1) \(\Omega \mathbf{v} = \omega \mathbf{v}\),
2) \(\left( \Omega - \omega I \right) \mathbf{v} = \mathbf{0}\),
3) \(\det \left( \Omega - \omega I \right) = 0\) if non-trivial eigenkets are to exist.
4) \(\det \left( \Omega - \omega I \right)\) is obtained as a product of the projection of \(\left( \Omega - \omega I \right) \mathbf{v} = \mathbf{0}\) onto the entire set of basis bras \(\{ \mathbf{e}_i^\dagger \}\) and has the general form: \(\sum_{m=0}^{n} c_{m} \omega^{m} = 0\) which is the characteristic equation.
Finding eigenvalues of \(\Omega\) has been reduced to root-finding. Finding eigenvectors has been reduced to solving a system of linear equations.
#+end_proposition
#+NAME: Roots of the Characteristic Equation
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Each distinct (or not) eigenvalue (\(n\) for the \(n\) roots of the characteristic polynomial for \(\Omega\) acting on vectors in \(\mathbb{V}^n\)) may not pair with a unique eigenket (up to a scaling factor). Some eigenvalues may not have a corresponding eigenket at all.
1) A polynomial \(P^n(x)=\sum_{i=0}^n a_i x^i\) is guaranteed to have \(n\) roots, not necessarily distinct or real.
2) Roots of \(P^n(x)\) are values of \(x\), say \(x^*\), such that \(P^n\left(x^*\right)=0\).
3) Real roots are points on the real axis that satisfy \(P^n(x)=0\).
4) Complex roots are points in the complex plane that satisfy \(P^n(x)=0\).
5) A root of the characteristic polynomial of a non-degenerate operator \(\Omega: \mathbb{V}^{n} \to \mathbb{V}^{n}\), upon substitution in the eigenvalue equation, yields a system of \(n\) linear equations. There are \(n\) such systems. The freedom to scale eigenkets can be used to eliminate one equation from every system. The solution to each system is the eigenket for the eigenvalue that generated it. A solution, in general, is not guaranteed.
6) For a degenerate operator \(\Lambda\) that has \(m\) roots out of \(n\), the \(i^{\text {th}}\) one repeating \(k_i\) times, \((n-m)\) systems with \(n-1\) equations each are obtained, the solutions of which (if they exist) yield \(n-m\) eigenkets, corresponding to the \(n-m\) distinct eigenvalues. Another \(m\) systems are obtained for the \(m\) degenerate roots, but only \(m - \sum_{i} k_{i}\) of them are unique. Each degenerate eigenvalue adds a degree of freedom to its eigenket: the eigenket of the \(i^{\text{th}}\) root (if it exists at all) lives in an eigenspace of dimensionality \(k_i\).
#+end_proposition
#+NAME: Eigenvalues of a Hermitian Operator
#+begin_theorem latex
The eigenvalues of a Hermitian operator are real.
#+end_theorem
#+NAME: Eigenvalues of a Unitary Operator
#+begin_theorem latex
The eigenvalues of a unitary operator are complex numbers of unit modulus.
#+end_theorem
#+NAME: Eigenvectors of a Non-Degenerate Unitary Operator
#+begin_theorem latex
The eigenvectors of a non-degenerate unitary operator are mutually orthogonal.
#+end_theorem
#+NAME: Diagonalization of Hermitian Operator
#+begin_theorem latex
To every Hermitian operator \(\Omega\), there exists (at least) a basis consisting of its orthonormal eigenvectors. It is diagonal in this eigenbasis and has its eigenvalues as its diagonal entries.
#+end_theorem
#+NAME: Similarity Transformation
#+begin_theorem latex
If \(\Omega\) is Hermitian, there exists a unitary matrix \(U\) (its columns being the eigenvectors of \(\Omega\)) such that \(U^{+} \Omega U^*\) is diagonal.
#+end_theorem  
#+NAME: Diagonalization and Eigenvalue Problem
#+ATTR_LATEX: :environment remark
#+begin_remark latex
Finding a basis that diagonalizes \(\Omega\) is reduced to solving its eigenvalue problem.
#+end_remark
#+NAME: Simultaneous Diagonalization
#+begin_theorem latex
If \(\Omega\) and \(\Lambda\) are two commuting Hermitian matrices/operators, there exists (at least) a basis of common eigenkets that diagonalizes them both.
#+end_theorem
#+NAME: c-number
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Consider
\begin{align*}
f(x)=\sum_{n=0}^{\infty} a_n x^n.
\end{align*}
In this equation \(x\) is a /c-number/.
#+end_definition
#+NAME: q-number
#+ATTR_LATEX: :environment definition
#+begin_definition latex
Consider
\begin{align*}
f(\Omega)=\sum_{n=0}^{\infty} a_n \Omega^n
\end{align*}
In this equation \(\Omega\) is a /q-number/.
#+end_definition
*** Matrix factorization
**** QR decomposition
***** Motivation
+ The column spaces of a matrix \(A\) are successive spaces spanned by the columns \(a_1, a_2, \ldots\) of \(A\)
\begin{align*}
\left\langle a_1\right\rangle \subseteq\left\langle a_1, a_2\right\rangle \subseteq\left\langle a_1, a_2, a_3\right\rangle \subseteq \ldots
\end{align*}
where \(\langle\cdots\rangle\) denotes the subspace spanned by whatever vectors are included in the brackets.
+ The idea of QR factorization is to construct a sequence of orthonormal vectors, \(q_1, q_2, \ldots\) that span these successive spaces.
+ Assume for the moment that \(A \in \mathbb{C}^{m \times n}(m \geq n)\), we want the sequence \(q_1, q_2, \ldots\) to have the property:
\begin{align*}
\left\langle q_1, q_2, \ldots, q_j\right\rangle=\left\langle a_1, a_2, \ldots, a_j\right\rangle \quad j=1, \ldots, n
\end{align*}
+ This amounts to
\[
\left[ \begin{array}{c|c|c|c}
   & & & \\
    & & & \\
    a_1 & a_2 & \cdots & a_n
   & & & \\
    & & &
\end{array} \right] = 
\left[ \begin{array}{c|c|c|c}
   & & & \\
    & & & \\
    q_1 & q_2 & \cdots & q_n
   & & & \\
    & & &
\end{array} \right]
\left[ \begin{array}{cccc}
    r_{11} & r_{12} & \cdots & r_{1n} \\
    & r_{22} & \cdots & \vdots \\
    & & \ddots & \vdots \\
    & & & r_{nn}
\end{array} \right]
\]
+ Then \(a_1, \ldots, a_k\) can be expressed as a linear combination of \(q_1, \ldots q_k\), and vice versa!
+ Written out the equations are:
\begin{align*}
a_1 & =r_{11} q_1 \\
a_2 & =r_{12} q_1+r_{22} q_2 \\
& \vdots \\
a_n & =r_{n 1} q_1+r_{2 n} q_2+\cdots+r_{n n} q_n
\end{align*}
- If \(A \in \mathbb{R}^{m \times n}\), vectors \(q_1, q_2, \ldots, q_n\) are orthonormal \(m\)-vectors:
\begin{align*}
q_i^T q_j=\delta_{i j}
\end{align*}
+ The diagonal elements \(r_{ii}\) are non-zero. If any \(r_{ii} < 0\), we can switch the signs of \(r_{ii}, \ldots, r_{in}\) and the corresponding \(q_i\) to ensure \(r_{ii} > 0\), which guarantees the uniqueness of \(Q\) and \(R\).
***** Definition
****** Square matrix
#+NAME: QR decomposition (real square matrix)
#+begin_definition latex
Let \( A \) be any real square matrix. It can be decomposed as
\[
A = QR,
\]
where \( Q \) is an orthogonal matrix (its columns are orthogonal unit vectors, implying that \( Q^{\top} = Q^{-1} \)), and \( R \) is an upper triangular matrix.
#+end_definition
#+NAME: QR decomposition (complex square matrix)
#+begin_definition latex
Let \( A \) be any complex square matrix. It can be decomposed as
\[
A = QR,
\]
where \( Q \) is a unitary matrix (its conjugate transpose \( Q^{\dagger} = Q^{-1} \)), and \( R \) is an upper triangular matrix.
#+end_definition
****** Rectangular matrix
#+NAME: QR decomposition (rectangular matrix)
#+begin_definition latex
Let \( A \) be a complex \( m \times n \) matrix, where \( m \geq n \). \( A \) can be factorized into the product of an \( m \times m \) unitary matrix \( Q \) and an \( m \times n \) upper triangular matrix \( R \). The matrix \( R \) can be further partitioned as:
\[
A = Q R = Q \left[ \begin{array}{c}
R_1 \\
0 
\end{array} \right] = \left[ \begin{array}{ll}
Q_1 & Q_2 
\end{array} \right] \left[ \begin{array}{c}
R_1 \\
0
\end{array} \right] = Q_1 R_1,
\]
where \( R_1 \) is an \( n \times n \) upper triangular matrix, \( 0 \) represents an \((m-n) \times n\) zero matrix, \( Q_1 \) is \( m \times n \), and \( Q_2 \) is \( m \times (m-n) \). Both \( Q_1 \) and \( Q_2 \) contain orthogonal columns.
#+end_definition
***** Example
\begin{align*}
{\left[\begin{array}{ccc}-1 & -1 & 1 \\ 1 & 3 & 3 \\ -1 & -1 & 5 \\ 1 & 3 & 7\end{array}\right] } & =\left[\begin{array}{ccc}-1 / 2 & 1 / 2 & -1 / 2 \\ 1 / 2 & 1 / 2 & -1 / 2 \\ -1 / 2 & 1 / 2 & 1 / 2 \\ 1 / 2 & 1 / 2 & 1 / 2\end{array}\right]\left[\begin{array}{lll}2 & 4 & 2 \\ 0 & 2 & 8 \\ 0 & 0 & 4\end{array}\right] \\
& =
\left[ \begin{array}{c|c|c|c}
   & & & \\
    & & & \\
    q_1 & q_2 & \cdots & q_n
   & & & \\
    & & &
\end{array} \right]
\left[\begin{array}{llll}r_{11} & r_{12} & \cdots & r_{1 n} \\ & r_{22} & & \\ & & \ddots & \vdots \\ & & & r_{n n}\end{array}\right] \\ & =Q R.
\end{align*}
***** Properties
#+NAME: Uniqueness of QR factorization
#+begin_corollary latex
For an invertible matrix \( A \), the QR factorization is unique provided that the diagonal elements of \( R \) are positive. If any \(r_{ii} < 0\), we can switch the signs of \(r_{ii}, \ldots, r_{in}\) and the corresponding \(q_i\) to ensure \(r_{ii} > 0\), which guarantees the uniqueness of \(Q\) and \(R\).
#+end_corollary
#+NAME: Orthonormal Basis of Column Span
#+begin_corollary latex
For any \( 1 \leq k \leq n \), the first \( k \) columns of \( Q \) form an orthonormal basis for the span of the first \( k \) columns of \( A \).
#+end_corollary
#+NAME: Orthonormal Basis of Column Space
#+begin_corollary latex
If \( A \) has \( n \) linearly independent columns, then the first \( n \) columns of \( Q \) provide an orthonormal basis for the column space of \( A \).
#+end_corollary
+ The fact that any column \(k\) of \(A\) only depends on the first \(k\) columns of \(Q\) corresponds to the triangular form of \(R\).
#+NAME: Uniqueness in QR decomposition (rectangular matrix)
#+begin_corollary latex
If \( R_1 \) in the QR decomposition of a rectangular matrix \( A \) has positive diagonal elements, then \( R_1 \) and \( Q_1 \) are unique. However, \( Q_2 \) is generally not unique unless additional constraints are specified.
#+end_corollary
#+NAME: Relation to Cholesky decomposition
#+begin_corollary latex
For a complex matrix \( A \), the matrix \( R_1 \) in the QR decomposition is equal to the upper triangular factor in the Cholesky decomposition of \( A^* A \). If \( A \) is real, this simplifies to \( A^\top A \).
#+end_corollary
***** Application
+ Given a matrix \( A \) admits a QR decomposition, linear systems of the form \( Ax = b \) can be solved /without the need for matrix inversion/. Such a solution relies on the method of backward substitution:
#+NAME: Backward substitution
#+begin_definition latex
Let \( U \) be an upper triangular matrix of the form
\begin{align*}
U=\left[\begin{array}{ccccc}
u_{1,1} & u_{1,2} & \ldots & u_{1, n-1} & u_{1,n} \\
 & u_{2,2} & \ldots & u_{2, n-1} & u_{2,n} \\
 &  & \ddots & \vdots & \vdots \\
 &  &  & u_{n-1, n-1} & u_{n-1, n} \\
0 &  &  &  & u_{n, n}
\end{array}\right].
\end{align*}
Consider the matrix equation \( U \mathbf{x} = \mathbf{b} \). The associated system of equations are
\begin{align*}
u_{n,n} x_n &= b_n, \\
u_{n-1, n-1} x_{n-1} + u_{n-1, n} x_n &= b_{n-1}, \\
& \vdots \\
u_{1,1} x_1 + u_{1,2} x_2 + \ldots + u_{1,n} x_n &= b_1.
\end{align*}
The solution for each variable \( x_k \) is computed starting from the last equation:
\begin{align*}
x_n &= \frac{b_n}{u_{n,n}}, \\
x_{n-1} &= \frac{b_{n-1} - u_{n-1, n} x_n}{u_{n-1, n-1}}, \\
& \vdots \\
x_1 &= \frac{b_1 - \sum_{j=2}^{n} u_{1,j} x_j}{u_{1,1}}.
\end{align*}
This iterative process of solving the system \( U \mathbf{x} = \mathbf{b} \) is referred to as the method of backward substitution.
#+end_definition
#+BEGIN_COMMENT
Notice that backward substitution, like forward substitution, does not require inverting the matrix.
#+END_COMMENT
+ Given a matrix \( A \) admits a QR decomposition, the steps for solving a linear systems of the form \( Ax = b \) are:
#+NAME: Solving linear systems using QR Decomposition
#+begin_steps latex
Let \( A \) be a matrix that admits a QR decomposition. The linear system \( Ax = b \) can be solved as follows:
1. Compute the QR decomposition of \( A \):
   Decompose matrix \( A \) into the product of an orthogonal matrix \( Q \) and an upper triangular matrix \( R \):
   \[
   A = QR.
   \]
2. Compute \( y \) by projecting \( b \) onto the columns of \( Q \):
   Multiply the transpose of \( Q \) with vector \( b \) to compute \( y \):
   \[
   y = Q^T b.
   \]
   This step projects \( b \) onto the column space of \( Q \), effectively transforming the original system into a new coordinate system defined by \( Q \).
3. Solve for \( x \) using backward substitution:
   Since \( R \) is an upper triangular matrix, solve the equation \( Rx = y \) using backward substitution:
   \[
   R x = y.
   \]
   This process involves solving for \( x_n \) first, then \( x_{n-1} \), and so on until all entries of \( x \) are determined.
\begin{align*}
x_n &= \frac{y_n}{r_{n,n}}, \\
x_{n-1} &= \frac{y_{n-1} - r_{n-1, n} x_n}{r_{n-1, n-1}}, \\
& \vdots \\
x_1 &= \frac{y_1 - \sum_{j=2}^{n} r_{1,j} x_j}{r_{1,1}}.
\end{align*}
This method leverages the orthogonality of \( Q \) and the triangular structure of \( R \) to simplify the solution process, avoiding the need for matrix inversion and thereby enhancing computational efficiency and stability.
#+end_steps
***** Algorithm
****** Classical Gram-Schmidt
Consider the Gram-Schmidt process applied to the columns of a full column rank matrix \(A = \left[\mathbf{a}_1 \cdots \mathbf{a}_n\right]\), with the inner product defined as \(\langle\mathbf{v}, \mathbf{w}\rangle = \mathbf{v}^{\top} \mathbf{w}\) for real vectors or \(\langle\mathbf{v}, \mathbf{w}\rangle = \mathbf{v}^{\dagger} \mathbf{w}\) for complex vectors.
Define the projection of \(\mathbf{a}\) onto \(\mathbf{q}\):
\[
\operatorname{proj}_{\mathbf{q}_i} \mathbf{a}_j = \frac{\langle \mathbf{q}_i, \mathbf{a}_j \rangle}{\langle \mathbf{q}_i, \mathbf{q}_i \rangle} \mathbf{q}_i
\]
This projector can be represented explicitly. Let \(Q_{j-1}\) denote the \(m \times(j-1)\) matrix containing the first \((j-1)\) columns of \(Q\)
\[
Q_{j-1} \left[ \begin{array}{c|c|c|c}
   & & & \\
    & & & \\
    \mathbf{q}_1 & \mathbf{q}_2 & \cdots & \mathbf{q}_{j-1}
   & & & \\
    & & &
\end{array} \right]
\]
Then \(\operatorname{proj}_{\mathbf{q}_j}\) is given by
\begin{align*}
\operatorname{proj}_{\mathbf{q}_j} = I - Q_{j-1} Q_{j-1}^{\ast}
\end{align*}
The process for \(k\) vectors is then defined as:
\begin{align*}
\mathbf{q}_1 & = \mathbf{a}_1, & \mathbf{e}_1 & = \frac{\mathbf{q}_1}{\|\mathbf{q}_1\|}, \\
\mathbf{q}_2 & = \mathbf{a}_2 - \operatorname{proj}_{\mathbf{q}_1} \mathbf{a}_2, & \mathbf{e}_2 & = \frac{\mathbf{q}_2}{\|\mathbf{q}_2\|}, \\
& \vdots & & \vdots \\
\mathbf{q}_k & = \mathbf{a}_k - \sum_{j=1}^{k-1} \operatorname{proj}_{\mathbf{q}_j} \mathbf{a}_k, & \mathbf{e}_k & = \frac{\mathbf{q}_k}{\|\mathbf{q}_k\|}.
\end{align*}
We can now express the \(\mathbf{a}_i\) over our newly computed orthonormal basis:
\begin{align*}
\mathbf{a}_1 & =\left\langle\mathbf{e}_1, \mathbf{a}_1\right\rangle \mathbf{e}_1 \\
\mathbf{a}_2 & =\left\langle\mathbf{e}_1, \mathbf{a}_2\right\rangle \mathbf{e}_1+\left\langle\mathbf{e}_2, \mathbf{a}_2\right\rangle \mathbf{e}_2 \\
\mathbf{a}_3 & =\left\langle\mathbf{e}_1, \mathbf{a}_3\right\rangle \mathbf{e}_1+\left\langle\mathbf{e}_2, \mathbf{a}_3\right\rangle \mathbf{e}_2+\left\langle\mathbf{e}_3, \mathbf{a}_3\right\rangle \mathbf{e}_3 \\
& \vdots \\
\mathbf{a}_k & =\sum_{j=1}^k\left\langle\mathbf{e}_j, \mathbf{a}_k\right\rangle \mathbf{e}_j
\end{align*}
where \(\left\langle\mathbf{e}_i, \mathbf{a}_i\right\rangle=\left\|\mathbf{q}_i\right\|\). This can be written in matrix form:
\[
A=Q R
\]
where:
\[
Q=\left[\begin{array}{lll}
\mathbf{e}_1 & \cdots & \mathbf{e}_n
\end{array}\right]
\]
and
\[
R=\left[\begin{array}{ccccc}
\left\langle\mathbf{e}_1, \mathbf{a}_1\right\rangle & \left\langle\mathbf{e}_1, \mathbf{a}_2\right\rangle & \left\langle\mathbf{e}_1, \mathbf{a}_3\right\rangle & \cdots & \left\langle\mathbf{e}_1, \mathbf{a}_n\right\rangle \\
0 & \left\langle\mathbf{e}_2, \mathbf{a}_2\right\rangle & \left\langle\mathbf{e}_2, \mathbf{a}_3\right\rangle & \cdots & \left\langle\mathbf{e}_2, \mathbf{a}_n\right\rangle \\
0 & 0 & \left\langle\mathbf{e}_3, \mathbf{a}_3\right\rangle & \cdots & \left\langle\mathbf{e}_3, \mathbf{a}_n\right\rangle \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \left\langle\mathbf{e}_n, \mathbf{a}_n\right\rangle
\end{array}\right]
\]
\begin{algorithm}
\caption{Classical Gram-Schmidt}
\begin{algorithmic}[1]
\For{\(j = 1\) to \(n\)}
    \State \(\mathbf{q}_j = \mathbf{a}_j\)
    \For{\(i = 1\) to \(j - 1\)}
        \State \(r_{ij} = \langle \mathbf{e}_i, \mathbf{a}_j \rangle\)
        \State \(\mathbf{q}_j = \mathbf{q}_j - r_{ij} \mathbf{e}_i\)
    \EndFor
    \State \(r_{jj} = \|\mathbf{q}_j\|\)
    \State \(\mathbf{e}_j = \frac{\mathbf{q}_j}{r_{jj}}\)
\EndFor
\end{algorithmic}
\end{algorithm}
In exact arithmetic, this algorithm is stable, but on a computer, we always have round-off errors, so in practice, the classical Gram-Schmidt algorithm for QR decomposition shows significant numerical instability.
******* Examples
******** Example 1
Given matrix \( A \):
\[
A = \begin{bmatrix}
12 & -51 & 4 \\
6 & 167 & -68 \\
-4 & 24 & -41
\end{bmatrix}
\]
********* Step 1: Compute \( \mathbf{u}_1 \) and \( \mathbf{q}_1 \)
+ Set \( \mathbf{u}_1 \) to \( \mathbf{a}_1 \) and normalize:
\[
\mathbf{u}_1 = \mathbf{a}_1 = \begin{bmatrix} 12 \\ 6 \\ -4 \end{bmatrix}, \quad r_{11} = \|\mathbf{u}_1\| = 14, \quad \mathbf{q}_1 = \frac{\mathbf{u}_1}{r_{11}} = \begin{bmatrix} 6/7 \\ 3/7 \\ -2/7 \end{bmatrix}
\]
********* Step 2: Compute \( \mathbf{u}_2 \) and \( \mathbf{q}_2 \)
+ Compute projection of \( \mathbf{a}_2 \) onto \( \mathbf{q}_1 \) and orthogonalize:
\[
r_{12} = \mathbf{q}_1^T \mathbf{a}_2 = 21, \quad \mathbf{u}_2 = \mathbf{a}_2 - r_{12} \mathbf{q}_1 = \begin{bmatrix} -69 \\ 158 \\ 30 \end{bmatrix}, \quad r_{22} = \|\mathbf{u}_2\| = 175, \quad \mathbf{q}_2 = \frac{\mathbf{u}_2}{r_{22}} = \begin{bmatrix} -69/175 \\ 158/175 \\ 6/35 \end{bmatrix}
\]
********* Step 3: Compute \( \mathbf{u}_3 \) and \( \mathbf{q}_3 \)
+ Compute projections of \( \mathbf{a}_3 \) onto \( \mathbf{q}_1 \) and \( \mathbf{q}_2 \), then orthogonalize:
\[
r_{13} = \mathbf{q}_1^T \mathbf{a}_3 = -14, \quad r_{23} = \mathbf{q}_2^T \mathbf{a}_3 = -70, \quad \mathbf{u}_3 = \mathbf{a}_3 - r_{13} \mathbf{q}_1 - r_{23} \mathbf{q}_2 = \begin{bmatrix} -58/5 \\ 6/5 \\ -33 \end{bmatrix}, \quad r_{33} = \|\mathbf{u}_3\| = 35, \quad \mathbf{q}_3 = \frac{\mathbf{u}_3}{r_{33}} = \begin{bmatrix} -58/175 \\ 6/175 \\ -33/35 \end{bmatrix}
\]
********* Final QR Decomposition
+ The orthonormal matrix \( Q \) and the upper triangular matrix \( R \) are given by:
\[
Q = \begin{bmatrix} 6/7 & -69/175 & -58/175 \\ 3/7 & 158/175 & 6/175 \\ -2/7 & 6/35 & -33/35 \end{bmatrix}, \quad R = \begin{bmatrix} 14 & 21 & -14 \\ 0 & 175 & -70 \\ 0 & 0 & 35 \end{bmatrix}
\]
******** Example 2
Consider the matrix \( A \)
\[
A = \begin{bmatrix}
-1 & -1 & 1 \\
1 & 3 & 3 \\
-1 & -1 & 5 \\
1 & 3 & 7
\end{bmatrix}
\]
********* Step 1: Compute \( \mathbf{q}_1 \)
- Compute \( \mathbf{u}_1 \) and normalize to get \( \mathbf{q}_1 \):
\[
\mathbf{u}_1 = \mathbf{a}_1 = \begin{bmatrix} -1 \\ 1 \\ -1 \\ 1 \end{bmatrix}, \quad r_{11} = \|\mathbf{u}_1\| = 2, \quad \mathbf{q}_1 = \frac{\mathbf{u}_1}{r_{11}} = \begin{bmatrix} -1/2 \\ 1/2 \\ -1/2 \\ 1/2 \end{bmatrix}
\]
********* Step 2: Compute \( \mathbf{q}_2 \)
- Project \(\mathbf{a}_2\) onto \(\mathbf{q}_1\) and orthogonalize to find \( \mathbf{q}_2 \):
\[
r_{12} = \mathbf{q}_1^T \mathbf{a}_2 = 4, \quad \tilde{\mathbf{q}}_2 = \mathbf{a}_2 - r_{12} \mathbf{q}_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}, \quad r_{22} = \|\tilde{\mathbf{q}}_2\| = 2, \quad \mathbf{q}_2 = \frac{\tilde{\mathbf{q}}_2}{r_{22}} = \begin{bmatrix} 1/2 \\ 1/2 \\ 1/2 \\ 1/2 \end{bmatrix}
\]
********* Step 3: Compute \( \mathbf{q}_3 \)
- Project \(\mathbf{a}_3\) onto \(\mathbf{q}_1\) and \(\mathbf{q}_2\), and orthogonalize to find \( \mathbf{q}_3 \):
\[
r_{13} = \mathbf{q}_1^T \mathbf{a}_3 = 3, \quad r_{23} = \mathbf{q}_2^T \mathbf{a}_3 = 8, \quad \tilde{\mathbf{q}}_3 = \mathbf{a}_3 - r_{13} \mathbf{q}_1 - r_{23} \mathbf{q}_2 = \begin{bmatrix} -2 \\ -2 \\ 2 \\ 2 \end{bmatrix}, \quad r_{33} = \|\tilde{\mathbf{q}}_3\| = 4, \quad \mathbf{q}_3 = \frac{\tilde{\mathbf{q}}_3}{r_{33}} = \begin{bmatrix} -1/2 \\ -1/2 \\ 1/2 \\ 1/2 \end{bmatrix}
\]
********* Final QR Decomposition
- The matrices \( Q \) and \( R \) from the Gram-Schmidt process are:
\[
Q = \begin{bmatrix} -1/2 & 1/2 & -1/2 \\ 1/2 & 1/2 & -1/2 \\ -1/2 & 1/2 & 1/2 \\ 1/2 & 1/2 & 1/2 \end{bmatrix}, \quad R = \begin{bmatrix} 2 & 4 & 3 \\ 0 & 2 & 8 \\ 0 & 0 & 4 \end{bmatrix}
\]
****** Modified Gram-Schmidt
Given a matrix \( A \) defined by columns \( \mathbf{a}_1, \cdots, \mathbf{a}_n \), the modified Gram-Schmidt process constructs an orthonormal matrix \( Q \) and an upper triangular matrix \( R \) through an iterative stabilization of the classical process:
\[
A = [\mathbf{a}_1 \cdots \mathbf{a}_n]
\]
Define the projection of vector \( \mathbf{a}_k \) onto vector \( \mathbf{q}_i \):
\[
\operatorname{proj}_{\mathbf{q}_i}(\mathbf{a}_k) = \frac{\langle \mathbf{q}_i, \mathbf{a}_k \rangle}{\langle \mathbf{q}_i, \mathbf{q}_i \rangle} \mathbf{q}_i
\]
The process for vector \(\mathbf{a}_k\) is then defined as:
\begin{align*}
\mathbf{q}_k^{(1)} & = \mathbf{a}_k - \operatorname{proj}_{\mathbf{q}_1} \mathbf{a}_k, && \\
\mathbf{q}_k^{(2)} & = \mathbf{q}_k^{(1)} - \operatorname{proj}_{\mathbf{q}_2} \mathbf{q}_k^{(1)}, && \\
& \vdots && \\
\mathbf{q}_k^{(k-2)} & = \mathbf{q}_k^{(k-3)} - \operatorname{proj}_{\mathbf{q}_{k-2}} \mathbf{q}_k^{(k-3)}, && \\
\mathbf{q}_k^{(k-1)} & = \mathbf{q}_k^{(k-2)} - \operatorname{proj}_{\mathbf{q}_{k-1}} \mathbf{q}_k^{(k-2)}, & \mathbf{e}_k &= \frac{\mathbf{q}_k^{(k-1)}}{\|\mathbf{q}_k^{(k-1)}\|}.
\end{align*}
We can now express the \(\mathbf{a}_i\) over our newly computed orthonormal basis:
\begin{align*}
\begin{aligned}
\mathbf{a}_1 & =\left\langle\mathbf{e}_1, \mathbf{a}_1\right\rangle \mathbf{e}_1 \\
\mathbf{a}_2 & =\left\langle\mathbf{e}_1, \mathbf{a}_2\right\rangle \mathbf{e}_1+\left\langle\mathbf{e}_2, \mathbf{a}_2\right\rangle \mathbf{e}_2 \\
\mathbf{a}_3 & =\left\langle\mathbf{e}_1, \mathbf{a}_3\right\rangle \mathbf{e}_1+\left\langle\mathbf{e}_2, \mathbf{a}_3\right\rangle \mathbf{e}_2+\left\langle\mathbf{e}_3, \mathbf{a}_3\right\rangle \mathbf{e}_3 \\
& \vdots \\
\mathbf{a}_k & =\sum_{j=1}^k\left\langle\mathbf{e}_j, \mathbf{a}_k\right\rangle \mathbf{e}_j
\end{aligned}
\end{align*}
where \(\left\langle\mathbf{e}_i, \mathbf{a}_i\right\rangle=\left\|\mathbf{q}_i\right\|\). This can be written in matrix form:
\begin{align*}
A=Q R
\end{align*}
where:
\begin{align*}
Q=\left[\begin{array}{lll}
\mathbf{e}_1 & \cdots & \mathbf{e}_n
\end{array}\right]
\end{align*}
and
\begin{align*}
R=\left[\begin{array}{ccccc}
\left\langle\mathbf{e}_1, \mathbf{a}_1\right\rangle & \left\langle\mathbf{e}_1, \mathbf{a}_2\right\rangle & \left\langle\mathbf{e}_1, \mathbf{a}_3\right\rangle & \cdots & \left\langle\mathbf{e}_1, \mathbf{a}_n\right\rangle \\
0 & \left\langle\mathbf{e}_2, \mathbf{a}_2\right\rangle & \left\langle\mathbf{e}_2, \mathbf{a}_3\right\rangle & \cdots & \left\langle\mathbf{e}_2, \mathbf{a}_n\right\rangle \\
0 & 0 & \left\langle\mathbf{e}_3, \mathbf{a}_3\right\rangle & \cdots & \left\langle\mathbf{e}_3, \mathbf{a}_n\right\rangle \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \left\langle\mathbf{e}_n, \mathbf{a}_n\right\rangle
\end{array}\right]
\end{align*}
\begin{algorithm}
\caption{Modified Gram-Schmidt}
\begin{algorithmic}[1]
\For{\(k = 1\) to \(n\)}
    \State \(\mathbf{q}_k = \mathbf{a}_k\)
\EndFor
\For{\(k = 1\) to \(n\)}
    \State \(r_{kk} = \| \mathbf{q}_k \|\)  
    \State \(e_{k} = \mathbf{q}_k/r_{kk}\)  
    \For{\(j = 1\) to \(k - 1\)}
        \State \(r_{kj} = \langle \mathbf{e}_k, \mathbf{q}_j \rangle\)
        \State \(\mathbf{q}_j = \mathbf{q}_j - r_{kj} \mathbf{e}_k\)
    \EndFor
    \State \(r_{kk} = \|\mathbf{q}_k\|\)
    \State \(\mathbf{e}_k = \mathbf{q}_k/r_{kk}\)
\EndFor
\end{algorithmic}
\end{algorithm}
In practice, the modified Gram-Schmidt process, despite being equivalent in theory to the classical approach, exhibits superior numerical stability, making it preferred in computational implementations.
In contrast to classical Gram-Schmidt algorithm, \( \operatorname{proj}_{\mathbf{q}_i}(\mathbf{a}_k) \) in the modified Gram-Schmidt algorithm can be computed by a sequence of of \((j-1)\) projections. If \(\operatorname{proj}_{\mathbf{q}_k}\) is given by \( \operatorname{proj}_{\mathbf{q}_k} = I - Q_{k-1} Q_{k-1}^{\ast} \) then
\[
\operatorname{proj}_{\mathbf{q}_k} = \operatorname{proj}_{\mathbf{q}_{k-1}} \operatorname{proj}_{\mathbf{q}_{k-2}} \cdots \operatorname{proj}_{\mathbf{q}_2} \operatorname{proj}_{\mathbf{q}_1}
\]
Each outer step of the algorithm can be interpreted as a right multiplication by a square upper-triangular matrix. Beginning with \(A\), the first iteration multiplies the first column \(\mathbf{a}_1\) with \(\frac{1}{r_{11}}\) and then subtracts \(r_{1 j}\) times the result from each of the remaining columns \(\mathbf{a}_j\).
This is equivalent to right-multiplication by a matrix \(R_1\) :
- In general, step \(i\) subtracts \(r_{i j} / r_{i i}\) times column \(i\) of the current A from columns, \(j>i\) and replaces column \(i\) by \(1 / r_{i i}\) times itself. This corresponds to multiplication by upper triangular matrix \(R_i\) :
\begin{align*}
R_2=\left[\begin{array}{cccc}
1 & & & \\
& \frac{1}{r_{22}} & \frac{-r_{23}}{r_{22}} & \cdots \\
& & 1 & \\
& & & \ddots
\end{array}\right] R_3=\left[\begin{array}{cccc}
1 & & & \\
& 1 & & \\
& & \frac{1}{r_{33}} & \cdots \\
& & & \ddots
\end{array}\right], \ldots
\end{align*}
- At the end of the iteration:
\begin{align*}
A \underbrace{R_1 R_2 \cdots R_n}_{R^{-1}}=Q
\end{align*}
- This shows that Gram-Schmidt is a method of triangular orthogonalization: It applies triangular operations on the right of a matrix to reduce it to a matrix of orthonormal columns.
****** Householder transformation
#+NAME: Reflection hyperplane
#+begin_definition latex
The reflection hyperplane can be defined by its normal vector, a unit vector \(\mathbf{w}_{\bot}\) (\( \| \mathbf{w}_{\bot} \| = 1 \)) that is orthogonal to the hyperplane, i.e., if \( \mathbf{w} \) is a vector that lies on the hyperplane then
\[ \langle \mathbf{w}_{\bot}, \mathbf{w} \rangle = \sum_{i=1}^{n} w_i \langle \mathbf{w}_{\bot}, \mathbf{e}_i \rangle = 0.\]
It follows that \(\langle \mathbf{w}_{\bot}, \mathbf{e}_i \rangle = 0\) for all \( i \).
#+end_definition
#+NAME: Householder transformation
#+begin_definition latex
The Householder transformation of a vector \(\mathbf{v}\) about the reflection hyperplane defined by the normal vector \(\mathbf{w}_{\bot}\) is a linear transformation given by
\begin{align*}
H \mathbf{v} = \mathbf{v} - 2 \langle \mathbf{w}_{\bot}, \mathbf{v} \rangle \mathbf{w}_{\bot} = \mathbf{v} - 2 (\mathbf{w}_{\bot} \otimes \mathbf{w}_{\bot}) \mathbf{v} = (I - 2 \mathbf{w}_{\bot} \otimes \mathbf{w}_{\bot}) \mathbf{v}.
\end{align*}
#+end_definition
#+NAME: Householder matrix
#+begin_definition latex
The Householder transformation defines the Householder matrix which is defined in terms of the outer product of the normal vector of the reflection hyperplane with its adjoint.
\[H \equiv I - 2 \mathbf{w}_{\bot} \otimes \mathbf{w}_{\bot}.\]
#+end_definition
#+NAME: The Householder matrix is Hermitian
#+begin_theorem latex
The Householder matrix \(H\) is Hermitian, i.e.,
\[H = H^{\dagger}.\]
#+end_theorem
#+NAME: The Householder matrix is unitary
#+begin_theorem latex
The Householder matrix \(H\) is unitary, i.e.,
\[H^{\dagger} H = H H^{\dagger} = I.\]
It follows that \(H^{-1}=H^{\dagger}\).
#+end_theorem
#+NAME: The Householder matrix is involutory
#+begin_theorem latex
The Householder matrix \(H\) is involutory, i.e.,
\[H H = I.\]
It follows that \(H = H^{-1}\).
#+end_theorem
#+NAME: Eigenvalues of Householder matrix
#+begin_theorem latex
A Householder matrix has eigenvalues \(\pm 1\).
#+end_theorem
#+NAME: Eigenvalues of Householder matrix
#+begin_proof latex
To see this, notice that if \(\mathbf{v}\) is orthogonal to the vector \(\mathbf{w}_{\bot}\) which was used to create the reflector, then \(H \mathbf{v} = \mathbf{v}\), i.e., 1 is an eigenvalue of multiplicity \(n-1\), since there are \(n-1\) independent vectors orthogonal to \(\mathbf{w}_{\bot}\). Also, notice \(H \mathbf{w}_{\bot} = - \mathbf{w}_{\bot}\), and so -1 is an eigenvalue with multiplicity 1.
#+end_proof
#+NAME: Determinant of Householder matrix
#+begin_theorem latex
\[\det (H) = - 1.\]
#+end_theorem
#+NAME: Eigenvalues of Householder matrix
#+begin_proof latex
The determinant of a matrix is the product of its eigenvalues. Since the eigenvalues of the Householder matrix are \( -1 \) and \( 1 \), it immediately follows that \( \det (H) = - 1 \).
#+end_proof
#+NAME: Orthogonality
#+ATTR_LATEX: :environment definition
#+begin_definition
\(\mathbf{v}\) and \(\mathbf{w}\) are called orthogonal (to each other) iff \(\langle \mathbf{v}, \mathbf{w} \rangle = 0\).
#+end_definition
**** LU decomposition
- Gram-Schmidt: \(A=Q R\) by triangular orthogonalization.
- Householder: \(A=Q R\) by orthogonal triangularization.
- Gaussian Elimination: \(A=L U\) by triangular triangularization.
- Consider an \(m \times m\) matrix. Suppose \(x_k\) denotes the \(k\) th column of the matrix beginning at step \(k\). Then \(L_k\) must be chosen such that:
\begin{align*}
x_k=\left[\begin{array}{c}
x_{1 k} \\
\vdots \\
x_{k k} \\
x_{k+1, k} \\
\vdots \\
x_{m k}
\end{array}\right] \xrightarrow{L_k} L_k x_k=\left[\begin{array}{c}
x_{1 k} \\
\vdots \\
x_{k k} \\
0 \\
\vdots \\
0
\end{array}\right]
\end{align*}
- To do this, we subtract \(l_{j k}\) times row \(k\) from row \(j\) :
\begin{align*}
l_{j k}=\frac{x_{j k}}{x_{k k}} \quad(k<j \leq m)
\end{align*}
- The matrix \(L_k\) takes the form:
\begin{align*}
\left[\begin{array}{cccccc}
1 & & & & & \\
& \ddots & & & & \\
& & 1 & & & \\
& & -l_{k+1, k} & 1 & & \\
& & \vdots & & \ddots & \\
& & -l_{m k} & & & 1
\end{array}\right]
\end{align*}
- Define \(l_k\) as:
\begin{align*}
l_k=\left[\begin{array}{c}
0 \\
\vdots \\
0 \\
l_{k+1, k} \\
\vdots \\
l_{m k}
\end{array}\right]
\end{align*}
- Then \(L_k=I-l_k e_k^*\) where \(e_k\) is the column vector with 1 in position \(k\) and 0 otherwise.
- One can easily check that \(e_k^* l_k=0\).
- Therefore consider:
\begin{align*}
\left(I-l_k e_k^*\right)\left(I+l_k e_k^*\right)=I-l_k e_k^* l_k e_k^*=I
\end{align*}

That is the inverse of \(L_k\) is \(I+l_k e_k^*\).
- Consider the product:
\begin{align*}
L_k^{-1} L_{k+1}^{-1}=\left(I+l_k e_k^*\right)\left(I+l_{k+1} e_{k+1}^*\right)=I+l_k e_k^*+l_{k+1} e_{k+1}^*
\end{align*}

Thus \(L_k^{-1} L_{k+1}^{-1}\) is just a lower triangular matrix with entries of both \(L_k^{-1}\) and \(L_{k+1}^{-1}\) inserted in the usual places.
- As a result, we can write the full matrix \(L\) as:
\begin{align*}
L=L_1^{-1} L_2^{-1} \ldots L_m^{-1}=\left[\begin{array}{ccccc}
1 & & & & \\
l_{21} & 1 & & & \\
l_{31} & l_{32} & 1 & & \\
\vdots & \vdots & \ddots & \ddots & \\
l_{m 1} & l_{m 2} & \cdots & l_{m, m-1} & 1
\end{array}\right]
\end{align*}
**** Cholesky Decomposition
- Hermitian positive definite matrices can be decomposed into triangular factors twice as quickly as general matrices.
- The standard algorithm for this is the Cholesky factorization, which is a variant of Gaussian elimination that operates on left and right of the matrix at once.
- For a complex matrix \(A \in \mathbb{C}^{m \times m}\), Hermitian matrices are \(A=A^*\).
- A Hermitian matrix is positive definite iff for any \(x \in \mathbb{C}^m\), \(x^* A x>0\). The eigenvalues of Hermitian positive definite matrix are always positive and real.
- Consider what happens if we apply a single step of Gaussian elimination to a Hermitian matrix \(A\) with 1 in the upper left position:
\begin{align*}
A=\left[\begin{array}{cc}
1 & w^* \\
w & K
\end{array}\right]=\left[\begin{array}{ll}
1 & 0 \\
w & I
\end{array}\right]\left[\begin{array}{cc}
1 & w^* \\
0 & K-w w^*
\end{array}\right]
\end{align*}
- Gaussian elimination would now proceed with introducing zeros in the next column. However, in Cholesky factorization, they are introduced in the first row to keep the hermiticity of the matrix.
\begin{align*}
\left[\begin{array}{cc}
1 & w^* \\
0 & K-w w^*
\end{array}\right]=\left[\begin{array}{cc}
1 & 0 \\
0 & K-w w^*
\end{array}\right]\left[\begin{array}{cc}
1 & w^* \\
0 & I
\end{array}\right]
\end{align*}
- Combining the two steps:
\begin{align*}
A=\left[\begin{array}{cc}
1 & w^* \\
w & K
\end{array}\right]=\left[\begin{array}{ll}
1 & 0 \\
w & I
\end{array}\right]\left[\begin{array}{cc}
1 & 0 \\
0 & K-w w^*
\end{array}\right]\left[\begin{array}{cc}
1 & w^* \\
0 & I
\end{array}\right]
\end{align*}
- The idea of Cholesky decomposition is to continue this process till the matrix is reduced to identity!
- In general, we need this to work for any \(a_{11}>0\). The generalization of this achieved by adjusting the algorithm and introducing \(\alpha=\sqrt{a_{11}}\)
\begin{align*}
\begin{aligned}
A= & {\left[\begin{array}{cc}
a_{11} & w^* \\
w & K
\end{array}\right]=\left[\begin{array}{cc}
\alpha & 0 \\
w / \alpha & I
\end{array}\right]\left[\begin{array}{cc}
1 & 0 \\
0 & K-w w^* / a_{11}
\end{array}\right]\left[\begin{array}{cc}
\alpha & w^* / \alpha \\
0 & I
\end{array}\right] } \\
& =R_1^* A_1 R_1
\end{aligned}
\end{align*}
- If the upper left entry of the submatrix \(K-w w^* / a_{11}\) is positive, the process can be continued further:
\begin{align*}
\begin{aligned}
A & =\underbrace{R_1^* R_2^* \cdots R_m^*}_{R^*} \underbrace{R_m \cdots R_2 R_1}_R \\
& =R^* R \quad r_{j j}>0
\end{aligned}
\end{align*}
where \(R\) is upper triangular.
- The only thing left hanging is that how do we know that the upper left entry of \(K-w w^* / a_{11}\) is positive? It has to be because, \(K-w w^* / a_{11}\) is positive definite as it is the principle submatrix of the positive definite matrix \(R_1^{-*} A R_1^{-1}\).
The following algorithm computes the factor \(R^* R\) of complex Hermitian \(A\) :
```
Algorithm 10 Cholesky factorization
    \(R=A\)
    for \(k=1\) to \(m\) do
        for \(j=k+1\) to \(m\) do
            \(R_{j, j: m}=R_{j, j: m}-R_{k, j: m} \overline{R_{k j}} / R_{k k}\)
        end for
        \(R_{k, k: m}=R_{k, k: m} / \sqrt{R_{k k}}\)
    end for
```
- Work for Householder orthogonalization \(\sim 2 m n^2-\frac{2}{3} n^3\)
- Work for (modified) Gram-Schmidt: \(\sim 2 m n^2\)
- Work for Gaussian elimination: \(\sim \frac{2}{3} m^3\)
- Work for Cholesky factorization: \(\sim \frac{1}{3} m^3\)
**** Singular Value Decomposition (SVD)
- Singular Value Decomposition (SVD) is the generalization of the eigendecomposition of a positive semidefinite normal matrix to any \(m \times n\) matrix.
- It is a factorization of a matrix \(M\) into:
\begin{align*}
M=U \Sigma V^*
\end{align*}
where \(U\) is \(m \times m\) a unitary matrix, \(\Sigma\) is a \(m \times n\) rectangular diagonal matrix with non-negative real numbers on the diagonal and \(V\) is a \(n \times n\) unitary matrix.
- The diagonal entries \(\sigma_i\) of \(\Sigma\) are known as the singular values of \(M\). The columns of \(U\) and the columns of \(V\) are called the left-singular vectors and right-singular vectors of \(M\), respectively.
- The left-singular vectors of \(M\) are a set of orthonormal eigenvectors of \(M M^*\).
- The right-singular vectors of \(\mathrm{M}\) are a set of orthonormal eigenvectors of \(M^* M\).
- The non-zero singular values of \(M\) (found on the diagonal entries of \(\Sigma\) ) are the square roots of the non-zero eigenvalues of both \(M^* M\) and \(M M^*\).
- This is useful for dimensionality reduction; eg Principal Component Analysis (PCA) etc.

Rank, Null space and Range
- The right-singular vectors corresponding to vanishing singular values of \(M\) span the null space of \(M\)
- The left-singular vectors corresponding to the non-zero singular values of \(M\) span the range of \(M\).
- The rank of \(M\) equals the number of non-zero singular values which is the same as the number of non-zero diagonal elements in \(\Sigma\).

SVD and eigen decomposition
- Eigenvalue decomposition of a square matrix:
\begin{align*}
A=X \Lambda X^{-1}
\end{align*}
where \(\Lambda\) is a diagonal matrix and \(X\) contains linearly independent eigenvectors of \(A\).
- SVD is the generalization of eigen decomposition to rectangular matrices.
- SVD uses two bases (left and right singular vectors) while eigenvalue decomposition uses only one (just the eigenvectors).
- In applications, SVD is relevant for problems involving the matrix itself where as eigen decomposition is useful to compute iterated forms of the matrix - such as matrix powers or exponentials etc.

#+ATTR_HTML: :width 600px
#+CAPTION: Physical interpretation of SVD
[[file:~/.local/images/svd.png]]

**** Schur Factorization
- One final factorization:
\begin{align*}
A=Q T Q^*
\end{align*}
where \(Q\) is unitary and \(T\) is upper-triangular.
- Since \(A\) and \(T\) are similar, eigenvalues of \(A\) neccesarily appear on the diagonal of \(T\).
- Diagonalization algorithms use this factorization.

Diagonalization and Schur Factorization
- Any eigenvalue solver has to be iterative!
- Most of the general purpose eigenvalue algorithms proceed by computing the Schur factorization:
\begin{align*}
\underbrace{Q_j^* \cdots Q_2^* Q_1^*}_{Q^*} A \underbrace{Q_1 Q_2 \cdots Q_j}_Q
\end{align*}
converges to an upper triangular matrix \(T\) as \(j \rightarrow \infty\).
*** Solving linear systems
**** Matrix vector product
- Let \(x\) be a \(n\)-dimensional column vector and \(A\) be an \(m \times n\) matrix ( \(m\) rows and \(n\) columns).
- The matrix vector product \(b=A x\) is the \(m\)-dimensional column vector:
\begin{align*}
b_i=\sum_{j=1}^n a_{i j} x_j \quad i=1, \ldots, m .
\end{align*}
- Let \(a_j\) denote column \(j\) of \(A\), an \(m\)-vector. Then rewriting the above equation:
\begin{align*}
b=A x=\sum_{j=1}^n x_j a_j
\end{align*}
\[
b = \left[ \begin{array}{c|c|c|c}
    & & & \\
    & & & \\
    a_1 & a_2 & \cdots & a_n \\
    & & & \\
    & & & 
\end{array} \right]
\left[ \begin{array}{c}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
\end{array} \right]
= x_1 \left[ \begin{array}{c}
    \\
    \\
    a_1 \\
    \\
    \\
\end{array} \right] +
x_2 \left[ \begin{array}{c}
    \\
    \\
    a_2 \\
    \\
    \\
\end{array} \right] + \cdots +
x_n \left[ \begin{array}{c}
    \\
    \\
    a_n \\
    \\
    \\
\end{array} \right]
\]
- \(b\) can also be thought of as a linear combination of the columns \(a_j\)
- \(A x=b\) is usually thought of as \(A\) acting on \(x\) to produce \(b\)
- \(A x=b\) can also be thought of as \(x\) acting on \(A\) to produce \(b\) !
- From \(x=A^{-1} b, x\) can be thought of just as the result of application of \(A^{-1}\) to \(b\).
- Alternatively, \(A^{-1} b\) is the vector of coefficients of the expansion of \(b\) in the basis of columns of \(A\).
**** Gaussian elimination
- Gaussian Elimination/LU decomposition transforms a full linear system into an upper-triangular one by applying simple linear transformations to the left.
- It is similar to Householder triangularization - the difference is that the transformations applied are not unitary.
- This is done by subtracting multiples of each row from subsequent rows!
- This elimination method is equivalent to multiplying \(A\) by a sequence of lower-triangular matrices \(L_k\) on the left:
\begin{align*}
\underbrace{L_{m-1} \cdots L_2 L_1}_{L^{-1}} A=U
\end{align*}
- Setting \(L=L_1^{-1} L_2^{-1} \cdots L_{m-1}^{-1}\) gives (note inverse of a lower triangular matrix is lower triangular)
\begin{align*}
A=L U
\end{align*}
- The matrix \(L_k\) are chosen such that it introduces zeros below the diagonal in the \(k\) th column by subtracting multiples of row \(k\) from rows \(k+1, \ldots, m\).
- As the first \(k-1\) entries are already zero, this operation does not destroy any zeroes previously obtained.
- For example, in the \(4 \times 4\) case, the zeroes are introduced in the following way:
\begin{align*}
\begin{aligned}
{\left[\begin{array}{cccc}
\times & \times & \times & \times \\
\times & \times & \times & \times \\
\times & \times & \times & \times \\
\times & \times & \times & \times
\end{array}\right] \xrightarrow{L_1}\left[\begin{array}{cccc}
\times & \times & \times & \times \\
\mathbf{0} & \times & \times & \times \\
\mathbf{0} & \times & \times & \times \\
\mathbf{0} & \times & \times & \times
\end{array}\right] \xrightarrow{L_2}\left[\begin{array}{cccc}
\times & \times & \times & \times \\
& \times & \times & \times \\
\mathbf{0} & \times & \times \\
\mathbf{0} & \times & \times
\end{array}\right] } \\
\xrightarrow{L_3}\left[\begin{array}{cccc}
\times & \times & \times & \times \\
& \times & \times & \times \\
& & \times & \times \\
& \mathbf{0} & \times
\end{array}\right]
\end{aligned}
\end{align*}
***** No Pivoting
The following algorithm computes the factor \(L U\) of \(A\) :
```
Algorithm 4 Gaussian Elimination without Pivoting
    \(U=A, L=I\)
    for \(k=1\) to \(m-1\) do
        for \(j=k+1\) to \(m\) do
            \(l_{j k}=u_{j k} / u_{k k}\)
            \(u_{j, k: m}=u_{j, k: m}-l_{j k} u_{k, k: m}\)
        end for
    end for
```
- Work for Householder orthogonalization \(\sim 2 m n^2-\frac{2}{3} n^3\)
- Work for (modified) Gram-Schmidt: \(\sim 2 m n^2\)
- Work for Gaussian elimination: \(\sim \frac{2}{3} m^3\)
***** Pivoting
- Gaussian elimination can become unstable/fail completely if the diagonal entries of the matrix \(A\) are very small/zero.
- \(x_{k k}\) element plays an important role in Gaussian elimination and is called a pivot.
- If \(x_{k k}=0\) then we need a different (modified) algorithm. Even if \(x_{k k} \neq 0\), but is small, there is a need for a more stable method.
- In principle, there is no need to pick only \(x_{k k}\) as the pivot. In principle, we can use any element of \(X_{k: m, k: m}\) as the pivot!
- We can interchange columns/rows among themselves to bring a large number to the diagonal - rather than work with a smaller number.
- This is crucial for stability of the algorithm.
***** Partial Pivoting
- If any element of \(X_{k: m, k: m}\) can be considered a pivot, then searching for the largest number will cost \(\mathcal{O}(m-k)^2\) flops per step - overall cost for \(m\) steps \(\mathcal{O}\left(m^3\right)\).
- This strategy - is expensive an called complete pivoting.
- In practice, equally good pivots can be found by choosing the largest element from \((m-k+1)\) subdiagonal entries in column \(k\). This can be achieved in \(\mathcal{O}(m-k)\) operations and overall cost for finding the pivot is \(\mathcal{O}\left(m^2\right)\).
- Then only rows are interchanged and it is called partial pivoting.
- The interchange of rows can be represented by the application of the Permutation operator.
- This can be visualied as:
\begin{align*}
\begin{aligned}
& {\left[\begin{array}{ccccc}
\times & \times & \times & \times & \times \\
& \times & \times & \times & \times \\
& \times & \times & \times & \times \\
& \boldsymbol{x}_{\boldsymbol{i k}} & \times & \times & \times \\
& \times & \times & \times & \times
\end{array}\right] \xrightarrow{P_1}\left[\begin{array}{ccccc}
\times & \times & \times & \times & \times \\
& \boldsymbol{x}_{\boldsymbol{i k} \boldsymbol{k}} & \times & \times & \times \\
& \times & \times & \times & \times \\
& \times & \times & \times & \times \\
& \times & \times & \times & \times
\end{array}\right]} \\
& \xrightarrow{L_1}\left[\begin{array}{ccccc}
\times & \times & \times & \times & \times \\
& x_{i k} & \times & \times & \times \\
& \mathbf{0} & \times & \times & \times \\
& \mathbf{0} & \times & \times & \times \\
& \mathbf{0} & \times & \times & \times
\end{array}\right] \\
&
\end{aligned}
\end{align*}
- Then the upper triangular matrix can be:
\begin{align*}
L_{m-1} P_{m-1} \cdots L_2 P_2 L_1 P_1 A=U
\end{align*}
- Consider the following definition:
\begin{align*}
L_k^{\prime}=P_{m-1} \ldots P_{k+1} L_k P_{k+1}^{-1} \ldots P_{m-1}^{-1}
\end{align*}
- Then:
\begin{align*}
\begin{aligned}
U= & L_{m-1} P_{m-1} \cdots L_2 P_2 L_1 P_1 A \\
& =\left(L_{m-1}^{\prime} \cdots L_2^{\prime} L_1^{\prime}\right)\left(P_{m-1} \cdots P_2 P_1\right) A
\end{aligned}
\end{align*}
- Equivalent to solving \(P A=L U\).
The following algorithm computes the factor \(L U\) of \(A\) :
```
Algorithm 5 Gaussian Elimination with Partial Pivoting
    \(U=A, L=I, P=1\)
    for \(k=1\) to \(m-1\) do
        Select \(i \geq k\) to maximize \(\left|u_{i k}\right|\)
        \(u_{k, k: m} \leftrightarrow u_{i, k: m}\)
        \(l_{k, k-1} \leftrightarrow l_{i, 1: k-1}\)
        \(p_{k,:} \leftrightarrow p_{i,:}\)
        for \(j=k+1\) to \(m\) do
            \(l_{j k}=u_{j k} / u_{k k}\)
            \(u_{j, k: m}=u_{j, k: m}-l_{j k} u_{k, k: m}\)
        end for
    end for
```
**** Jacobi Method
- Simplest iterative scheme to solve \(A x=b\).
- The method can be motivated by rewriting the \(3 \times 3\) system as follows:
\begin{align*}
\begin{aligned}
& x_1=\left(b_1-a_{12} x_2-a_{13} x_3\right) / a_{11} \\
& x_2=\left(b_2-a_{21} x_1-a_{23} x_3\right) / a_{22} \\
& x_3=\left(b_3-a_{31} x_1-a_{32} x_2\right) / a_{33}
\end{aligned}
\end{align*}

- Suppose \(x^{(k)}\) is an approximation to \(x=A^{-1} b\). A natural way to generate a new approximation \(x^{(k+1)}\) is to compute:
\begin{align*}
\begin{aligned}
& x_1^{(k+1)}=\left(b_1-a_{12} x_2^{(k)}-a_{13} x_3^{(k)}\right) / a_{11} \\
& x_2^{(k+1)}=\left(b_2-a_{21} x_1^{(k)}-a_{23} x_3^{(k)}\right) / a_{22} \\
& x_3^{(k+1)}=\left(b_3-a_{31} x_1^{(k)}-a_{32} x_2^{(k)}\right) / a_{33}
\end{aligned}
\end{align*}

For a general \(n\) we have:
\begin{align*}
x_i^{(k+1)}=\left(b_i-\sum_{j=1}^{i-1} a_{i j} x_j^{(k)}-\sum_{j=i+1}^n a_{i j} x_j^{(k)}\right) / a_{i i}
\end{align*}

**** Gauss-Seidel Method
- Jacobi method does not use the most updated values when computing \(x^{(k+1)}\).
- For example, \(x_1^{(k)}\) is used in the calculation of \(x_2^{(k+1)}\), even though \(x_1^{(k+1)}\) is known.
- If we revise Jacobi iteration so that we use the most current estimate then we obtain Gausss-Seidel iteration:
\begin{align*}
x_i^{(k+1)}=\left(b_i-\sum_{j=1}^{i-1} a_{i j} x_j^{(k+1)}-\sum_{j=i+1}^n a_{i j} x_j^{(k)}\right) / a_{i i}
\end{align*}
Jacobi and Gauss-Seidel Iterations
For both Jacobi and Gauss-Seidel iterations, the transition from \(x^{(k)}\) to \(x^{(k+1)}\) can be written in terms of the matrices, \(L, D\), and \(U\) defined as:
\begin{align*}
\begin{aligned}
L=\left[\begin{array}{ccccc}
0 & 0 & \cdots & \cdots & 0 \\
a_{21} & 0 & \cdots & & 0 \\
a_{31} & a_{32} & \ddots & & \vdots \\
\vdots & & & 0 & 0 \\
a_{n 1} & \cdots & \cdots & a_{n, n-1} & 0
\end{array}\right] U=\left[\begin{array}{ccccc}
0 & a_{21} & \cdots & \cdots & a_{1 n} \\
0 & 0 & \cdots & & a_{2 n} \\
\vdots & & \ddots & & \vdots \\
0 & 0 & & 0 & a_{n-1,1} \\
0 & 0 & 0 & 0 & 0
\end{array}\right. \\
D=\operatorname{diag}\left(a_{11}, \cdots, a_{n n}\right)
\end{aligned}
\end{align*}
- Jacobi step has form: \(M_J x^{(k+1)}=N_J x^{(k)}+b\), where \(M_J=D\) and \(N_J=-(L+U)\).
- Gauss-Seidel step has form: \(M_G x^{(k+1)}=N_G x^{(k)}+b\), where \(M_G=(D+L)\) and \(N_J=-U\).
**** Steepest Descent Method
- Consider the function ( \(b \in \mathbb{R}^n\) and \(A \in \mathbb{R}^{n \times n}\), symmetric and positive definite):
\begin{align*}
\phi(x)=\frac{1}{2} x^T A x-x^T b
\end{align*}
- Minimum value of \(\phi(x)=-b^T A^{-1} b / 2\) is achieved by setting \(x=A^{-1} b\).
- Thus minimizing \(\phi\) is equivalent to solving \(A x=b\), if \(\mathrm{A}\) is symmetric positive definite.
- At a current point \(x_c\), the function \(\phi\) decreases most rapidly in the direction of the negative gradient:
\begin{align*}
-\nabla \phi\left(x_c\right)=b-A x_c \equiv r_c
\end{align*}
called the residual of \(x_c\).
Steepest Descent Method
- If the residual is non-zero, then there exists a positive \(\alpha\) such that:
\begin{align*}
\phi\left(x_c+\alpha r_c\right)<\phi\left(x_c\right)
\end{align*}

 Now:
\begin{align*}
\phi\left(x_c+\alpha r_c\right)=\phi\left(x_c\right)-\alpha r_c^T r_c+\frac{1}{2} \alpha^2 r_c^T A r_c
\end{align*}
- Thus minimizing this wrt \(\alpha, \alpha=r_c^T r_c / r_c^T A r_c\).
```
Algorithm 1 Steepest Descent
    \(x_0, r_0=b-A x_0\)
    for \(n=1,2,3, \ldots\) do
        \(\alpha_n=\left(r_{n-1}^T r_{n-1}\right) /\left(r_{n-1}^T A r_{n-1}\right) \quad\) \{Step length
        \(x_n=x_{n-1}+\alpha_n r_{n-1} \quad\) \{Approximate solution\}
        \(r_n=b-A x_n \quad\) \{Residual\}
    end for
```
**** GMRES - Generalized Minimal Residuals
- Arnoldi method can be used to find eigenvalues. GMRES is its analog for \(A x=b\) problems.
- Let \(\mathcal{K}_n\) denote the Krylov subspace \(\left\langle b, A b, \ldots, A^{n-1} b\right\rangle\) then GMRES tries to do the following:
- At step \(n\), one approximates the exact solution, \(x_*\) (ie \(\left.A x_*=b\right)\), by the vector \(x_n \in \mathcal{K}_n\) that minimizes the norm of the residual, \(r_n=b-A x_n\).
- In other words, the \(x_n\) is determined by solving a least squares problem.
- One obvious way to solve this problem would be the following.
- Let \(K_n\) be the Krylov matrix so that
\begin{align*}
A K_n=\left[A b\left|A^2 b\right| \cdots \mid A^n b\right]
\end{align*}
- The column space of this matrix is \(A \mathcal{K}_n\). Thus the problem reduces to finding a vector \(c \in \mathbb{C}^n\) such that
\begin{align*}
\left\|A K_n c-b\right\|=\text { minimum }
\end{align*}
- This can be achieved by QR factorization of \(A K_n\). Once \(c\) is found, we can set \(x_n=K_n c\).
- The above procedure is numerically unstable!
- Instead use Arnoldi iteration to construct a sequence of Krylov subspace matrices, \(Q_n\), whose columns \(q_1, q_2, \ldots\) span the successive Krylov subspaces \(\mathcal{K}_n\).
- Then one can write \(x_n=Q_n y\) instead of \(x_n=K_n c\).
- The least squares problem reduces to finding a vector \(y \in \mathbb{C}^n\) such that:
\begin{align*}
\left\|A Q_n y-b\right\|=\text { minimum }
\end{align*}
GMRES - Generalized Minimal Residuals
Using \(A Q_n=Q_{n+1} H_n\), we get:
\begin{align*}
\left\|Q_{n+1} H_n y-b\right\|=\text { minimum }
\end{align*}
- Now both vectors inside the norm are in the column space of \(Q_{n+1}\). So multiplying on the left by \(Q_{n+1}^*\) does not change that norm :
\begin{align*}
\left\|H_n y-Q_{n+1}^* b\right\|=\text { minimum }
\end{align*}
- As \(b\) was the first vector in the column space of \(Q_{n+1}\), \(Q_{n+1}^* b=\|b\| e_1\), where \(e_1=(1,0,0, \ldots)^*\) So the problem reduces to:
\begin{align*}
\left\|H_n y-\right\| b\left\|e_1\right\|=\text { minimum }
\end{align*}
```
Algorithm 2 GMRES
    \(q_1=b /|| b||\)
    for \(n=1,2,3, \ldots\) do
        \(v=A q_n\)
        for \(j=1\) to \(n\) do
            \(h_{j n}=q_j^* v\)
            \(v=v-h_{j n} q_j\)
        end for
        \(h_{n+1, n}=\|v\|\)
        \(q_{n+1}=v / h_{n+1, n}\)
        Find \(y\) to minimize ||\(H_n y-\|b\| \mid e_1 \|\)
        \(x_n=Q_n y\)
    end for
```
**** Conjugate gradients
- Conjugate gradient iteration is the "original" Krylov subspace method - and a mainstay of scientific computing!
- Conjugate gradient is to GMRES what Lanczos is to Arnoldi!
- It is applicable to solving \(A x=b\) problems where \(A\) is a real, positive definite, symmetric matrix.
- In this case, the GMRES iteration reduces substantially and no upper Hessenberg matrix has to be constructed.
- Conjugate gradient can be described as: It is a system of recurrence formulas that generates the unique sequence of iterates \(\left\{x_n \in \mathcal{K}_n\right\}\) with the property that at each step \(n\) \(\left\|e_n\right\|_A\) is minimized.
```
Algorithm 3 Conjugate gradients
    \(x_0=0, r_0=b, p_0=r_0\)
    for \(n=1,2,3, \ldots\) do
        \(\alpha_n=\left(r_{n-1}^T r_{n-1}\right) /\left(p_{n-1}^T A p_{n-1}\right) \quad\) \{Step length\}
        \(x_n=x_{n-1}+\alpha_n p_{n-1} \quad\) \{Approximate solution\}
        \(r_n=r_{n-1}-\alpha_n A p_{n-1} \quad\) \{Residual\}
        \(\beta_n=\left(r_n^T r_n\right) /\left(r_{n-1}^T r_{n-1}\right) \quad\) \{Improvement of the step \(\}\)
        \(p_n=r_n+\beta_n p_{n-1} \quad\) \{Search direction\}
    end for
```
- It can be shown that
\begin{align*}
r_n^T r_j=0 \quad(j<n)
\end{align*}
- The search directions:
\begin{align*}
p_n^T A p_j=0 \quad(j<n)
\end{align*}
- All the \(x, p\) and \(r\) belong to the Krylov subspace:
\begin{align*}
\begin{aligned}
\mathcal{K}_n= & \left\langle x_1, x_2, \ldots, x_n\right\rangle=\left\langle p_0, p_1, \ldots, p_{n-1}\right\rangle \\
& \left\langle r_0, r_1, \ldots, r_{n-1}\right\rangle=\left\langle b, A b, \ldots A^{n-1} b\right\rangle
\end{aligned}
\end{align*}

- Let the CG iteration be applied to a symmetric positive definite matrix problem \(A x=b\). If the iteration has not already converged, then \(x_n\) is the unique point in \(\mathcal{K}_n\) that minimizes \(\left\|e_n\right\|_A\). The convergence is monotonic:
\begin{align*}
\left\|e_n\right\|_A \leq\left\|e_{n-1}\right\|_A
\end{align*}
and \(e_n=0\) is achieved for some \(n \leq m\).
- We know that \(x_n\) belongs to \(\mathcal{K}_n\). To show that it is unique point that minimizes \(\|e\|_A\), consider the arbitrary point, \(x=x_n-\Delta x \in \mathcal{K}_n\). The error \(e=x_*-x=e_n+\Delta x\). Calculate
\begin{align*}
\begin{aligned}
\|e\|_A^2 & =\left(e_n+A\right)^T A\left(e_n+A\right) \\
& =e_n^T A e_n+(\Delta x)^T A(\Delta x)+2 e_n^T A(\Delta x)
\end{aligned}
\end{align*}
- We know that the final term: \(2 r_n^T(\Delta x)\) is the inner product of \(r_n\) with a vector in \(\mathcal{K}_n\) which has to be 0 .
So,
\begin{align*}
\|e\|_A^2=e_n^T A e_n+(\Delta x)^T A(\Delta x)
\end{align*}
and since \(A\) is positive definite, this is minimum when \(\Delta x=0\).
- The monotonicity follows from the fact that \(\mathcal{K}_n \subseteq \mathcal{K}_{n+1}\).

*** Finding eigenvalues
Two phases of eigenvalue computations
- Whether or not \(A\) is hermitian, the sequence is usually split into two phases - first a direct method is applied to produce a upper-Hessenberg matrix \(\mathrm{H}\), that is, a matrix with zeros below the first subdiagonal.
- In the second phase, an iteration is used to generate a formally infinite sequence of Hessenberg matrices that converge to a triangular form.
\begin{align*}
\begin{aligned}
& {\left[\begin{array}{ccccc}
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times
\end{array}\right] \xrightarrow{\text { Phase } 1}} \\
& {\left[\begin{array}{ccccc}
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
& \times & \times & \times & \times \\
& & \times & \times & \times \\
& & & \times & \times
\end{array}\right]} \\
& {\left[\begin{array}{lllll}
\times & x & \times & x & \times
\end{array}\right]} \\
& \times \times \times \\
& \times \times \times \\
& \times \quad \times \\
&
\end{aligned}
\end{align*}
\(\xrightarrow{\text { Phase2 }}\)
Reduction to Hessenberg/Tridiagonal form
- Use Householder reflectors to introduce zeros, but leave the first row as it is!
- Upon applying with \(Q_1\) on the right, it will not destroy the zeroes that you have!
\begin{align*}
\begin{aligned}
& {\left[\begin{array}{ccccc}
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times
\end{array}\right] \xrightarrow{Q_1^*}\left[\begin{array}{ccccc}
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
\mathbf{0} & \times & \times & \times & \times \\
\mathbf{0} & \times & \times & \times & \times \\
\mathbf{0} & \times & \times & \times & \times
\end{array}\right] } \\
& \xrightarrow{Q_1}\left[\begin{array}{ccccc}
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
0 & \times & \times & \times & \times \\
0 & \times & \times & \times & \times \\
0 & \times & \times & \times & \times
\end{array}\right]
\end{aligned}
\end{align*}
Reduction to Hessenberg/Tridiagonal form
- If the matrix is Hermitian, it will reduce to a tridiagonal form!
\begin{align*}
\begin{aligned}
& {\left[\begin{array}{ccccc}
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times
\end{array}\right] \xrightarrow{Q_1^*}\left[\begin{array}{ccccc}
\times & \times & \times & \times & \times \\
\times & \times & \times & \times & \times \\
0 & \times & \times & \times & \times \\
\mathbf{0} & \times & \times & \times & \times \\
0 & \times & \times & \times & \times
\end{array}\right]} \\
& \xrightarrow{Q_1}\left[\begin{array}{ccccc}
\times & \times & \mathbf{0} & \mathbf{0} & \mathbf{0} \\
\times & \times & \times & \times & \times \\
0 & \times & \times & \times & \times \\
0 & \times & \times & \times & \times \\
0 & \times & \times & \times & \times
\end{array}\right] \\
&
\end{aligned}
\end{align*}
Householder reduction to Hessenberg Form
The following algorithm computes the Householder reduction of \(A\) to Hessenberg form:
```
Algorithm 11 Householder reduction to Hessenberg form
    for \(k=1\) to \(m-2\) do
        \(x=A_{k+1: m, k}\)
        \(v_k=\operatorname{sign}\left(x_1\right)\|x\|_2 e_1+x\)
        \(v_k=v_k /\left\|v_k\right\|_2\)
        \(A_{k+1: m, k: n}=A_{k+1: m, k: n}-2 v_k\left(v_k^* A_{k+1: m, k: n}\right)\)
        \(A_{1: m, k+1: n}=A_{1: m, k+1: n}-2 v_k\left(v_k^* A_{1: m, k+1: n}\right)\)
    end for
```
Householder reduction to Tridiagonal form
- Reduces a symmetric/hermitian matrix to tridiagonal form.
- Work done \(\sim \frac{4}{3} m^3\) Flops
**** Rayleigh Quotient
- For a given complex Hermitian matrix \(M\) and nonzero vector \(\mathbf{X}\), the Rayleigh quotient \(r(x)\), is defined as:
\begin{align*}
r(x)=\frac{x^* M x}{x^* x} .
\end{align*}
- It addresses the question, given an \(x\), what scalar, \(\alpha\), "acts most like the eigenvalue" for \(x\) in the sense of minimizing \(\|M x-\alpha x\|\) ?
- Calculating the gradient:
\begin{align*}
\nabla r(x)=\frac{2}{x^* x}(M x-r(x) x)
\end{align*}
- This shows that the eigenvectors of \(M\) are the stationary points of the function \(r(x)\) and the eigenvalues of \(M\) are the values of \(r(x)\) at these stationary points.
- The Rayleigh quotient is a quadratically accurate estimate of the eigenvalue!
Power Iteration
- Suppose \(v^{(0)}\) is a normalized vector. The following is expected to produce a sequence \(v^{(i)}\) that converges to an eigenvector corresponding to the largest eigenvalue of \(A\).
```
Algorithm 2 Power Iteration
    \(v^{(0)}=\) some vector with \(\left\|v^{(0)}\right\|=1\)
    for \(k=1,2, \ldots\) do
        \(w=A v^{(k-1)}\)
        \(v^{(k)}=w /\|w\|\)
        \(\lambda_k=\left(v^{(k)}\right)^T A v^{(k)}\)
    end for
```
**** Power Iteration
- We can analyze the power iteration as:
\begin{align*}
v^{(0)}=a_1 q_1+a_2 q_2+\ldots+a_m q_m
\end{align*}
where \(q_i\) are orthonormal eigenvectors with corresponding eigenvalues satisfying \(\left|\lambda_1\right| \geq\left|\lambda_2\right| \geq \cdots \geq|\lambda|_m \geq 0\).
- Then one can write:
\begin{align*}
\begin{aligned}
v^{(k)} & =c_k A^k v^{(0)} \\
& =c_k \lambda_1^k\left(a_1 q_1+a_2\left(\lambda_2 / \lambda_1\right)^k q_2+\cdots+a_m\left(\lambda_m / \lambda_1\right)^k q_k\right.
\end{aligned}
\end{align*}
- In the limit \(k \rightarrow \infty\),
\begin{align*}
\left\|v^{(k)}-\left( \pm q_1\right)\right\|=\mathcal{O}\left(\left|\frac{\lambda_2}{\lambda_1}\right|^k\right) \quad\left|\lambda^{(k)}-\lambda_1\right|=\mathcal{O}\left(\left|\frac{\lambda_2}{\lambda_1}\right|^{2 k}\right)
\end{align*}
**** Inverse Iteration
- For any \(\mu\) that is not an eigenvalue of \(A\), the eigenvectors of \((A-\mu I)^{-1}\) has the same eigenvectors as \(A\) and the corresponding eigenvalues are \(\left(\lambda_j-\mu\right)^{-1}\).
- Then by a judicious choice of \(\mu\) close to \(\lambda_J,\left(\lambda_J-\mu\right)^{-1}\) can be made much larger for \(j \neq J\).
- Applying the power iteration should converge to \(q_J\).
```
Algorithm 6 Inverse Iteration
    \(v^{(0)}=\) some vector with \(\left\|v^{(0)}\right\|=1\)
    for \(k=1,2, \ldots\) do
        Solve \((A-\mu I) w=v^{(k-1)}\) for \(w\)
        \(v^{(k)}=w /\|w\|\)
        \(\lambda_k=\left(v^{(k)}\right)^T A v^{(k)}\)
    end for
```
**** Rayleigh Quotient Iteration
- The Rayleigh Quotient is a method for obtaining eigenvalue from and eigenvector estimate while the Inverse iteration obtains an eigenvector from estimate of the eigenvalue.
- In this algorithm, one combines the two:
```
Algorithm 8 Rayleigh quotient Iteration
    \(v^{(0)}=\) some vector with \(\left\|v^{(0)}\right\|=1\)
    \(\lambda^{(0)}=\left(v^{(0)}\right)^T A v^{(0)}\)
    for \(k=1,2, \ldots\) do
        Solve \(\left(A-\lambda^{(k-1)} I\right) w=v^{(k-1)}\) for \(w\)
        \(v^{(k)}=w /\|w\|\)
        \(\lambda^{(k)}=\left(v^{(k)}\right)^T A v^{(k)}\)
    end for
```
**** Projection into Krylov Subspace
- Iterative methods are based on projecting an \(m\)-dimensional problem into a lower dimensional Krylov subspace.
- Given a matrix \(A\) and a vector \(b\), the associated sequence of vectors:
\begin{align*}
b, A b, A^2 b, A^3 b \ldots
\end{align*}
is called Krylov sequence or Krylov subspace.
**** Arnoldi Iteration
- For a matrix \(A\), a complete reduction into Hessenberg form by an orthogonality transformation:
\begin{align*}
A Q=Q H
\end{align*}
is out of question if \(m\) is large.
- Instead consider the first \(n\) columns of the above factorization.
- Let \(Q_n\) be a \(m \times n\) matrix whose columns are the first columns of \(Q\) :
\begin{align*}
\left[\begin{array}{l|l|l|l} 
& & & \\
& & & \\
q_1 & q_2 & \ldots & q_n \\
& & &
\end{array}\right]
\end{align*}
```
Algorithm 9 Arnoldi Iteration
    \(b=\) some vector with \(q_1=b / \| b\)
    for \(n=1,2,3, \ldots\) do
        \(v=A q_n\)
        for \(j=1\) to \(n\) do
            \(h_{j n}=q_j^* v\)
            \(v=v-h_{n j} q_j\)
        end for
        \(h_{n+1, n}=\|v\|\)
        \(q_{n+1}=v / h_{n+1, m}\)
    end for
```
The Arnoldi process can be described as a systematic construction of orthonormal bases using successive Krylov subspaces - done using the Gram-Schmidt orthogonalization procedure.
\begin{align*}
K_n=Q_n R_n
\end{align*}

Let \(H_n\) be the (upper Hessenberg) matrix formed by the numbers \(h_{j, k}\) computed by the algorithm:
\begin{align*}
H_n=\left[\begin{array}{ccccc}
h_{1,1} & h_{1,2} & h_{1,3} & \ldots & h_{1, n} \\
h_{2,1} & h_{2,2} & h_{2,3} & \cdots & h_{2, n} \\
0 & h_{3,2} & h_{3,3} & \cdots & h_{3, n} \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
0 & \cdots & 0 & h_{n, n-1} & h_{n, n}
\end{array}\right] .
\end{align*}

We then have
\begin{align*}
H_n=Q_n^* A Q_n .
\end{align*}

This yields an alternative interpretation of the Arnoldi iteration as a (partial) orthogonal reduction of \(A\) to Hessenberg form. The matrix \(H_n\) can be viewed as the representation in the basis formed by the Arnoldi vectors of the orthogonal projection of \(A\) onto the Krylov subspace.
- Let \(\tilde{H}_n\) be the \((n+1) \times n\) upper-left section of \(\mathrm{H}\),
\begin{align*}
\tilde{H}_n=\left[\begin{array}{ccccc}
h_{11} & & \cdots & & h_{1 n} \\
h_{21} & h_{22} & & & \\
& \ddots & \ddots & & \vdots \\
& & & h_{n, n-1} & h_{n, n} \\
& & & & h_{n+1, n}
\end{array}\right]
\end{align*}
- Then we have
\begin{align*}
A Q_n=Q_{n+1} \tilde{H}_n
\end{align*}
- The \(n^{\text {th }}\) column of this equation:
\begin{align*}
A q_n=h_{1 n} q_1+\cdots+h_{n n} q_n+h_{n+1, n} q_{n+1}
\end{align*}

This indicates that \(q_{(n+1)}\), satisfies an \((n+1)\) term recurrence relation involving itself and previous Krylov subspace vectors.
- Arnoldi iteration is simply the modified Gram-Schmidt iteration that implements the above equation.
- To find the eigenvalues of \(A\), one expects that since \(H_n\) is a projection of \(A\), eigenvalues of \(H_n\) must be related to that of \(A\).
- These eigenvalues are called the Ritz value or Arnoldi eigenvalue estimates.
**** Lanczos Iteration
- For a symmetric matrix, since the matrix, \(H_n\) is both Hessenberg and symmetric - It just becomes a triangular matrix.
- Again just like Arnoldi, we find the eigenvalues of this triangular matrix.
- This leads to a three term recurrence relation.
```
Algorithm 13 Lanczos Iteration
    \(\beta_0=0, q_0=0, b=\) some vector with \(q_1=b / \| b\)
    for \(n=1,2,3, \ldots\) do
        \(v=A q_n\)
        \(\alpha_n=q_n^T v\)
        \(v=v-\beta_{n-1} q_{n-1}-\alpha_n q_n\)
        \(\beta_n=\|v\|\)
        \(q_{n+1}=v / \beta_n\)
    end for
```
** Random Numbers and Monte Carlo
:LOGBOOK:
CLOCK: [2024-06-22 Sat 13:07]--[2024-06-22 Sat 18:00] =>  4:53
CLOCK: [2024-06-22 Sat 09:22]--[2024-06-22 Sat 11:16] =>  1:54
CLOCK: [2024-06-22 Sat 07:44]--[2024-06-22 Sat 08:57] =>  1:13
CLOCK: [2024-06-22 Sat 02:15]--[2024-06-22 Sat 04:21] =>  2:06
CLOCK: [2024-06-15 Sat 04:22]--[2024-06-15 Sat 04:52] =>  0:30
:END:
*** Random numbers
To make programs behave randomly we use random numbers. Technically, we use /pseudorandom numbers/, which are not really random at all. They only look random, being generated by a deterministic formula referred to (inaccurately) as a random number generator.
**** Random number generators (RNGs)
***** Types of RNGs
****** Linear congruential generator (LCG)
Consider the following recurrence relation
\begin{align*}
x^{\prime}=(a x+c) \bmod m
\end{align*}
where \(a\), \(c\), and \(m\) are integer constants and \(x\) is an integer variable. Now suppose we take that new value and plug it back in on the righthand side of the equation again and get another value, and so on, generating a stream of integers. Here's a program in Python to do exactly that, starting from the value \(x=1\) :
#+NAME: Linear congruential generator (LCG)
#+begin_src python :results output
from pylab import plot,show
N = 10
a = 1664525
c = 1013904223
m = 4294967296
x = 1
results = []
for i in range(N):
    x = (a*x+c)%m
    results.append(x)
print(results)
#+end_src

#+RESULTS:
: [1015568748, 1586005467, 2165703038, 3027450565, 217083232, 1587069247, 3327581586, 2388811721, 70837908, 2745540835]

This program calculates the first 10 numbers in the sequence generated by \( x^{\prime}=(a x+c) \bmod m \) with \(a=1664525, c=1013904223\), and \(m=4294967296\).
This is a /linear congruential random number generator/. It generates a string of apparently random integers, one after another, simply by iterating the same equation over and over. It is the most famous of random number generators.
***** RNG seeds
Seeding a random number generator (RNG) like the Mersenne Twister in Python allows programmers to specify the starting point of a sequence of random numbers, fixing the entire sequence from that point forward. This is done by using the seed function from the random package, which takes an integer seed as its argument. For example, seeding with the integer 42 will produce a predictable sequence of numbers.
The seed need not necessarily represent the first number in the sequence of generated random numbers (as it does for the LCG) but some mathematical transform on it. The form of the transformation depends on the RNG in use. Nevertheless, a seeding an RNG fixes the entire sequence of random numbers and is determined by the value of the seed that was used.
The benefit of seeding is particularly apparent during debugging. Without seeding, a program using random numbers might behave differently on each run, making it difficult to reproduce and troubleshoot errors. Seeding ensures that the sequence of random numbers is consistent across multiple runs, allowing for reliable reproduction of errors and facilitating debugging. Once issues are resolved, the seed can be removed to restore true randomness in the program's execution.
**** The inverse transform method
#+NAME: The inverse transform method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Let \( F \) be a cumulative distribution function (CDF), and define its generalized inverse function as
\[
F^{-1}(z) = \inf \{x \mid F(x) \geq z\}, \quad \text{for } 0 < z < 1.
\]
If \( Z \) is a uniform random variable on \([0, 1]\), then the random variable \( X = F^{-1}(Z) \) has \( F \) as its CDF.
#+end_proposition
#+NAME: The inverse transform method
#+ATTR_LATEX: :environment proof
#+begin_proof latex
To show that \( X = F^{-1}(Z) \) has \( F \) as its CDF, we calculate:
\[
\mathbb{P}(F^{-1}(Z) \leq x) = \mathbb{P}(Z \leq F(x)).
\]
This equality follows from the definition of the generalized inverse:
\[
\{ z : F^{-1}(z) \leq x \} = \{ z : z \leq F(x) \}.
\]
Given that \( Z \) is uniform on \([0, 1]\), the probability that \( Z \) does not exceed a value \( z \) is exactly \( z \), thus:
\[
\mathbb{P}(Z \leq F(x)) = F(x).
\]
Hence, \( \mathbb{P}(F^{-1}(Z) \leq x) = F(x) \), showing that \( X \) is distributed according to \( F \).
#+end_proof
#+NAME: Oracle of randomness
#+ATTR_LATEX: :environment axiom
#+begin_axiom latex
We have an oracle of randomness that yields random numbers \( z \) on demand from the standard uniform distribution with a PDF
\[
p(z) = \begin{cases}
1, & z \in [0, ~ 1], \\
0, & \text{ otherwise }.
\]
#+end_axiom
***** Uniform distribution
#+NAME: Uniform distribution
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
Given \( Z = z \) from the standard uniform distribution, \(X = a + Z(b-a) \) is distributed according to the density
\[
p(X=x) = \begin{cases}
(b-a)^{-1}, & x \in [a,~b], \\
0, & \text{otherwise}
\end{cases}
\]
of the uniform distribution (\( b > a \)).
#+end_corollary
#+NAME: Uniform distribution
#+ATTR_LATEX: :environment proof
#+begin_proof latex
For \( a \leq x \leq b \), the corresponding CDF is
\begin{align*}
F(x) \equiv \int_{-\infty}^x p(t) \, dt =  \int_{a}^x p(t) \, dt = \int_a^x \frac{1}{b-a} \, dt
& = \frac{1}{b-a} \int_a^x dt = \frac{x - a}{b-a}
\end{align*}
The inverse of the CDF is given by \( F^{-1}(z) = \inf \{x \mid F(x) \geq z\} \) for \( 0 < z < 1 \). For the uniform distribution
\[
F^{-1}(z) = a + z \cdot (b-a), \quad 0 < z < 1.
\]
Given that \( Z \) uniform on \( [0, 1] \)
\[
\boxed{
X = a + Z \cdot (b-a)
}
\]
is distributed according to \( p \) by appeal to the inverse transform method.
#+end_proof
***** Bernoulli distribution
#+NAME: Bernoulli distribution
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
Given \( Z = z \) from the standard uniform distribution, 
\[ k = \begin{cases}
0, & 0 < z < 1 - p, \\
1, & 1 - p \leq z < 1
\end{cases}
\] 
is distributed according to the mass function
\[
p(k) = \begin{cases}
p, & k = 1, \\
1-p, & k = 0, \\
0, & \text{ otherwise. }
\]
of the Bernoulli distribution.
#+end_corollary
#+NAME: Bernoulli distribution
#+ATTR_LATEX: :environment proof
#+begin_proof latex
For \( k \in \{0,~1\} \), the corresponding CDF is
\[
F(k) = \begin{cases}
0, & k < 0, \\
1-p, & 0 \leq k < 1, \\
1, & k \geq 1.
\end{cases}
\]
The inverse of the CDF is given by \( F^{-1}(z) = \inf \{k \mid F(k) \geq z\} \) for \( 0 < z < 1 \). For the Bernoulli distribution
\[
F^{-1}(z) = \begin{cases}
0, & 0 < z < 1  - p, \\
1, & 1 - p \leq z < 1.
\end{cases}
\]
Given that \( Z \) uniform on \( [0, 1] \)
\[
\boxed{
k = \begin{cases}
0, & 0 < z < 1  - p, \\
1, & 1 - p \leq z < 1.
\end{cases}
}
\]
is distributed according to \( p(k) \) by appeal to the inverse transform method.
#+end_proof
***** Exponential distribution
#+NAME: Exponential distribution
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
Given \( Z = z \) and \( W = w \) both from the standard uniform distribution, \( X = - \frac{1}{\lambda} \ln(1 - Z) \) is distributed according to the density
\[
p(X=x) = \lambda e^{-\lambda x}, \quad x \geq 0,
\]
of the exponential distribution (\(\lambda > 0\)).
#+end_corollary

#+NAME: Exponential distribution
#+ATTR_LATEX: :environment proof
#+begin_proof latex
For \( x \geq 0 \), the corresponding CDF is
\[
F(x) = \int_0^x \lambda e^{-\lambda t} \, dt = 1 - e^{-\lambda x}.
\]
The inverse of the CDF is given by \( F^{-1}(z) = \inf \{k \mid F(k) \geq z\} \) for \( 0 < z < 1 \). For the exponential distribution
\[
F^{-1}(z) = -\frac{1}{\lambda} \ln(1 - z), \quad 0 < z < 1.
\]
Given that \( Z \) is uniform on \([0, 1]\),
\[
\boxed{
X = -\frac{1}{\lambda} \ln(1 - Z)
}
\]
is distributed according to \( p(x) = \lambda e^{-\lambda x} \) by appeal to the inverse transform method, as \( F^{-1}(Z) \) transforms \( Z \) into a variable with the exponential distribution's CDF.
#+end_proof
****** Example: Decay of an isotope
The decay of the radioisotope thallium-208 to stable lead-208 has a half-life of 3.053 minutes. Beginning with 1000 thallium atoms, we simulate the decay process, capturing its randomness using random numbers.

The number \(N(t)\) of atoms at time \(t\) follows the exponential decay law:
\[
N(t) = N(0) 2^{-t/\tau}
\]
where \(\tau\) is the half-life. The fraction of remaining atoms is:
\[
\frac{N(t)}{N(0)} = 2^{-t/\tau}
\]
The probability \(p(t)\) that a single atom decays by time \(t\) is:
\[
p(t) = 1 - 2^{-t/\tau}
\]
To simulate, we partition the 1000 atoms into thallium and lead sets, initially placing all atoms in the thallium set. Time is divided into 1-second steps. Each step is Bernoulli trial where decays occurs probability \(p(t)\). Decayed atoms are transferred from the thallium set to the lead set. The simulation iteratively updates the counts of both sets. Here's a program to perform the calculation and make a plot of the number of atoms of each type as a function of time for 1000 seconds:
#+NAME: Decay of an isotope
#+begin_src python :results output
from random import random
import numpy as np
import matplotlib.pyplot as plt
import os

# Constants
NTl = 1000  # Initial number of thallium atoms
NPb = 0     # Initial number of lead atoms
tmax = 100  # Total time
h = 1       # Time step
p = 0.01    # Probability of decay per time step

# Ensure the directory exists
save_path = os.path.expanduser('~/.local/images')
os.makedirs(save_path, exist_ok=True)

# Lists of plot points
tpoints = np.arange(0.0, tmax, h)
Tlpoints = []
Pbpoints = []

# Main loop
for t in tpoints:
    Tlpoints.append(NTl)
    Pbpoints.append(NPb)
    decay = 0
    for i in range(NTl):
        if random() < p:
            decay += 1
    NTl -= decay
    NPb += decay

# Plotting the results
plt.plot(tpoints, Tlpoints, label='Thallium')
plt.plot(tpoints, Pbpoints, label='Lead')
plt.xlabel("Time")
plt.ylabel("Number of atoms")
plt.legend()

# Save the figure
file_path = os.path.join(save_path, 'atom_decay_plot.png')
plt.savefig(file_path)
plt.close()
#+end_src

#+HTML_ATTR: :width 100px
[[file:~/.local/images/atom_decay_plot.png]]
#+CAPTION: The number of thallium atoms decays roughly exponentially, but some random variation around the exponential curve is also visible.

***** Gaussian distribution
#+NAME: Gaussian distribution
#+ATTR_LATEX: :environment corollary
#+begin_corollary latex
Given \( Z = z \) and \( W = w \) from the standard uniform distribution, let \(r = \sqrt{-2 \sigma^2 \ln (1 - z)}\) and \( \theta = 2 \pi w \cdot \). \( X = r \cos \theta \) and \( X = r \sin \theta \) are distributed according to the density
\[
p(X=x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{x^2}{2 \sigma^2}\right)
\]
of the Gaussian distribution (\(\sigma > 0\)).
#+end_corollary

#+NAME: Gaussian distribution
#+ATTR_LATEX: :environment proof
#+begin_proof latex
To generate random variables from the Gaussian distribution using the polar coordinate method, we start with the density function:
\[
p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{x^2}{2 \sigma^2}\right).
\]
While the transformation method fails due to the non-analytic nature of the Gaussian CDF, an alternative is the polar coordinate transformation.
Consider two independent Gaussian random variables, \( x \) and \( y \), with density functions \( p(x) \) and \( p(y) \). The joint density function in Cartesian coordinates is:
\[
p(x) p(y) = \frac{1}{2 \pi \sigma^2} \exp \left(-\frac{x^2 + y^2}{2 \sigma^2}\right) \, \mathrm{d}x \, \mathrm{d}y.
\]
Transforming into polar coordinates (\( r, \theta \)), where \( r = \sqrt{x^2 + y^2} \) and \( \theta = \tan^{-1}\left(\frac{y}{x}\right) \), the joint density becomes:
\[
p(r, \theta) = \frac{r}{\sigma^2} \exp \left(-\frac{r^2}{2 \sigma^2}\right) \, \mathrm{d}r \times \frac{1}{2 \pi} \, \mathrm{d}\theta.
\]
Here, \( p(r) \) and \( p(\theta) \) are the radial and angular components, respectively. \( p(\theta) = \frac{1}{2\pi} \) directly implies uniform distribution over \([0, 2\pi]\). The distribution for \( r \), obtained by the transformation \( r = \sqrt{-2 \sigma^2 \ln(1-z)} \) from a uniformly distributed \( z \), reflects the conversion from uniform to exponential form, which, when transformed back to Cartesian coordinates using \( x = r \cos \theta \) and \( y = r \sin \theta \), results in two independent Gaussian variables.
The key steps involve generating \( r \) from its transformed CDF:
\[
1 - \exp \left(-\frac{r^2}{2 \sigma^2}\right) = z,
\]
yielding:
\[
r = \sqrt{-2 \sigma^2 \ln(1-z)}.
\]
Thus, using \( r \) and \( \theta \), and converting back to Cartesian coordinates, we generate \( x = r \cos \theta \) and \( y = r \sin \theta \) as independent Gaussian random variables.
#+end_proof
****** Example: Rutherford scattering
At the beginning of the 20th century, Ernest Rutherford and his collaborators showed that when an \(\alpha\) particle, i.e., a helium nucleus of two protons and two neutrons, passes close to an atom as shown in Fig. 10.3, it is scattered, primarily by electrical repulsion from the positively charged nucleus of the atom, and that the angle \(\theta\) through which it is scattered obeys
\begin{align*}
\tan \frac{1}{2} \theta=\frac{Z e^{2}}{2 \pi \epsilon_{0} E b}
\end{align*}
where \(\mathrm{Z}\) is the atomic number of the nucleus, \(e\) is the electron charge, \(\epsilon_{0}\) is the permittivity of free space, \(E\) is the kinetic energy of the incident \(\alpha\) particle, and \(b\) is the impact parameter, i.e., the perpendicular distance between the particle's initial trajectory and the axis running through the nucleus.
Consider a beam of \(\alpha\) particles with energy \(7.7~\mathrm{MeV}\) that has a Gaussian profile in both its \(x\) and \(y\) axes with standard deviation \(\sigma=a_{0} / 100\), where \(a_{0}\) is the Bohr radius. The beam is fired directly at a gold atom. Let us simulate the scattering process for one million \(\alpha\) particles and calculate the fraction of particles that "bounce back" on scattering, i.e., that scatter through angles greater than \(90^{\circ}\). The threshold value of the impact parameter for which this happens can be calculated from Eq. (10.19) with \(\theta=90^{\circ}\), which gives
\begin{align*}
b=\frac{Z e^{2}}{2 \pi \varepsilon_{0} E}
\end{align*}
If \(b\) is less than this value then the particle bounces back.
#+NAME: Rutherford
#+begin_src python :results output
	from math import sqrt,log,cos,sin,pi
	from random import random
	# Constants
	Z = 79
	e = 1.602e-19
	E = 7.7e6*e
	epsilon0 = 8.854e-12
	a0 = 5.292e-11
	sigma = a0/100
	N = 1000000
	# Function to generate two Gaussian random numbers
	def gaussian():
			r = sqrt(-2*sigma*sigma*log(1-random()))
			theta = 2*pi*random()
			x = r*cos(theta)
			y = r*sin(theta)
			return x,y
	# Main program
	count = 0
	for i in range(N):
			x,y = gaussian()
			b = sqrt(x*x+y*y)
			if b<Z*e*e/(2*pi*epsilon0*E):
					count += 1
  print(count,"particles were reflected out of",N)
#+end_src

#+RESULTS: Rutherford
: 1511 particles were reflected out of 1000000
In this program we first define our constants, then define a function that generates two Gaussian random numbers with the appropriate standard deviation using the method described in the previous section. In the main program we use these numbers to calculate a value for the impact parameter of an \(\alpha\) particle and compare it to the threshold value
\begin{align*}
b=\frac{Z e^{2}}{2 \pi \varepsilon_{0} E}.
\end{align*}
We then repeat these operations a million times and count the total number of particles that bounce back. If we run the program it prints the output show above. The number produced varies a little from run to run because of the randomness in the calculation, but this is a typical figure. In other words, about \(0.15 \%\) of particles bounce back.
Rutherford, who did not yet know about the atomic nucleus when he did his experiments in 1909, was amazed to observe such back-scattering and famously wrote
#+begin_quote
[having an \(\alpha\) particle bounce back] was about as credible as if you had fired a 15-inch shell at a piece of tissue paper and it had come back and hit you.
#+end_quote
*** Monte Carlo integration
**** Methods
+ If a standard quadrature technique such as the /trapezoidal rule/ or /Simpson's rule/ or /Gaussian quadrature/ suffices, do not bother with MC integration.
+ MC integration is best suited for pathological functions \( f \) such as
  \[
  I=\int_{0}^{2} f(x) \mathrm{d} x, \quad \sin ^{2}\left[\frac{1}{x(2-x)}\right] 
  \]
  where the domain and range of the integrand are bounded, so that the integral is well defined, but the variations of \( f \) with respect to \( x \) are arbitrarily large.
+ MC integration can be used in the evaluation of very high dimensional integrals.
***** Naive MC integration
#+NAME: Naive Monte Carlo integration
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
Suppose we want to evaluate the integral
\begin{align*}
I=\int_{a}^{b} f(x) \mathrm{d} x.
\end{align*}
Let \( A \) is a bounding region such that the domain and range of \( f \) are entirely contained within this area. Suppose we sample \( N \) points \( (x_{i},~y_{i})_{i=1\ldots N} \) from within this area uniformly at random so that \( k \) of these points satisfy \( y_{i} \leq f(x_{i}) \). An approximation for \( I \) is and the associated  are
\[
I \simeq \frac{k A}{N} = \frac{A}{N} \sum_{i} \mathds{1} (y_{i} \leq f(x_{i})) \qquad \langle I^{2} \rangle_{c}^{1/2} = I^{1/2} (A-I)^{1/2} / \sqrt{N} \backsim \mathcal{O} (1/ \sqrt{N})
\]
where \(\langle I^{2} \rangle_{c}^{1/2}\) is the /approximation error/.
#+end_proposition
#+NAME: Naive Monte Carlo integration
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Consider a single sample \( (x, ~ y) \in A \) chosen uniformly at random from the bounding area \( A \). The probability that that \( y \leq f(x) \) is \(p = I / A\) and \( y > f(x) \) occurs with probability \( 1 - p \). Now consider \( N \) random points \( (x_{i},~y_{i})_{i=1\ldots N} \) in the bounding area and evaluate \( k \equiv \sum_{i} \mathds{1} (y_{i} \leq f(x_{i}))\). Clearly, \( k / N \) must approximately equal the probability \(p\), i.e., \( k/N = I/A \). It follows that
\[
I \simeq \frac{k A}{N} = \frac{A}{N} \sum_{i} \mathds{1} (y_{i} \leq f(x_{i})).
\]
Each sample of \( \mathds{1} (y_{i} \leq f(x_{i})) \) is a Bernoulli trial and \( k = \sum_{i=1}^{N} \mathds{1} (y_{i} \leq f(x_{i})) \) is distributed according to a binomial distribution
\[
p(k)=\binom{N}{k} p^{k} ~ (1-p)^{N-k}.
\]
The first two moments of the binomial distribution are
\[
\langle k\rangle=\sum_{k=0}^N k p(k) = N p, \quad \langle k^2 \rangle = \sum_{k=0}^N k^2 p(k) = N(N-1) p^2 + N p,
\]
and the variance of this distribution is \( \langle k^{2} \rangle_{c} = \langle k^2 \rangle-\langle k\rangle^2=N p(1-p)\). On substituting \( p = I /A \)
\[
\langle k^{2} \rangle_{c} = N p(1-p)=N (I/A) (1-I/A) = N I (A - I) / A^{2}.
\]
From \( I \simeq kA / N \) we have 
\[
\langle I^{2} \rangle_{c} = (A/N)^{2} \langle k^{2} \rangle_{c} = I (A-I)/ N \qquad \langle I^{2} \rangle_{c}^{1/2} = \frac{\sqrt{I (A - I)}}{\sqrt{N}}
\]
#+end_proof
Here's a program to estimate the integral
\begin{align*}
I=\int_{0}^{2} \sin ^{2}\left[\frac{1}{x(2-x)}\right] \mathrm{d} x
\end{align*}
using \(N=\) 10000 points with the naive Monte Carlo integration method.

#+NAME: MC integration
#+begin_src python :results output
	from math import sin
	from random import random
	def f(x):
			return (sin(1/(x*(2-x))))**2
	N = 10000
	count = 0
	for i in range(N):
			x = 2*random()
			y = random()
			if y<f(x):
					count += 1
	I = 2*count/N
	print(I)
#+end_src
The approximation error for an integral from \(x=a\) to \(x=b\) with \(N\) uniform samples (\( h = (b-a)/ N \)):
+ Naive MC integration :: \( \mathcal{O} (1/\sqrt{N}) \).
+ Trapezoidal rule :: \( \mathcal{O} (h^{2}) = \mathcal{O} (1/N^{2}) \),
+ Simpson's rule :: \( \mathcal{O} (h^{4}) = \mathcal{O} (1/N^{4}) \),
***** The mean value method
#+NAME: The mean value method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
The integral
\begin{align*}
I=\int_{a}^{b} f(x) \mathrm{d} x
\end{align*}
can be approximated using \( N \) numbers \(x_{1} \ldots x_{N}\) chosen uniformly at random from the interval \( [a,~b] \) as
\[
I \simeq \frac{b-a}{N} \sum_{i=1}^{N} f\left(x_{i}\right) \qquad \langle I^{2} \rangle_{c}^{1/2} = (b-a) \langle f^{2} \rangle_{c}^{1/2}/\sqrt{N}
\]
where \(\langle I^{2} \rangle_{c}^{1/2}\) is the /approximation error/.
#+end_proposition
#+NAME: The mean value method
#+ATTR_LATEX: :environment proof
#+begin_proof latex
The average value of \(f\) in the range from \(a\) to \(b\), by definition, is
\[
\langle f\rangle=\frac{1}{b-a} \int_{a}^{b} f(x) ~\mathrm{d} x=\frac{I}{b-a} \implies I = (b-a) \langle f \rangle, \qquad \langle f\rangle = \lim_{N\to\infty} N^{-1} \sum_{i=1}^{N} f(x_{i})
\]
For \(N\) points \(x_{1} \ldots x_{N}\) chosen uniformly at random between \(a\) and \(b\), we can approximate \(\langle f\rangle\) using \(f(x_{i})\) at, i.e., \( \(\langle f\rangle \simeq N^{-1} \sum_{i=1}^{N} f(x_{i})\). Substituting into \( I = (b-a) \langle f \rangle \) we recover the fundamental formula for the mean value method
\[
I \simeq \frac{b-a}{N} \sum_{i=1}^{N} f (x_{i}).
\]
With
\[
\langle f\rangle = N^{-1} \sum_{i=1}^{N} f\left(x_{i}\right), \quad \langle f^{2} \rangle = N^{-1}  \sum_{i=1}^{N}\left[f\left(x_{i}\right)\right]^{2}.
\]
the variance of \( f \) can be estimated as \( \langle f^{2} \rangle_{c} = \langle f^{2} \rangle - \langle f \rangle^{2} \). Thereafter, by appeal to the central limit theorem, we have
\[
\langle I^{2} \rangle_{c} = \lim_{N \to \infty} \bigg(\frac{b-a}{N}\bigg)^{2} N \langle f^{2} \rangle_{c} \implies \langle I^{2} \rangle_{c}^{1/2} = (b-a) \langle f^{2} \rangle_{c}^{1/2} / \sqrt{N}.
\]
#+end_proof
The approximation error is still \( \mathcal{O}( 1 / \sqrt{N})\) but \((b-a) \langle f^{2} \rangle_{c}^{1/2} < I^{1/2} (A-I)^{1/2} \), so the mean value method is a better approximation relative to naive MC integration.
***** The importance sampling method
#+NAME: The importance sampling method
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
The integral
\begin{align*}
I=\int_{a}^{b} f(x) \mathrm{d} x
\end{align*}
can be approximated using \( N \) numbers \(x_{1} \ldots x_{N}\) chosen drawn from \( p(x) =\big( \int_{a}^{b} w(x) ~ \mathrm{d} x\big)^{-1} w(x) \), where \( w(x) \) is an integrable, over the interval \( [a,~b] \) as
\[
I \simeq N^{-1} \bigg(\int_{a}^{b} w(x)~ \mathrm{d} x \bigg) \sum_{i=1}^{N} [f(x_{i})/w(x_i)],  \quad \langle I^{2} \rangle_{c}^{1/2} = \bigg(\int_{a}^{b} w(x) ~ \mathrm{d} x \bigg) ~ \langle f^{2}/w^{2} \rangle_{c, w}^{1/2} \big/ \sqrt{N}
\]
where \(\langle I^{2} \rangle_{c}^{1/2}\) is the /approximation error/ and \( \langle f^{2} / w^{2} \rangle_{c,w} = \langle g^{2} \rangle_{c, w} = \langle g^{2} \rangle_{w} - \langle g \rangle_{w}^{2} \).
#+end_proposition

#+NAME: The importance sampling method
#+ATTR_LATEX: :environment proof
#+begin_proof latex
Let \( f \) be such a terminally ill function defined over \( [a,~b] \)and suppose we want to evaluate
  \[
  I = \int_{a}^{b} f(x) ~ \mathrm{d}x.
  \]
+ In the mean value method, the mean value \( \langle f \rangle \) is the expected value of \( f \) with respect to a uniform distribution over the interval \(x \in [a,~b] \).
+ The importance sampling method introduces an auxiliary function \( g(x) \) and a (unnormalized) density function \( w (x) \) defined by
  \[
  \langle g\rangle_{w} = \bigg(\int_{a}^{b} w(x) ~ \mathrm{d} x\bigg)^{-1} \int_{a}^{b} g(x) ~ w(x) ~ \mathrm{d} x, \qquad  f(x) \equiv g(x) ~ w(x).
  \]
generalizes introduces a second function \( g(x) \) also defined over \( [a,~b] \) and its /weighted/ mean value \( \langle g \rangle_{w} \) where the expected value of \( g \) is with respect to some arbitrary density \( w(x) \) over the interval \( x \in [a,~b] \). We have \( g(x) = f(x)/ w(x) \). Under the action of \( \langle \cdot \rangle_{w} \) we have
  \[
\langle f(x)/w(x) \rangle_{w} = \bigg(\int_{a}^{b} w(x) ~ \mathrm{d} x\bigg)^{-1} \int_{a}^{b} f(x) ~ \mathrm{d} x = I~\bigg(\int_{a}^{b} w(x) ~ \mathrm{d} x\bigg)^{-1}
  \]
with an equivalent rewrite
\[
I = \langle f(x)/w(x) \rangle_{w} \int_{a}^{b} w(x) ~ \mathrm{d} x.
\]
Using \( N \) non-uniformly points \( x_{i},\ldots,x_{N} \) distributed according to \(p(x) =  w(x) \bigg(\int_{a}^{b} w(x) ~ \mathrm{d} x\bigg)^{-1} \), we have
\[
\langle g \rangle_{w} = \int_{a}^{b} p(x) g(x) ~ \mathrm{d} x \simeq N^{-1} \sum_{i=1}^{N} g(x_{i}).
\]
Since \( \langle g(x) \rangle_{w} = \langle f(x)/ w(x) \rangle_{w} \) we get
\[
I \simeq N^{-1} \bigg(\int_{a}^{b} w(x)~ \mathrm{d} x \bigg) \sum_{i=1}^{N} [f(x_{i})/w(x_i)].
\]
With
\[
\langle f/w\rangle_{c, w} = N^{-1} \sum_{i=1}^{N} f\left(x_{i}\right) / w(x_{i}), \quad \langle f^{2}/ w^{2} \rangle_{c, w} = N^{-1}  \sum_{i=1}^{N}\left[f\left(x_{i}\right)/w(x_{i})\right]^{2}.
\]
\( \langle f^{2}/g^{2} \rangle_{c, w} \) can be estimated as \( \langle f^{2} / g^{2} \rangle_{c, w} = \langle f^{2} / w^{2} \rangle_{w} - \langle f / w \rangle_{w}^{2} \). Thereafter, by appeal to the central limit theorem, we have
\[
\langle I^{2} \rangle_{c} = N^{-2} \bigg(\int_{a}^{b} w(x) ~ \mathrm{d} x \bigg)^{2} ~ N \langle f^{2}/w^{2} \rangle_{c, w} \big \implies \langle I^{2} \rangle_{c}^{1/2} = \bigg(\int_{a}^{b} w(x) ~ \mathrm{d} x \bigg) ~ \langle f^{2}/w^{2} \rangle_{c, w}^{1/2} \big/ \sqrt{N}.
\]
#+end_proof
+ The choice of \( w(x) \) depends on the functional form of \( f \) and the pathology we wish do us no ill. As long as \( w \) is integrable over the domain of integration, anything flies.
+ The approximation error is still \( \mathcal{O}( 1 / \sqrt{N})\).
****** Example 1:
As an example of usage of importance sampling, consider the integral
  \[
  I=\int_{0}^{1} \frac{x^{-1 / 2}}{\mathrm{e}^{x}+1} \mathrm{~d} x.
  \]
  which arises in the theory of Fermi gases. The integral is well-defined and finite-valued despite the divergence at \( x = 0 \). Using mean value method will lead to problems because close to \( x = 0 \), small differences \( \delta x \) can lead to arbitrarily large variations \( \delta I \).
We want the rid ourselves of the divergence at \( x = 0 \) so a natural choice for \( w \) is \( w(x) = x^{-1/2} \) so that \( f(x)/w(x) = (\exp (x) + 1)^{-1} \), free of all pathologies. To sample the points \( x_{1}\ldots x_{N} \), we need the functional form for \( p(x) = \big(\int_{a}^{b} w(x) ~ \mathrm{d} x\big)^{-1} w(x) \)
\[
p(x) = \frac{x^{-1/2}}{\int_{0}^{1} x^{-1/2} ~ \mathrm{d} x} = \frac{x^{-1/2}}{2}.
\]
Given \(Z = z \) from the standard uniform distribution, \( X = 1 / 4Z^{2} \) is distributed according to \( p \) (using the inverse transform sampling method). Let \( x_{1}\ldots x_{N} \) denote \( N \) such points. An importance sampling approximation for the integral \( I \) is 
\[
I \simeq 2 N^{-1} \sum_{i=1}^{N} (\exp (x_{i}) + 1)^{-1}
\]

Now we have we would write a program that draws random \(x\) values in the range from zero to one from the distribution

then plug them into Eq. (10.42) to get a value for the integral, noting that
\begin{align*}
\int_{0}^{1} w(x) \mathrm{d} x=\int_{0}^{1} x^{-1 / 2} \mathrm{~d} x=2 \tag{10.45}
\end{align*}
Exercise 10.8 asks you to write a program to perform this calculation and find a value for the integral.
Importance sampling has other uses as well. For instance, it can be used to evaluate integrals over infinite domains. An integral from 0 to \(\infty\) can't be done using the ordinary mean value method, because we would have to generate random points \(x_{i}\) uniformly between 0 and \(\infty\), which is impossible because
their distribution would not be integrable. But importance sampling allows us to generate points from some other, nonuniform distribution, one that extends out to infinity but is integrable, such as an exponential distribution. 
To be fair, as we saw in Section 5.8, a one-dimensional integral out to infinity can also be performed-and probably with greater accuracy-using conventional methods such as the trapezoidal rule in combination with a suitable change of variables. Once again, however, if the integrand took a pathological form, or if the integral were in a high-dimensional space, then Monte Carlo integration might become an attractive method.
In this situation the nonuniform distribution of points is no longer a downside of the importance sampling method but has become a positive advantage that we exploit to perform an integral that otherwise would not be possible. We will see another example of this type of approach in the next section, when we look at the technique of Monte Carlo simulation, particularly as applied to thermal physics.
****** Example 2
In scenarios where the cost of evaluating a function \( f \) varies across its domain, we can leverage importance sampling, regardless of any pathologies in \( f \). We can arrange for \( w \) to favors regions where \( f \) is cheaper to evaluate, so that we effectively sample more from these areas. For example, using \( w(x) = \exp(-x) \) over \( [0, \infty) \) emphasizes lower \( x \) values, making the rarer high \( x \) values contribute disproportionately more to the integral \( I \). This approach can provide a more efficient approximation compared to uniform sampling methods like the mean value technique.
\[
I \simeq \frac{1}{N} \sum_{i=1}^{N} \exp (x_{i}) f(x_{i}) \int_{0}^{\infty} \exp(-x) \mathrm{~d} x = \frac{1}{N} \sum_{i=1}^{N} \exp (x_{i}) f (x_{i}).
\]
****** Example 3

**** Applications
***** Integral of multi-dimensional functions
+ Numerical integration is higher dimensional spaces is intractable because of the /curse of dimensionality/: using \( N \) points along each dimension requires \( N^{D} \) function evaluations where \( D \) is the dimension. Monte Carlo integration may give adequate results.
#+NAME: The mean value method (multiple dimensions)
#+ATTR_LATEX: :environment proposition
#+begin_proposition latex
The integral
\begin{align*}
I= \idotsint\limits_{V} f(\mathbf{r}) ~\mathrm{d} \mathbf{r}
\end{align*}
can be approximated using \( N \) numbers \(\mathbf{r}_{1} \ldots \mathbf{r}_{N}\) chosen uniformly at random from the volume \( V \) as
\[
I \simeq \frac{V}{N} \sum_{i=1}^{N} f\left(\mathbf{r}_{i}\right) \qquad \langle I^{2} \rangle_{c}^{1/2} = V \langle f^{2} \rangle_{c}^{1/2}/\sqrt{N}.
\]
where \(\langle I^{2} \rangle_{c}^{1/2}\) is the /approximation error/.
#+end_proposition
+ The mean value method generalizes to the integral of a function \(f(r)\) over a volume \(V\) in a high-dimensional space
  \[
  I \simeq \frac{V}{N} \sum_{i=1}^{N} f (\mathbf{r}_{i} )
  \]
  where the points \(\mathbf{r}_{i}\) are picked uniformly at random from the volume \(V\).
#+NAME: Financial mathematics
#+ATTR_LATEX: :environment remark
#+begin_remark latex
An important application of this type of integral is in financial mathematics. The mathematical techniques used for predicting the values of portfolios of stocks and bonds require the evaluation of integrals over many variables, and those integrals need to be done rapidly in order to make quick trading decisions. Monte Carlo integration is a standard technique for doing this. Connections like this between physics and finance are one of the reasons why Wall Street and other trading centers often employ physicists.
#+end_remark
***** Monte Carlo simulations
The fundamental task in statistical mechanics is to calculate the average value of a quantity in a physical system at thermal equilibrium at temperature \( T \). The probability of the system occupying state \( i \) with energy \(E_i\) is given by the Gibbs-Boltzmann distribution
\[
P(E_i) = \exp (- \beta E_{i}) / Z, \quad Z = \sum_i \exp(-\beta E_i), \quad \beta = 1/ k_B T
\]
and \( k_B \) is Boltzmann's constant. The average value of a quantity \( X \) is, by definition,

\[
\langle X \rangle = \sum_i X_i P(E_i).
\]
In most cases, this sum cannot be calculated exactly due to the immense number of states, typically around \( 10^{23} \) for a mole of gas. Direct evaluation is infeasible; thus, numerical methods like Monte Carlo integration are used. The Monte Carlo method approximates the sum by randomly sampling states \( k = 1 \ldots N \):
\[ \langle X \rangle \approx \bigg(\sum_{k=1}^N P(E_k) \bigg)^{-1} \sum_{k=1}^N X_k P(E_k) \]
which is just a discrete version of
\[
  \langle g\rangle_{w} = \bigg(\int_{a}^{b} w(x) ~ \mathrm{d} x\bigg)^{-1} \int_{a}^{b} g(x) ~ w(x) ~ \mathrm{d} x
  \]
if we make the change \( \int_{a}^{b} \to \sum_{k=1}^{N} \), \( g(x) \to g_{i} \) and identify \( w_{i} = X_{i} ~ P(E_{i}) / g_{i} \) so that
\[
\langle X_{i} P(E_{i}) / w_{i} \rangle_{w} = \frac{\sum_{i} w_{i} X_{i} P(E_{i}) / w_{i}}{\sum_{i} w_{i}} = \frac{\sum_{i} X_{i} P(E_{i})}{\sum_{i} w_{i}} = \frac{\langle X \rangle}{\sum_{i} w_{i}}
\]
or
\[
\langle X \rangle = \langle X_{i} P(E_{i}) / w_{i} \rangle_{w} \sum_{i} w_{i}.
\]

Unfortunately, Eq. (10.49) doesn't work. The problem is that the Boltzmann probability (10.47) is exponentially small for any state with energy \(E_{i} \gg k_{B} T\), which is most of the states in most cases. That means that almost all of the states we choose for Eq. (10.49) will be ones that make very little contribution to the original sum in Eq. (10.48). There is typically just a small fraction of states that actually contribute significantly to the value of the sum, so few that we are unlikely to pick them in our random sample of terms. This means that the value of the sum we estimate from Eq. (10.49) will not be a good approximation to the true value, since the most important terms are missing. In the worst case there are so many states that make small contributions and so few that make large ones that we end up picking not one single one of the large ones, and our approximation to the sum will be very bad indeed.
But we have already seen a method that can solve this problem for us: importance sampling. Recall that importance sampling allowed us, for example, to get around divergences in integrals and other pathologies, but it did so at the expense of forcing us to choose our random samples from a nonuniform distribution. In Section 10.2.3 these nonuniform samples were mostly an annoyance-something we had to put up with to get the other benefits of the method. Now, however, this very nonuniformity becomes an advantage.
Importance sampling lets us calculate a correct value of an integral or a sum using a set of samples drawn nonuniformly. We have said that in our statistical mechanics calculations only a small fraction of the terms in our sum contribute significantly. Let us choose our nonuniform distribution of samples so that it focuses precisely on this small set, while ignoring the many terms that contribute little. It is, in fact, this application of importance sampling that gives the method its name. Importance sampling chooses the samples that are most important in the sum and neglects the ones that matter less.
Here's how it works in detail. For any quantity \(g_{i}\) that depends on the states \(i\), we can define a weighted average over states
\begin{align*}
\langle g\rangle_{w}=\frac{\sum_{i} w_{i} g_{i}}{\sum_{i} w_{i}} \tag{10.50}
\end{align*}
where \(w_{i}\) is any set of weights we choose. Making the particular choice \(g_{i}=\) \(X_{i} P\left(E_{i}\right) / w_{i}\) this gives
\begin{align*}
\left\langle\frac{X_{i} P\left(E_{i}\right)}{w_{i}}\right\rangle_{w}=\frac{\sum_{i} w_{i} X_{i} P\left(E_{i}\right) / w_{i}}{\sum_{i} w_{i}}=\frac{\sum_{i} X_{i} P\left(E_{i}\right)}{\sum_{i} w_{i}}=\frac{\langle X\rangle}{\sum_{i} w_{i}} \tag{10.51}
\end{align*}
where we have used Eq. (10.48). Rearranging this expression we get
\begin{align*}
\langle X\rangle=\left\langle\frac{X_{i} P\left(E_{i}\right)}{w_{i}}\right\rangle_{z v} \sum_{i} \tau w_{i} \tag{10.52}
\end{align*}
We can evaluate this expression approximately by selecting a set of \(N\) sample states randomly but nonuniformly, such that the probability of choosing state \(i\) is
\begin{align*}
p_{i}=\frac{w_{i}}{\sum_{j} w_{j}} \tag{10.53}
\end{align*}
in which case the equivalent of Eq. (10.41) is
\begin{align*}
\langle g\rangle_{w} \simeq \frac{1}{N} \sum_{k=1}^{N} g_{k} \tag{10.54}
\end{align*}
where \(g_{k}\) is the value of the quantity \(g\) in the \(k\) th state we select. Combining this result with Eq. (10.52), we then get
\begin{align*}
\langle X\rangle \simeq \frac{1}{N} \sum_{k=1}^{N} \frac{X_{k} P\left(E_{k}\right)}{w_{k}} \sum_{i} w_{i} \tag{10.55}
\end{align*}
Note that the first sum here is over only those states \(k\) that we sample, but the second is over all states \(i\). This second sum is usually evaluated analytically.
Our goal is to choose the weights \(w_{i}\) so that most of the samples are in the region where \(P\left(E_{i}\right)\) is big and very few are in the region where it is small, so that we pick up the big terms in the sum and ignore the small ones. We should also choose \(w_{i}\) so that the sum \(\sum_{i} w_{i}\) in Eq. (10.55) can be performed analytically-if we can't do this sum, then the equation is useless.
But both of these requirements are easy to satisfy. Let us simply choose \(w_{i}=P\left(E_{i}\right)\). Then we have \(\sum_{i} w_{i}=\sum_{i} P\left(E_{i}\right)=1\) by definition, and Eq. (10.55) simplifies to
\begin{align*}
\langle X\rangle \simeq \frac{1}{N} \sum_{k=1}^{N} X_{k} \tag{10.56}
\end{align*}
In other words, we just choose \(N\) states in proportion to their Boltzmann probabilities and take the average of the quantity \(\mathrm{X}\) over all of them, and that's it. The more states we use the more accurate the answer will be, but even with only a few states we already get a much more accurate answer than the purely random samples we considered to begin with.
There is another way of looking at this process, which is more physical and less mathematical (and therefore probably appeals more to physicists). In the real system we are studying-in the real world-the system passes, as we have said, through a succession of states with probabilities \(P\left(E_{i}\right)\), and the average value of \(X\) is, by definition, the average of the values in the states that the system passes through. If we do our Monte Carlo calculation according to the recipe above then we are, in effect, just simulating this process: we choose a set of states \(k\) of the system in proportion to \(P\left(E_{i}\right)\). You can think of these as the states the system passes through and we calculate the average value of \(\mathrm{X}\) in those states. So our calculation really is just an imitation of nature, and hence it should not be surprising that it gives a correct value for the quantity we are interested in. Just as our simulation of radioactive decay in Example 10.1 was an imitation on the computer of the physics of real radioactive decay, so our simulation of a thermal system is an imitation of the physics of a real thermal system. This is why we call this technique Monte Carlo "simulation."
This Monte Carlo simulation mimics the natural process where the system transitions through states according to \( P(E_i) \).  By sampling states proportionally to \( P(E_i) \), we effectively simulate the physical behavior of the system, ensuring accurate estimation of \( \langle X \rangle \).
However, this straightforward approach fails because \( P(E_i) \) is exponentially small for \( E_i \gg k_B T \), leading to insignificant contributions from most states.
Monte Carlo simulation is the name given to any computer simulation that uses random numbers to simulate a random physical process in order to estimate something about the outcome of that process. An example is the calculation we did of the decay of radioactive isotopes in Example 10.1. Another is the Rutherford scattering calculation in Exercise 10.2.
Although Monte Carlo simulation finds uses in every branch of physics, there is one area where it is used more than any other, and that is statistical mechanics. Because statistical mechanics is fundamentally about random (or apparently random) processes, Monte Carlo simulation assumes a particular importance in the field. In this section we look at Monte Carlo simulation, focusing particularly on statistical mechanics.
*** The Markov chain method
Unfortunately, we're not done yet. There is a catch. The catch is that it's not easy to pick states with probability \(P\left(E_{i}\right)\). We have a formula for \(P\left(E_{i}\right)\) in Eq. (10.47), but it includes the factor \(Z\), the so-called partition function, which is defined as a sum over states. We don't know the value of this sum in general and it's not easy to calculate. After all, the whole point of Monte Carlo simu-
lation is to calculate sums over states and if such sums were easy we wouldn't be doing the calculation in the first place. So if we don't know the value of the partition function and we can't easily calculate it, how do we choose states with probability \(P\left(E_{i}\right)\) ? Remarkably, it turns out that we can do it without knowing the partition function, using a device called a Markov chain.
We want to choose a set of states for the sum (10.56). We will do so by generating a string of states one after another-the Markov chain. Consider a single step in this process and suppose that the previous state in the chain, the state for the step before this one, was state \(i\). For the new state, instead of choosing randomly from all possible states, let us make some change, usually small, to state \(i\) so as to create a new state. If the system we are studying is a gas, for instance, we might move one of the molecules in the gas to a new quantum level, but leave the other molecules where they are. The choice of the new state is determined probabilistically by a set of transition probabilities \(T_{i j}\) that give the probability of changing from state \(i\) to state \(j\). It turns out that, if we choose \(T_{i j}\) right for all \(i\) and \(j\), we can arrange that the probability of visiting any particular state on any step of the Markov chain is precisely the Boltzmann probability \(P\left(E_{i}\right)\), so that when we take many steps and generate the entire chain the complete set of states that we move through is a correct sample of our Boltzmann distribution. Then all we have to do to evaluate Eq. (10.56) is measure the quantity \(X\) of interest in each state we move through and take the average. The trick lies in how we choose \(T_{i j}\) to achieve this.
Given that we must end up in some state on every step of the Markov chain it must be the case that
\begin{align*}
\sum_{j} T_{i j}=1 \tag{10.57}
\end{align*}
The secret of the Markov chain method is to choose the \(T_{i j}\) so that they satisfy this condition and also so that
\begin{align*}
\frac{T_{i j}}{T_{j i}}=\frac{P\left(E_{j}\right)}{P\left(E_{i}\right)}=\frac{\mathrm{e}^{-\beta E_{j}} / Z}{\mathrm{e}^{-\beta E_{i}} / \mathrm{Z}}=\mathrm{e}^{-\beta\left(E_{j}-E_{i}\right)} \tag{10.58}
\end{align*}
In other words, we are choosing a particular value for the ratio of the probability to go from \(i\) to \(j\) and the probability to go back again from \(j\) to \(i\). Note that the value of the partition function \(\mathrm{Z}\) cancels out of this equation, so that we don't need to know it in order to satisfy the equation-this gets around the problem we had before of not knowing the value of \(Z\).
Suppose we can find a set of probabilities \(T_{i j}\) that satisfy Eq. (10.58) and suppose that the probability we are in state \(i\) on one particular step of the Markov
chain is equal to the Boltzmann probability \(P\left(E_{i}\right)\), for all \(i\). In that case, making use of Eq. (10.58), the probability of being in state \(j\) on the next step is the sum of the probabilities that we got there from every other possible state \(i\) :
\begin{align*}
\sum_{i} T_{i j} P\left(E_{i}\right)=\sum_{i} T_{j i} P\left(E_{j}\right)=P\left(E_{j}\right) \sum_{i} T_{j i}=P\left(E_{j}\right) \tag{10.59}
\end{align*}
where we have used Eqs. (10.57) and (10.58). In other words, if we have the correct probability of being in every state on one step, then we have the correct probability on the next step too, and hence we will have it for ever afterwards.
This proves that if we can once get a Boltzmann distribution over states then we will keep a Boltzmann distribution. We say that the Boltzmann distribution is a fixed point of the Markov chain. We have not, however, proved that if we start with some other distribution over states we will converge to the Boltzmann distribution, but in fact we can prove this: we can prove that if we wait long enough the distribution of states in the Markov chain will always converge to the Boltzmann distribution. The proof is not complicated, but it's quite long, so it is relegated to Appendix D. The important point is that if you start off the system in any random state and run the Markov chain for long enough, the distribution over states will converge to the Boltzmann distribution. This is exactly what we need to make our calculation work.
We still need to work out what the values of the \(T_{i j}\) should be. We have assumed that it's possible to find a set of values that satisfy Eq. (10.58), but we haven't actually done it. In fact, it's not that hard: Equation (10.58) leaves us quite a lot of latitude about how we choose the probabilities, and there are many different choices that will get the job don. The most common choice by far, however, is the choice that leads to the so-called Metropolis algorithm. To understand how this algorithm works, note first that we are allowed to visit the same state more than once in our Markov chain, and indeed we can even visit the same state twice on two consecutive steps. Another way to say this is that the probability \(T_{i i}\) of moving from a state to itself is allowed to be nonzero, in which case the "move" is no move at all-we just stay in the state we're already in. Bearing this in mind, the Metropolis algorithm then works as follows.
Suppose once again that the previous state in the chain is state \(i\) and we generate a new state \(j\) by making some change to state \(i\). We will choose the particular change we make uniformly at random from a specified set of possible changes, sometimes called a move set. To take our previous example of the gas, for instance, we might choose to change the state of one molecule of the gas, with the molecule chosen uniformly at random and the state we are moving to chosen randomly to be up or down one state in energy from the present one. A uniform choice of this kind does not, generally, satisfy Eq. (10.58). But then we either accept or reject the new state with acceptance probability \(P_{a}\) given by
\begin{align*}
P_{a}= \begin{cases}1 & \text { if } E_{j} \leq E_{i}  \tag{10.60}\\ \mathrm{e}^{-\beta\left(E_{j}-E_{i}\right)} & \text { if } E_{j}>E_{i}\end{cases}
\end{align*}
If the move is rejected then we do nothing-the system remains in its old state \(i\) for one more step, which is equivalent to moving from state \(i\) to the same exact state \(i\), something that, as we have said, is allowed. If the move is accepted, on the other hand, then we change the system to the new state.
Another way of expressing Eq. (10.60) is to say that if the proposed move will decrease the energy of the system or keep it the same (i.e., \(E_{j} \leq E_{i}\) ), then we definitely accept it-we change the molecule to its new state, or whatever the move requires. On the other hand, if the proposed move will increase the energy, then we may still accept it-with the probability given in Eq. (10.60)otherwise we reject it.
Under this scheme the total probability \(T_{i j}\) of making a move from state \(i\) to state \(j\) is the probability that we choose that move out of all possibilities, which is just 1 / \(M\) if there are \(M\) possibilities, times the probability that we accept the move. So for instance if \(E_{j}>E_{i}\) then
\begin{align*}
T_{i j}=\frac{1}{M} \times \mathrm{e}^{-\beta\left(E_{j}-E_{i}\right)} \quad \text { and } \quad T_{j i}=\frac{1}{M} \times 1 \tag{10.61}
\end{align*}
and the ratio of the two is
\begin{align*}
\frac{T_{i j}}{T_{j i}}=\frac{\mathrm{e}^{-\beta\left(E_{i}-E_{i}\right)} / M}{1 / M}=\mathrm{e}^{-\beta\left(E_{j}-E_{i}\right)} \tag{10.62}
\end{align*}
which agrees with Eq. (10.58). Conversely, if \(E_{i} \geq E_{j}\) then
\begin{align*}
T_{i j}=\frac{1}{M} \times 1, \quad T_{j i}=\frac{1}{M} \times \mathrm{e}^{-\beta\left(E_{i}-E_{j}\right)} \tag{10.63}
\end{align*}
and the ratio of the two is
\begin{align*}
\frac{T_{i j}}{T_{j i}}=\frac{1 / M}{\mathrm{e}^{-\beta\left(E_{i}-E_{j}\right)} / M}=\mathrm{e}^{-\beta\left(E_{j}-E_{i}\right)} \tag{10.64}
\end{align*}
which again agrees with Eq. (10.58). Either way, we have found a set of transition probabilities \(T_{i j}\) that satisfies Eq. (10.58).
Our complete Markov chain Monte Carlo simulation now involves the following steps:
1. Choose a random starting state.
2. Choose a move uniformly at random from an allowed set of moves, such as changing a single molecule to a new state.
3. Calculate the value of the acceptance probability \(P_{a}\) given in Eq. (10.60).
4. With probability \(P_{a}\) accept the move, meaning the state of the system changes to the new state; otherwise reject it, meaning the system stays in its current state for one more step of the calculation.
5. Measure the value of the quantity of interest \(X\) in the current state and add it to a running sum of such measurements.
6. Repeat from step 2.
When we have done many such steps, we take our running sum and divide it by the total number of steps we took to get an estimate of the average value \(\langle X\rangle\), Eq. (10.56).
A simple way to implement Eq. (10.60) in a program is to calculate the change in energy \(E_{j}-E_{i}\) and then generate a random number \(z\) uniformly between zero and one and accept the move if \(z<\mathrm{e}^{-\beta\left(E_{j}-E_{i}\right)}\). Note that if \(E_{j} \leq E_{i}\) then the exponential will be greater than or equal to one, which means the move will always be acceptedin agreement with Eq. (10.60)-while if \(E_{j}>E_{i}\) the move will be accepted with probability \(\mathrm{e}^{-\beta\left(E_{j}-E_{i}\right)}\)-again in agreement with Eq. (10.60). Thus, for instance, if we have a variable deltaE in our program that stores the change in energy \(E_{j}-E_{i}\), then a single if statement of the form
if random()<exp(-beta*deltaE):
will accept or reject moves with the correct probabilities.
Although the Metropolis method is straightforward in practice, it does have some subtleties:
1. The steps where you reject a move and don't end up changing to a new state still count as steps. When you calculate your running sum of \(X\) values, you need to add to the sum even on the steps where you stay in the state you're already in. This means that when the system stays in the same state for two steps you will add the same number \(X\) to the sum two times. This may seem odd, but it's the correct thing to do.
2. In order for Eq. (10.62) to be correct it's crucial that the number of possible
moves \(M\) that you are choosing between be the same when going from \(i\) to \(j\) as it is when going from \(j\) to \(i\). Care must be taken when choosing the move set to make sure that this is the case.
3. You also need to choose the move set so that you can actually get to every possible state. It is possible to choose sets such that Eq. (10.58) is satisfied but nonetheless some states are inaccessible-there is no path of moves that will take you there from your starting state. A move set for which all states are accessible is said to be ergodic, and ergodicity is a requirement for the Metropolis algorithm to work. See Appendix D for some additional discussion of this point.
4. Although we have said that the Markov chain always converges to the correct Boltzmann probability distribution, we haven't said how long it takes to do it-how long the simulation takes to equilibrate. There is no universal rule of thumb telling you how long equilibration takes but, as we'll see in the following example, it's often obvious just by looking at the results.
***** Example: Monte Carlo simulation of an ideal gas
The quantum states of a particle or atom of mass \(m\) in a cubic box of length \(L\) on each side have three integer quantum numbers \(n_{x}, n_{y}, n_{z}=1 \ldots \infty\) and energies given by
\begin{align*}
E\left(n_{x}, n_{y}, n_{x}\right)=\frac{\pi^{2} \hbar^{2}}{2 m L^{2}}\left(n_{x}^{2}+n_{y}^{2}+n_{z}^{2}\right) \tag{10.65}
\end{align*}
An ideal gas is a gas of \(N\) such atoms that do not interact with one another in any way, so that their total energy is just the sum of the energies of the individual particles:
\begin{align*}
E=\sum_{i=1}^{N} E\left(n_{x}^{(i)}, n_{y}^{(i)}, n_{z}^{(i)}\right) \tag{10.66}
\end{align*}
where \(n_{x}^{(i)}\) is the value of the quantum number \(n_{x}\) for the \(i\) th atom, and similarly for \(n_{y}^{(i)}\) and \(n_{z}^{(i)}\). In this example we will perform a Monte Carlo simulation of such an ideal gas and use it to calculate the internal energy of the gas.
First we need to choose a move set. In this case we will let the move set be the set of all moves of a single atom to one of the six neighboring states where \(n_{x}, n_{y}\), or \(n_{z}\) differs by \(\pm 1\). In other words, on each step of the Monte Carlo simulation we will choose a random particle, choose one of the three quantum numbers \(n_{x}, n_{y}\), or \(n_{z}\), and choose a random change, either +1 or -1 ,
for the quantum number. When we make such a move, just one term in the total energy, Eq. (10.66), will change, the term for the atom \(i\) that changes state. If, for example, \(n_{x}\) for atom \(i\) increases by 1 , then the change will be
\begin{align*}
\Delta E & =\frac{\pi^{2} \hbar^{2}}{2 m L^{2}}\left[\left(n_{x}+1\right)^{2}+n_{y}^{2}+n_{z}^{2}\right]-\frac{\pi^{2} \hbar^{2}}{2 m L^{2}}\left(n_{x}^{2}+n_{y}^{2}+n_{z}^{2}\right) \\
& =\frac{\pi^{2} \hbar^{2}}{2 m L^{2}}\left[\left(n_{x}+1\right)^{2}-n_{x}^{2}\right]=\frac{\pi^{2} \hbar^{2}}{2 m L^{2}}\left(2 n_{x}+1\right) . \tag{10.67}
\end{align*}
Similarly if \(n_{x}\) decreases by 1 we have
\begin{align*}
\Delta E=\frac{\pi^{2} \hbar^{2}}{2 m L^{2}}\left(-2 n_{x}+1\right) \tag{10.68}
\end{align*}
We will write a program to perform the simulation for \(N=1000\) particles when \(k_{B} T=10\). For simplicity, we work with units where \(m=\hbar=1\) and set the size of box to be \(L=1\). Since, as we have seen, it doesn't matter what state we start our system in, let us just start with all particles in the ground state \(n_{x}=n_{y}=n_{z}=1\). Then we select at random the particle, axis, and sign for the first move, calculate the energy change \(\Delta E=E_{j}-E_{i}\), and then either accept or reject the move according to the Metropolis probability, Eq. (10.60). Then we simply repeat the whole process for many moves. (Notice that if a particle is in an \(n=1\) state and we happen to choose to decrease the value of the quantum number \(n\), then the move is always rejected, because there is no lower value of \(n\).)
Here is a program to perform the entire calculation, and make a plot of the total energy of the gas as a function of time for 250800 Monte Carlo moves:
```
from random import random,randrange
from math import exp,pi
from numpy import ones
from pylab import plot,ylabel,show
```
![](https://cdn.mathpix.com/cropped/2024_06_11_8eb9d957c8c8db71ffe0g-42.jpg?height=40&width=156&top_left_y=1703&top_left_x=120)
```
N}=100
steps = 250000
# Create a 2D array to store the quantum numbers
n = ones([N,3],int)
# Main loop
eplot = []
E = 3*N*pi*pi/2
```
File: mcsim.py
```
for k in range(steps):
    # Choose the particle and the move
    i = randrange(N)
    j = randrange(3)
    if random()<0.5:
        dn = 1
        dE = (2*n[i, j]+1)*pi*pi/2
    else:
        dn = -1
        dE = (-2*n[i,j]+1)*pi*pi/2
    # Decide whether to accept the move
    if n[i,j]>1 or dn==1:
        if random()<exp(-dE/T):
            n[i,j] += dn
            E += dE
    eplot. append(E)
# Make the graph
plot(eplot)
ylabel("Energy")
show()
```
Note how on each step of the calculation we choose one of the \(N\) particles using randrange \((\mathrm{N})\), one of the three quantum numbers with randrange (3), and a direction, up or down, with \(50: 50\) probability, by testing whether random () \(<0.5\).
The program runs quickly-it takes just a few seconds to run through all 250000 steps. Figure 10.5 shows the plot it produces. As we can see, the energy starts low at the beginning of the calculation (because we start it off in the ground state), but quickly rises and then levels off. Eyeballing the plot roughly, we might say that the calculation has equilibrated after about 50000 steps, or perhaps 100000 to be on the safe side. After this point, we could, for instance, start accumulating values of the energy to calculate an estimate of the average energy of the gas. If we did this, it looks like we'd get a value around \(E=25000\) (in the dimensionless units used in the calculation). In principle, we could then go back and run the calculation again for another value of the temperature, and another, perhaps making a graph of the calculated value of energy as a function of temperature.
Because the program is a random one it will not give the same answer every
![](https://cdn.mathpix.com/cropped/2024_06_11_8eb9d957c8c8db71ffe0g-44.jpg?height=647&width=928&top_left_y=340&top_left_x=219)
Figure 10.5: Internal energy of an ideal gas. The energy of an ideal gas of 1000 particles calculated by Monte Carlo simulation, as described in the text. From the shape of the curve it appears that the simulation equilibrates after about 50000 steps.
time we run it, even if we use the same values of the parameters. But the longer we run it for, the more accurate our answers will be, and the more similar they will be from one run to another. If we want highly accurate answers, therefore, we should run for many Monte Carlo'steps. If we are content with less accurate answers, then we can do a shorter run and get an answer sooner. Like many calculations in computational physics, Monte Carlo simulations are a compromise between speed and accuracy.
The Monte Carlo method can be extended to many other systems studied in statistical physics, and to the measurement of a wide variety of quantities. One can estimate heat capacity, entropy, free energy, pressure, magnetization, and many other things using straightforward extensions of the techniques described here.
\section*{Exercise 10.9: The Ising model}
The Ising model is a theoretical model of a magnet. The magnetization of a magnetic material is made up of the combination of many small magnetic dipoles spread
throughout the material. If these dipoles point in random directions then the overall magnetization of the system will be close to zero, but if they line up so that all or most of them point in the same direction then the system can acquire a macroscopic magnetic moment-it becomes magnetized. The Ising model is a model of this process in which the individual moments are represented by dipoles or "spins" arranged on a grid or lattice:
![](https://cdn.mathpix.com/cropped/2024_06_11_8eb9d957c8c8db71ffe0g-45.jpg?height=500&width=473&top_left_y=567&top_left_x=813)
In this case we are using a square lattice in two dimensions, although the model can be defined in principle for any lattice in any number of dimensions.
The spins themselves, in this simple model, are restricted to point in only two directions, up and down. Mathematically the spins are represented by variables \(s_{i}= \pm 1\) on the points of the lattice, +1 for up-pointing spins and -1 for down-pointing ones. Dipoles in real magnets can typically point in any spatial direction, not just up or down, but the Ising model, with its restriction to just the two directions, captures a lot of the important physics while being significantly simpler to understand.
Another important feature of many magnetic materials is that the individual dipoles in the material may interact magnetically in such a way that it is energetically favorable for them to line up in the same direction. The magnetic potential energy due to the interaction of two dipoles is proportional to their dot product, but in the Ising model this simplifies to just the product \(s_{i} s_{j}\) for spins on sites \(i\) and \(j\) of the lattice, since the spins are one-dimensional scalars, not vectors. Then the actual energy of interaction is \(-J s_{i} s_{j}\), where \(J\) is a positive interaction constant. The minus sign ensures that the interactions are ferromagnetic, meaning the energy is lower when dipoles are lined up. A ferromagnetic interaction implies that the material will magnetize if given the chance. (In some materials the interaction has the opposite sign so that the dipoles prefer to be antialigned. Such a material is said to be antiferromngnetic, but we will not look at the antiferromagnetic case here.)
Normally it is assumed that spins interact only with those that are immediately adjacent to them on the lattice, which gives a total energy for the entire system equal to
\begin{align*}
E=-J \sum_{\langle i j\rangle} s_{i} s_{j}
\end{align*}
*** Simulated annealing
One of the most interesting uses of the Monte Carlo method in recent years has been in numerical optimization. In Section 6.4 we looked at methods for finding maxima or minima of functions, methods such as golden ratio search and gradient descent. As we pointed out, however, those methods are of limited use if one wants to find the global maximum or minimum-they can find a local minimum, a point in a function that is lower than the immediately surrounding points, but they do not guarantee that it is the lowest point anywhere. There are many problems, both in physics and elsewhere, that require us to find the highest or lowest overall value of a function-the global maximum or minimum-or at least an approximation to it. If, for example, we are looking for the ground state energy of a quantum system then it is important to find the overall minimum of the energy. An engineer trying to find the design parameters of an automobile that maximize its fuel efficiency wants the global maximum, not merely a local maximum. For such optimization problems the methods of Chapter 6 will not work and another approach is needed. Global optimization problems are among the hardest of computational tasks and it can demand both substantial computing resources and considerable ingenuity to solve them. One of the most promising approaches, proposed in 1985 by the physicist Scott Kirkpatrick, borrows ideas from statistical physics and goes under the name of simulated annealing.
For a physical system in equilibrium at temperature \(T\), the probability that at any moment the system is in a state \(i\) is given by the Boltzmann probability of Eq. (10.47), which for convenience we reproduce here:
\begin{align*}
P\left(E_{i}\right)=\frac{\mathrm{e}^{-\beta E_{i}}}{\mathrm{Z}}, \quad \text { with } \quad \mathrm{Z}=\sum_{i} \mathrm{e}^{-\beta E_{i}} \tag{10.69}
\end{align*}
where \(\beta=1 / k_{B} T\). Let us assume our system has a single unique ground state and let us choose our energy scale so that \(E_{i}=0\) in the ground state and \(E_{i}>0\) for all other states. Now suppose we cool down the system to absolute zero. In the limit \(T \rightarrow 0\) we have \(\beta \rightarrow \infty\) so \(\mathrm{e}^{-\beta E_{i}} \rightarrow 0\) except for the ground state, where \(e^{-\beta E_{i}}=e^{0}=1\). Thus in this limit \(Z=1\) and
\begin{align*}
P\left(E_{i}\right)= \begin{cases}1 & \text { for } E_{i}=0  \tag{10.70}\\ 0 & \text { for } E_{i}>0\end{cases}
\end{align*}
In other words, the system will definitely be in the ground state. Thus one way to find the ground state of the system is to cool it down to \(T=0\) (or as close to it as we can get) and see what state it lands in. This in turn suggests a computational strategy for finding the ground state: let us simulate the system at temperature T, using the Markov chain Monte Carlo method of Section 10.3 , then lower the temperature to zero and the system should find its way to the ground state. Kirkpatrick's insight was to realize that this same approach could be used to find the minimum of any function, not just the energy of a physical system. We can take any mathematical function \(f(x, y, z, \ldots)\) and treat the independent variables \(x, y, z\) as defining a "state" of the "system" and \(f\) as being the energy of that system, then perform a Monte Carlo simulation. Taking the temperature to zero will again cause the system to fall into its ground state, i.e., the state with the lowest value of \(f\), and hence we find the minimum of the function. This is the basic idea behind simulated annealing. The method can also be used to maximize functions-we treat \(-f\) as the energy of the system (instead of \(+f\) ), so that the minimum of energy corresponds to the maximum of \(f\).
There is a catch, however. Recall that on each step of a thermal Monte Carlo simulation we choose a move from a given set of possibilities, changing the state of the system to some other nearby state, then we accept or reject that move with probability given by Eq. (10.60). Now suppose that the system finds itself in a local minimum of the energy, a state in which all other nearby states have higher energy. In that case, all proposed Monte Carlo moves will be to states with higher energy, and if we then set \(T=0\) the acceptance probability of Eq. (10.60) becomes zero for every move, because \(\beta \rightarrow \infty\) and \(E_{j}>E_{i}\). This means that every move will be rejected and the system will never be able escape from its local minimum. In principle there is still a global minimum that has a lower energy, but our computer program will never find it because it gets trapped in the local minimum. Luckily there is a trick that can get around this problem, and once again it is inspired by our knowledge of the physical world.
It has been well known for centuries, among glassworkers, metalworkers, ceramicists, and others, that when you are working with hot materials, like molten glass or metal, one must cool the materials slowly to produce a hardy, solid final product. Glass cooled too quickly will develop defects and weaknesses, or even shatter. Metal cooled too quickly will be soft or brittle. If one cools slowly, on the other hand-if one anneals the material-the end result will be rigid and sturdy. Why is this? The answer is that when materials are hot their atoms or molecules, which are in a rapid state of motion, are disordered; the motion randomizes their positions and prevents them coming to rest in an orderly arrangement. If one cools such a material rapidly, that disorder gets
frozen into the material, and this creates flaws in the structure, cracks and defects that weaken the material. In the language of thermal physics, the system has become trapped at a local minimum of the energy, in a manner exactly analogous to what happens in our Monte Carlo simulation. If, on the other hand, one cools the material slowly then the disorder no longer gets frozen in and the atoms have time to rearrange and reach an ordered state, such as the regular lattice seen in the crystal structure of metals, which is the ground state of the system and contains no flaws, and is correspondingly stronger.
Thus while a rapidly cooled system can get stuck in a local energy minimum, an annealed system, one that is cooled sufficiently slowly, can find its way to the ground state. Simulated annealing applies the same idea in a computational setting. It mimics the slow cooling of a material on the computer by using a Monte Carlo simulation with a temperature parameter that is gradually lowered from an initially high value towards zero. It can be shown rigorously that this method is guaranteed to find the ground state if we cool slowly enough. Unfortunately, the rate of cooling required can often be so slow as to be impractical, so instead we typically just cool as slowly as we reasonably can, in the knowledge that we may not find the exact ground state of the system, but we will probably find a state close to it.
Some of the most interesting applications of simulated annealing are not to physics problems, but to problems in other areas of science and engineering. Like the problem of optimizing an automobile's fuel consumption, many technical challenges can be phrased in terms of finding the global minimum or maximum of a function of one or more parameters. Simulated annealing is one of the most widely used methods for solving such problems and has been applied in engineering, computer science, biology, economics, mathematics, statistics, and chemistry, as well as, of course, physics.
In practice, implementation of the method is quite straightforward-we perform a Monte Carlo simulation of the system of interest just as in Section 10.3.2 and slowly lower the temperature until the state of the system stops changing. The final state that the system comes to rest in is our estimate of the ground state. The only questions we need to answer are what the initial temperature should be and how fast we should cool.
The initial temperature should be chosen so that the system equilibrates quickly. To achieve this we should choose the thermal energy \(k_{B} T\) to be significantly greater than the typical energy change accompanying a single Monte Carlo move. Then we will have \(\beta\left(E_{j}-E_{i}\right) \ll 1\) in Eq. (10.60) and hence \(P_{a} \simeq 1\) for all Monte Carlo moves, meaning that most moves will be accepted and the
state of the system will be rapidly randomized, no matter what the starting state. As for the rate of cooling, one typically specifies a cooling schedule, i.e., a trajectory for the temperature as a function of time, and the most common choice by far is the exponential cooling schedule
\begin{align*}
T=T_{0} \mathrm{e}^{-\mathrm{t} / \tau} \tag{10.71}
\end{align*}
where \(T_{0}\) is the initial temperature and \(\tau\) is a time constant. Some trial and error may be necessary to find a good value for the time constant. Typically one gets better results the slower the cooling, i.e., the larger the value of \(\tau\), but slower cooling also means that it takes the system longer to reach a temperature low enough that its state no longer changes (at which point the simulation can stop, since nothing more is happening). As with most computer calculations, therefore, the choice of \(\tau\) is often a compromise between having a program that gives good answers and having one that runs in reasonable time.
***** Example: The traveling salesman
As an example of the use of simulated annealing, we will solve one of the most famous of optimization problems, the traveling salesman problem, which involves finding the shortest route that visits a given set of locations on a map. The traveling salesman problem is of interest not because we actually care about traveling salesmen-most of us don't. It's interesting because it belongs to the class of NP-hard problems, problems that are unusually difficult to solve computationally. Thus the problem provides a special challenge for us as computational scientists and a method that can find a good solution to this problem will probably work well for other problems too.
The definition of the problem is as follows. A traveling salesman, hawking his employer's products, wishes to visit \(N\) given cities. We'll assume that he can travel (let's say by air) in a straight line between any pair of cities. Given the coordinates of the cities, the problem is to devise the shortest tour,
Figure 10.6: The traveling salesman problem. Starting from a given city (circled) a salesman wants to find the shortest tour that visits each city once and then returns to the starting point. in terms of total distance traveled, that starts and ends at the same city and visits all \(N\) cities-see Fig. 10.6.
To make the problem simple, we will consider it in a flat two-dimensional space (as opposed to the surface of the Earth, which is curved, making the
problem harder), and we'll solve it for the case where the \(N\) city locations are chosen at random within a square of unit length on each side.
Let us number the cities in the order in which the salesman visits them, in Python fashion starting from zero-see Fig. 10.6 again-and let us denote the position of city \(i\) by the two-dimensional vector \(\mathbf{r}_{i}=\left(x_{i}, y_{i}\right)\), with \(\mathbf{r}_{N}=\mathbf{r}_{0}\), since the tour ends where it begins. Then the total distance \(D\) traveled by the salesman over the whole tour is
\begin{align*}
D=\sum_{i=0}^{N-1}\left|\mathbf{r}_{i+1}-\mathbf{r}_{i}\right| \tag{10.72}
\end{align*}
We want to minimize this quantity over the set of all possible tours, i.e., over all possible choices of the order of the cities. To do this using simulated annealing we need to choose a set of moves for the Markov chain. In this case a suitable set of moves is swaps of pairs of cities in the tour, so the calculation will involve setting up an initial tour and repeatedly trying out a swap of two cities. If a swap shortens the tour, then we always accept it and proceed to the next move. If it lengthens the tour then we accept it with the Metropolis probability given in Eq. (10.60), with the energy \(E\) replaced by the distance \(D\), otherwise we reject it. If the move is rejected we need to swap the two cities back to where they were previously before we try another move. Also on each step we need to calculate the new value of the temperature according to the cooling schedule, Eq. (10.71).
Here is a full program to perform the calculation and display an animated picture of the tour on the screen as it runs:
#+NAME: Salesman
#+begin_src python
	from math import sqrt,exp
	from numpy import empty
	from random import random,randrange
	from visual import sphere,curve,display,rate
	N = 25
	R = 0.02
	Tmax = 10.0
	Tmin = 1e-3
	tau = 1e4
	# Function to calculate the magnitude of a vector
	def mag(x):
			return sqrt(x[0]**2+x[1]**2)
	# Function to calculate the total length of the tour
	def distance():
			s = 0.0
			for i in range(N):
					s += mag(r[i+1]-r[i])
			return s
	# Choose N city locations and calculate the initial distance
	r = empty([N+1,2],float)
	for i in range(N):
			r [i,0] = random()
			r[i,1] = random()
	D = distance()
	# Set up the graphics
	display(center = [0.5,0.5])
	for i in range(N):
			sphere(pos=r [i],radius=R)
	l = curve(pos=r,radius=R/2)
	# Main loop
	t = 0
	T = Tmax
	while T>Tmin:
	t += 1
	T = Tmax*exp(-t/tau) # Cooling ,
	if t%100==0: # Update the visualization every 100 moves
			1.pos = r
			rate(25)
	# Choose two cities to swap and make sure they are distinct
	while i==j:
			i,j = randrange(1,N),randrange(1,N)
	# Swap them and calculate the change in distance
	oldD = D
	r[i,1],r[j,1] = r[j,1],r[i,1]
	D = distance()
	deltaD = D - oldD
#+end_src
Figure 10.7: Solutions of the traveling salesman problem. (a) A solution to one of the random traveling salesman problems generated by the program given in the text, as found by the program using simulated annealing. (b) The solution found by the program for another instance of the problem. In this case the solution is clearly not perfect-it could be improved by swapping the two cities near the center, where the paths cross.
\begin{align*}
& \text { \# If the move is rejected, swap them back again } \\
& \text { if random()>=exp(-deltaD/T): } \\
& \quad r[i, 0], r[j, 0]=r[j, 0], r[i, 0] \\
& \quad r[i, 1], r[j, 1]=r[j, 1], r[i, 1] \\
& \quad D=\text { oldD }
\end{align*}
For a tour of 25 cities the program takes about 30 seconds to run on the author's computer. Figure 10.7a shows the result from a typical run, where the computer appears to have found a reasonable tour around the given set of cities. As we have said, however, simulated annealing usually involves compromises between speed of execution and quality of the results and can occasionally return imperfect answers. Figure 10.7b shows an example: in this case the computer has found a pretty good solution to the problem but it's not perfect. The tour could clearly be improved by swapping the two cities in the middle, where the paths cross.
These results are typical of the simulated annealing method: it gives good but not necessarily perfect results for a wide range of problems, and is a good, general-purpose method for performing optimization on the computer.
** Quadrature
:LOGBOOK:
CLOCK: [2024-06-11 Tue 22:15]--[2024-06-12 Wed 02:58] =>  4:43
CLOCK: [2024-06-11 Tue 16:54]--[2024-06-11 Tue 21:29] =>  4:35
CLOCK: [2024-06-11 Tue 00:26]--[2024-06-11 Tue 02:11] =>  1:45
:END:
*** Polynomial interpolation
**** Lagrange interpolating polynomial
Given a set of \(k+1\) nodes \(\left\{x_0, x_1, \ldots, x_k\right\}\), which must all be distinct, \(x_j \neq x_m\) for indices \(j \neq m\), the Lagrange basis for polynomials of degree \(\leq k\) for those nodes is the set of polynomials \(\left\{\ell_0(x), \ell_1(x), \ldots, \ell_k(x)\right\}\) each of degree \(k\) which take values \(\ell_j\left(x_m\right)=0\) if \(m \neq j\) and \(\ell_j\left(x_j\right)=1\). Using the Kronecker delta this can be written \(\ell_j\left(x_m\right)=\delta_{j m}\). Each basis polynomial can be explicitly described by the product:
\begin{align*}
\begin{aligned}
\ell_j(x) & =\frac{\left(x-x_0\right)}{\left(x_j-x_0\right)} \cdots \frac{\left(x-x_{j-1}\right)}{\left(x_j-x_{j-1}\right)} \frac{\left(x-x_{j+1}\right)}{\left(x_j-x_{j+1}\right)} \cdots \frac{\left(x-x_k\right)}{\left(x_j-x_k\right)} \\
& =\prod_{\substack{0 \leq m \leq k \\
m \neq j}} \frac{x-x_m}{x_j-x_m}
\end{aligned}
\end{align*}
Notice that the numerator \(\prod_{m \neq j}\left(x-x_m\right)\) has \(k\) roots at the nodes \(\left\{x_m\right\}_{m \neq j}\) while the denominator \(\prod_{m \neq j}\left(x_j-x_m\right)\) scales the resulting polynomial so that \(\ell_j\left(x_j\right)=1\).
The Lagrange interpolating polynomial for those nodes through the corresponding values \(\left\{y_0, y_1, \ldots, y_k\right\}\) is the linear combination:
\begin{align*}
L(x)=\sum_{j=0}^k y_j \ell_j(x) .
\end{align*}
Each basis polynomial has degree \(k\), so the sum \(L(x)\) has degree \(\leq k\), and it interpolates the data because
\begin{align*}
L\left(x_m\right)=\sum_{j=0}^k y_j \ell_j\left(x_m\right)=\sum_{j=0}^k y_j \delta_{m j}=y_m
\end{align*}
The interpolating polynomial is unique. Proof: assume the polynomial \(M(x)\) of degree \(\leq k\) interpolates the data. Then the difference \(M(x)-L(x)\) is zero at \(k+1\) distinct nodes \(\left\{x_0, x_1, \ldots, x_k\right\}\). But the only polynomial of degree \(\leq k\) with more than \(k\) roots is the constant zero function, so \(M(x)-L(x)=0\), or \(M(x)=L(x)\).
**** Newton interpolating polynomial
If one writes the Lagrange interpolation polynomial with slightly different basis functions, one obtains the Newton's interpolation formula given by:
\[
P_n(x) = \alpha_0 + \alpha_1\left(x-x_0\right) + \alpha_2\left(x-x_1\right)\left(x-x_0\right) + \ldots + \alpha_n\left(x-x_{n-1}\right) \ldots \left(x-x_1\right) \left(x-x_0\right)
\]
For \(i=0\):
\[
f_0 = P_n\left(x_0\right) = \alpha_0
\]
For \(i=1\):
\[
f_1 = P_n\left(x_1\right) = \alpha_0 + \alpha_1\left(x_1 - x_0\right)
\]
\[
\alpha_1 = \frac{f_1 - f_0}{x_1 - x_0}
\]
For \(i=2\):
\[
f_2 = P_n\left(x_2\right) = \alpha_0 + \alpha_1\left(x_2 - x_0\right) + \alpha_2\left(x_2 - x_1\right)\left(x_2 - x_0\right)
\]
\[
\alpha_2 = \frac{\left(f_2 - f_1\right)/\left(x_2 - x_1\right) - \left(f_1 - f_0\right)/\left(x_1 - x_0\right)}{x_2 - x_0}
\]
Similarly, we can find \(\alpha_3, \ldots, \alpha_{n-1}\).
To express \(\alpha_i\), \(i=0, \ldots, n-1\), in a compact manner, let us first define the following notation called divided differences:
\[
f\left[x_k\right] = f_k
\]
\[
f\left[x_k, x_{k+1}\right] = \frac{f\left[x_{k+1}\right] - f\left[x_k\right]}{x_{k+1} - x_k}
\]
\[
f\left[x_k, x_{k+1}, x_{k+2}\right] = \frac{f\left[x_{k+1}, x_{k+2}\right] - f\left[x_k, x_{k+1}\right]}{x_{k+2} - x_k}
\]
\[
f\left[x_k, x_{k+1}, \ldots, x_i, x_{i+1}\right] = \frac{f\left[x_{k+1}, \ldots, x_{i+1}\right] - f\left[x_k, \ldots, x_i\right]}{x_{i+1} - x_k}
\]
With this notation:
\[
\alpha_0 = f\left[x_0\right]
\]
\[
\alpha_1 = f\left[x_0, x_1\right]
\]
\[
\alpha_2 = f\left[x_0, x_1, x_2\right]
\]
\[
\alpha_n = f\left[x_0, x_1, \ldots, x_n\right]
\]
Now the polynomial can be rewritten as:
\[
P_n(x) = \sum_{k=0}^n f\left[x_0, \ldots, x_k\right] \prod_{i=0}^{k-1}\left(x-x_i\right)
\]
This is called Newton's Divided Difference interpolation polynomial.
Sometimes in practice, the data points \(x_i\) are equally spaced points:
\[
x_i = x_0 + i \cdot h, \quad i=0, 1, 2, \ldots, n
\]
where \(x_0\) is the starting point and \(h\) is the step size.
In this case, it is enough to calculate simple differences rather than the divided differences as in the non-uniformly placed data set case.
These simple differences can be forward differences \(\left(\Delta f_i\right)\) or backward differences \(\left(\nabla f_i\right)\). We will first look at forward differences and the interpolation polynomial based on forward differences.
The first order forward difference \(\Delta f_i\) is defined as:
\[
\Delta f_i = f_{i+1} - f_i
\]
The second order forward difference \(\Delta^2 f_i\) is defined as:
\[
\Delta^2 f_i = \Delta f_{i+1} - \Delta f_i
\]
The \(k^{\text{th}}\) order forward difference \(\Delta^k f_i\) is defined as:
\[
\Delta^k f_i = \Delta^{k-1} f_{i+1} - \Delta^{k-1} f_i
\]
Then the first divided difference \(f\left[x_0, x_1\right]\):
\[
f\left[x_0, x_1\right] = \frac{f_1 - f_0}{h} = \frac{\Delta f_0}{h}
\]
\[
\therefore \Delta f_0 = h f\left[x_0, x_1\right]
\]
By the definition of second order forward difference \(\Delta^2 f_0\), we get:
\[
\Delta^2 f_0 = \Delta f_1 - \Delta f_0 = h\left\{f\left[x_1, x_2\right] - f\left[x_0, x_1\right]\right\} = h^2\left\{\left(f\left[x_1, x_2\right] - f\left[x_0, x_1\right]\right)/\left(x_2 - x_0\right)\right\} = 2 h^2 f\left[x_0, x_1, x_2\right]
\]
In general:
\[
\Delta^k f_i = k!h^k f\left[x_i, x_{i+1}, x_{i+2}, \ldots, x_{i+k}\right]
\]
The Newton forward difference interpolation polynomial may be written as follows:
\[
P_n(x) = \sum_{k=0}^n \frac{\Delta^k f_0}{k!h^k} \prod_{i=0}^{k-1}\left(x - x_i\right)
\]
We can rewrite the above in a simpler way:
\[
x = x_0 + s h, \quad p_n(s) = P_n(x)
\]
\[
x_k = x_0 + k h
\]
\[
x - x_k = (s - k) h
\]
Then:
\[
p_n(s) = \sum_{k=0}^n \frac{\Delta^k f_0}{k!h^k} \prod_{i=0}^{k-1}(s - i) h = \sum_{k=0}^n \frac{\Delta^k f_0}{k!h^k}[s(s-1) \ldots (s-k+1)] h^k = \sum_{k=0}^n \binom{s}{k} \Delta^k f_0
\]
**** Errors on interpolation
+ When interpolating a given function \(f\) by a polynomial of degree \(n\) at the nodes \(x_0, \ldots, x_n\), we get the error:
\[
f(x) - P_n(x) = f\left[x_0, \ldots, x_n, x\right] \prod_{i=0}^n \left(x - x_i\right)
\]
If \(f\) is \(n+1\) times continuously differentiable, then for each \(x\) in the interval, there exists \(\xi\) in that interval such that:
\[
f(x) - p_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \prod_{i=0}^n \left(x - x_i\right)
\]
*** Richardson extrapolation
Richardson extrapolation is a sequence acceleration method, used to improve the rate of convergence of a sequence. Let \(A(h)\) be an approximation of \(A\) that depends on a positive step size \(h\) with an error formula of the form:
\[
A - A(h) = a_0 h^{k_0} + a_1 h^{k_1} + a_2 h^{k_2} + \cdots
\]
where the \(a_i\) are unknown constants and the \(k_i\) are known constants such that \(h^{k_i} > h^{k_{i+1}}\).
The exact value sought can be given by:
\[
A = A(h) + a_0 h^{k_0} + a_1 h^{k_1} + a_2 h^{k_2} + \cdots = A(h) + a_0 h^{k_0} + \mathcal{O}\left(h^{k_1}\right)
\]
Using the step sizes \(h\) and \(h / t\) for some \(t\), the two formulas for \(A\) are:
\[
A = A(h) + a_0 h^{k_0} + \mathcal{O}\left(h^{k_1}\right)
\]
\[
A = A\left(\frac{h}{t}\right) + a_0 \left(\frac{h}{t}\right)^{k_0} + \mathcal{O}\left(h^{k_1}\right)
\]
Multiplying the second equation by \(t^{k_0}\) and subtracting the first equation gives:
\[
\left(t^{k_0} - 1\right) A = t^{k_0} A\left(\frac{h}{t}\right) - A(h) + \mathcal{O}\left(h^{k_1}\right)
\]
which can be solved for \(A\) to give:
\[
A = \frac{t^{k_0} A\left(\frac{h}{t}\right) - A(h)}{t^{k_0} - 1} + \mathcal{O}\left(h^{k_1}\right)
\]
By this process, we have achieved a better approximation of \(A\) by subtracting the largest term in the error which was \(\mathcal{O}\left(h^{k_0}\right)\). This process can be repeated to remove more error terms to get even better approximations.
A general recurrence relation beginning with \(A_0 = A(h)\) can be defined for the approximations by:
\[
A_{i+1}(h) = \frac{t^{k_i} A_i\left(\frac{h}{t}\right) - A_i(h)}{t^{k_i} - 1}
\]
where \(k_{i+1}\) satisfies:
\[
A = A_{i+1}(h) + \mathcal{O}\left(h^{k_{i+1}}\right)
\]
*** Single-variable integrals
**** Newton-Cotes formulas
***** Rectangular rule
The *Rectangular rule* is a method for numerical integration that approximates the definite integral \( \int_{a}^{b} f(x) \mathrm{d}x \) for a continuous function \( f \) defined for \(a \leq x \leq b\). The rectangular rule uses the Riemann sum.
#+NAME: Riemann sum
#+begin_definition latex
Let \(f(x)\) be a continuous function defined for \(a \leq x \leq b\). The Riemann sum is defined by
\[
R \equiv \sum_{i=1}^N f\left(x_i^*\right) \Delta x, \qquad \Delta x = \frac{b-a}{N}.
\]
Here \(f\left(x_i^{\ast}\right)\) is the value of the function at an arbitrary point, \(x_i^{\ast}\), in the interval \([x_{i-1},\,x_{i}]\), where \(x_i = x_{i-1} + \Delta x\).
#+end_definition
The Riemann sum is an approximation for the [[id:ee101e01-9277-4714-a82f-132d0ba11aa8][definite integral]] of \( f(x) \) over the interval \( [a,\,b] \) which becomes exact in the limit of \( N \to \infty \)
\[
I = \int_a^b f(x) \, dx = \lim_{N \to \infty} \sum_{i=1}^N f\left(x_i^*\right) \Delta x = \lim_{N \to \infty} R.
\]
#+NAME: Left endpoint Riemann sum
#+begin_definition latex
The left endpoint Riemann sum \( R_l \) is defined by the choice \( x^{\ast} = x_{i-1} \) in a Riemann sum
\[
R_l \equiv \sum_{i=1}^N f\left(x_{i-1}\right) \Delta x, \qquad \Delta x = \frac{b-a}{N}.
\]
#+end_definition
#+NAME: Right endpoint Riemann sum
#+begin_definition latex
The right endpoint Riemann sum \( R_r \) is defined by the choice \( x^{\ast} = x_{i} \) in a Riemann sum
\[
R_r \equiv \sum_{i=1}^N f\left(x_{i}\right) \Delta x, \qquad \Delta x = \frac{b-a}{N}.
\]
#+end_definition
#+NAME: midpoint Riemann sum
#+begin_definition latex
The midpoint Riemann sum \( R_m \) is defined by the choice \( x^{\ast} = (x_{i-1} + x_{i})/2 \) in a Riemann sum
\[
R_m \equiv \sum_{i=1}^N f\bigg(\frac{x_{i-1} + x_{i}}{2}\bigg) \Delta x, \qquad \Delta x = \frac{b-a}{N}.
\]
#+end_definition
+ The left endpoint, right endpoint, and midpoint Riemann sum all approximate the definite integral of \( f(x) \) over the interval \( [a,\,b] \)
\begin{align*}
I &= \int_a^b f(x) \, dx = \lim_{N \to \infty} \sum_{i=1}^N f\left(x_{i-1}\right) \Delta x = R_l, \\
I &= \int_a^b f(x) \, dx = \lim_{N \to \infty} \sum_{i=1}^N f\left(x_i\right) \Delta x = R_r, \\
I &= \int_a^b f(x) \, dx = \lim_{N \to \infty} \sum_{i=1}^N f\left(\frac{x_{i-1}+x_i}{2}\right) \Delta x = R_m
\end{align*}
where \(\Delta x = (b-a)/N\). These approximations become exact in the limit of \( N \to \infty \). Clearly then
\[
\lim_{N \to \infty} R_l = \lim_{N \to \infty} R_r = \lim_{N \to \infty} R_m = I.
\]
#+begin_src latex :file ~/.local/images/riemann-left.png :results file graphics
  \begin{tikzpicture}
    % Define function
    \def\func{sqrt(x) + 1}
    % Draw axes
    \draw[->] (-0.5, 0) -- (7.5, 0) node[right] {\(x\)};
    \draw[->] (0, -0.5) -- (0, 4) node[above] {\(f(x)\)};
    % Draw rectangles
    \foreach \i in {1, 2, 3, 4, 5} {
      \draw[fill=green!20] (\i, 0) rectangle (\i+1, {sqrt(\i) + 1});
      \draw[green] (\i, {sqrt(\i) + 1}) -- (\i+1, {sqrt(\i) + 1});
    }
    % Draw the curve
    \draw[thick, domain=0.5:6.5] plot (\x, {sqrt(\x) + 1});
    % Label points a and b
    \node[below] at (1, 0) {\textcolor{red}{\(a\)}};
    \node[below] at (6, 0) {\textcolor{red}{\(b\)}};
  \end{tikzpicture}
#+end_src
#+RESULTS:
[[file:~/.local/images/riemann-left.png]]
#+begin_src latex :file ~/.local/images/riemann-right.png :results file graphics
\begin{tikzpicture}
    % Define function
    \def\func{sqrt(x) + 1}
    % Draw axes
    \draw[->] (-0.5, 0) -- (7.5, 0) node[right] {\(x\)};
    \draw[->] (0, -0.5) -- (0, 4) node[above] {\(f(x)\)};
    % Draw rectangles
    \foreach \i in {2, 3, 4, 5, 6} {
        \draw[fill=red!20] (\i, 0) rectangle (\i-1, {sqrt(\i) + 1});
        \draw[red] (\i, {sqrt(\i) + 1}) -- (\i-1, {sqrt(\i) + 1});
    }
    % Draw the curve
    \draw[thick, domain=0.5:6.5] plot (\x, {sqrt(\x) + 1});
    % Label points a and b
    \node[below] at (1, 0) {\textcolor{red}{\(a\)}};
    \node[below] at (6, 0) {\textcolor{red}{\(b\)}};
\end{tikzpicture}
#+end_src
#+RESULTS:
[[file:~/.local/images/riemann-right.png]]
#+begin_src latex :file ~/.local/images/riemann-mid.png :results file graphics
\begin{tikzpicture}
    % Define function
    \def\func{sqrt(x) + 1}
    % Draw axes
    \draw[->] (-0.5, 0) -- (7.5, 0) node[right] {\(x\)};
    \draw[->] (0, -0.5) -- (0, 4) node[above] {\(f(x)\)};
    % Draw rectangles
    \foreach \i in {1, 2, 3, 4, 5} {
        \pgfmathsetmacro{\midpoint}{\i + 0.5}
        \draw[fill=blue!20] (\i, 0) rectangle (\i+1, {sqrt(\midpoint) + 1});
        \draw[blue] (\i, {sqrt(\midpoint) + 1}) -- (\i+1, {sqrt(\midpoint) + 1});
    }
    % Draw the curve
    \draw[thick, domain=0.5:6.5] plot (\x, {sqrt(\x) + 1});
    % Label points a and b
    \node[below] at (1, 0) {\textcolor{red}{\(a\)}};
    \node[below] at (6, 0) {\textcolor{red}{\(b\)}};
\end{tikzpicture}
#+end_src
#+RESULTS:
[[file:~/.local/images/riemann-mid.png]]
***** Trapezoidal rule
The *trapezoidal rule* is a method for [[id:916e7c98-ca7a-4286-b0ee-c22370f18eb9][numerical integration]].
#+NAME: Trapezoidal rule
#+begin_definition latex
Let \(f(x)\) be a continuous function defined for \(a \leq x \leq b\). The trapezoidal rule approximates the definite integral \( I(a,\,b)=\int_{a}^{b} f(x) \mathrm{d} x \). Let \(h=(b-a) / N\) for \( 0 < N < \infty \). An approximation for \(I(a,\,b)\) is given by the trapezoidal rule
\begin{equation*}
\boxed{
I(a, b) \simeq h \bigg[\frac{1}{2} f(a)+\frac{1}{2} f(b)+\sum_{k=1}^{N-1} f(a+k h)\bigg].
}
\end{equation*}
#+end_definition
+ For fixed \( N \), the Trapezoidal rule approximates the definite integral \( I(a,\,b) = \int_{a}^{b} f(x) \mathrm{d}x \) much better than the [[id:c21d20bd-d964-4ad4-9caa-dd4a8e0ec547][Riemann sum]] as illustrated in the two figures below.
#+HTML_ATTR: :width 200px
[[file:~/.local/images/comphy-5-1a.png]]
#+HTML_ATTR: :width 200px
[[file:~/.local/images/comphy-5-1b.png]]
+ The approximation constructed using the extended Trapezoidal rule becomes exact in the limit \( N \to \infty \)
\[
I(a, b) = \lim_{N \to \infty} \sum_{k=1}^{N} A_{k} = \lim_{h \to 0} \left[\frac{1}{2} f(a)+\frac{1}{2} f(b)+\sum_{k=1}^{N-1} f(a+k h)\right] \cdot h.
\]
#+HTML_ATTR: :width 200px
[[file:~/.local/images/comphy-5-1c.png]]
****** Derivation
Let \(f(x)\) be a continuous function defined for \(a \leq x \leq b\). The trapezoidal rule approximates the definite integral \( I(a,\,b)=\int_{a}^{b} f(x) \mathrm{d} x \). Let \(h=(b-a) / N\) for \( 0 < N < \infty \). An approximation for \(I(a + (k-1)\,h,\, a + k \cdot h)\) is given by the area of the trapezium formed by the points \( x_{k-1} \), \( x_{k} \), \( f(a + (k-1) \cdot h) \), and \( f(a + k \cdot h) \) in the \( x \)-\( y \) plane
\[
A_{k}=\frac{1}{2} h[f(a+(k-1) h)+f(a+k h)].
\]
The integral \( I(a,\,b) \) can be expressed as a sum over \( k \) of \( A_{k} \)
\begin{align*}
I(a, b) \simeq \sum_{k=1}^{N} A_{k} & =\frac{1}{2} h \sum_{k=1}^{N}[f(a+(k-1) h)+f(a+k h)] \\
& =h\left[\frac{1}{2} f(a)+f(a+h)+f(a+2 h)+\ldots+\frac{1}{2} f(b)\right] \\
& =h\left[\frac{1}{2} f(a)+\frac{1}{2} f(b)+\sum_{k=1}^{N-1} f(a+k h)\right].
\end{align*}
This is the Trapezoidal rule.
****** Example
*Use the trapezoidal rule to approximate the integral of* \(x^{4}-2 x+1\) *from* \(x=0\) to \(x=2\)
\[
I_{N} (0,\,2) = \int_{0}^{2} \mathrm{d}x\, (x^4 - 2x + 1).
\]
*Give numerical values of* \( I_N (0,\,2) \) *and the approximation error* \( (I_{N} - I_{\infty}) / I_{\infty} \) *for* \( N = 10,\,100,\,1000 \).
This definite integral can be evaluated exactly
\begin{align*}
I_{\infty} (0,\,2) = \int_{0}^{2}\left(x^{4}-2 x+1\right) \mathrm{d} x=\left[\frac{1}{5} x^{5}-x^{2}+x\right]_{0}^{2}=4.4.
\end{align*}
Now let us approximate this integral using the trapezoidal rule
#+begin_src python :results output
  import numpy as np
  def f(x):
      return x**4 - 2*x + 1
  def trapezoidal_rule(f, a, b, N=10):
    h = (b-a) / N
    s = 0.5*f(a) + 0.5*f(b)
    for k in range(1, N):
      s += f(a+k*h)
    return s*h
  N_values = [10, 100, 1000]
  a = 0.0
  b = 2.0
  I_exact = 4.4
  for N in N_values:
    I = trapezoidal_rule(f, a, b, N)
    print(f"N = {N}, I_N = {np.around(I, 4)}, approximation error={np.around(((I - I_exact)/I_exact)*100, 4)}%")
#+end_src
#+RESULTS:
: N = 10, I_N = 4.5066, approximation error=2.4218%
: N = 100, I_N = 4.4011, approximation error=0.0242%
: N = 1000, I_N = 4.4, approximation error=0.0002%
****** Euler-Maclaurin formula
The [[id:fdd9852f-2b53-4496-9156-8e30a5312527][approximation error]] of the trapezoidal rule is given by its [[id:5fafbff8-1129-4da9-9200-63a0075344e8][EulerMaclaurin formula]].
#+NAME: Euler-Maclaurin formula for the trapezoidal rule
#+begin_theorem latex
Consider the definite integral \(\int_{a}^{b} f(x) \mathrm{d} x\), where \( f \), \( \mathrm{D}_x f \), and \( \mathrm{D}_x^2 f \) is a continuous function for \( a \leq x \leq b \). The first term in the Euler-Maclaurin formula for the trapezoidal approximation
\[
\int_{a}^{b} f(x) \mathrm{d} x \approx h \left[\frac{1}{2} f(a)+\frac{1}{2} f(b)+\sum_{k=1}^{N-1} f(a+k h)\right]
\]
is given by
\begin{equation*}
\boxed{
\epsilon=\frac{h^{2}}{12} [\mathrm{D}_x f(a)-\mathrm{D}_x f(b)].
}
\end{equation*}
#+end_theorem
#+NAME: Euler-Maclaurin formula for the trapezoidal rule
#+begin_proof latex
Consider the definite integral \(\int_{a}^{b} f(x) \mathrm{d} x\), where \( f \), \( \mathrm{D}_x f \), and \( \mathrm{D}_x^2 f \) is a continuous function for \( a \leq x \leq b \). Let \(x_{k}=a+k h\). Consider the interval \( [x_{k-1},\,x_{k}] \). Taylor expansion of \(f(x)\) about \(x = x_{k-1}\) is
\begin{align*}
f(x)=f(x_{k-1})+(x-x_{k-1}) \mathrm{D}_{x} f(x_{k-1})+\frac{1}{2}(x-x_{k-1})^{2} \mathrm{D}_{x}^{2} f(x_{k-1})+\ldots
\end{align*}
Integrating this expression from \(x_{k-1}\) to \(x_{k}\) yields
\begin{align*}
\int_{x_{k-1}}^{x_{k}} f(x) \mathrm{d} x=f(x_{k-1}) \int_{x_{k-1}}^{x_{k}} \mathrm{~d} x & +\mathrm{D}_{x} f(x_{k-1}) \int_{x_{k-1}}^{x_{k}}(x-x_{k-1}) \mathrm{d} x \\
& +\frac{1}{2} \mathrm{D}_{x}^{2} f(x_{k-1}) \int_{x_{k-1}}^{x_{k}}(x-x_{k-1})^{2} \mathrm{~d} x+\ldots
\end{align*}
With \(u=x-x_{k-1}\) we get
\[
\int_{x_{k-1}}^{x_{k}} f(x) \mathrm{d} x = f(x_{k-1}) \int_{0}^{h} \mathrm{~d} u+\mathrm{D}_{x} f(x_{k-1}) \int_{0}^{h} u \mathrm{~d} u+\frac{1}{2} \mathrm{D}_{x}^{2} f(x_{k-1}) \int_{0}^{h} u^{2} \mathrm{~d} u+\ldots
\]
which yields
\[
\int_{x_{k-1}}^{x_{k}} f(x) \mathrm{d} x = h f(x_{k-1})+\frac{1}{2} h^{2} \mathrm{D}_{x} f(x_{k-1})+\frac{1}{6} h^{3} \mathrm{D}_{x}^{2} f(x_{k-1})+\mathrm{O}(h^{4}). \tag{1}
\]
Taylor expansion of \( f(x) \) about \( x = x_{k} \) will yield a similar result
\begin{align*}
\int_{x_{k-1}}^{x_{k}} f(x) \mathrm{d} x=h f(x_{k})-\frac{1}{2} h^{2} \mathrm{D}_{x} f(x_{k})+\frac{1}{6} h^{3} \mathrm{D}_{x}^{2} f(x_{k})-\mathrm{O}(h^{4}). \tag{2}
\end{align*}
Averaging both sides of (1) and (2) yields
\begin{align*}
\int_{x_{k-1}}^{x_{k}} f(x) \mathrm{d} x=\frac{h}{2} [f(x_{k-1})+ f(x_{k})]+\frac{h^{2}}{4} [\mathrm{D}_{x} f(x_{k-1})-\mathrm{D}_{x} f(x_{k})] + \frac{h^{3}}{12} [\mathrm{D}_{x}^{2} f(x_{k-1})+\mathrm{D}_{x}^{2} f(x_{k})]+\mathrm{O}(h^{4}).
\end{align*}
\(\int_{a}^{b} f(x) \mathrm{d} x\) can be expressed as a sum over \( k \)
\begin{align*}
&\int_{a}^{b} f(x) \mathrm{d} x = \sum_{k=1}^{N} \int_{x_{k-1}}^{x_{k}} f(x) \mathrm{d} x \\
&\quad = \frac{h}{2} \sum_{k=1}^{N}[f(x_{k-1})+f(x_{k})]+\frac{h^{2}}{4} [\mathrm{D}_{x} f(a)-\mathrm{D}_{x} f(b)] + \frac{h^{3}}{12} \sum_{k=1}^{N}[\mathrm{D}_{x}^{2} f(x_{k-1})+\mathrm{D}_{x}^{2} f(x_{k})]+\mathrm{O}(h^{4}). \tag{3}
\end{align*}
Consider now a trapezoidal approximation for the integral \( \int_{a}^{b} \mathrm{D}_x^{2} f(x) \mathrm{d} x \):
\[
\int_{a}^{b} \mathrm{D}_x^{2} f(x) \mathrm{d} x=\frac{1}{2} h \sum_{k=1}^{N}[\mathrm{D}_x^{2} f(x_{k-1})+\mathrm{D}_x^{2} f(x_{k})]+\mathrm{O}(h^{2}).
\]
Multiplying by \(\frac{1}{6} h^{2}\) and rearranging
\[
\frac{h^{3}}{12} \sum_{k=1}^{N}[\mathrm{D}_x^{2} f(x_{k-1})+\mathrm{D}_x^{2} f(x_{k})] & =\frac{h^{2}}{6} \int_{a}^{b} \mathrm{D}_x^{2} f(x) \mathrm{d} x+\mathrm{O}(h^{4}) =\frac{h^{2}}{6} [\mathrm{D}_x f(b)-\mathrm{D}_x f(a)]+\mathrm{O}(h^{4})
\]
since the integral of \(\mathrm{D}_x^{2} f(x)\) is just \(\mathrm{D}_x f(x)\). Substituting this result into (3) and simplifying we obtain
\begin{align*}
&\int_{a}^{b} f(x) \mathrm{d} x = \sum_{k=1}^{N} \int_{x_{k-1}}^{x_{k}} f(x) \mathrm{d} x \\
&\quad = \frac{h}{2} \sum_{k=1}^{N}[f(x_{k-1})+f(x_{k})]+\frac{h^{2}}{12} [\mathrm{D}_{x} f(a)-\mathrm{D}_{x} f(b)] + \mathrm{O}(h^{4}). \tag{4}
\end{align*}
The first term on the right-hand side of (4) is precisely equal to the extended trapezoidal rule. When we use the trapezoidal rule, we evaluate only this sum and discard all the terms following. The discarded terms - the rest of the series - gives the Euler-Maclaurin formula for the trapezoidal rule. The first term in the formula is
\begin{equation*}
\epsilon=\frac{h^{2}}{12} [\mathrm{D}_x f(a)-\mathrm{D}_x f(b)].
\end{equation*}
#+end_proof
***** Simpson's rules
****** Simpson's 1/3 rule
The *Simpson's rule* is a method for [[id:916e7c98-ca7a-4286-b0ee-c22370f18eb9][numerical integration]].
#+NAME: Simpson's rule
#+begin_definition latex
Let \(f(x)\) be a continuous function defined for \(a \leq x \leq b\). The Simpson's rule approximates the definite integral \( I(a,\,b)=\int_{a}^{b} f(x) \mathrm{d} x \). Let \(h=(b-a) / N\) for \( 0 < N < \infty \). Without loss of generality assume \( a \leq - h \). An approximation for \(I(a,\,b)\) is given by the Simpson's rule
\begin{align*}
I(a, b) &\simeq \frac{1}{3} h\bigg[f(a)+f(b)+4 \sum_{\substack{k \text { odd } \\
1 \ldots N-1}} f(a+k h)+2 \sum_{\substack{k \text { even } \\
2 \ldots N-2}} f(a+k h)\bigg]
\end{align*}
Note that the total number of slices must be even for this to work.
#+end_definition
+ For fixed \( N \), the Simpson's rule approximates the definite integral \( I(a,\,b) = \int_{a}^{b} f(x) \mathrm{d}x \) much better than the [[id:2bc5a9e9-a0cc-4b7f-b9b8-6ec19d84254a][Trapezoidal rule]].
+ The approximation constructed using the Simpson's rule becomes exact in the limit \( h \to 0 \)
\[
I(a, b) = \lim_{h \to 0} \bigg(\frac{1}{3} h\bigg[f(a)+f(b)+4 \sum_{\substack{k \text { odd } \\
1 \ldots N-1}} f(a+k h)+2 \sum_{\substack{k \text { even } \\
2 \ldots N-2}} f(a+k h)\bigg] \bigg).
\]
******* Derivation
Let \(f(x)\) be a continuous function defined for \(a \leq x \leq b\). The Simpson's rule approximates the definite integral \( I(a,\,b)=\int_{a}^{b} f(x) \mathrm{d} x \). Let \(h=(b-a) / N\) for \( 0 < N < \infty \). Without loss of generality assume \( a \leq - h \). To evaluate \(I(-h,\,h)\), suppose we fit a quadratic \(A x^{2}+B x+C\) through these points. 
#+HTML_ATTR: :width 200px
[[file:~/.local/images/comphy-5-2.png]]
By definition
\begin{align*}
f(-h)=A h^{2}-B h+C, \quad f(0)=C, \quad f(h)=A h^{2}+B h+C
\end{align*}
Solving these equations simultaneously for \(A\), \(B\), and \(C\) yields
\begin{align*}
A=\frac{1}{h^{2}}\bigg[\frac{1}{2} f(-h)-f(0)+\frac{1}{2} f(h)\bigg], \quad
B=\frac{1}{2 h}[f(h)-f(-h)], \quad
C=f(0)
\end{align*}
and the area under the curve of \(f(x)\) from \(-h\) to \(+h\) is given approximately by the area under the quadratic:
\begin{align*}
\int_{-h}^{h}\left(A x^{2}+B x+C\right) \mathrm{d} x=\frac{2}{3} A h^{3}+2 C h=\frac{1}{3} h[f(-h)+4 f(0)+f(h)].
\end{align*}
If, as before, we are integrating from \(x=a\) to \(x=b\) in slices of width \(h\) then the three points bounding the first pair of slices fall at \(x=a, a+h\) and \(a+2 h\), those bounding the second pair at \(a+2 h\), \(a+3 h, a+4 h\), and so forth. Then the approximate value of the entire integral is given by
\begin{align*}
I(a, b) \simeq \frac{1}{3} h & {[f(a)+4 f(a+h)+f(a+2 h)] } \\
& +\frac{1}{3} h[f(a+2 h)+4 f(a+3 h)+f(a+4 h)]+\ldots \\
& +\frac{1}{3} h[f(a+(N-2) h)+4 f(a+(N-1) h)+f(b)]
\end{align*}
Note that the total number of slices must be even for this to work. Collecting terms together, we now have
\begin{align*}
I(a, b) & \simeq \frac{1}{3} h[f(a)+4 f(a+h)+2 f(a+2 h)+4 f(a+3 h)+\ldots+f(b)] \\
& =\frac{1}{3} h\bigg[f(a)+f(b)+4 \sum_{\substack{k \text { odd } \\
1 \ldots N-1}} f(a+k h)+2 \sum_{\substack{k \text { even } \\
2 \ldots N-2}} f(a+k h)\bigg].
\end{align*}
This is the Simpson's rule. An alternative expression of the Simpson's rule is
\[
I(a, b) \simeq \frac{1}{3} h\bigg[f(a)+f(b)+4 \sum_{k=1}^{N / 2} f(a+(2 k-1) h)+2 \sum_{k=1}^{N / 2-1} f(a+2 k h)\bigg]
\]
******* Example
*Use the Simpson's rule to approximate the integral of* \(x^{4}-2 x+1\) *from* \(x=0\) to \(x=2\).
\[
I_{N} (0,\,2) = \int_{0}^{2} \mathrm{d}x\, (x^4 - 2x + 1).
\]
*Give numerical values of* \( I_N (0,\,2) \) *and the approximation error* \( (I_{N} - I_{\infty}) / I_{\infty} \) *for* \( N = 10,\,100,\,1000 \) *and compare with approximation error for the trapezoidal rule.*
This definite integral can be evaluated exactly
\begin{align*}
I_{\infty} (0,\,2) = \int_{0}^{2}\left(x^{4}-2 x+1\right) \mathrm{d} x=\left[\frac{1}{5} x^{5}-x^{2}+x\right]_{0}^{2}=4.4.
\end{align*}
Now let us approximate this integral using the trapezoidal rule and Simpson's rule
#+begin_src python :results output
  import numpy as np
  def f(x):
      return x**4 - 2*x + 1
  def trapezoidal_rule(f, a, b, N=10):
      h = (b - a) / N
      s = 0.5 * f(a) + 0.5 * f(b)
      for k in range(1, N):
          s += f(a + k * h)
      return s * h
  def simpsons_rule(f, a, b, N=10):
      if N % 2 != 0:
          raise ValueError("N must be an even integer.")
      h = (b - a) / N
      s = f(a) + f(b)
      for k in range(1, N, 2):
          s += 4 * f(a + k * h)
      for k in range(2, N - 1, 2):
          s += 2 * f(a + k * h)
      return (h / 3) * s
  def main():
      N_values = [10, 100, 1000]
      a = 0.0
      b = 2.0
      I_exact = 4.4
      print("Trapezoidal rule")
      for N in N_values:
          I = trapezoidal_rule(f, a, b, N)
          approximation_error = ((I - I_exact) / I_exact) * 100
          print(f"N = {N}, I_N = {np.around(I, 4)}, approximation error = {np.around(approximation_error, 4)}%")
      print("Simpson's rule")
      for N in N_values:
          I = simpsons_rule(f, a, b, N)
          approximation_error = ((I - I_exact) / I_exact) * 100
          print(f"N = {N}, I_N = {np.around(I, 4)}, approximation error = {np.around(approximation_error, 4)}%")
  if __name__ == "__main__":
      main()
#+end_src
#+RESULTS:
: Trapezoidal rule
: N = 10, I_N = 4.5066, approximation error = 2.4218%
: N = 100, I_N = 4.4011, approximation error = 0.0242%
: N = 1000, I_N = 4.4, approximation error = 0.0002%
: Simpson's rule
: N = 10, I_N = 4.4004, approximation error = 0.0097%
: N = 100, I_N = 4.4, approximation error = 0.0%
: N = 1000, I_N = 4.4, approximation error = 0.0%
******* Euler-Maclaurin formula
The [[id:fdd9852f-2b53-4496-9156-8e30a5312527][approximation error]] of the Simpson's rule is given by its [[id:5fafbff8-1129-4da9-9200-63a0075344e8][EulerMaclaurin formula]].
#+NAME: Euler-Maclaurin formula for the Simpson's rule
#+begin_theorem latex
Consider the definite integral \(\int_{a}^{b} f(x) \mathrm{d} x\), where \( f \), \( \mathrm{D}_x f \), \( \mathrm{D}_x^2 f \), and \( \mathrm{D}_x^3 f \) is a continuous function for \( a \leq x \leq b \). The first term in the Euler-Maclaurin formula for the Simpson's approximation
\[
\int_{a}^{b} f(x) \mathrm{d} x \approx \frac{1}{3} h\bigg[f(a)+f(b)+4 \sum_{k=1}^{N / 2} f(a+(2 k-1) h)+2 \sum_{k=1}^{N / 2-1} f(a+2 k h)\bigg]
\]
is given by
\begin{equation*}
\boxed{
\epsilon=\frac{h^{4}}{90} [\mathrm{D}_x^3 f (a)- \mathrm{D}_x^3 f(b)].
}
\end{equation*}
#+end_theorem
****** Simpson's 3/8 rule
The *Simpson's 3/8 rule* is a method for [[id:916e7c98-ca7a-4286-b0ee-c22370f18eb9][numerical integration]].
#+NAME: Simpson's 3/8 rule
#+begin_definition latex
Let \(f(x)\) be a continuous function defined for \(a \leq x \leq b\). The Simpson's 3/8 rule approximates the definite integral \( I(a,\,b)=\int_{a}^{b} f(x) \mathrm{d} x \). Let \(h=(b-a) / N\) for \( 0 < N < \infty \). An approximation for \(I(a,\,b)\) is given by the Simpson's 3/8 rule
\begin{align*}
I(a, b) &\simeq \frac{3h}{8} \bigg[f(x_0)+3 \sum_{k \mod 3 \neq 0}^{N-1} f(x_k) + 2 \sum_{k=1}^{N / 3-1} f(x_{3k})+f(x_n)\bigg].
\end{align*}
Note that the total number of slices must be a multiple of \( 3 \).
#+end_definition
******* Derivation
Let \(f(x)\) be a continuous function defined for \(a \leq x \leq b\). The Simpson's 3/8 rule approximates the definite integral \( I(a,\,b)=\int_{a}^{b} f(x) \mathrm{d} x \). Let \(h=(b-a) / N\) for \( 0 < N < \infty \). Without loss of generality assume \( a \leq - h \). To evaluate \(I(-3h/2,\,3h/2)\), suppose we fit a cubic \(A x^{3}+B x^2 + C x + D\) through the four points \( - 3h/2 \), \( - h/2 \), \( h/2 \), and \( 3h/2 \).
By definition
\begin{align*}
& f(-3h/2) = A(-3h/2)^3 + B(-3h/2)^2 + C(-3h/2) + D, \\
& f(-h/2) = A(-h/2)^3 + B(-h/2)^2 + C(-h/2) + D, \\
& f(h/2) = A(h/2)^3 + B(h/2)^2 + C(h/2) + D, \\
& f(3h/2) = A(3h/2)^3 + B(3h/2)^2 + C(3h/2) + D.
\end{align*}
#+begin_src python :results output
from sympy import symbols, Eq, solve, latex, init_printing
# Initialize pretty printing for LaTeX output
init_printing(use_latex=True)
# Define symbols for coefficients and points
A, B, C, D = symbols('A B C D')
h, f_m3h2, f_3h2, f_mh2, f_h2 = symbols('h f(-3h/2) f(3h/2) f(-h/2) f(h/2)')
# Equations based on the cubic polynomial values at points 3h/2 and h/2
eq1 = Eq(A*(-3*h/2)**3 + B*(-3*h/2)**2 + C*(-3*h/2) + D, f_m3h2)
eq2 = Eq(A*(-h/2)**3 + B*(-h/2)**2 + C*(-h/2) + D, f_mh2)
eq3 = Eq(A*(h/2)**3 + B*(h/2)**2 + C*(h/2) + D, f_h2)
eq4 = Eq(A*(3*h/2)**3 + B*(3*h/2)**2 + C*(3*h/2) + D, f_3h2)
# Solve the system of equations for A, B, C, and D
solutions_abcd = solve((eq1, eq2, eq3, eq4), (A, B, C, D))
# Print each solution in LaTeX format
for var in solutions_abcd:
    print(latex(Eq(var, solutions_abcd[var])))
#+end_src
#+RESULTS:
: A = \frac{- f(-3h/2) + 3 f(-h/2) + f(3h/2) - 3 f(h/2)}{6 h^{3}}
: B = \frac{f(-3h/2) - f(-h/2) + f(3h/2) - f(h/2)}{4 h^{2}}
: C = \frac{f(-3h/2) - 27 f(-h/2) - f(3h/2) + 27 f(h/2)}{24 h}
: D = - \frac{f(-3h/2)}{16} + \frac{9 f(-h/2)}{16} - \frac{f(3h/2)}{16} + \frac{9 f(h/2)}{16}
Solving these equations simultaneously for \(A\), \(B\), \( C \) and \(D\) yields
\begin{align*}
&A=\frac{f(3 h / 2)-3 f(h / 2)-f(-3 h / 2)+3 f(-h / 2)}{6 h^3} \\
&B=\frac{f(3 h / 2)-f(h / 2)+f(-3 h / 2)-f(-h / 2)}{4 h^2} \\
&C=\frac{-f(3 h / 2)+27 f(h / 2)+f(-3 h / 2)-27 f(-h / 2)}{24 h} \\
&D=\frac{-f(3 h / 2)+9 f(h / 2)-f(-3 h / 2)+9 f(-h / 2)}{16}
\end{align*}
and the area under the curve of \(f(x)\) from \(-3h/2\) to \(3h/2\) is given approximately by the area under the cubic:
\[
\int_{-3h/2}^{3h/2} (Ax^3 + Bx^2 + Cx + D)\, \mathrm{d} x = \frac{3}{8} h[f(-3h/2)+3 f(-h/2)+3 f(h/2)+f(3h/2)].
\]
If we are integrating from \(x=a\) to \(x=b\) in slices of width \(h\) then the four points bounding the first pair of slices fall at \(x=a\), \(x = a+h\), \( x = a + 2h \), and \(x = a+3 h\), those bounding the second pair at \(x = a+3 h\), \(x = a+4 h\), \(x=a+5 h\), and \( x = a + 6h \) and so forth. Then the approximate value of the entire integral is given by
\begin{align*}
\int_a^b f(x) \, \mathrm{d} x &\approx \frac{3}{8} h \sum_{i=1}^{n / 3}\left[f\left(x_{3 i-3}\right)+3 f\left(x_{3 i-2}\right)+3 f\left(x_{3 i-1}\right)+f\left(x_{3 i}\right)\right] \\
& =\frac{3}{8} h\left[f\left(x_0\right)+3 f\left(x_1\right)+3 f\left(x_2\right)+2 f\left(x_3\right)+3 f\left(x_4\right)+3 f\left(x_5\right)+2 f\left(x_6\right)+\cdots+2 f\left(x_{n-3}\right)+3 f\left(x_{n-2}\right)+3 f\left(x_{n-1}\right)+f\left(x_n\right)\right] \\
& =\frac{3}{8} h\left\lceil f\left(x_0\right)+3 \sum^{n-1} f\left(x_i\right)+2 \sum^{n / 3-1} f\left(x_{3 i}\right)+f\left(x_n\right)\right\rceil.
\end{align*}
\begin{align*}
I(a, b) \simeq \frac{3}{8} h & {[f(a)+3 f(a+h) + 3 f(a + 2h) + f(a+3h)] } \\
& +\frac{3}{8} h[f(a+ 3h)+ 3 f(a+4h) + 3 f(a + 5h) + f(a+6h)]+\ldots \\
& +\frac{3}{8} h[f(a+(N-3) h) +3 f(a+(N-2) h) + 3 f(a+(N-1) h) + f(b)]
\end{align*}
Note that the total number of slices must be a multiple of \( 3 \) for this to work. Collecting terms together, we now have
\begin{align*}
I(a, b) & \simeq \frac{3}{8} h[f(a)+3 f(a+h)+3 f(a+2 h)+ 2f(a+3 h)+\ldots+f(b)] \\
& =\frac{3}{8} h\bigg[f(a)+f(b) + 3 \sum_{k \mod 3 = 2}^{N-1} f(a+k h) + 3 \sum_{k \mod 3 = 1}^{N-2} f(a+k h) + 2 \sum_{k \mod 3 = 0}^{N-3} f(a+k h) \bigg].
\end{align*}
This is the Simpson's 3/8 rule.
***** Romberg's method
- Romberg's method is used to estimate the definite integral:
\[
I = \int_a^b f(x) \, dx
\]
by applying Richardson extrapolation repeatedly on the trapezium rule.
- The estimates generate a triangular array.
- Romberg's method evaluates the integrand at equally spaced points.
As already discussed in previous lectures, the trapezoidal rule is:
\[
I_n^{(0)} = h\left[\frac{1}{2} f_0 + f_1 + \ldots + f_{n-1} + \frac{1}{2} f_n\right]
\]
where \(h = \frac{b - a}{n}\), \(x_i = x_0 + i h\), \(x_0 = a\), \(x_n = b\).
The error for this rule \(\left(\mathcal{O}\left(h^2\right)\right)\) only has even powers of \(h\):
\[
I = I_n^{(0)} + A h^2 + B h^4 + C h^6 + \ldots
\]
where \(A\), \(B\), \(C\) are related to derivatives of \(f(x)\) at the endpoints and numerical weights. The exact expressions are called the Euler-Maclaurin formula.
To obtain a more accurate estimate for \(I\), we will eliminate the leading contribution to the error term of order \(h^2\), by taking \(n\) to be even and determining the trapezoidal rule for \(\frac{n}{2}\) intervals as well as for \(n\) intervals.
Since the width of one interval is now \(2h\), we have:
\[
I_{\frac{n}{2}}^{(0)} = 2h\left[\frac{1}{2} f_0 + f_1 + \ldots + f_{n-1} + \frac{1}{2} f_n\right]
\]
\[
I = I_{\frac{n}{2}}^{(0)} + A(2h)^2 + B(2h)^4 + C(2h)^6 + \ldots
\]
Combining and eliminating the leading \(h^2\) term:
\[
I = \frac{4 I_n^{(0)} - I_{\frac{n}{2}}^{(0)}}{3} - 4 B h^4 - 20 C h^6 + \ldots
\]
As a result, the next level of approximation becomes:
\[
I_n^{(1)} = \frac{4 I_n^{(0)} - I_{\frac{n}{2}}^{(0)}}{3}
\]
The integral \(I\) can be written as:
\[
I = I_n^{(1)} + B^{\prime} h^4 + C^{\prime} h^6 + \ldots
\]
with \(B^{\prime} = -4 B\) and \(C^{\prime} = -20 C\).
In terms of the weighted sum, this expression reduces to:
\[
I_n^{(1)} = \frac{h}{3}\left[f_0 + 4 f_1 + 2 f_2 + \ldots + 2 f_{n-1} + f_n\right]
\]
which is Simpson's rule!
One can keep repeating this to get the next approximation to \(I\). Formulae differ from the Newton-Cotes. In general:
\[
I_n^{(k)} = \frac{4^k I_n^{(k-1)} - I_{\frac{n}{2}}^{(k-1)}}{4^k - 1}
\]
for \(k=1, 2, 3, \ldots\) which will have an error \(\mathcal{O}\left(h^{2k+2}\right)\).
As a result, better approximations can be found by using the table:
\begin{tabular}{|cc|ccccc|}
\hline
\(n\) & \(k \rightarrow\) & 0 & 1 & 2 & 3 & \(\cdots\) \\
\(\downarrow\) & & & & & & \\
\hline
\hline
1 & & \(I_1^{(0)}\) & & & & \\
2 & & \(I_2^{(0)}\) & \(I_2^{(1)}\) & & & \\
4 & & \(I_4^{(0)}\) & \(I_4^{(1)}\) & \(I_4^{(2)}\) & & \\
8 & & \(I_8^{(0)}\) & \(I_8^{(1)}\) & \(I_8^{(2)}\) & \(I_8^{(3)}\) & \\
\(\vdots\) & & \(\vdots\) & \(\vdots\) & \(\vdots\) & \(\vdots\) & \(\ddots\) \\
\hline
\end{tabular}
\begin{tabular}{llllllll} 
\(n, k\) & 0 & 1 & 2 & 3 & 4 & 5 & \\
1 & 0.62500000000 & & & & & \\
2 & 0.53472222222 & 0.50462962963 & & & & \\
4 & 0.50899376417 & 0.50041761149 & 0.50013681028 & & & \\
8 & 0.50227085033 & 0.50002987904 & 0.50000403021 & 0.50000192259 & & \\
16 & 0.50056917013 & 0.50000194339 & 0.50000008102 & 0.50000001833 & 0.50000001086 & \\
32 & 0.50014238459 & 0.50000012275 & 0.50000000137 & 0.50000000010 & 0.50000000003 & 0.50000000002
\end{tabular}
To reach close to machine accuracy with double precision, Romberg integration needs 64 intervals, while Simpson's rule would need about 1900 intervals, and the trapezium rule would need no less than \(3.8 \times 10^6\) intervals.
***** Higher order integration
Quadrature is a weighted sum of a finite number of sample values of the integrand:
\[
\int_a^b f(x) \, dx = \sum_{i=1}^n f\left(x_i\right) w_i
\]
| Name      | Degree | Weights                                      |
|-----------+--------+----------------------------------------------|
| Trapezoid |      1 | \((h/2, h/2)\)                               |
| Simpson's |      2 | \((h/3, 4h/3, h/3)\)                         |
| \(3/8\)   |      3 | \((3h/8, 9h/8, 9h/8, 3h/8)\)                 |
| Milne     |      4 | \((14h/45, 64h/45, 24h/45, 64h/45, 14h/45)\) |
**** Gaussian Quadrature
- Newton-Cotes Formulae
- Use evenly-spaced functional values.
- Did not use the flexibility we have to select the quadrature points.
- In fact, a quadrature has several degrees of freedom.
\[
I[f] = \sum_{i=1}^m c_i f\left(x_i\right)
\]
A formula with \(m\) function evaluations requires \(2m\) numbers to be specified, \(c_i\) and \(x_i\).
- Select both these weights and locations so that a higher order polynomial can be integrated.
- Price: functional values must now be evaluated at non-uniformly distributed points to achieve higher accuracy.
- Weights are no longer simple numbers.
- Usually derived for an interval such as \([-1, 1]\).
- Other intervals \([a, b]\) determined by mapping to \([-1, 1]\).
In general, in Gaussian quadrature, the points are placed non-uniformly. More points are closer to the edges than in the middle.
For example:
\[
\int_1^2 \frac{1}{x^2} \, dx = 0.5
\]
| \(n\) |           Integral |
|-------+--------------------|
|     1 | 0.4444444444444447 |
|     2 | 0.4970414201183431 |
|     3 | 0.4998740236835472 |
|     4 | 0.4999951475626201 |
|     5 | 0.4999998234768075 |
|     6 | 0.4999999938120432 |
|     7 | 0.4999999997886506 |
|     8 | 0.4999999999929189 |
|     9 | 0.4999999999997659 |
|    10 | 0.4999999999999911 |
To reach close to machine accuracy with double precision, Romberg integration needs 64 intervals, while Simpson's rule would need about 1900 intervals, and the trapezium rule would need no less than \(3.8 \times 10^6\). Gaussian quadrature needs only 10 points.
***** Finding quadrature nodes and weights
+ One way is through the theory of orthogonal polynomials.
+ Here we will do it via brute force.
+ Set up equations by requiring that the \(2m\) points guarantee that a polynomial of degree \(2m-1\) is integrated exactly.
+ The general process is non-linear:
  + Involves a polynomial function involving the unknown point and its product with unknown weight.
  + Can be solved by using a multidimensional nonlinear solver.
  + Alternatively, it can sometimes be done step by step.
***** Examples
****** Example 1: Gaussian quadrature on [-1, 1]
\[
I[f] = \int_{-1}^1 f(x) \, dx = \sum_{i=1}^n c_i f\left(x_i\right) = c_1 f_1 + c_2 f_2 + \ldots + c_{n-1} f_{n-1} + c_n f_n
\]
Two function evaluations: Choose \(\left(c_1, c_2, x_1, x_2\right)\) such that the method yields the "exact integral" for \(f(x) = x^0, x^1, x^2, x^3\). For \(n = 2\), the method is accurate up to \(2n-1 = 3\) degree polynomial.
****** Example 2: Gaussian quadrature on [-1, 1]
For \(n = 2\):
\[
\int_{-1}^1 f(x) \, dx = c_1 f\left(x_1\right) + c_2 f\left(x_2\right)
\]
Integral for \(f(x) = x^0, x^1, x^2, x^3\) leads to four equations and four unknowns:
\[
\begin{aligned}
& f = 1 \Longrightarrow \int_{-1}^1 1 \, dx = 2 = c_1 + c_2 \\
& f = x \Longrightarrow \int_{-1}^1 x \, dx = 0 = c_1 x_1 + c_2 x_2 \\
& f = x^2 \Longrightarrow \int_{-1}^1 x^2 \, dx = \frac{2}{3} = c_1 x_1^2 + c_2 x_2^2 \\
& \left\{\begin{array}{l}
c_1 = c_2 = 1 \\
x_1 = -x_2 = \frac{1}{\sqrt{3}}
\end{array}\right. \\
& f = x^3 \Longrightarrow \int_{-1}^1 x^3 \, dx = 0 = c_1 x_1^3 + c_2 x_2^3 \\
& I = \int_{-1}^1 f(x) \, dx = f\left(\frac{1}{\sqrt{3}}\right) + f\left(-\frac{1}{\sqrt{3}}\right)
\end{aligned}
\]
****** Example 3: Gaussian quadrature on [-1, 1]
For \(n = 3\):
\[
\int_{-1}^1 f(x) \, dx = c_1 f\left(x_1\right) + c_2 f\left(x_2\right) + c_3 f\left(x_3\right)
\]
Integral for \(f(x) = x^0, x^1, x^2, x^3, x^4, x^5\) leads to six equations and six unknowns:
\[
\begin{aligned}
& f = 1 \Longrightarrow \int_{-1}^1 1 \, dx = 2 = c_1 + c_2 + c_3 \\
& f = x \Longrightarrow \int_{-1}^1 x \, dx = 0 = c_1 x_1 + c_2 x_2 + c_3 x_3 \\
& f = x^2 \Longrightarrow \int_{-1}^1 x^2 \, dx = \frac{2}{3} = c_1 x_1^2 + c_2 x_2^2 + c_3 x_3^2 \\
& f = x^3 \Longrightarrow \int_{-1}^1 x^3 \, dx = 0 = c_1 x_1^3 + c_2 x_2^3 + c_3 x_3^3 \\
& f = x^4 \Longrightarrow \int_{-1}^1 x^4 \, dx = \frac{2}{5} = c_1 x_1^4 + c_2 x_2^4 + c_3 x_3^4 \\
& f = x^5 \Longrightarrow \int_{-1}^1 x^5 \, dx = 0 = c_1 x_1^5 + c_2 x_2^5 + c_3 x_3^5
\end{aligned}
\]
Solving these equations, we get:
\[
\begin{aligned}
& c_1 = \frac{5}{9}, \quad c_2 = \frac{8}{9}, \quad c_3 = \frac{5}{9} \\
& x_1 = -\sqrt{\frac{3}{5}}, \quad x_2 = 0, \quad x_3 = \sqrt{\frac{3}{5}}
\end{aligned}
\]
Thus:
\[
I = \int_{-1}^1 f(x) \, dx = \frac{5}{9} f\left(-\sqrt{\frac{3}{5}}\right) + \frac{8}{9} f(0) + \frac{5}{9} f\left(\sqrt{\frac{3}{5}}\right)
\]
****** Example 4: Gaussian quadrature on [a, b]
Define:
\[
t = \frac{b - a}{2} x + \frac{b + a}{2}
\]
At \(x = -1\), \(t = a\) and \(x = 1\), \(t = b\):
\[
I = \int_a^b f(x) \, dx = \int_{-1}^1 f\left(\frac{b - a}{2} x + \frac{b + a}{2}\right) \frac{b - a}{2} \, dx = \int_{-1}^1 g(x) \, dx
\]
**** Adaptive integration
- Non-adaptive quadrature: We continue to subdivide all subintervals, say by half, until the overall error estimate falls below the desired tolerance.
  - This method, although unbiased, may often be very inefficient if the function is not equally smooth throughout the domain of integration.
- Adaptive quadrature: The domain of integration is selectively refined. This reflects the behavior of the particular integrand function on a specific subinterval.
  - The integrand is sampled densely in regions where it is difficult to integrate and sparsely in regions where it is easy.
  - It is a very practical way to approach the problem of having an integrand which has some "interesting" behavior.
  - Can be quite efficient - uses 1384 sampling points.
  - However, if the interval of integration is very wide but the "interesting" behavior of the integrand is confined to a narrow range, sampling by automatic routine may miss the interesting part of the integrand behavior, and the resulting value for the integral may be completely wrong.
  - Plot to see the interesting part.
**** Special cases
***** Oscillating integrand
\[
\int_a^b f(x) \cos ^n(\omega x) \, dx
\]
Use methods or programs specially designed to calculate integrals with oscillating functions:
- Filon's method
- Clenshaw-Curtis method
***** Improper integral
Improper integrals are integrals whose integrand is unbounded in the range of integration.
- Change of variable
- Elimination of the singularity
- Ignoring the singularity
- Truncation of the interval
- Using the Cauchy principal value
****** Change of variable
Sometimes it is possible to find a change of variable that eliminates the singularity.
\[
I=\int_0^1 x^{-\frac{1}{n}} g(x) \, dx
\]
substituting \(x=t^n\), the integral becomes:
\[
I=n \int_0^1 t^{n-2} g\left(t^n\right) \, dt
\]
which is proper! But one has to be careful to not trade one problem for another:
\[
I=\int_0^1 \log (x) f(x) \, dx
\]
substituting \(t=-\log (x)\),
\[
I=-\int_0^{\infty} t e^{-t} f\left(e^{-t}\right) \, dt
\]
****** Elimination of the singularity
General idea: Subtract from the singular integrand \(f(x)\) a function, \(g(x)\).
- \(g(x)\) integral is known in closed form.
- \(f(x)-g(x)\) is no longer singular.
This means that \(g(x)\) has to mimic the behavior of \(f(x)\) closely to its singular point.
\[
\int_0^1 \frac{\cos x}{\sqrt{x}} \, dx =\int_0^1 \frac{1}{\sqrt{x}} \, dx+\int_0^1 \frac{\cos (x)-1}{\sqrt{x}} \, dx =2+\int_0^1 \frac{\cos (x)-1}{\sqrt{x}} \, dx
\]
But \(\cos (x)-1 \approx-\frac{x^2}{2}\) near \(x=0\) making the new integrand proper that can be integrated easily.
****** Ignoring the singularity
It is also possible to avoid the integrand singularities and apply the standard quadrature formulae. If we want to compute:
\[
\int_0^1 f(x) \, dx
\]
where \(f(x)\) is unbounded in the neighborhood of \(x=0\),
- Then we set \(f(0)=0\) (or any other value) and use any sequence of quadrature methods.
- Another option: use a sequence of quadrature methods that do not involve the value of \(f(x)\) at \(x=0\).
Proceeding to the limit
\(1>r_1>r_2>\ldots\) is a sequence of points that converges to 0. For example, if \(r_n=2^{-n}\), then
\[
\int_0^1 f(x) \, dx=\int_{r_1}^1 f(x) \, dx+\int_{r_2}^{r_1} f(x) \, dx+\int_{r_3}^{r_2} f(x) \, dx+\ldots
\]
Each of the above integrals is a proper integral. The evaluation can be terminated if
\[
\left|\int_{r_n}^{r_{n+1}} f(x) \, dx\right| \leq \epsilon
\]
****** Truncation of the interval
\[
\int_0^1 f(x) \, dx=\int_0^r f(x) \, dx+\int_r^1 f(x) \, dx
\]
then if
\[
\left|\int_0^r f(x) \, dx\right| \leq \epsilon,
\]
we can simply evaluate
\[
\int_r^1 f(x) \, dx
\]
For example, assume \(|g(x)|<1 \forall x \in(0,1]\), then
\[
\left|\frac{g(x)}{x^{\frac{1}{2}}+x^{\frac{1}{3}}}\right| \leq \frac{1}{2 x^{\frac{1}{2}}} \Longrightarrow\left|\int_0^r \frac{g(x)}{x^{\frac{1}{2}}+x^{\frac{1}{3}}} \, dx\right| \leq \int_0^r \frac{dx}{2 x^{\frac{1}{2}}}=r^{\frac{1}{2}}
\]
If we chose \(r \leq 10^{-8}\), we get an accuracy of \(10^{-4}\).
****** Using the Cauchy principal value
Reduction of the CPV to one-sided improper integral is possible. Consider \(f(x)\) unbounded in \(x=c\) with \(a<c<b\).
Suppose that \(\mathrm{P} \int_a^b f(x) \, dx\) exists:
\[
\mathrm{P} \int_a^b f(x) \, dx=\lim _{r \rightarrow 0}\left[\int_a^{c-r} f(x) \, dx+\int_{c+r}^b f(x) \, dx\right]
\]
Consider \(c=0\) and \(b=-a\).
Decompose \(f(x)\) into its odd and even parts:
\[
g(x)=\frac{1}{2}[f(x)-f(-x)] \quad h(x)=\frac{1}{2}[f(x)+f(-x)]
\]
\[
\int_{-a}^{-r} f(x) \, dx+\int_{+r}^a f(x) \, dx=\int_{-a}^{-r} g(x) \, dx+\int_{+r}^a g(x) \, dx+\int_{-a}^{-r} h(x) \, dx+\int_{+r}^a h(x) \, dx=2 \int_{+r}^a h(x) \, dx
\]
Therefore:
\[
\mathrm{P} \int_{-a}^a f(x) \, dx=2 \lim _{r \rightarrow 0^{+}} \int_r^a h(x) \, dx
\]
\[
\mathrm{P} \int_{-1}^1 \frac{1}{x} \, dx=0 \\
\mathrm{P} \int_{-1}^1 \frac{e^x}{x} \, dx=2 \int_0^1 \frac{\sinh (x)}{x} \, dx
\]
The method of subtracting the singularity may also be used.
\[
I(x)=\mathrm{P} \int_a^b \frac{f(t)}{t-x} \, dt \quad a<x<b \\
I(x)=\int_a^b \frac{f(t)-f(x)}{t-x} \, dt+f(x) \mathrm{P} \int_a^b \frac{dt}{t-x} =\int_a^b \frac{f(t)-f(x)}{t-x} \, dt+f(x) \log \frac{b-x}{x-a}
\]
Consider the function:
\[
\phi(t, x) =\frac{f(t)-f(x)}{t-x} \quad t \neq x \\
\phi(x, x) =f^{\prime}(x) \quad t=x
\]
and solve
\[
\int_a^b \phi(t, x) \, dt
\]
It may be useful to consider:
\[
\int_{x-h}^{x+h} \phi(t, x) \, dt=\int_{-h}^h \frac{f(t+x)-f(x)}{t} \, dt
\]
If \(f(t+x)\) can be expanded in a Taylor series:
\[
\int_{x-h}^{x+h} \phi(t, x) \, dt =\int_{-h}^h\left(f^{\prime}(x)+\frac{t f^{\prime \prime}(x)}{2!}+\frac{t^2 f^{\prime \prime \prime}(x)}{3!}+\ldots\right) =2 h f^{\prime}(x)+\frac{h^3 f^{\prime \prime \prime}(x)}{9}+\ldots
\]
***** Indefinite integral
\[
\int_a^{\infty} f(x) \, dx \quad \int_{-\infty}^{\infty} f(x) \, dx
\]
- Change of variables (common one is):
\[
z=\frac{x-a}{1+x-a}
\]
then
\[
\int_a^{\infty} f(x) \, dx=\int_0^1 \frac{1}{(1-z)^2} f\left(\frac{z}{1-z}+a\right) \, dz
\]
For \(\int_{-\infty}^{\infty}\) use
\[
x=\frac{z}{1-z^2} \quad \text { or } \quad x=\tan z
\]
\[
\int_a^{\infty} f(x) \, dx \quad \int_{-\infty}^{\infty} f(x) \, dx
\]
- Replace infinite limits of integration by carefully chosen finite values. Use asymptotic behavior to evaluate "tail" contribution! (For \(a \gg 1\) ):
\[
\int_0^{\infty} \frac{\sqrt{x}}{x^2+1} \, dx =\int_0^a \frac{\sqrt{x}}{x^2+1} \, dx+\int_a^{\infty} \frac{\sqrt{x}}{x^2+1} \, dx \approx \int_0^a \frac{\sqrt{x}}{x^2+1} \, dx+\int_a^{\infty} \frac{1}{x^{3 / 2}} \, dx =\int_0^a \frac{\sqrt{x}}{x^2+1} \, dx+\frac{2}{\sqrt{a}}
\]
- Use nonlinear quadrature rules designed for infinite range intervals.
*** Multi-variable integrals
- Use automatic one-dimensional quadrature routine for each dimension, one for the outer integral and another for the inner integral.
- Monte-Carlo method (effective for large dimensions)
\[
\int_0^1 dx_1 \int_0^1 dx_2 \ldots \int_0^1 dx_7\left(x_1+x_2+\ldots+x_7\right)^2
\]
*** Conclusions
- Analyze the integrand and plot it to see any potential pitfalls.
- For smooth functions, all methods work well.
- For oscillating functions, functions with singularities, functions with high and narrow peaks, etc., one should use special methods and programs.
- A very good set of quadrature methods is available through SciPy called QUADPACK. For your projects, use these whenever possible.
** Ordinary Differential Equations
:LOGBOOK:
CLOCK: [2024-06-14 Fri 10:34]--[2024-06-14 Fri 15:41] =>  5:07
CLOCK: [2024-06-14 Fri 07:02]--[2024-06-14 Fri 09:58] =>  2:56
CLOCK: [2024-06-14 Fri 00:35]--[2024-06-14 Fri 06:15] =>  5:40
:END:
*** Classification
*** Errors
There are five types of errors that can arise while numerically solving an ODE:
+ Initial value errors :: These are errors in the initial data
+ Algebraic errors :: ?
+ Truncation errors :: cased by truncating the Taylor series approximation (decreases with decreasing of the step size).
+ Round-off errors :: caused by finite word length (increases as the step size decreases: more steps and small difference between large numbers leading to catastrophic cancellation)
+ Inherited errors :: the sum of all accumulated errors from all previous steps (means that the initial condition for the next is incorrect)
*** Initial value problems (IVPs)
**** Preliminaries
***** Definition
Without loss of generality to higher-order systems (see the sections on IVPs defined by simultaneous ODEs and higher-order ODEs), we restrict ourselves to linear first-order differential equations, because a linear higher-order ODE can be converted into a larger system of linear first-order equations by introducing extra variables. 
#+NAME: Initial value problem involving a first-order ODE 
#+begin_definition latex
A first-order differential equation is an initial value problem of the form
\begin{align*}
\mathrm{D}_t x(t) = f(t, x(t)), \quad x\left(0\right)=x_0
\end{align*}
where \(f\) is a function \(f:\left[0, \infty\right) \times \mathbb{R} \rightarrow \mathbb{R}\), and the initial condition \(x_0 \in \mathbb{R}\).
#+end_definition
***** Finite-difference approximations (FDAs)
Initial values problems are solving using finite difference methods. The objective of a finite difference method for solving an ODE is to transform a calculus problem into an algebra problem by:
1) Approximating the exact derivatives in the ODE by algebraic /finite difference approximations/,
2) Substituting the FDA into ODE to obtain an algebraic /finite difference equation/,
3) Transforming the continuous physical domain into a discrete finite difference grid,
4) Solving the resulting algebraic finite difference equation.
There are three distinct groups of finite difference methods for solving ODEs appearing in initial value problems:
- Single-point methods :: advance the solution from one grid point to the next grid point using only the data at a single grid point. An example of single point method is the /4th order Runge-Kutta method/.
- Extrapolation methods :: evaluate the solution at a grid point for several values of grid size and extrapolate those results to get for a more accurate solution.
- Multi-point methods :: advance the solution form one grid point to the next using the data at several known points. An example of a multi-point method is the /4th order Adams-Bashforth-Moulton method/.
The rest of this section develops elementary FDAs for the previously defined IVP.
****** Forward difference
Expanding \(x(t + h)\) in a Taylor's series at \( x (t) \) we have \(x(t + h) = x(t) + \mathrm{D}_t x(t) \, h + \mathcal{O} (h^{2}) \). Using \( \mathrm{D}_t x(t) = f(t, x(t))\) we get
\[
\boxed{
x(t+h) = x(t) + h \, f(t,\,x(t))  + \mathcal{O} (h^{2}).
}
\]
When \( h \) is only allowed to assume discrete values and terms that are \( \mathcal{O} (h^{2}) \) are dropped from the right hand side, this scheme is called the /forward difference approximation/:
\[
\boxed{
x_{n+1} = x_{n} + h f(t_n + h,\, x_{n}).
}
\]
****** Backward difference
Expanding \(x(t - h)\) in a Taylor's series at \( x (t) \) we have \(x(t- h) = x(t) - \mathrm{D}_t x(t) \, h + \mathcal{O} (h^{2}) \). Using \( \mathrm{D}_t x(t) = f(t, x(t))\) and transforming \( t \to t + h \) we get
\[
\boxed{
x(t+h) = x(t) + h \, f(t+h,\,x(t+h))  + \mathcal{O} (h^{2}).
}
\]
When \( h \) is only allowed to assume discrete values and terms that are \( \mathcal{O} (h^{2}) \) are dropped from the right hand side, this scheme is called the /backward difference approximation/:
\[
\boxed{
x_{n+1} = x_{n} + h f(t_n + h,\, x_{n+1}).
}
\]
****** Central difference
Expanding \(x(t + h)\) in a Taylor's series at \( x (t) \) we have
\[
x(t + h) = x(t) + \mathrm{D}_t x(t) \, h + \frac{1}{2!} \mathrm{D}_t^{2} x(t) \, h^{2} + \mathcal{O} (h^{3}). \tag{1}
\]
Expanding \(x(t - h)\) in a Taylor's series at \( x (t) \) we have
\[
x(t- h) = x(t) - \mathrm{D}_t x(t) \, h + \frac{1}{2!} \mathrm{D}_t^{2} x(t) \, h^{2} + \mathcal{O} (h^{3}). \tag{2}
\]
Subtracting (2) from (1) and transforming \( h \to h / 2 \) we obtain
\[
x(t + h / 2) - x(t- h/2) = \mathrm{D}_t x(t) \, h + \mathcal{O} (h^{3})
\]
Using \( \mathrm{D}_t x(t) = f(t, x(t))\) and transforming \( t \to t + h/2 \) we get
\[
\boxed{
x(t+h) = x(t) + h \, f(t + h/2,\,x(t + h/2))  + \mathcal{O} (h^{3}).
}
\]
******* Forward central difference
Starting from \(x(t+h) = x(t) + h \, f(t + h/2,\,x(t + h/2))  + \mathcal{O} (h^{3})\) we can expand \( x(t + h/2) \) in a Taylor series about \( x(t) \) to obtain \( x(t + h/2) = x(t) + (h/2) \mathrm{D}_t x(t) + \mathcal{O}(h^2) \) and use \( \mathrm{D}_t x(t) = f(t, x(t))\) to obtain
\[
\boxed{
x(t+h) = x(t) + h \, f(t + h/2,\,x(t) + (h/2) \cdot f(t,\,x(t)))  + \mathcal{O} (h^{3}).
}
\]
Note that while the approximation \( x(t + h/2) \approx x(t) + (h/2) \mathrm{D}_t x(t) \) by itself introduces error that is \( \mathcal{O} (h^2) \), it appears as an argument of \( f \) which multiplies a factor of \( h \) so that the overall we have a leading order error of \( \mathcal{O} (h^3) \). When \( h \) is only allowed to assume discrete values and terms that are \( \mathcal{O} (h^{3}) \) are dropped from the right hand side, this scheme is called the /forward central difference approximation/:
\[
\boxed{
x_{n+1} = x_{n} + h f(t_n + h/2,\, x_n + (h/2) \cdot f(t_n,\,y_n)).
}
\]
******* Backward central difference
Starting from \(x(t+h) = x(t) + h \, f(t + h/2,\,x(t + h/2))  + \mathcal{O} (h^{2})\) we can expand \( x(t + h/2) \) in a Taylor series about \( x(t + h) \) to obtain \( x(t + h/2) = x(t+h) - (h/2) \mathrm{D}_t x(t+h) + \mathcal{O}(h^2) \) and use \( \mathrm{D}_t x(t) = f(t, x(t))\) to obtain
\[
\boxed{
x(t+h) = x(t) + h \, f(t + h/2,\,x(t+h) - (h/2) \cdot f(t+h,\,x(t+h)))  + \mathcal{O} (h^{3}).
}
\]
Note that while the approximation \( x(t + h/2) \approx x(t) - (h/2) \mathrm{D}_t x(t+h) \) by itself introduces error that is \( \mathcal{O} (h^2) \), it appears as an argument of \( f \) which multiplies a factor of \( h \) so that the overall we have a leading order error of \( \mathcal{O} (h^3) \). When \( h \) is only allowed to assume discrete values and terms that are \( \mathcal{O} (h^{2}) \) are dropped from the right hand side, this scheme is called the /backwards central difference approximation/:
\[
\boxed{
x_{n+1} = x_{n} + h f(t_n + h/2,\, x_{n+1} - (h/2) \cdot f(t_{n+1},\,x_{n+1})).
}
\]
***** IVPs defined by simultaneous ODEs
IVPs can involve more than one /dependent variables/, i.e., are defined by a system of simultaneous linear first-order ODEs where, in general, the derivative of each dependent variable with respect to the /independent variable/ can depend on any or all of the dependent variables as well as the single independent variable:
\begin{align*}
\mathrm{D}_t x(t) = f_x (t,\,x(t), y(t)) \quad \mathrm{D}_t y(t) = f_y (t,\,x(t),\, y(t))
\end{align*}
where \(f_x\) and \(f_y\) are possibly nonlinear functions of \(x\), \(y\) and \(t\). These equations can be written in a vector form as:
\begin{align*}
\mathrm{D}_t \mathbf{r} (t) = \mathbf{f} (t,\,\mathbf{r}(t))
\end{align*}
where \(\mathbf{r} (t) = (x(t),\, y(t),\, \ldots)\) and \(\mathbf{f}\) is a vector of functions, \(\mathbf{f}(t,\,\mathbf{r}(t)) = (f_x (t,\,\mathbf{r}(t)),\, f_y(t,\,\mathbf{r}(t)),\, \ldots)\). As far as Computationally, these are solved using the same finite-difference methods we use for solving an single ODE. In fact, we can substitute \(x(t) \to \mathbf{r} (t) \) in all of the identities in the previous section and they shall remain just as true. 
Thus, /any method that can be used to solve an IVP defined by a linear first-order ODE with one dependent variable can be used to solve an IVP defined by a system of simultaneous linear first-order ODEs with more than one dependent variables./
***** IVPs defined by higher-order ODEs
Any order \( n \) linear ODE can be reduced to a system of \( n \) simultaneous ODEs by introducing extra variables. For example, the general second-order differential equation of the form \( \mathrm{D}_t^{2} x(t) = f (t,\, x(t), \,\mathrm{D}_t x(t)) \) reduces to \( \mathrm{D}_t x(t) = y(t) \) and \( \mathrm{D}_t y(t) = f(t,\,x(t),\,y(t)) \). Similarly, the general third-order ODE \( \mathrm{D}_t^{3} = f(t,\,x(t),\, \mathrm{D}_t x(t),\, \mathrm{D}_t^{2} x(t)) \) reduces to \( \mathrm{D}_t x(t) = y(t) \), \( \mathrm{D}_t y(t) = z(t) \), and \( \mathrm{D}_t z(t) = f(t,\, x(t),\,y(t),\,z(t)) \). As a concrete example, the system \( \mathrm{D}_t x(t) = v(t) \) and \( \mathrm{D}_t v(t) = -(k/m) x(t) \) of linear first-order ODEs is completely equivalent to the linear second-order ODE that defines the simple harmonic oscillator \(\mathrm{D}_t^{2} x(t) = (k/m) x(t)  \).  
Thus, /any method that can be used to solve an IVP defined by a system of simultaneous linear first-order ODEs can be used to solve an IVP defined by a linear higher-order ODE/. In the previous section we saw that any method that can be used to solve an IVP defined by a linear first-order ODE with one dependent variable can be used to solve an IVP defined by a system of simultaneous linear first-order ODEs. /Therefore, methods that solve the IVP defined by a linear first-order ODE also make accessible a considerably larger class of IVPs./
***** Solution over an infinite range
In some cases, we want to march along the independent variable, say \( t \), from some initial value to \(t=\infty\). Such problems are usually solved using a suitably chosen change of variables so that as \( t \to \infty \), the transformed variable, say \( u \), tends to some finite value, i.e., \( u \to u_{f} \) where \( -\infty < u_{f} < \infty \). An example of such a coordinate transform is \( t \to u/(1-u) \). It is clear that as \( u \to 1 \), \( t \to \infty \). Let \( u(t) \) be a function of \( t \) that denotes such a general coordinate transformation. We also assume the existence of the inverse transform: \( t(u) \) is a well-defined function that 1) has the image and domain of \( u(t) \) as its domain and image respectively and 2) for all \( t \) satisfies \(t(u(t)) = t\). Using the chain rule, a rewrite for the ODE  \( \mathrm{D}_t x(t) = f(t, x(t)) \) is \( \mathrm{D}_u x(t(u)) = (\mathrm{D}_t u(t))^{-1} \cdot f(t, x(t(u))) \). Because \( t(u(t)) = t \), we have \( \mathrm{D}_u t(u) \cdot \mathrm{D}_t u(t) = 1 \). It follows that \( (\mathrm{D}_t u(t))^{-1} = \mathrm{D}_u  t(u) \). Substituting this result in \( \mathrm{D}_u x(t(u)) = (\mathrm{D}_t u(t))^{-1} \cdot f(t, x(t)) \) we get \( \mathrm{D}_u x(t(u)) = \mathrm{D}_u t(u) \cdot f(t,\, x(t(u))) \). It is now possible to eliminate \( t \) by substituting \( t(u) \); on doing so we get the \( \mathrm{D}_u x(u) = g(u,\, x(u)) \) where \( g(u,\,x(u)) \equiv \mathrm{D}_u t(u) \cdot f(t(u),\,x(t(u))) \). As an example, consider the ODE \( \mathrm{D}_t x(t) = (x^2 + t^2)^{-1}\) with \(x=1\) at \(t=0\). To obtain \( x(t) \) for \( t \in [0,\, \infty) \), we introduce \( u (t) = t/(1+t) \). Clearly, \( t(u) = u / (1-u) \) so that the ODE transforms into \( \mathrm{D}_t x(u) = \mathrm{D}_u [u/(1-u)] \cdot (x^2 + [u/(1-u)]^{2})^{-1} \) and simplifies to \( \mathrm{D}_u x(u) = (x^{2}(1 -  u)^{2} + u^{2})^{-1} \) so that in stepping from \( u = 0 \) to \( u = 1 \), we will have stepped from \( t = 0 \) to \( t = \infty \) to obtain \( x(t) \) for \( t \in [0,\, \infty) \).
**** Euler's methods
***** Types
****** Explicit Euler's method
The explicit Euler method refers to first-order forward difference approximation in solving the IVP described a first order ODE.
#+NAME: Explicit Euler's method
#+begin_definition latex
Let \(f\) is a function \(f:\left[0, \infty\right) \times \mathbb{R} \rightarrow \mathbb{R}\). Consider the IVP
\begin{align*}
\mathrm{D}_t x(t) = f(t, x(t)), \quad x\left(0\right)=x_0.
\end{align*}
The explicit Euler's method is a first-order forward difference approximation that solves this IVP:
\[
x_{n+1} \approx x_{n} + h \, f(t_{n},\,x_{n})
\]
#+end_definition
It is explicit since \(f\left(t_n, x_n\right)\) does not depend on \(x_{n+1}\). The explicit Euler's method is a example of a single point method since it requires only one known point. A limitation of this method (overcome by the implicit Euler's method) is that the method is /conditionally stable/ for \(\Delta t \leq \Delta t_{c r}\).
****** Implicit Euler's method
#+NAME: Implicit Euler's method
#+begin_definition latex
Let \(f\) is a function \(f:\left[0, \infty\right) \times \mathbb{R} \rightarrow \mathbb{R}\). Further assume that \( f \) is linear. Consider the IVP
\begin{align*}
\mathrm{D}_t x(t) = f(t, x(t)), \quad x\left(0\right)=x_0.
\end{align*}
The implicit Euler's method is a first-order backward difference approximation that solves this IVP:
\[
x_{n+1} \approx x_{n} + h \, f(t_{n+1},\,x_{n+1}).
\]
#+end_definition
It is implicit since \(f(t_{n+1}, x_{n+1})\) depends on \(x_{n+1}\). The implicit Euler's method is a example of a single point method since it requires only one known point. For linear first-order ODEs this method is /unconditionally stable/. However, if \(f(x, t)\) is non-linear, a method specialized for non-linear ODEs should be preferred over this method.
The explicit Euler's method simply requires the evaluation of the right hand side of the forward finite difference approximation that defines it. In contrast, when using the implicit Euler's method, a transcendental equation must be solved for every step, possibly by using a suitable root-finding method. Thus the use of the implicit Euler's method is computationally expensive relative to the use of the explicit Euler's method.
***** Errors
****** Truncation errors
******* Local truncation error (\( \mathcal{O} (h^2) \))
The local truncation error is \(\mathcal{O}\left(h^2\right)\).
The local truncation error of the Euler method is the error made in a single step. It is the difference between the numerical solution after one step, \(x_1\), and the exact solution at time \(t_1=t_0+h\). The numerical solution is given by
\begin{align*}
x_1 = x_0 + h f(t_0, x_0)
\end{align*}
For the exact solution, we use the Taylor expansion
\begin{align*}
x(t_0 + h) = x(t_0) + h \mathrm{D}_t x (t_0) + \frac{1}{2} h^2 \mathrm{D}_t^{2} x (t_0) + \mathcal{O} (h^3)
\end{align*}
The local truncation error (LTE) introduced by therical method starts from an initial
\begin{align*}
\mathrm{LTE} = x (t_0 + h) - x_1 = \frac{1}{2} h^2 \mathrm{D}_t^2 x (t_0) + \mathcal{O} (h^3).
\end{align*}
This result is valid if \(x(t)\) has a bounded third derivative. This shows that for small \(h\), the local truncation error is approximately proportional to \(h^2\). This makes the Euler method less accurate than higher-order techniques such as Runge-Kutta methods and linear multistep methods, for which the local truncation error is proportional to a higher power of the step size.
A slightly different formulation for the local truncation error can be obtained by using the Lagrange form for the remainder term in Taylor's theorem. If \(x\) has a continuous second derivative, then there exists a \(\xi \in\left[t_0, t_0+h\right]\) such that
\begin{align*}
\mathrm{LTE}=x (t_0 + h) - x_1 = \frac{1}{2} h^2 \mathrm{D}_t x (\xi).
\end{align*}
In the above expressions for the error, the second derivative of the unknown exact solution \(x\) can be replaced by an expression involving the right-hand side of the differential equation. Indeed, it follows from the equation \(\mathrm{D}_t x = f (t, x)\) that
\[
\mathrm{D}_{t}^{2} x (t_0)= \partial_t f (t_0, x (t_0)) + \partial_x f (t_0, x(t_0)) f(t_0, x(t_0))
\]
******* Global truncation error (\( \mathcal{O} (h) \))
The global error accumulated after \(n\) steps \(\mathcal{O}(h)\).
The global truncation error is the error at a fixed time \(t_i\), after however many steps the method needs to take to reach that time from the initial time. The global truncation error is the cumulative effect of the local truncation errors committed in each step. The number of steps is easily determined to be \(\frac{t_i-t_0}{h}\), which is proportional to \(\frac{1}{h}\), and the error committed in each step is proportional to \(h^2\) (see the previous section). Thus, it is to be expected that the global truncation error will be proportional to \(h\).
This intuitive reasoning can be made precise. If the solution \(x\) has a bounded second derivative and \(f\) is Lipschitz continuous in its second argument, then the global truncation error (denoted as \(\left|x\left(t_i\right)-x_i\right|\) ) is bounded by
\begin{align*}
\left|x\left(t_i\right)-x_i\right| \leq \frac{h M}{2 L}\left(\exp [L(t_i-t_0)]-1\right)
\end{align*}
where \(M\) is an upper bound on the second derivative of \(x\) on the given interval and \(L\) is the Lipschitz constant of \(f\). Or more simply, when \(\mathrm{D}_t x(t)=f(t, x)\), the value \(L=\max \left(\left| \mathrm{D}_x [f(t, x)]\right|\right)\) (such that \(t\) is treated as a constant). In contrast, \(M=\max \left(\left| \mathrm{D}_t^{2} [x(t)]\right|\right)\) where function \(x(t)\) is the exact solution which only contains the \(t\) variable.
The precise form of this bound is of little practical importance, as in most cases the bound vastly overestimates the actual error committed by the Euler method. What is important is that it shows that the global truncation error is (approximately) proportional to \(h\). For this reason, the Euler method is said to be first order.
Given the differential equation 
\[ \mathrm{D}_t x (t) = 1 + (t - x(t))^2, \]
and the exact solution 
\[ y(t) = t + \frac{1}{t - 1}, \]
we want to find \( M \) and \( L \) for \( 2 \leq t \leq 3 \).
First, we determine \( L \):
\[
L = \max \left( \left| \mathrm{D}_{x} [ f(t, x) ] \right| \right) = \max_{2 \leq t \leq 3} \left( \left| \mathrm{D}_x \left[ 1 + (t - x)^2 \right] \right| \right) = \max_{2 \leq t \leq 3} \left( \left| 2 (t - x) \right| \right).
\]
Substituting \( x = t + \frac{1}{t - 1} \), we get:
\[
L = \max_{2 \leq t \leq 3} \left( \left| 2 \left( t - \left( t + \frac{1}{t - 1} \right) \right) \right| \right) = \max_{2 \leq t \leq 3} \left( \left| -\frac{2}{t - 1} \right| \right).
\]
For \( 2 \leq t \leq 3 \), the maximum value of \( \left| -\frac{2}{t - 1} \right| \) is 2. Thus, \( L = 2 \).
Next, we determine \( M \):
\[
M = \max \left( \left| \mathrm{D}_t^{2}  [ x(t) ] \right| \right) = \max_{2 \leq t \leq 3} \left( \left| \mathrm{D}_t^{2} \left[ t + \frac{1}{1 - t} \right] \right| \right) = \max_{2 \leq t \leq 3} \left( \left| \frac{2}{(-t + 1)^3} \right| \right).
\]
For \( 2 \leq t \leq 3 \), the maximum value of \( \left| \frac{2}{(-t + 1)^3} \right| \) is 2. Thus, \( M = 2 \).
We can now find the error bound at \( t = 2.5 \) and \( h = 0.5 \):
\[
\text{error bound} = \frac{h M}{2 L} \left( \exp (L (t_i - t_0)) - 1 \right) = \frac{0.5 \cdot 2}{2 \cdot 2} \left( \exp (2 (2.5 - 2)) - 1 \right) = 0.42957.
\]
Here, \( t_0 = 2 \) because it is the lower bound for \( t \) in \( 2 \leq t \leq 3 \).
****** Round-off errors
******* Local round-off error (\( \sim \epsilon_{\text{mach}} x_{n} \))
In step \(n\) of the Euler method, the rounding error is roughly of the magnitude \(\epsilon_{\text{mach}} x_n\) where \(\epsilon_{\text{mach}}\) is the machine epsilon.
******* Global round-off error (\( \mathcal{O}(1/\sqrt{h}) \))
Assuming that the rounding errors are independent random variables, the expected total rounding error is proportional to \(\epsilon_{\text{mach}}/\sqrt{h}\). 
Thus, for extremely small values of the step size the truncation error will be small but the effect of rounding error may be big. Most of the effect of rounding error can be easily avoided if compensated summation is used in the formula for the Euler method.
***** Numerical stability
- Consider the 'linear test equation':
\begin{align*}
\frac{d x(t)}{d t}=\lambda x(t)
\end{align*}
where \(\lambda \in \mathbb{C}\) and \(x(t=0)=x_0 \neq 0\). The exact solution of this equation is:
\begin{align*}
x(t)=x_0 e^{\lambda t}
\end{align*}
For \(\Re(\lambda)<0\), then the solution \(x(t \rightarrow \infty) \rightarrow 0\).
\begin{align*}
& x_{n+1}=x_n+\lambda \delta t x_n \\
& x_{n+1}=(1+\lambda \delta t)^n x_0
\end{align*}
So for the case when \(\Re(\lambda)<0\) :
\begin{align*}
|1+\lambda \delta t|<1
\end{align*}
If we restrict that \(\lambda \in \mathbb{R}\) :
\begin{align*}
-1<1+\lambda \delta t<1 \\
-2<\lambda \delta t<0 \\
0<\delta t<t-\frac{2}{\lambda}
\end{align*}
(as \(\delta t>0\) and \(\lambda<0\) ).
The condition of stability is:
\begin{align*}
\delta t<-\frac{2}{\lambda}
\end{align*}
\begin{align*}
x_{n+1} & =x_n+\lambda \delta t x_{n+1} \\
x_{n+1} & =\frac{1}{1-\lambda \delta t} x_n \\
x_{n+1} & =\left(\frac{1}{1-\lambda \delta t}\right)^n x_0
\end{align*}
So for the case when \(\Re(\lambda)<0\) :
\begin{align*}
\left|\frac{1}{1-\lambda \delta t}\right|<1
\end{align*}
If we restrict that \(\lambda \in \mathbb{R}\) :
\begin{align*}
|1-\lambda \delta t|>1 \\
1-\lambda \delta t>1 \text { or } 1-\lambda \delta t<-1 \\
\lambda \delta t<0 \text { or } \lambda \delta t>2
\end{align*}
As \(\delta t>0\) and \(\lambda<0\), the condition of stability is always satified. Thus, implicit Euler method is unconditionally stable.
The Euler method can also be numerically unstable, especially for stiff equations, meaning that the numerical solution grows very large for equations where the exact solution does not. This can be illustrated using the linear equation
\begin{align*}
\mathrm{D}_t x (t) = -2.3 y(t), \quad y(0)=1.
\end{align*}
The exact solution is \(x(t) = \exp (-2.3 t)\), which decays to zero as \(t \rightarrow \infty\). However, if the Euler method is applied to this equation with step size \(h=1\), then the numerical solution is qualitatively wrong: It oscillates and grows. This is what it means to be unstable. If a smaller step size is used, for instance \(h=0.7\), then the numerical solution does decay to zero.
If the Euler method is applied to the linear equation \(\mathrm{D}_t x(t) = k y(t)\), then the numerical solution is unstable if the product \(h k\) is outside the region
\begin{align*}
\{z \in \mathbf{C}|| z+1 \mid \leq 1\}.
\end{align*}
This region is called the (linear) stability region. In the example, \(k=-2.3\), so if \(h=1\) then \(h k=-2.3\) which is outside the stability region, and thus the numerical solution is unstable.
This limitation - along with its slow convergence of error with \(h\) - means that the Euler method is not often used, except as a simple example of numerical integration[citation needed]. Frequently models of physical systems contain terms representing fast-decaying elements (i.e. with large negative exponential arguments). Even when these are not of interest in the overall solution, the instability they can induce means that an exceptionally small timestep would be required if the Euler method is used.
**** Runge-Kutta methods
***** Types
****** Explicit Runge-Kutta methods
The family of explicit Runge-Kutta methods is given by
\[
x_{n+1} = x_n + h \sum_{i=1}^s b_i k_i
\]
where
\begin{align*}
k_1 & = f(t_n, x_n) \\
k_2 & = f(t_n + c_2 h, x_n + (a_{21} k_1) h) \\
k_3 & = f(t_n + c_3 h, x_n + (a_{31} k_1 + a_{32} k_2) h) \\
& \vdots \\
k_s & = f(t_n + c_s h, x_n + (a_{s1} k_1 + a_{s2} k_2 + \cdots + a_{s, s-1} k_{s-1}) h)
\end{align*}
In general, a Runge-Kutta method of order \(s\) can be written as:
\[
x_{t+h} = x_t + h \cdot \sum_{i=1}^s a_i k_i + \mathcal{O}\left(h^{s+1}\right)
\]
where:
\[
k_i = \sum_{j=1}^s \beta_{ij} f\left(t_n+\alpha_i h, k_j\right)
\]
are the increments obtained by evaluating the derivatives of \(x_t\) at the \(i\)-th order.
To specify a particular method, one needs to provide the integer \(s\) (the number of stages), and the coefficients \(a_{ij}\) (for \(1 \leq j < i \leq s\)), \(b_i\) (for \(i = 1, 2, \ldots, s\)) and \(c_i\) (for \(i = 2, 3, \ldots, s\)). The matrix \([a_{ij}]\) is called the Runge-Kutta matrix, while the \(b_i\) and \(c_i\) are known as the weights and the nodes. These data are usually arranged in a mnemonic device, known as a Butcher tableau (after John C. Butcher):
\[
\begin{array}{c|cccccc}
0 &  &  &  &  &  &  \\
c_{2} & a_{21} &  &  &  &  &  \\
c_{3} & a_{31} & a_{32} &  &  &  &  \\
\vdots & \vdots & \vdots & \ddots &  &  &  \\
c_{s} & a_{s1} & a_{s2} & \cdots & a_{s,s-1} &  &  \\
\hline
 & b_{1} & b_{2} & \cdots & b_{s-1} & b_{s} &  \\
\hline
 & b_{1}^{*} & b_{2}^{*} & \cdots & b_{s-1}^{*} & b_{s}^{*} &
\end{array}
\]
The Butcher tableau for a generic second-order method is:
\[
\begin{array}{c|ccc}
0 & 0 & 0 & 0 \\
\alpha & \alpha & 0 & 0 \\
1 - \frac{1}{2\alpha} & 1 - \frac{1}{2\alpha} & \frac{1}{2\alpha} & 0 \\
\hline
 & 1/6 & 2/3 & 1/6 \\
\end{array}
\]
for \( \alpha \neq 0 \). 
\begin{align*}
x_{n+1}=x_n+h\bigg[\bigg(1-\frac{1}{2 \alpha}\bigg) f(t_n, x_n) + \frac{1}{2 \alpha} f(t_n+\alpha h, x_n+\alpha h f(t_n, x_n))\bigg]
\end{align*}
In this family, \( \alpha = 1/2 \) yields the (explicit) midpoint method, \( \alpha = 1 \) yields the Heun's method, and \( \alpha = 2/3 \) yields the Ralston's method.
The Butcher tableau for a generic third-order method is:
\[
\begin{array}{c|ccccc}
0 & 0 & 0 & 0 & 0 & 0 \\
\alpha & \alpha & 0 & 0 & 0 & 0 \\
1 & 1 + \frac{1 - \alpha}{\alpha(3\alpha - 2)} & -\frac{1 - \alpha}{\alpha(3\alpha - 2)} & 0 & 0 & 0 \\
\hline
 & \frac{1}{2} & \frac{1}{6\alpha} & \frac{1}{6\alpha(1 - \alpha)} & \frac{2 - 3\alpha}{6(1 - \alpha)} & 0 \\
\end{array}
\]
for \(\alpha \neq 0, \frac{2}{3}, 1\):
A Taylor series expansion shows that the Runge-Kutta method is consistent if and only if
\[
\sum_{i=1}^s b_i = 1
\]
There are also accompanying requirements if one requires the method to have a certain order \(p\), meaning that the local truncation error is \(\mathrm{O}(h^{p+1})\). These can be derived from the definition of the truncation error itself. For example, a two-stage method has order 2 if \(b_1 + b_2 = 1\), \(b_2 c_2 = \frac{1}{2}\), and \(b_2 a_{21} = \frac{1}{2}\). Note that a popular condition for determining coefficients is 
\[
\sum_{j=1}^{i-1} a_{ij} = c_i \quad \text{for} \quad i=2, \ldots, s
\]
This condition alone, however, is neither sufficient, nor necessary for consistency. In general, if an explicit \(s\)-stage Runge-Kutta method has order \(p\), then it can be proven that the number of stages must satisfy \(s \geq p\), and if \(p \geq 5\), then \(s \geq p+1\). However, it is not known whether these bounds are sharp in all cases. In some cases, it is proven that the bound cannot be achieved. For instance, Butcher proved that for \(p > 6\), there is no explicit method with \(s = p + 1\) stages. Butcher also proved that for \(p > 7\), there is no explicit Runge-Kutta method with \(p + 2\) stages. In general, however, it remains an open problem what the precise minimum number of stages \(s\) is for an explicit Runge-Kutta method to have order \(p\). Some values which are known are:
\[
\begin{array}{c|cccccccc}
p & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\hline
\min s & 1 & 2 & 3 & 4 & 6 & 7 & 9 & 11
\end{array}
\]
The provable bound above then imply that we can not find methods of orders \(p = 1, 2, \ldots, 6\) that require fewer stages than the methods we already know for these orders. The work of Butcher also proves that 7th and 8th order methods have a minimum of 9 and 11 stages, respectively. An example of an explicit method of order 6 with 7 stages can be found in the literature. Explicit methods of order 7 with 9 stages and explicit methods of order 8 with 11 stages are also known. See references for a summary.
******* Examples
******** Explicit Euler's method (RK1)
The simplest RungeKutta method is the (explicit) Euler method. It is of order 1. This is the only consistent explicit RungeKutta method with one stage. The lack of stability and accuracy limits its popularity mainly to use as a simple introductory example of a numeric solution method. The corresponding tableau is 
\[
\begin{array}{c|c}
0 & 0 \\
\hline
1 & 1 \\
\end{array}
\]
******** Explicit midpoint method (RK2)
The (explicit) midpoint method is a second-order method with two stages (see also the implicit midpoint method below):
\[
\begin{array}{c|cc}
0 & 0 & 0 \\
1/2 & 1/2 & 0 \\
\hline
 & 0 & 1 \\
\end{array}
\]
******** Heun's method (RK2)
Heun's method is a second-order method with two stages. It is also known as the explicit trapezoid rule, improved Euler's method, or modified Euler's method:
\[
\begin{array}{c|cc}
0 & 0 & 0 \\
1 & 1 & 0 \\
\hline
 & 1/2 & 1/2 \\
\end{array}
\]
******** Ralston's method (RK2)
Ralston's method is a second-order method with two stages and a minimum local error bound:
\[
\begin{array}{c|cc}
0 & 0 & 0 \\
2/3 & 2/3 & 0 \\
\hline
 & 1/4 & 3/4 \\
\end{array}
\]
with the corresponding equations
\begin{align*}
k_1 & =f\left(t_n, y_n\right) \\
k_2 & =f\left(t_n+\frac{2}{3} h, y_n+\frac{2}{3} h k_1\right) \\
y_{n+1} & =y_n+h\left(\frac{1}{4} k_1+\frac{3}{4} k_2\right)
\end{align*}
Let us solve the initial-value problem
\begin{align*}
\mathrm{D}_t x(t) = \tan (x) + 1, \quad x_0 = 1, t \in[1,1.1]
\end{align*}
with step size \(h=0.025\), so the method needs to take four steps.
The method proceeds as follows:
\begin{align*}
& t_0=1: \\
&\qquad \qquad x_0=1 \\
& t_1=1.025: \\
&\qquad \qquad x_0=1 \quad k_1=2.557407725 \quad k_2=f\left(t_0+\frac{2}{3} h, y_0+\frac{2}{3} h k_1\right)=2.7138981400
 \\
& x_1=y_0+h\left(\frac{1}{4} k_1+\frac{3}{4} k_2\right)=\underline{1.066869388} \\
& t_2 = 1.050: \\
&\qquad \qquad x_1= 1.066869388 \quad k_1=2.813524695 \quad k_2=f\left(t_1+\frac{2}{3} h, y_1+\frac{2}{3} h k_1\right) \\
& x_2=y_1+h\left(\frac{1}{4} k_1+\frac{3}{4} k_2\right)=\underline{1.141332181
} \\
\end{align*}
******** Kutta's method
\[
\begin{array}{c|cccc}
0 & 0 & 0 & 0 & 0 \\
1/2 & 1/2 & 0 & 0 & 0 \\
1 & -1 & 2 & 0 & 0 \\
\hline
 & 1/6 & 2/3 & 1/6 & 0 \\
\end{array}
\]
******** Huen's third order method (RK3)
\[
\begin{array}{c|cccc}
0 & 0 & 0 & 0 & 0 \\
1/3 & 1/3 & 0 & 0 & 0 \\
2/3 & 0 & 2/3 & 0 & 0 \\
\hline
 & 1/4 & 0 & 3/4 & 0 \\
\end{array}
\]
******** Van der Houwen's/Wray method (RK3)
\[
\begin{array}{c|cccc}
0 & 0 & 0 & 0 & 0 \\
8/15 & 8/15 & 0 & 0 & 0 \\
2/3 & 1/4 & 5/12 & 0 & 0 \\
\hline
 & 1/4 & 0 & 3/4 & 0 \\
\end{array}
\]
******** Ralston's third order method (RK3)
Ralston's third-order method is used in the BogackiShampine method (an adaptive Runge-Kutta method).
\[
\begin{array}{c|ccc}
0 & 0 & 0 & 0 \\
1/2 & 1/2 & 0 & 0 \\
3/4 & 0 & 3/4 & 0 \\
\hline
 & 2/9 & 1/3 & 4/9 \\
\end{array}
\]
******** Strong stability preserving Runge-Kutta method (SSPRK3)
\[
\begin{array}{c|ccc}
0 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
1/2 & 1/4 & 1/4 & 0 \\
\hline
 & 1/6 & 1/6 & 2/3 \\
\end{array}
\]
******** Classical Runge-Kutta method (RK4)
The "original" RungeKutta method.
\[
\begin{array}{c|cccc}
0 & 0 & 0 & 0 & 0 \\
1/2 & 1/2 & 0 & 0 & 0 \\
1/2 & 0 & 1/2 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 \\
\hline
 & 1/6 & 1/3 & 1/3 & 1/6 \\
\end{array}
\]
Recall that a Runge-Kutta method of order \(s\) can be written as:
\[
x_{t+h} = x_t + h \cdot \sum_{i=1}^s a_i k_i + \mathcal{O}\left(h^{s+1}\right)
\]
where:
\[
k_i = \sum_{j=1}^s \beta_{ij} f\left(t_n+\alpha_i h, k_j\right)
\]
are the increments obtained by evaluating the derivatives of \(x_t\) at the \(i\)-th order. For the Runge-Kutta fourth-order method, using \(s=4\) and evaluating at the starting point, midpoint, and endpoint of any interval \((t, t+h)\), we choose:
\[
\begin{array}{cc}
\alpha_i & \beta_{ij} \\
\alpha_1 = 0 & \beta_{21} = \frac{1}{2} \\
\alpha_2 = \frac{1}{2} & \beta_{32} = \frac{1}{2} \\
\alpha_3 = \frac{1}{2} & \beta_{43} = 1 \\
\alpha_4 = 1 &
\end{array}
\]
with \(\beta_{ij} = 0\) otherwise.
We begin by defining the following quantities:
\begin{align*}
& x_{t+h}^1 = x_t + h f(t, x_t) \\
& x_{t+h}^2 = x_t + h f\left(t+\frac{h}{2}, x_{t+h / 2}^1\right) \\
& x_{t+h}^3 = x_t + h f\left(t+\frac{h}{2}, x_{t+h / 2}^2\right)
\end{align*}
where \(x_{t+h / 2}^1 = \frac{x_t + x_{t+h}^1}{2}\) and \(x_{t+h / 2}^2 = \frac{x_t + x_{t+h}^2}{2}\). Defining:
\begin{align*}
& k_1 = f(t, x_t) \\
& k_2 = f\left(t+\frac{h}{2}, x_{t+h / 2}^1\right) = f\left(t+\frac{h}{2}, x_t + \frac{h}{2} k_1\right) \\
& k_3 = f\left(t+\frac{h}{2}, x_{t+h / 2}^2\right) = f\left(t+\frac{h}{2}, x_t + \frac{h}{2} k_2\right) \\
& k_4 = f\left(t+h, x_{t+h}^3\right) = f\left(t+h, x_t + h k_3\right)
\end{align*}
we can show that the following equalities hold up to \(\mathcal{O}\left(h^2\right)\):
\begin{align*}
k_2 & = f\left(t+\frac{h}{2}, x_{t+h / 2}^1\right) = f(t, x_t) + \frac{h}{2} D_t f(t, x_t) \\
k_3 & = f\left(t+\frac{h}{2}, x_{t+h / 2}^2\right) = f(t, x_t) + \frac{h}{2} D_t\left[f(t, x_t) + \frac{h}{2} D_t f(t, x_t)\right] \\
k_4 & = f\left(t+h, x_{t+h}^3\right) = f(t, x_t) + h D_t\left[f(t, x_t) + \frac{h}{2} D_t\left[f(t, x_t) + \frac{h}{2} D_t f(t, x_t)\right]\right]
\end{align*}
where:
\[
D_t f(t, x_t) = \frac{\partial}{\partial x} f(t, x_t) \dot{x}_t + \frac{\partial}{\partial t} f(t, x_t) = f_x(t, x_t) \dot{x}_t + f_t(t, x_t) := \ddot{x}_t
\]
is the total derivative of \(f\) with respect to time.
Expressing the general formula using the derived quantities, we obtain:
\begin{align*}
x_{t+h} = & x_t + h\left\{a \cdot f(t, x_t) + b \cdot\left[f(t, x_t) + \frac{h}{2} D_t f(t, x_t)\right] + c \cdot\left[f(t, x_t) + \frac{h}{2} D_t\left[f(t, x_t) + \frac{h}{2} D_t f(t, x_t)\right]\right] + \right. \\
& \left. + d \cdot\left[f(t, x_t) + h D_t\left[f(t, x_t) + \frac{h}{2} D_t\left[f(t, x_t) + \frac{h}{2} D_t f(t, x_t)\right]\right]\right]\right\} + \mathcal{O}\left(h^5\right)
\end{align*}
and comparing this with the Taylor series expansion of \(x_{t+h}\) around \(t\), we obtain a system of constraints on the coefficients, which when solved gives \(a = \frac{1}{6}, b = \frac{1}{3}, c = \frac{1}{3}, d = \frac{1}{6}\) as stated above.
******** 3/8-rule Runge-Kutta method (RK4)
This method called the 3/8-rule doesn't have as much notoriety as the "classic" method, but is just as classic because it was proposed in the same paper (Kutta, 1901). The primary advantage this method has is that almost all of the error coefficients are smaller than in the popular method, but it requires slightly more FLOPs (floating-point operations) per time step. Its Butcher tableau is 
\[
\begin{array}{c|cccc}
0 & 0 & 0 & 0 & 0 \\
1/3 & 1/3 & 0 & 0 & 0 \\
2/3 & -1/3 & 1 & 0 & 0 \\
1 & 1 & -1 & 1 & 0 \\
\hline
 & 1/8 & 3/8 & 3/8 & 1/8 \\
\end{array}
\]
******** Ralston's fourth order method (RK4)
This fourth order method has minimum truncation error.
\[
\begin{array}{c|cccccc}
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\frac{2}{5} & \frac{2}{5} & 0 & 0 & 0 & 0 & 0 \\
\frac{14 - 3\sqrt{5}}{16} & \frac{14 - 3\sqrt{5}}{16} & \frac{2\sqrt{5}}{5} & 0 & 0 & 0 & 0 \\
\frac{14 - 3\sqrt{5}}{16} & -\frac{2,889 + 1,428\sqrt{5}}{1,024} & \frac{3,785 - 1,620\sqrt{5}}{1,024} & 0 & 0 & 0 & 0 \\
1 & \frac{-3,365 + 2,094\sqrt{5}}{6,040} & -\frac{975 - 3,046\sqrt{5}}{2,552} & \frac{467,040 + 203,968\sqrt{5}}{240,845} & 0 & 0 & 0 \\
\hline
 & \frac{263 - 24\sqrt{5}}{1,812} & \frac{125 - 1,000\sqrt{5}}{3,828} & \frac{3,426,304 + 1,661,952\sqrt{5}}{5,924,787} & \frac{30 - 4\sqrt{5}}{123} & 0 & 0 \\
\end{array}
\]
****** Adaptive Runge-Kutta methods
Adaptive methods are designed to produce an estimate of the local truncation error of a single Runge-Kutta step. This is done by having two methods, one with order \(p\) and one with order \(p-1\). These methods are interwoven, i.e., they have common intermediate steps. Thanks to this, estimating the error has little or negligible computational cost compared to a step with the higher-order method.
During the integration, the step size is adapted such that the estimated error stays below a user-defined threshold: If the error is too high, a step is repeated with a lower step size; if the error is much smaller, the step size is increased to save time. This results in an (almost) optimal step size, which saves computation time. Moreover, the user does not have to spend time on finding an appropriate step size.
The lower-order step is given by
\begin{align*}
x_{n+1}^* = x_n + h \sum_{i=1}^s b_i^* k_i
\end{align*}
where \(k_i\) are the same as for the higher-order method. Then the error is
\begin{align*}
e_{n+1} = x_{n+1} - x_{n+1}^* = h \sum_{i=1}^s \left(b_i - b_i^*\right) k_i,
\end{align*}
which is \(O(h^p)\). The Butcher tableau for this kind of method is extended to give the values of \(b_i^{\ast}\):
\[
\begin{array}{c|ccccc}
0 &  &  &  &  &  \\
c_{2} & a_{21} &  &  &  &  \\
c_{3} & a_{31} & a_{32} &  &  &  \\
\vdots & \vdots & \vdots & \ddots &  &  \\
c_{s} & a_{s1} & a_{s2} & \cdots & a_{s,s-1} &  \\
\hline
 & b_{1} & b_{2} & \cdots & b_{s-1} & b_{s} \\
 & b_{1}^{\ast} & b_{2}^{\ast} & \cdots & b_{s-1}^{\ast} & b_{s}^{\ast}
\end{array}
\]
******* Examples
******** Heun-Euler method (RK1 and RK2)
The /Heun-Euler method/ has two methods of order 1 and 2. Its extended Butcher tableau is:
\[
\begin{array}{c|cc}
0 &  &  \\
1 & 1 &  \\
\hline
1/2 & 1/2 & 1/2 \\
1 & 1 & 0 \\
\end{array}
\]
******** Fehlberg method (RK1(2))
The Fehlberg method has two methods of orders 1 and 2. Its extended Butcher Tableau is:
\[
\begin{array}{c|cc}
0 &  &  \\
1/2 & 1/2 &  \\
1 & 1/256 & 255/256 \\
\hline
 & 1/512 & 255/256 & 1/512 \\
 & 1/256 & 255/256 & 0 \\
\end{array}
\]
The first row of \(b\) coefficients gives the second-order accurate solution, and the second row has order one.
******** BogackiShampine method (RK2 and RK3)
The /Bogacki-Shampine method/ has two methods of orders 2 and 3. Its extended Butcher Tableau is:
\begin{tabular}{l|llll}
0 & & & & \\
\(1 / 2\) & \(1 / 2\) & & & \\
\(3 / 4\) & 0 & \(3 / 4\) & & \\
1 & \(2 / 9\) & \(1 / 3\) & \(4 / 9\) & \\
\hline & \(2 / 9\) & \(1 / 3\) & \(4 / 9\) & 0 \\
& \(7 / 24\) & \(1 / 4\) & \(1 / 3\) & \(1 / 8\)
\end{tabular}
The first row of \(b\) coefficients gives the third-order accurate solution, and the second row has order two.
******** RungeKuttaFehlberg method (RKF45)
The /RungeKuttaFehlberg method/ has two methods of order 4 and 5. Its extended Butcher tableau is:
\[
\begin{array}{c|ccccc}
0 &  &  &  &  &  \\
1/4 & 1/4 &  &  &  &  \\
3/8 & 3/32 & 9/32 &  &  &  \\
12/13 & 1932/2197 & -7200/2197 & 7296/2197 &  &  \\
1 & 439/216 & -8 & 3680/513 & -845/4104 &  \\
1/2 & -8/27 & 2 & -3544/2565 & 1859/4104 & -11/40 \\
\hline
 & 16/135 & 0 & 6656/12825 & 28561/56430 & -9/50 & 2/55 \\
 & 25/216 & 0 & 1408/2565 & 2197/4104 & -1/5 & 0 \\
\end{array}
\]
******** CashKarp method (RK4 and RK5)
The /Cash-Karp method/ has two methods of order 4 and 5. Its extended Butcher tableau is:
\[
\begin{array}{l|llllll}
0 & & & & & \\
1 / 5 & 1 / 5 & & & & & \\
3 / 10 & 3 / 40 & 9 / 40 & & & & \\
3 / 5 & 3 / 10 & -9 / 10 & 6 / 5 & & & \\
1 & -11 / 54 & 5 / 2 & -70 / 27 & 35 / 27 & & \\
7 / 8 & 1631 / 55296 & 175 / 512 & 575 / 13824 & 44275 / 110592 & 253 / 4096 & \\
\hline & 37 / 378 & 0 & 250 / 621 & 125 / 594 & 0 & 512 / 1771 \\
& 2825 / 27648 & 0 & 18575 / 48384 & 13525 / 55296 & 277 / 14336 & 1 / 4
\end{array}
\]
******** DormandPrince method (RK4 and RK5)
The /Dormand-Prince method/ has two methods of order 4 and 5. Its extended Butcher tableau is:
\begin{tabular}{l|llllllll}
0 & & & & & & \\
\(1 / 5\) & \(1 / 5\) & & & & & \\
\(3 / 10\) & \(3 / 40\) & \(9 / 40\) & & & & & \\
\(4 / 5\) & \(44 / 45\) & \(-56 / 15\) & \(32 / 9\) & & & & \\
\(8 / 9\) & \(19372 / 6561\) & \(-25360 / 2187\) & \(64448 / 6561\) & \(-212 / 729\) & & & \\
1 & \(9017 / 3168\) & \(-355 / 33\) & \(46732 / 5247\) & \(49 / 176\) & \(-5103 / 18656\) & & \\
1 & \(35 / 384\) & 0 & \(500 / 1113\) & \(125 / 192\) & \(-2187 / 6784\) & \(11 / 84\) & \\
\hline & \(35 / 384\) & 0 & \(500 / 1113\) & \(125 / 192\) & \(-2187 / 6784\) & \(11 / 84\) & 0 \\
& \(5179 / 57600\) & 0 & \(7571 / 16695\) & \(393 / 640\) & \(-92097 / 339200\) & \(187 / 2100\) & \(1 / 40\)
\end{tabular}
The first row of \(b\) coefficients gives the fifth-order accurate solution, and the second row gives the fourth-order accurate solution.
****** Implicit Runge-Kutta methods
All Runge-Kutta methods mentioned up to now are explicit methods. Explicit Runge-Kutta methods are generally unsuitable for the solution of stiff equations because their region of absolute stability is small; in particular, it is bounded. This issue is especially important in the solution of partial differential equations.
The instability of explicit Runge-Kutta methods motivates the development of implicit methods. An implicit Runge-Kutta method has the form
\begin{align*}
y_{n+1}=y_n+h \sum_{i=1}^s b_i k_i
\end{align*}
where
\begin{align*}
k_i=f\left(t_n+c_i h, y_n+h \sum_{j=1}^s a_{i j} k_j\right), \quad i=1, \ldots, s .{ }^{[25]}
\end{align*}
The difference with an explicit method is that in an explicit method, the sum over \(j\) only goes up to \(i-1\). This also shows up in the Butcher tableau: the coefficient matrix \(a_{i j}\) of an explicit method is lower triangular. In an implicit method, the sum over \(j\) goes up to \(s\) and the coefficient matrix is not triangular, yielding a Butcher tableau of the form \({ }^{[18]}\)
\begin{tabular}{c|cccc|c}
\(c_1\) & \(a_{11}\) & \(a_{12}\) & \(\ldots\) & \(a_{1 s}\) \\
\(c_2\) & \(a_{21}\) & \(a_{22}\) & \(\ldots\) & \(a_{2 s}\) \\
\(\vdots\) & \(\vdots\) & \(\vdots\) & \(\ddots\) & \(\vdots\) \\
\(c_s\) & \(a_{s 1}\) & \(a_{s 2}\) & \(\ldots\) & \(a_{s s}\) \\
\hline & \(b_1\) & \(b_2\) & \(\ldots\) & \(b_s\) \\
& \(b_1^*\) & \(b_2^*\) & \(\ldots\) & \(b_s^*\)
\end{tabular}
See Adaptive Runge-Kutta methods above for the explanation of the \(b^{\ast}\) row.
The consequence of this difference is that at every step, a system of algebraic equations has to be solved. This increases the computational cost considerably. If a method with \(s\) stages is used to solve a differential equation with \(m\) components, then the system of algebraic equations has \(m s\) components. This can be contrasted with implicit linear multistep methods (the other big family of methods for ODEs): an implicit \(s\)-step linear multistep method needs to solve a system of algebraic equations with only \(m\) components, so the size of the system does not increase as the number of steps increases.
******* Examples
******** Implicit Euler's method (RK1)
\section*{Backward Euler}
The (implicit) Euler method is first order. Unconditionally stable and non-oscillatory for linear diffusion problems.
\[
\begin{array}{c|c}
1 & 1 \\
\hline
 & 1 \\
\end{array}
\]
******** Implicit midpoint method (RK2)
The implicit midpoint method is of second order. It is the simplest method in the class of collocation methods known as the Gauss-Legendre methods. It is a symplectic integrator.
\[
\begin{array}{c|c}
1/2 & 1/2 \\
\hline
 & 1 \\
\end{array}
\]
******** Crank-Nicolson method
The Crank-Nicolson method corresponds to the implicit trapezoidal rule and is a second-order accurate and A-stable method.
\[
\begin{array}{c|cc}
0 & 0 & 0 \\
1 & 1/2 & 1/2 \\
\hline
 & 1/2 & 1/2 \\
\end{array}
\]
******** Gauss-Legendre methods
These methods are based on the points of GaussLegendre quadrature. The GaussLegendre method of order four has Butcher tableau:
\[
\begin{array}{c|cc}
1/2 - \sqrt{3}/6 & 1/4 & 1/4 - \sqrt{3}/6 \\
1/2 + \sqrt{3}/6 & 1/4 + \sqrt{3}/6 & 1/4 \\
\hline
 & 1/2 & 1/2 \\
\end{array}
\]
The GaussLegendre method of order six has Butcher tableau:
\[
\begin{array}{c|ccc}
1/2 - \sqrt{15}/10 & 5/36 & 2/9 - \sqrt{15}/15 & 5/36 - \sqrt{15}/30 \\
1/2 & 5/36 + \sqrt{15}/24 & 2/9 & 5/36 - \sqrt{15}/24 \\
1/2 + \sqrt{15}/10 & 5/36 + \sqrt{15}/30 & 2/9 + \sqrt{15}/15 & 5/36 \\
\hline
 & 5/18 & 8/18 & 5/18 \\
\end{array}
\]
******** Diagonally implicit Runge-Kutta methods
Diagonally Implicit Runge-Kutta (DIRK) formulae have been widely used for the numerical solution of stiff initial value problems; the advantage of this approach is that here the solution may be found sequentially as opposed to simultaneously. The (implicit) midpoint method is the simplest method from this class.
********* Kraaijevanger and Spijker's method
\[
\begin{array}{c|ccc}
1/2 & 1/2 & 0 & 0 \\
3/2 & -1/2 & 2 & 0 \\
\hline
 & -1/2 & 3/2 & 0 \\
\end{array}
\]
********* Pareschi and Russo's method
\[
\begin{array}{c|ccc}
x & x & 0 & 0 \\
1 - x & 1 - 2x & x & 0 \\
\hline
 & 1/2 & 1/2 & 0 \\
\end{array}
\]
This diagonally implicit Runge-Kutta method is A-stable if and only if \(x \geq \frac{1}{4}\). Moreover, this method is L-stability if and only if \(x\) equals one of the roots of the polynomial \(x^2 - 2x + \frac{1}{2}\), i.e. if \(x = 1 \pm \frac{\sqrt{2}}{2}\).
********* Qin and Zhang's method
Qin and Zhang's method corresponds to Pareschi and Russo's method with \(x = 1/4\). It is a symplectic integrator.
\[
\begin{array}{c|ccc}
1/4 & 1/4 & 0 & 0 \\
3/4 & 1/2 & 1/4 & 0 \\
\hline
 & 1/2 & 1/2 & 0 \\
\end{array}
\]
********* Two-stage 2nd order diagonally implicit Runge-Kutta method
\[
\begin{array}{c|cc}
x & x & 0 \\
1 & 1 - x & x \\
\hline
 & 1 - x & x \\
\end{array}
\]
Again, this diagonally implicit Runge-Kutta method is A-stable if and only if \(x \geq \frac{1}{4}\). As the previous method, this method is again L-stable if and only if \(x\) equals one of the roots of the polynomial \(x^2 - 2x + \frac{1}{2}\), i.e. if \(x = 1 \pm \frac{\sqrt{2}}{2}\).
********* Crouzeix's two-stage method (RK3)
Crouzeix's two-stage method is an order 3 diagonally implicit Runge-Kutta method.
\[
\begin{array}{c|ccc}
1/2 + \frac{\sqrt{3}}{6} & 1/2 & 1/2 - \frac{\sqrt{3}}{6} & 0 \\
1/2 - \frac{\sqrt{3}}{6} & \frac{\sqrt{3}}{6} & 1/2 & 0 \\
\hline
 & 1/2 & 1/2 & 0 \\
\end{array}
\]
********* Crouzeix's three-stage method (RK4)
Crouzeix's three-stage method is an order 4 diagonally implicit Runge-Kutta method.
\[
\begin{array}{c|ccc}
\frac{1 + \alpha}{2} & \frac{1 + \alpha}{2} & 0 & 0 \\
\alpha & \frac{\alpha}{2} & \frac{1 + \alpha}{2} & 0 \\
1 - \alpha & 1 + \alpha - \frac{1 + 2\alpha}{3\alpha^2} & -\frac{1 + \alpha}{3\alpha^2} & \frac{1 + \alpha}{2} \\
\hline
 & \frac{1}{6\alpha^2} & \frac{1}{3(1 - 2\alpha)} & \frac{1 + \alpha}{2} \\
\end{array}
\]
with \(\alpha = \frac{2}{\sqrt{3}} \cos \frac{\pi}{18}\).
********* Three-stage 3rd order L-stable diagonally implicit Runge-Kutta method
\[
\begin{array}{c|ccc}
x & x & 0 & 0 \\
1 + x & 1 - x & x & 0 \\
1 & -3x^2/2 + 4x - 1/4 & 3x^2/2 - 5x + 5/4 & x \\
\hline
 & -3x^2/2 + 4x - 1/4 & 3x^2/2 - 5x + 5/4 & x \\
\end{array}
\]
with \(x = 0.4358665215\).
********* Nrsett's three-stage method
Nrsett's three-stage method is an order 4 diagonally implicit Runge-Kutta method that is L-stable.
\[
\begin{array}{c|ccc}
x & x & 0 & 0 \\
1/2 & 1/2 - x & x & 0 \\
1 - x & 2x & 1 - 4x & x \\
\hline
 & \frac{1}{6(1 - 2x)^2} & \frac{3(1 - 2x)}{2} & \frac{1}{6(1 - 2x)^2} \\
\end{array}
\]
with \(x\) one of the three roots of the cubic equation \(x^3 - 3x^2/2 + x/2 - 1/24 = 0\). The three roots of this cubic equation are approximately \(x_1 = 1.06858\), \(x_2 = 0.30254\), and \(x_3 = 0.12889\). The root \(x_1\) gives the best stability properties for initial value problems.
********* Four-stage 3rd order L-stable diagonally implicit Runge-Kutta method
\[
\begin{array}{c|cccc}
1/2 & 1/2 & 0 & 0 & 0 \\
2/3 & 1/6 & 1/2 & 0 & 0 \\
1/2 & -1/2 & 1/2 & 1/2 & 0 \\
1 & 3/2 & -3/2 & 1/2 & 1/2 \\
\hline
 & 3/2 & -3/2 & 1/2 & 1/2 \\
\end{array}
\]
******** Lobatto methods
There are three main families of Lobatto methods, called IIIA, IIIB and IIIC (in classical mathematical literature, the symbols I and II are reserved for two types of Radau methods). These are named after Rehuel Lobatto as a reference to the Lobatto quadrature rule, but were introduced by Byron L. Ehle in his thesis. All are implicit methods, have order \(2s - 2\) and they all have \(c_1 = 0\) and \(c_s = 1\). Unlike any explicit method, it's possible for these methods to have the order greater than the number of stages. Lobatto lived before the classic fourth-order method was popularized by Runge and Kutta.
********* Lobatto IIIA methods
The Lobatto IIIA methods are collocation methods. The second-order method is known as the trapezoidal rule:
\[
\begin{array}{c|cc}
0 & 0 & 0 \\
1 & 1/2 & 1/2 \\
\hline
 & 1/2 & 1/2 \\
\end{array}
\]
The fourth-order method is given by
\[
\begin{array}{c|cccc}
0 & 0 & 0 & 0 & 0 \\
1/2 & 5/24 & 1/3 & -1/24 & 0 \\
1 & 1/6 & 2/3 & 1/6 & 0 \\
\hline
 & 1/6 & 2/3 & 1/6 & 0 \\
 & -1/2 & 2 & -1/2 & 0 \\
\end{array}
\]
These methods are A-stable, but not L-stable and B-stable.
********* Lobatto IIIB methods
The Lobatto IIIB methods are not collocation methods, but they can be viewed as discontinuous collocation methods. The second-order method is given by
\[
\begin{array}{c|cc}
0 & 1/2 & 0 \\
1 & 1/2 & 0 \\
\hline
 & 1/2 & 1/2 \\
\end{array}
\]
The fourth-order method is given by
\[
\begin{array}{c|ccc}
0 & 1/6 & -1/6 & 0 \\
1/2 & 1/6 & 1/3 & 0 \\
1 & 1/6 & 5/6 & 0 \\
\hline
 & 1/6 & 2/3 & 1/6 \\
 & -1/2 & 2 & -1/2 \\
\end{array}
\]
Lobatto IIIB methods are A-stable, but not L-stable and B-stable.
********* Lobatto IIIC methods
The Lobatto IIIC methods also are discontinuous collocation methods. The second-order method is given by
\[
\begin{array}{c|cc}
0 & 1/2 & -1/2 \\
1 & 1/2 & 1/2 \\
\hline
 & 1/2 & 1/2 \\
\end{array}
\]
The fourth-order method is given by
\[
\begin{array}{c|ccc}
0 & 1/6 & -1/3 & 1/6 \\
1/2 & 1/6 & 5/12 & -1/12 \\
1 & 1/6 & 2/3 & 1/6 \\
\hline
 & 1/6 & 2/3 & 1/6 \\
 & -1/2 & 2 & -1/2 \\
\end{array}
\]
They are L-stable. They are also algebraically stable and thus B-stable, that makes them suitable for stiff problems.
********* Lobatto IIIC* methods
The Lobatto IIIC* methods are also known as Lobatto III methods (Butcher, 2008), Butcher's Lobatto methods (Hairer et al., 1993), and Lobatto IIIC methods (Sun, 2000) in the literature. The second-order method is given by
\[
\begin{array}{c|ccc}
0 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 \\
\hline
 & 1/2 & 1/2 & 0 \\
\end{array}
\]
Butcher's three-stage, fourth-order method is given by
\[
\begin{array}{c|ccc}
0 & 0 & 0 & 0 \\
1/2 & 1/4 & 1/4 & 0 \\
1 & 0 & 1 & 0 \\
\hline
 & 1/6 & 2/3 & 1/6 \\
\end{array}
\]
These methods are not A-stable, B-stable or L-stable. The Lobatto IIIC* method for \(s = 2\) is sometimes called the explicit trapezoidal rule.
********* Generalized Lobatto methods
One can consider a very general family of methods with three real parameters \((\alpha_A, \alpha_B, \alpha_C)\) by considering Lobatto coefficients of the form
\[
a_{ij}(\alpha_A, \alpha_B, \alpha_C) = \alpha_A a_{ij}^A + \alpha_B a_{ij}^B + \alpha_C a_{ij}^C + \alpha_{C*} a_{ij}^{C*},
\]
where 
\[
\alpha_{C*} = 1 - \alpha_A - \alpha_B - \alpha_C.
\]
For example, Lobatto IIID family introduced in (Nrsett and Wanner, 1981), also called Lobatto IIINW, are given by
\[
\begin{array}{c|cc}
0 & 1/2 & 1/2 \\
1 & -1/2 & 1/2 \\
\hline
 & 1/2 & 1/2 \\
\end{array}
\]
and
\[
\begin{array}{c|ccc}
0 & 1/6 & 0 & -1/6 \\
1/2 & 1/12 & 5/12 & 0 \\
1 & 1/6 & 2/3 & 1/6 \\
\hline
 & 1/6 & 2/3 & 1/6 \\
 & -1/2 & 2 & -1/2 \\
\end{array}
\]
These methods correspond to \(\alpha_A = 2\), \(\alpha_B = 2\), \(\alpha_C = -1\), and \(\alpha_{C*} = -2\). The methods are L-stable. They are algebraically stable and thus B-stable.
******** Randau methods
Radau methods are fully implicit methods (matrix \(A\) of such methods can have any structure). Radau methods attain order \(2s - 1\) for \(s\) stages. Radau methods are A-stable, but expensive to implement. Also they can suffer from order reduction. The first order Radau method is similar to backward Euler method.
********* Randau IA methods
The third-order method is given by
\[
\begin{array}{c|ccc}
0 & 1/4 & -1/4 & 0 \\
2/3 & 1/4 & 5/12 & 0 \\
\hline
 & 1/4 & 3/4 & 0 \\
\end{array}
\]
The fifth-order method is given by
\[
\begin{array}{c|cccc}
0 & 1/9 & \frac{-1 - \sqrt{6}}{18} & \frac{-1 + \sqrt{6}}{18} & 0 \\
\frac{3}{5} - \frac{\sqrt{6}}{10} & \frac{11}{45} & \frac{11 + \sqrt{6}}{45} & \frac{11 - \sqrt{6}}{45} & \frac{-43 + \sqrt{6}}{360} \\
\frac{3}{5} + \frac{\sqrt{6}}{10} & \frac{11}{45} & \frac{43 - \sqrt{6}}{45} & \frac{11 + \sqrt{6}}{45} & \frac{11 + \sqrt{6}}{45} \\
\hline
 & \frac{4}{9} & \frac{4 + \sqrt{6}}{9} & \frac{4 - \sqrt{6}}{9} & \frac{\sqrt{6}}{36} \\
\end{array}
\]
********* Randau IIA methods
The \(c_i\) of this method are zeros of 
\[
\frac{d^s - 1}{dx^s - 1}(x^s - 1(x - 1)^s).
\]
The third-order method is given by
\[
\begin{array}{c|ccc}
1/3 & 5/12 & -1/12 & 0 \\
1 & 3/4 & 1/4 & 0 \\
\hline
 & 3/4 & 1/4 & 0 \\
\end{array}
\]
The fifth-order method is given by
\[
\begin{array}{c|cccc}
\frac{2 - \sqrt{6}}{10} & \frac{11}{45} & \frac{7\sqrt{6}}{360} & \frac{37}{225} & \frac{169\sqrt{6}}{1800} \\
\frac{2 + \sqrt{6}}{10} & \frac{169\sqrt{6}}{1800} & \frac{11}{45} & \frac{7\sqrt{6}}{360} & \frac{2\sqrt{6}}{225} \\
1 & \frac{4}{9} & \frac{\sqrt{6}}{36} & \frac{4}{9} & \frac{\sqrt{6}}{36} \\
\hline
 & \frac{4}{9} & \frac{\sqrt{6}}{36} & \frac{4}{9} & \frac{\sqrt{6}}{36} \\
 & \frac{4}{9} & \frac{\sqrt{6}}{36} & \frac{4}{9} & \frac{\sqrt{6}}{36} \\
\end{array}
\]
***** Errors
***** Numerical stability
****** A stability
The advantage of implicit Runge-Kutta methods over explicit ones is their greater stability, especially when applied to stiff equations. Consider the linear test equation \(y^{\prime}=\lambda y\). A Runge-Kutta method applied to this equation reduces to the iteration \(y_{n+1}=r(h \lambda) y_n\), with \(r\) given by
\begin{align*}
r(z)=1+z b^T(I-z A)^{-1} e=\frac{\operatorname{det}\left(I-z A+z e b^T\right)}{\operatorname{det}(I-z A)}
\end{align*}
where \(e\) stands for the vector of ones. The function \(r\) is called the stability function. \({ }^{30]}\) It follows from the formula that \(r\) is the quotient of two polynomials of degree \(s\) if the method has \(s\) stages. Explicit methods have a strictly lower triangular matrix \(A\), which implies that \(\operatorname{det}(I-z A)=1\) and that the stability function is a polynomial. \({ }^{[31]}\)
The numerical solution to the linear test equation decays to zero if \(|r(z)|<1\) with \(z=h \lambda\). The set of such \(z\) is called the domain of absolute stability. In particular, the method is said to be absolute stable if all \(z\) with \(\operatorname{Re}(z)<0\) are in the domain of absolute stability. The stability function of an explicit Runge-Kutta method is a polynomial, so explicit Runge-Kutta methods can never be A-stable. \({ }^{[31]}\)
If the method has order \(p\), then the stability function satisfies \(r(z)=\mathrm{e}^z+O\left(z^{p+1}\right)\) as \(z \rightarrow 0\). Thus, it is of interest to study quotients of polynomials of given degrees that approximate the exponential function the best. These are known as Pad approximants. A Pad approximant with numerator of degree \(m\) and denominator of degree \(n\) is A-stable if and only if \(m \leq n\) \(\leq m+2\).
The Gauss-Legendre method with \(s\) stages has order \(2 s\), so its stability function is the Pad approximant with \(m=n=s\). It follows that the method is A-stable. \({ }^{[33]}\) This shows that A-stable Runge-Kutta can have arbitrarily high order. In contrast, the order of A-stable linear multistep methods cannot exceed two. [34]
****** B stability
The \(A\)-stability concept for the solution of differential equations is related to the linear autonomous equation \(y^{\prime}=\lambda y\). Dahlquist proposed the investigation of stability of numerical schemes when applied to nonlinear systems that satisfy a monotonicity condition. The corresponding concepts were defined as G-stability for multistep methods (and the related oneleg methods) and B-stability (Butcher, 1975) for Runge-Kutta methods. A Runge-Kutta method applied to the non-linear system \(y^{\prime}=f(y)\), which verifies \(\langle f(y)-f(z), y-z\rangle \leq 0\), is called \(B\)-stable, if this condition implies \(\left\|y_{n+1}-z_{n+1}\right\| \leq\left\|y_n-z_n\right\|\) for two numerical solutions.
Let \(B, M\) and \(Q\) be three \(s \times s\) matrices defined by
\begin{align*}
B=\operatorname{diag}\left(b_1, b_2, \ldots, b_s\right), M=B A+A^T B-b b^T, Q=B A^{-1}+A^{-T} B-A^{-T} b b^T A^{-1} .
\end{align*}
A Runge-Kutta method is said to be algebraically stable if the matrices \(B\) and \(M\) are both non-negative definite. A sufficient condition for \(B\)-stability is: \(B\) and \(Q\) are non-negative definite.
****** L stability
Within mathematics regarding differential equations, L-stability is a special case of A-stability, a property of Runge-Kutta methods for solving ordinary differential equations. A method is L-stable if it is A-stable and \(\phi(z) \rightarrow 0\) as \(z \rightarrow \infty\), where \(\phi\) is the stability function of the method (the stability function of a Runge-Kutta method is a rational function and thus the limit as \(z \rightarrow+\infty\) is the same as the limit as \(z \rightarrow-\infty\) ). L-stable methods are in general very good at integrating stiff equations.
**** Leapfrog method
***** Formulation
Recall the /central difference/ FDA where for an IVP \(\mathrm{D}_t x(t) = f(t, x(t)), \quad x\left(0\right)=x_0\)  where \(f\) is a function \(f:\left[0, \infty\right) \times \mathbb{R} \rightarrow \mathbb{R}\), and the initial condition \(x_0 \in \mathbb{R}\), we obtained
\[
x(t+h) = x(t) + h \, f(t + h/2,\,x(t + h/2))  + \mathcal{O} (h^{3}).
\]
Since \(x(t + h/2)\) is unknown, we approximate it using either the forward Euler's method
\[
x(t + h/2) = x(t) + (h/2) f(t, \, x(t)) + \mathcal{O}(h^2).
\]
or the backward Euler's method
\[
x(t + h/2) = x(t+h) - (h/2) f(t+h, \, x(t+h)) + \mathcal{O}(h^2).
\]
Note that: 
1) the first-order Euler march takes steps of size \(h/2\),
2) the second-order midpoint march takes steps of size \(h\), 
3) a given point along the forward (backward) Euler march \(x(t + h/2)\) depends on the point \(x(t)\) (\(x(t+h)\)) of the midpoint march, and 
4) the midpoint march leaps over the most recent point of the Euler march.
We obtain a new method by leaving the midpoint march unchanged and modifying the backward Euler march so that it takes steps of size \(h\). The equation for the backward Euler's method for \(x(t + h/2)\) transforms as \(h /2 \to h\):
\[
x(t + h) = x(t+2h) - h f(t+2h, \, x(t+2h)) + \mathcal{O}(h^2).
\]
Recall that the Euler march was introduced because \(x(t + h/2)\) is unknown. So we obtain \(x (t + h/2\) on the left hand side we transform \(t \to t - h/2\)
\[
x(t + h/2) = x(t+3h/2) - h f(t+h, \, x(t+h)) + \mathcal{O}(h^2),
\]
which after re-arranging yields
\[
x(t+3h/2)  = x(t + h/2) + h f(t+h, \, x(t+h)) + \mathcal{O}(h^2),
\]
Notice that we are now left with a forward Euler march where \(x(t+3h/2)\) depends on the point \(x(t+h/2)\) of its own march and the most-recent point \(x(t+h)\) of the midpoint march. Further notice that now the Euler march too leaps over the most recent point of the midpoint march. Since both marches now leap over each other in lockstep, this method is named the /leapfrog method/:
\[
\boxed{
x(t+h) \simeq x(t) + h \, f(t + h/2,\,x(t + h/2)), \quad x(t+3h/2) \simeq x(t+h/2) + h \, f(t + h,\,x(t+h)).
}
\]
Note that apart from \(x(0)\), we also need an initial value \(x(h/2)\) to unroll this recurrence. \(x(h/2)\) can either be specified by the IVP problem or we can approximate it by using a half step Euler march \(x(h/2) = x(0) + (h/2) f(0,\, x(0))\). Thereafter, a leapfrog march using the above equations can be unfolded. When \( h \) is only allowed to assume discrete values and terms that are \( \mathcal{O} (h^{3}) \) are dropped from the right hand side, we are left with
\[
\boxed{
x_{n+1} = x_{n} + h f(t_{n} + h/2, \, x_{n + 1/2} ), \quad x_{n + 3h/2} = x_{n + h/2} + h f(t_{n} + h, \, x_{n+1}).
}
\]
Apart from \(x_{0}\), we will need \(x_{h/2}\) which we can approximate as \(x_{0} + (h/2) f(0,\,x_{0})\).
***** Time-reversal symmetry
The leapfrog method is /time-reversal symmetric/ i.e.,
#+NAME: Time-reversal symmetry
#+begin_definition latex
Suppose \(x: \mathbb{R} \to \mathbb{R}\) is a differentiable function. An ordinary differential equation (ODE) for \(x(t)\) is said to exhibit time-reversal symmetry if the following conditions are met:
\[
\mathrm{D}_t x(0^{+}) = -\mathrm{D}_t x(0^{-}), \quad \text{and} \quad \mathrm{D}_t x(t) = -\mathrm{D}_t x(-t), \quad \forall t \neq 0.
\]
These conditions imply that \(x(t) = x(-t)\) for all \(t\), meaning the function \(x\) is symmetric about \(t = 0\).
#+end_definition
With \(h \to -h\), the leapfrog equations become
\[
x(t-h) \simeq x(t) - h \, f(t - h/2,\,x(t - h/2)), \quad x(t-3h/2) \simeq x(t-h/2) - h \, f(t - h,\,x(t-h)).
\]
Translating \(x(t)\) as \(t \to t + 3h/2\)
\[
x(t+h/2) \simeq x(t+3h/2) - h \, f(t+h ,\,x(t+h)), \quad x(t) \simeq x(t+h) - h \, f(t+h/2,\,x(t+h/2)).
\]
These equations give \(x(t)\) and \(x(t+h/2)\) in terms of \(x(t + 3h/2)\) and \(x(t + h)\) and \(x(t + 3h/2)\) but we can rearrange them to obtain the original leapfrog method equations
\[
x(t+h) \simeq x(t) + h \, f(t + h/2,\,x(t + h/2)), \quad x(t+3h/2) \simeq x(t+h/2) + h \, f(t + h,\,x(t+h)).
\]
Thus the leapfrog integrator is time-reversal symmetric.
method will give you a solution in either the forward or backward direction, but the solutions will not agree exactly, in general, even after you allow for rounding error.
Why is time-reversal symmetry important? It turns out that it has a couple of useful implications. One concerns the conservation of energy.
Consider as an illustration the frictionless nonlinear pendulum, which we studied in Example 8.6. The motion of the pendulum is given by Eqs. (8.45) and (8.46), which read
\begin{align*}
\frac{\mathrm{d} \theta}{\mathrm{d} t}=\omega, \quad \frac{\mathrm{d} \omega}{\mathrm{d} t}=-\frac{g}{\ell} \sin \theta .
\end{align*}
If we solve these equations using a Runge-Kutta method we can get a pretty good solution, as shown in Fig. 8.8 on page 361, but it is nonetheless only approximate, as nearly all computer calculations are. Among other things, this means that the total energy of the system, kinetic plus potential, is only approximately constant during the calculation. A frictionless pendulum should have constant energy, but the Runge-Kutta method isn't perfect and energies calculated using it tend to fluctuate and drift slightly over time. The top panel of Fig. 8.10 shows results from a solution of the equations above using the second-order Runge-Kutta method and the drift of the total energy with time is clearly visible. (We have deliberately used the less accurate second-order method in this case to make the drift larger and easier to see. With the fourthorder Runge-Kutta method, which is more accurate, the drift would be significantly smaller, though it would still be there.)
Now suppose we solve the same differential equations using the leapfrog method. Imagine doing so for one full swing of the pendulum. The pendulum starts at the furthest limit of its swing, swings all the way across, then all the way back again. In real life, the total energy of the system must remain constant throughout the motion, and in particular it must be the same when the pendulum returns to its initial point as it was when it started out. Our solution using the leapfrog method, on the other hand, is only approximate, so it's possible the energy might drift. Let us suppose for the sake of argument that it drifts upward, as it did for the Runge-Kutta method in the top panel of Fig. 8.10, so that its value at the end of the swing is slightly higher than at the beginning.
Now let us calculate the pendulum's motion once again, still using the leapfrog method but this time in reverse, starting at the end of the swing and solving backwards, with minus the step size that we used in our forward calculation. As we have shown, when we run the leapfrog method backwards in this fashion it will retrace its steps and end up exactly at the starting point of the motion again (apart from rounding error). Thus, if the energy increased during the forward calculation it must decrease when we do things in reverse.
But here's the thing. The physics of the pendulum is itself time-reversal symmetric. The motion of swinging across and back, the motion that the pendulum makes in a single period, is exactly the same backwards as it is forwards. Hence, when we perform the backward solution we are solving for the exact same motion and moreover doing it using the exact same method (since we are using the leapfrog method in both directions). This means that the values of the variables \(\theta\) and \(\omega\) will be exactly the same at each successive step of the solution in the reverse direction as they were going forward. Hence, if the energy increased during the forward solution it must also increase during the backward one.
Now we have a contradiction. We have shown that if the energy increases during the forward calculation then it must both decrease and increase during the backward one. Clearly this is impossible-it cannot do both-and hence we conclude that it cannot have increased during the forward calculation. An analogous argument shows it cannot decrease either, so the only remaining possibility is that it stays the same. In other words, the leapfrog method conserves energy. The total energy of the system will stay constant over time when we solve the equations using the leapfrog method, except for any small changes introduced by rounding error.
There are a couple of caveats. First, even though the energy is conserved we should not make the mistake of assuming this means our solution for the motion is exact. It isn't. The leapfrog method only gives approximate solutions for differential equations-as discussed in Section 8.5.1 the method is only accurate to second order on each step and has a third-order error. So the values we get for the angle \(\theta\) for our pendulum, for example, will not be exactly correct, even though the energy is constant.
Second, the argument we have given applies to a full swing of the pendulum. It tells us that the energy at the end of a full swing will be the same as it was at the beginning. It does not tell us that the energy will be conserved throughout the swing, and indeed, as we will see, it is not. The energy may fluctuate during the course of the pendulum swing, but it will always come back to the correct value at the end of the swing. More generally, if the leapfrog method is used to solve equations of motion for any periodic system, such as a pendulum or a planet orbiting a star, then energy will be conserved over any full period of the system (or many full periods), but it will not, in general, be conserved over fractions of a period.
If we can live with these limitations, however, the leapfrog method can be useful for solving the equations of motion of energy conserving physical systems over long periods of time. If we wait long enough, a solution using a Runge-Kutta method will drift in energy-the pendulum might run down and stop swinging, or the planet might fall out of orbit and into its star. But a solution using the leapfrog method will run forever.
As an example look again at Fig. 8.10. The bottom panel shows the total energy of the nonlinear pendulum calculated using the leapfrog method and we can see that indeed it is constant on average over long periods of timemany swings of the pendulum-even though it oscillates over the course of individual swings. As the figure shows, the accuracy of the energy in the short term is actually poorer than the second-order Runge-Kutta method (notice that the vertical scales are different in the two panels), but in the long term the leapfrog method will be far better than the Runge-Kutta method, as the latter drifts further and further from the true value of the energy.
****** Global truncation error
**** Verlet method
Suppose, as in the previous section, that we are using the leapfrog method to solve the classical equations of motion for a physical system. Such equations, derived from Newton's second law \(F=m a\), take the form of second-order differential equations
\begin{align*}
\frac{\mathrm{d}^2 x}{\mathrm{~d} t^2}=f(x, t),
\end{align*}
or the vector equivalent when there is more than one dependent variable. Examples include the motion of projectiles, the pendulum of the previous section, and the cometary orbit of Exercise 8.10. As we have seen, we can convert such equations of motion into coupled first-order equations
\begin{align*}
\frac{\mathrm{d} x}{\mathrm{~d} t}=v, \quad \frac{\mathrm{d} v}{\mathrm{~d} t}=f(x, t),
\end{align*}
where we use the variable name \(v\) here as a reminder that, when we are talking about equations of motion, the quantity it represents is a velocity (or sometimes an angular velocity, as in the case of the pendulum).
If we want to apply the leapfrog method to these equations the normal strategy would be to define a vector \(\mathbf{r}=(x, v)\), combine the two equations (8.68) into a single vector equation
\begin{align*}
\frac{\mathrm{d} \mathbf{r}}{\mathrm{d} t}=\mathrm{f}(\mathrm{r}, t),
\end{align*}
and then solve this equation for \(r\) using the leapfrog method.
Rather than going this route, however, let us instead write out the leapfrog method in full, as applied to (8.68). If we are given the value of \(x\) at some time \(t\) and the value of \(v\) at time \(t+\frac{1}{2} h\) then, applying the method, the value of \(x\) a time interval \(h\) later is
\begin{align*}
x(t+h)=x(t)+h v\left(t+\frac{1}{2} h\right) .
\end{align*}
And the value of \(v\) an interval \(h\) later is
\begin{align*}
v\left(t+\frac{3}{2} h\right)=v\left(t+\frac{1}{2} h\right)+h f(x(t+h), t+h) .
\end{align*}
We can derive a full solution to the problem by using just these two equations repeatedly, as many times as we wish. Notice that the equations involve the 
value of \(x\) only at time \(t\) plus integer multiples of \(h\) and the value of \(v\) only at half-integer multiples. We never need to calculate \(v\) at any of the integer points or \(x\) at the half integers. This is an improvement over the normal leapfrog method applied to the vector \(\mathbf{r}=(x, v)\), which would involve solving for both \(x\) and \(v\) at all points, integer and half-integer. Equations (8.70) and (8.71) require only half as much work to evaluate as the full leapfrog method.
This simplification works only for equations of motion or other differential equations that have the special structure of Eq. (8.68), where the right-hand side of the first equation depends on \(v\) but not \(x\) and the right-hand side of the second equation depends on \(x\) but not \(v\). Many physics problems, however, boil down to solving equations of motion, so the method is widely applicable.
A minor problem with the method arises if we want to calculate some quantity that depends on both \(x\) and \(v\), such as a the total energy of the system. Potential energy depends on position \(x\) while kinetic energy depends on velocity \(v\), so calculating the total energy, potential plus kinetic, at any time \(t\), requires us to know the values of both variables at that time. Unfortunately we know \(x\) only at the integer points and \(v\) only at the half-integer points, so we never know both at the same time.
But there's an easy solution to this problem. We can calculate the velocity at the integer points by doing an additional half step as follows. If we did know \(v(t+h)\) then we could calculate \(v\left(t+\frac{1}{2} h\right)\) from it by doing a half step backwards using Euler's method. That is, we would do Euler's method with a step size of \(-\frac{1}{2} h\) :
\begin{align*}
v\left(t+\frac{1}{2} h\right)=v(t+h)-\frac{1}{2} h f(x(t+h), t+h) .
\end{align*}
Alternatively, by rearranging this equation we can calculate \(v(t+h)\) from \(v(t+\) \(\left.\frac{1}{2} h\right)\) like this:
\begin{align*}
v(t+h)=v\left(t+\frac{1}{2} h\right)+\frac{1}{2} h f(x(t+h), t+h) .
\end{align*}
This equation gives us \(v\) at integer steps in terms of quantities we already know from our leapfrog calculation, allowing us to calculate total energy (or any other quantity) at each step.
A complete calculation combines Eqs. (8.70) and (8.71) with Eq. (8.73), plus an initial half step at the very beginning to get everything started. Putting it all together, here's what we have.
We are given the initial values of \(x\) and \(v\) at some time \(t\). Then
\begin{align*}
v\left(t+\frac{1}{2} h\right)=v(t)+\frac{1}{2} h f(x(t), t) .
\end{align*}
Then subsequent values of \(x\) and \(v\) are derived by repeatedly applying
\begin{align*}
x(t+h) & =x(t)+h v\left(t+\frac{1}{2} h\right), \\
k & =h f(x(t+h), t+h), \\
v(t+h) & =v\left(t+\frac{1}{2} h\right)+\frac{1}{2} k, \\
v\left(t+\frac{3}{2} h\right) & =v\left(t+\frac{1}{2} h\right)+k .
\end{align*}
This variant of the leapfrog method is called the Verlet method after physicist Loup Verlet, who discovered it in the 1960s (although it was known to others long before that, as far back as the eighteenth century).
The method can be easily extended to equations of motion in more than one dimension. If we wish to solve an equation of motion of the form
\begin{align*}
\frac{d^2 r}{d t^2}=f(r, t),
\end{align*}
where \(\mathrm{r}=(x, y, \ldots)\) is a \(d\)-dimensional vector, then, given initial conditions on \(\mathbf{r}\) and the velocity \(\mathbf{v}=\mathrm{dr} / \mathrm{d} t\), the appropriate generalization of the Verlet method involves first performing a half step to calculate \(\mathbf{v}\left(t+\frac{1}{2} h\right)\) :
\begin{align*}
\mathbf{v}\left(t+\frac{1}{2} h\right)=\mathbf{v}(t)+\frac{1}{2} h \mathbf{f}(\mathbf{r}(t), t),
\end{align*}
then repeatedly applying the equations
\begin{align*}
\mathbf{r}(t+h) & =\mathbf{r}(t)+h \mathbf{v}\left(t+\frac{1}{2} h\right), \\
\mathbf{k} & =h \mathbf{f}(\mathbf{r}(t+h), t \mathbf{t} h), \\
\mathbf{v}(t+h) & =\mathbf{v}\left(t+\frac{1}{2} h\right)+\frac{1}{2} \mathbf{k}, \\
\mathbf{v}\left(t+\frac{3}{2} h\right) & =\mathbf{v}\left(t+\frac{1}{2} h\right)+\mathbf{k} .
\end{align*}
**** Gragg's method
The leapfrog integrator is a modification of the midpoint method (central difference approximation) which is a second-order method with local truncation error is order \(\mathcal{O}(h^{3})\). A consequence of time reversal symmetry of the leapfrog integrator is that the change in solution going backwards is exactly the reverse of the change going forwards; in other words the backwards local truncation error \(\epsilon_{l} (-h)\) is \(\epsilon_{l} (-h) = - \epsilon_{l} (h)\). It would normally immediately follows that \(\epsilon_{l} (h)\) is an odd function of \(h\), i.e., 
\[
\epsilon_{l} (h) = c_{3} h^{3} + c_{5} h^{5} + c_{7} h^{7} + \cdots
\]
but the initial Euler half-step for obtaining \(x(h/2)\) has \(\epsilon_{l} (h)\) that is order \(\mathcal{O}(h^{2})\). This one extra step at the start of the march prevents us from claiming that \(\epsilon_{l} (h)\) for the leapfrog integrator is an odd function of \(h\). A way out of this quagmire is to use /Gragg's method/.
Suppose we wish to solve our differential equation from some initial time \(t\) forward to a later time \(t+H\) (where \(H\) is not necessarily small) using \(n\) leapfrog steps of size \(h=H / n\) each. A rewrite of the leapfrog integrator equations is
\begin{align*}
& x_0=x(t), \\
& y_1=x_0+\frac{1}{2} h f (t, \, x_0). \tag{1}
\end{align*}
which yield
\begin{align*}
& x_1=x_0+h f (t+\frac{1}{2} h, y_{1}), \\
& y_2=y_1+h f (t+h,\, x_{1}), \\
& x_2=x_1+h f (t+\frac{3}{2} h,\, y_{2}),
\end{align*}
and so forth. The variables \(x_m\) here represent the solution at integer multiples of \(h\) and the variables \(y_m\) at half-integer multiples. In general, we have
\begin{align*}
& y_{m+1}=y_m+h f(t+m h, \, x_m), \\
& x_{m+1}=x_m+h f(t+(m+\frac{1}{2}) h, \, y_{m+1}). \tag{2}
\end{align*}
The last two points in the solution are \(y_n=x (t+H-\frac{1}{2} h)\) and \(x_n=x(t+H)\). We have \(x(t+H\) = x_n\). However, we also have \(x(t+H)=y_n+\frac{1}{2} h f(t+H,\, x_{n})\). Averaging the two we get
\begin{align*}
x(t+H)=\frac{1}{2} [x_n+y_n+\frac{1}{2} h f (t+H,\,x_n)]. \tag{3}
\end{align*}
It turns out that if we calculate \(x(t+H)\) from this equation then the odd-order error terms that arise from the Euler's method step at the start of the leapfrog calculation cancel out, giving a local truncation error \(\epsilon_{l} (h)\) on (3) that now contains only odd powers of \(h\). This is called the Gragg's method or the modified midpoint method. This method combines the leapfrog method in the form of Eqs. (1) and (2) with Eq. (3) to make an estimate of \(x(t + H)\) that carries a leading order local truncation error \(\epsilon_l (h)\) of order \(\mathcal{O} (h^{3})\) and higher-order terms containing odd powers of \(h\) only.
Given a step size \(h\), it takes \(\Delta / h\) steps to cover an interval of size \(\Delta\). If each step is associated with a local truncation error of \(\epsilon_{l} (h)\), then the global truncation error \(\epsilon_t (h)\) is order \(\mathcal{O} (\epsilon_l (h) \times \Delta / h) \), i.e., one order of \(h\) lower than \(\epsilon_{l} (h)\). Thus, the global truncation error \(\epsilon_t (h) \) for a leapfrog integrator constructed using Gragg's method must be an even function of \(h\), i.e.,
\[
\epsilon_{t} (h) = c_{2} h^{2} + c_{5} h^{4} + c_{7} h^{6} + \cdots
\]
This property makes the leapfrog integrator a candidate for Richardson extrapolation and forms the basis for the /Bulirsch-Stoer method/.
**** Bulirsch-Stoer method
*** Boundary value problems (BVPs)
**** Shooting method
**** Relaxation method
**** Equilibrium method
*** Eigenvalue problems
**** Shooting method
**** Numerov's method
* Research
** TODO Question 2
2) *Recall your memory of the concept of perfectly reconstructible vectors. Does it in any way point towards a link between the modes of a distribution and representability?*
** TODO Question 3
3) *Does a mixture of product equal to a product of mixtures?*
** TODO Question 4
4) *Hierarchical models as marginals of hierarchical models.*
** TODO Question 5
5) *How does having a lot of minima imply better representability? Does the complexity give you the model capacity? What does it represent?*
** TODO Question 6
6) *Does your research work have any similarity with the research work in the field of Neural Architecture Search? If yes, what? If no, how does it differ?*
** TODO Question 9
9) *Read up on the theorem by Bengio and Delalleau.*
** TODO Question 12
12) *What are some examples from physics literature, where models like the RBM and the DBM have been put to use? Say for studying phase transitions, or learning compressed representation of Hubbard model quantum wave functions?*
** TODO Question 19
18) *Say I have the inherent structures of some configuration space? How can I use this information to draw conclusions about the basin of attraction around it? Can I use it to obtain the entropy of the system?*
