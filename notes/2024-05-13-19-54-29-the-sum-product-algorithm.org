:PROPERTIES:
:ID:       2ffed678-7405-484d-abc9-57879f6ca317
:END:
#+TITLE: The sum-product algorithm
#+FILETAGS: :literature:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

We shall now make use of the factor graph framework to derive a powerful class of efficient, exact inference algorithms that are applicable to tree-structured graphs. Here we shall focus on the problem of evaluating local marginals over nodes or subsets of nodes, which will lead us to the sum-product algorithm. Later we shall modify the technique to allow the most probable state to be found, giving rise to the max-sum algorithm.

Also we shall suppose that all of the variables in the model are discrete, and so marginalization corresponds to performing sums. The framework, however, is equally applicable to linear-Gaussian models in which case marginalization involves integration, and we shall consider an example of this in detail when we discuss linear dynamical systems.

There is an algorithm for exact inference on directed graphs without loops known as belief propagation, and is equivalent to a special case of the sum-product algorithm. Here we shall consider only the sum-product algorithm because it is simpler to derive and to apply, as well as being more general.

We shall assume that the original graph is an undirected tree or a directed tree or polytree, so that the corresponding factor graph has a tree structure. We first convert the original graph into a factor graph so that we can deal with both directed and undirected models using the same framework. Our goal is to exploit the structure of the graph to achieve two things: (i) to obtain an efficient, exact inference algorithm for finding marginals; (ii) in situations where several marginals are required to allow computations to be shared efficiently.

We begin by considering the problem of finding the marginal \(p(x)\) for particular variable node \(x\). For the moment, we shall suppose that all of the variables are hidden. Later we shall see how to modify the algorithm to incorporate evidence corresponding to observed variables. By definition, the marginal is obtained by summing the joint distribution over all variables except \(x\) so that

\begin{align*}
p(x)=\sum_{\mathbf{x} \backslash x} p(\mathbf{x})
\end{align*}

where \(\mathbf{x} \backslash x\) denotes the set of variables in \(\mathbf{x}\) with variable \(x\) omitted. The idea is to substitute for \(p(\mathbf{x})\) using the factor graph expression (8.59) and then interchange summations and products in order to obtain an efficient algorithm. Consider the fragment of graph shown in Figure 8.46 in which we see that the tree structure of the graph allows us to partition the factors in the joint distribution into groups, with one group associated with each of the factor nodes that is a neighbour of the variable node \(x\). We see that the joint distribution can be written as a product of the form

\begin{align*}
p(\mathbf{x})=\prod_{s \in \operatorname{ne}(x)} F_s\left(x, X_s\right)
\end{align*}

ne \((x)\) denotes the set of factor nodes that are neighbours of \(x\), and \(X_s\) denotes the set of all variables in the subtree connected to the variable node \(x\) via the factor node\(f_s\), and \(F_s\left(x, X_s\right)\) represents the product of all the factors in the group associated with factor \(f_s\).

Substituting (8.62) into (8.61) and interchanging the sums and products, we obtain

\begin{align*}
p(x) & =\prod_{s \in \operatorname{ne}(x)}\left[\sum_{X_{s}} F_{s}\left(x, X_{s}\right)\right] \\
& =\prod_{s \in \operatorname{ne}(x)} \mu_{f_{s} \rightarrow x}(x) .
\end{align*}

Here we have introduced a set of functions \(\mu_{f_{s} \rightarrow x}(x)\), defined by

\[
\mu_{f_{s} \rightarrow x}(x) \equiv \sum_{X_{s}} F_{s}\left(x, X_{s}\right)
\]

which can be viewed as /messages/ from the factor nodes \(f_{s}\) to the variable node \(x\). We see that the required marginal \(p(x)\) is given by the product of all the incoming messages arriving at node \(x\).

In order to evaluate these messages, we again turn to 

#+ATTR_HTML: :width 400px
[[file:~/.local/images/prml-8-46.png]]

and note that each factor \(F_{s}\left(x, X_{s}\right)\) is described by a factor (sub-)graph and so can itself be factorized. In particular, we can write

\[
F_{s}\left(x, X_{s}\right)=f_{s}\left(x, x_{1}, \ldots, x_{M}\right) G_{1}\left(x_{1}, X_{s 1}\right) \ldots G_{M}\left(x_{M}, X_{s M}\right)
\]

where, for convenience, we have denoted the variables associated with factor \(f_{x}\), in addition to \(x\), by \(x_{1}, \ldots, x_{M}\). This factorization is illustrated in the figure below

Note that the set of variables \(\left\{x, x_{1}, \ldots, x_{M}\right\}\) is the set of variables on which the factor \(f_{s}\) depends, and so it can also be denoted \(\mathbf{x}_{s}\), using the notation of (8.59).

Substituting (8.65) into (8.64) we obtain

\begin{align*}
\mu_{f_{s} \rightarrow x}(x) & =\sum_{x_{1}} \ldots \sum_{x_{M}} f_{s}\left(x, x_{1}, \ldots, x_{M}\right) \prod_{m \in \operatorname{ne}\left(f_{s}\right) \backslash x}\left[\sum_{X_{x m}} G_{m}\left(x_{m}, X_{s m}\right)\right] \\
& =\sum_{x_{1}} \ldots \sum_{x_{M}} f_{s}\left(x, x_{1}, \ldots, x_{M}\right) \prod_{m \in \operatorname{ne}\left(f_{s}\right) \backslash x} \mu_{x_{m} \rightarrow f_{s}}\left(x_{m}\right)
\end{align*}

Figure 8.47 Illustration of the factorization of the subgraph associated with factor node \(f_{s}\).

\begin{center}
\includegraphics[max width=\textwidth]{2023_08_27_4c5a80c0a42382702197g-421}
\end{center}

\(G_{m}\left(x_{m}, X_{s m}\right)\)

where ne \(\left(f_{s}\right)\) denotes the set of variable nodes that are neighbours of the factor node \(f_{s}\), and ne \(\left(f_{s}\right) \backslash x\) denotes the same set but with node \(x\) removed. Here we have defined the following messages from variable nodes to factor nodes

\[
\mu_{x_{m} \rightarrow f_{s}}\left(x_{m}\right) \equiv \sum_{X_{s m}} G_{m}\left(x_{m}, X_{s m}\right) .
\]

We have therefore introduced two distinct kinds of message, those that go from factor nodes to variable nodes denoted \(\mu_{f \rightarrow x}(x)\), and those that go from variable nodes to factor nodes denoted \(\mu_{x \rightarrow f}(x)\). In each case, we see that messages passed along a link are always a function of the variable associated with the variable node that link connects to.

The result (8.66) says that to evaluate the message sent by a factor node to a variable node along the link connecting them, take the product of the incoming messages along all other links coming into the factor node, multiply by the factor associated with that node, and then marginalize over all of the variables associated with the incoming messages. This is illustrated in the figure below

#+ATTR_HTML: :width 400px
[[file:~/.local/images/prml-8-47.png]]

It is important to note that a factor node can send a message to a variable node once it has received incoming messages from all other neighboring variable nodes.

Finally, we derive an expression for evaluating the messages from variable nodes to factor nodes, again by making use of the (sub-)graph factorization.

#+ATTR_HTML: :width 400px
[[file:~/.local/images/prml-8-48.png]]

From the figure above, we see that term \(G_{m}\left(x_{m}, X_{s m}\right)\) associated with node \(x_{m}\) is given by a product of terms \(F_{l}\left(x_{m}, X_{m l}\right)\) each associated with one of the factor nodes \(f_{l}\) that is linked to node \(x_{m}\) (excluding node \(f_{s}\) ), so that

\[
G_{m}\left(x_{m}, X_{s m}\right)=\prod_{l \in \operatorname{ne}\left(x_{m}\right) \backslash f_{s}} F_{l}\left(x_{m}, X_{m l}\right)
\]

where the product is taken over all neighbours of node \(x_{m}\) except for node \(f_{s}\). Note that each of the factors \(F_{l}\left(x_{m}, X_{m l}\right)\) represents a subtree of the original graph of precisely the same kind as introduced in (8.62). Substituting (8.68) into (8.67), we then obtain

\begin{align*}
\mu_{x_{m} \rightarrow f_{s}}\left(x_{m}\right) & =\prod_{l \in \operatorname{ne}\left(x_{m}\right) \backslash f_{s}}\left[\sum_{X_{m l}} F_{l}\left(x_{m}, X_{m l}\right)\right] \\
& =\prod_{l \in \operatorname{ne}\left(x_{m}\right) \backslash f_{s}} \mu_{f_{l} \rightarrow x_{m}}\left(x_{m}\right)
\end{align*}

where we have used the definition (8.64) of the messages passed from factor nodes to variable nodes. Thus to evaluate the message sent by a variable node to an adjacent factor node along the connecting link, we simply take the product of the incoming messages along all of the other links. Note that any variable node that has only two neighbors performs no computation but simply passes messages through unchanged. Also, we note that a variable node can send a message to a factor node once it has received incoming messages from all other neighboring factor nodes.

Recall that our goal is to calculate the marginal for variable node \(x\), and that this marginal is given by the product of incoming messages along all of the links arriving at that node. Each of these messages can be computed recursively in terms of other messages. In order to start this recursion, we can view the node \(x\) as the root of the tree and begin at the leaf nodes. From the definition (8.69), we see that if a leaf node is a variable node, then the message that it sends along its one and only link is given by

\[
\mu_{x \rightarrow f}(x)=1
\]

as illustrated below

#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-8-49a.png]]

Similarly, if the leaf node is a factor node, we see from (8.66) that the message sent should take the form

\[
\mu_{f \rightarrow x}(x)=f(x)
\]

as illustrated below

#+ATTR_HTML: :width 300px
[[file:~/.local/images/prml-8-49b.png]]

At this point, it is worth pausing to summarize the particular version of the sumproduct algorithm obtained so far for evaluating the marginal \(p(x)\). We start by viewing the variable node \(x\) as the root of the factor graph and initiating messages at the leaves of the graph using (8.70) and (8.71). The message passing steps (8.66) and (8.69) are then applied recursively until messages have been propagated along every link, and the root node has received messages from all of its neighbours. Each node can send a message towards the root once it has received messages from all of its other neighbours. Once the root node has received messages from all of its neighbours, the required marginal can be evaluated using (8.63). We shall illustrate this process shortly.

To see that each node will always receive enough messages to be able to send out a message, we can use a simple inductive argument as follows. Clearly, for a graph comprising a variable root node connected directly to several factor leaf nodes, the algorithm trivially involves sending messages of the form (8.71) directly from the leaves to the root. Now imagine building up a general graph by adding nodes one at a time, and suppose that for some particular graph we have a valid algorithm. When one more (variable or factor) node is added, it can be connected only by a single link because the overall graph must remain a tree, and so the new node will be a leaf node. It therefore sends a message to the node to which it is linked, which in turn will therefore receive all the messages it requires in order to send its own message towards the root, and so again we have a valid algorithm, thereby completing the proof.

Now suppose we wish to find the marginals for every variable node in the graph. This could be done by simply running the above algorithm afresh for each such node. However, this would be very wasteful as many of the required computations would be repeated. We can obtain a much more efficient procedure by 'overlaying' these multiple message passing algorithms to obtain the general sum-product algorithm as follows. Arbitrarily pick any (variable or factor) node and designate it as the root. Propagate messages from the leaves to the root as before. At this point, the root node will have received messages from all of its neighbors. It can therefore send out messages to all of its neighbors. These in turn will then have received messages from all of their neighbors and so can send out messages along the links going away from the root, and so on. In this way, messages are passed outwards from the root all the way to the leaves. By now, a message will have passed in both directions across every link in the graph, and every node will have received a message from all of its neighbors. Again a simple inductive argument can be Exercise 8.20 used to verify the validity of this message passing protocol. Because every variable node will have received messages from all of its neighbors, we can readily calculate the marginal distribution for every variable in the graph. The number of messages that have to be computed is given by twice the number of links in the graph and so involves only twice the computation involved in finding a single marginal. By comparison, if we had run the sum-product algorithm separately for each node, the amount of computation would exhibit quadratic growth with the size of the graph. Note that this algorithm is in fact independent of which node was designated as the root, and indeed the notion of one node having a special status was introduced only as a convenient way to explain the message passing protocol.

Next suppose we wish to find the marginal distributions \(p\left(\mathbf{x}_{s}\right)\) associated with the sets of variables belonging to each of the factors. By a similar argument to that used above, it is easy to see that the marginal associated with a factor is given by the product of messages arriving at the factor node and the local factor at that node

\[
p\left(\mathbf{x}_{s}\right)=f_{s}\left(\mathbf{x}_{s}\right) \prod_{i \in \operatorname{ne}\left(f_{s}\right)} \mu_{x_{i} \rightarrow f_{s}}\left(x_{i}\right)
\]

in complete analogy with the marginals at the variable nodes. If the factors are parameterized functions and we wish to learn the values of the parameters using the EM algorithm, then these marginals are precisely the quantities we will need to calculate in the E step, as we shall see in detail when we discuss the hidden Markov model in Chapter 13.

The message sent by a variable node to a factor node, as we have seen, is simply the product of the incoming messages on other links. We can if we wish view the sum-product algorithm in a slightly different form by eliminating messages from variable nodes to factor nodes and simply considering messages that are sent out by factor nodes. This is most easily seen by considering the example in Figure 8.50.

So far, we have rather neglected the issue of normalization. If the factor graph was derived from a directed graph, then the joint distribution is already correctly normalized, and so the marginals obtained by the sum-product algorithm will similarly be normalized correctly. However, if we started from an undirected graph, then in general there will be an unknown normalization coefficient \(1 / Z\). As with the simple chain example of Figure 8.38, this is easily handled by working with an unnormalized version \(\widetilde{p}(\mathbf{x})\) of the joint distribution, where \(p(\mathbf{x})=\widetilde{p}(\mathbf{x}) / Z\). We first run the sum-product algorithm to find the corresponding unnormalized marginals \(\widetilde{p}\left(x_{i}\right)\). The coefficient \(1 / Z\) is then easily obtained by normalizing any one of these marginals, and this is computationally efficient because the normalization is done over a single variable rather than over the entire set of variables as would be required to normalize \(\widetilde{p}(\mathbf{x})\) directly.

At this point, it may be helpful to consider a simple example to illustrate the operation of the sum-product algorithm.

#+ATTR_HTML: :width 400px
[[file:~/.local/images/prml-8-51.png]]

 The figure above shows a simple 4-node factor graph whose unnormalized joint distribution is given by

\[
\widetilde{p}(\mathbf{x})=f_{a}\left(x_{1}, x_{2}\right) f_{b}\left(x_{2}, x_{3}\right) f_{c}\left(x_{2}, x_{4}\right) .
\]

In order to apply the sum-product algorithm to this graph, let us designate node \(x_{3}\) as the root, in which case there are two leaf nodes \(x_{1}\) and \(x_{4}\). Starting with the leaf nodes, we then have the following sequence of six messages

\begin{align*}
\mu_{x_{1} \rightarrow f_{a}}\left(x_{1}\right) & =1 \\
\mu_{f_{a} \rightarrow x_{2}}\left(x_{2}\right) & =\sum_{x_{1}} f_{a}\left(x_{1}, x_{2}\right) \\
\mu_{x_{4} \rightarrow f_{c}}\left(x_{4}\right) & =1 \\
\mu_{f_{c} \rightarrow x_{2}}\left(x_{2}\right) & =\sum_{x_{4}} f_{c}\left(x_{2}, x_{4}\right) \\
\mu_{x_{2} \rightarrow f_{b}}\left(x_{2}\right) & =\mu_{f_{a} \rightarrow x_{2}}\left(x_{2}\right) \mu_{f_{c} \rightarrow x_{2}}\left(x_{2}\right) \\
\mu_{f_{b} \rightarrow x_{3}}\left(x_{3}\right) & =\sum_{x_{2}} f_{b}\left(x_{2}, x_{3}\right) \mu_{x_{2} \rightarrow f_{b}} .
\end{align*}

The direction of flow of these messages is illustrated in Figure 8.52. Once this message propagation is complete, we can then propagate messages from the root node out to the leaf nodes, and these are given by

\begin{align*}
\mu_{x_{3} \rightarrow f_{b}}\left(x_{3}\right) & =1 \\
\mu_{f_{b} \rightarrow x_{2}}\left(x_{2}\right) & =\sum_{x_{3}} f_{b}\left(x_{2}, x_{3}\right) \\
\mu_{x_{2} \rightarrow f_{a}}\left(x_{2}\right) & =\mu_{f_{b} \rightarrow x_{2}}\left(x_{2}\right) \mu_{f_{c} \rightarrow x_{2}}\left(x_{2}\right) \\
\mu_{f_{a} \rightarrow x_{1}}\left(x_{1}\right) & =\sum_{x_{2}} f_{a}\left(x_{1}, x_{2}\right) \mu_{x_{2} \rightarrow f_{a}}\left(x_{2}\right) \\
\mu_{x_{2} \rightarrow f_{c}}\left(x_{2}\right) & =\mu_{f_{a} \rightarrow x_{2}}\left(x_{2}\right) \mu_{f_{b} \rightarrow x_{2}}\left(x_{2}\right) \\
\mu_{f_{c} \rightarrow x_{4}}\left(x_{4}\right) & =\sum_{x_{2}} f_{c}\left(x_{2}, x_{4}\right) \mu_{x_{2} \rightarrow f_{c}}\left(x_{2}\right) .
\end{align*}

#+ATTR_HTML: :width 400px
[[file:~/.local/images/prml-8-51.png]]

Flow of messages for the sum-product algorithm applied to the example graph above.

#+ATTR_HTML: :width 400px
[[file:~/.local/images/prml-8-52a.png]]

From the leaf nodes \(x_{1}\) and \(x_{4}\) towards the root node \(x_{3}\). 

#+ATTR_HTML: :width 400px
[[file:~/.local/images/prml-8-52b.png]]

From the root node towards the leaf nodes.

One message has now passed in each direction across each link, and we can now evaluate the marginals. As a simple check, let us verify that the marginal \(p\left(x_{2}\right)\) is given by the correct expression. Using (8.63) and substituting for the messages using the above results, we have

\begin{align*}
\tilde{p}\left(x_{2}\right) & =\mu_{f_{a} \rightarrow x_{2}}\left(x_{2}\right) \mu_{f_{b} \rightarrow x_{2}}\left(x_{2}\right) \mu_{f_{c} \rightarrow x_{2}}\left(x_{2}\right) \\
& =\left[\sum_{x_{1}} f_{a}\left(x_{1}, x_{2}\right)\right]\left[\sum_{x_{3}} f_{b}\left(x_{2}, x_{3}\right)\right]\left[\sum_{x_{4}} f_{c}\left(x_{2}, x_{4}\right)\right] \\
& =\sum_{x_{1}} \sum_{x_{2}} \sum_{x_{4}} f_{a}\left(x_{1}, x_{2}\right) f_{b}\left(x_{2}, x_{3}\right) f_{c}\left(x_{2}, x_{4}\right) \\
& =\sum_{x_{1}} \sum_{x_{3}} \sum_{x_{4}} \tilde{p}(\mathbf{x})
\end{align*}

as required.

So far, we have assumed that all of the variables in the graph are hidden. In most practical applications, a subset of the variables will be observed, and we wish to calculate posterior distributions conditioned on these observations. Observed nodes are easily handled within the sum-product algorithm as follows. Suppose we partition \(\mathbf{x}\) into hidden variables \(\mathbf{h}\) and observed variables \(\mathbf{v}\), and that the observed value of \(\mathbf{v}\) is denoted \(\widehat{\mathbf{v}}\). Then we simply multiply the joint distribution \(p(\mathbf{x})\) by \(\prod_{i} I\left(v_{i}, \widehat{v}_{i}\right)\), where \(I(v, \widehat{v})=1\) if \(v=\widehat{v}\) and \(I(v, \widehat{v})=0\) otherwise. This product corresponds to \(p(\mathbf{h}, \mathbf{v}=\widehat{\mathbf{v}})\) and hence is an unnormalized version of \(p(\mathbf{h} \mid \mathbf{v}=\widehat{\mathbf{v}})\). By running the sum-product algorithm, we can efficiently calculate the posterior marginals \(p\left(h_{i} \mid \mathbf{v}=\widehat{\mathbf{v}}\right)\) up to a normalization coefficient whose value can be found efficiently using a local computation. Any summations over variables in \(\mathbf{v}\) then collapse into a single term.

\begin{center}
\begin{tabular}{c|cc}
 & \(x=0\) & \(x=1\) \\
\hline
\(y=0\) & 0.3 & 0.4 \\
\(y=1\) & 0.3 & 0.0 \\
\end{tabular}
\end{center}

We have assumed throughout this section that we are dealing with discrete variables. However, there is nothing specific to discrete variables either in the graphical framework or in the probabilistic construction of the sum-product algorithm. For continuous variables the summations are simply replaced by integrals. We shall give an example of the sum-product algorithm applied to a graph of linear-Gaussian variables when we consider linear dynamical systems.


