:PROPERTIES:
:ID:       748061c5-f975-4773-90e4-3a5b35ceb8cd
:END:
#+TITLE: Linear models for classification
#+FILETAGS: :literature:prml:hub:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

The goal in classification is to take an input vector \(\mathbf{x}\) and to assign it to one of \(K\) discrete classes \(\mathcal{C}_{k}\) where \(k=1, \ldots, K\). In the most common scenario, the classes are taken to be disjoint, so that each input is assigned to one and only one class. The input space is thereby divided into *decision regions* whose boundaries are called *decision boundaries* or *decision surfaces*. Models in which the decision surfaces are linear functions of the input vector \(\mathbf{x}\) are called /linear models for classification/. In linear models for classification, the decision boundaries are defined by \((D-1)\)-dimensional hyperplanes within the \(D\)-dimensional input space. Data sets whose classes can be separated exactly by linear decision surfaces are said to be linearly separable.

[[id:43aa39fd-b16a-429b-b59a-408240ae3523][Discriminant functions]]
[[id:623b54a5-2318-4774-922b-b0e5e42959da][Probabilistic generative models]]
[[id:7ba4082f-5e4b-416b-915c-d42ea674f52c][Probabilistic discriminative models]]
[[id:fe110473-ca15-4c42-ad93-f45a4ea7c4a2][The Laplace approximation]]
[[id:d4e1b57b-5547-45aa-b134-f32593415c23][Bayesian logistic regression]]

* Target variable representation
There are various ways of using target values to represent class labels:
1) For \( K = 2 \) classes, the most convenient representation is the binary representation in which there is a single target variable \(t \in\{0,1\}\) such that \(t=1\) represents class \(\mathcal{C}_{1}\) and \(t=0\) represents class \(\mathcal{C}_{2}\). 
2) For \(K>2\) classes, it is convenient to use a 1-of-\(K\) coding scheme in which \(\mathbf{t}\) is a vector of length \(K\) such that if the class is \(\mathcal{C}_{j}\), then all elements \(t_{k}\) of \(\mathbf{t}\) are zero except element \(t_{j}\), which takes the value 1. For instance, if we have \(K=5\) classes, then a pattern from class 2 would be given the target vector \(\mathbf{t}=(0,1,0,0,0)^{\mathrm{T}}\).

* Approaches to classification problems
1) construct a [[id:43aa39fd-b16a-429b-b59a-408240ae3523][discriminant function]] that directly assigns each vector \(\mathrm{x}\) to a specific class,
2) model the [[id:391465bc-1399-40f0-b049-738c1a64d6fb][conditional probability distribution]] \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)\) in an [[id:01b2eedd-71b0-428f-bc4b-146b7068af36][inference]] stage; use this distribution to make optimal decisions. This may be done in two ways:
   2.1) model the conditional probabilities \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)\) directly, by representing them as parametric models and then optimizing the parameters using a training set.
   2.2) model the class-conditional densities given by \(p\left(\mathbf{x} \mid \mathcal{C}_{k}\right)\), together with the prior probabilities \(p\left(\mathcal{C}_{k}\right)\) for the classes, and then we compute the required posterior probabilities using [[id:731eff69-04dd-45f2-b2a7-968e9c44dc3e][Bayes's theorem]]

       \begin{align*}
       p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)=\frac{p\left(\mathbf{x} \mid \mathcal{C}_{k}\right)              p\left(\mathcal{C}_{k}\right)}{p(\mathbf{x})}
      \end{align*}   
