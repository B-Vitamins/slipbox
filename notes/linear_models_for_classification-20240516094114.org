:PROPERTIES:
:ID:       748061c5-f975-4773-90e4-3a5b35ceb8cd
:END:
#+TITLE: Linear models for classification
#+FILETAGS: :literature:prml:hub:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

In the previous chapter, we explored a class of regression models having particularly simple analytical and computational properties. We now discuss an analogous class of models for solving classification problems. The goal in classification is to take an input vector \(\mathbf{x}\) and to assign it to one of \(K\) discrete classes \(\mathcal{C}_{k}\) where \(k=1, \ldots, K\). In the most common scenario, the classes are taken to be disjoint, so that each input is assigned to one and only one class. The input space is thereby divided into decision regions whose boundaries are called decision boundaries or decision surfaces. In this chapter, we consider linear models for classification, by which we mean that the decision surfaces are linear functions of the input vector \(\mathbf{x}\) and hence are defined by \((D-1)\)-dimensional hyperplanes within the \(D\)-dimensional input space. Data sets whose classes can be separated exactly by linear decision surfaces are said to be linearly separable.

For regression problems, the target variable \(\mathbf{t}\) was simply the vector of real numbers whose values we wish to predict. In the case of classification, there are various
ways of using target values to represent class labels. For probabilistic models, the most convenient, in the case of two-class problems, is the binary representation in which there is a single target variable \(t \in\{0,1\}\) such that \(t=1\) represents class \(\mathcal{C}_{1}\) and \(t=0\) represents class \(\mathcal{C}_{2}\). We can interpret the value of \(t\) as the probability that the class is \(\mathcal{C}_{1}\), with the values of probability taking only the extreme values of 0 and 1. For \(K>2\) classes, it is convenient to use a 1 -of- \(K\) coding scheme in which \(\mathbf{t}\) is a vector of length \(K\) such that if the class is \(\mathcal{C}_{j}\), then all elements \(t_{k}\) of \(\mathbf{t}\) are zero except element \(t_{j}\), which takes the value 1 . For instance, if we have \(K=5\) classes, then a pattern from class 2 would be given the target vector

\begin{align*}
\mathbf{t}=(0,1,0,0,0)^{\mathrm{T}} \tag{4.1}
\end{align*}

Again, we can interpret the value of \(t_{k}\) as the probability that the class is \(\mathcal{C}_{k}\). For nonprobabilistic models, alternative choices of target variable representation will sometimes prove convenient.

In Chapter 1, we identified three distinct approaches to the classification problem. The simplest involves constructing a discriminant function that directly assigns each vector \(\mathrm{x}\) to a specific class. A more powerful approach, however, models the conditional probability distribution \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)\) in an inference stage, and then subsequently uses this distribution to make optimal decisions. By separating inference and decision, we gain numerous benefits, as discussed in Section 1.5.4. There are two different approaches to determining the conditional probabilities \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)\). One technique is to model them directly, for example by representing them as parametric models and then optimizing the parameters using a training set. Alternatively, we can adopt a generative approach in which we model the class-conditional densities given by \(p\left(\mathbf{x} \mid \mathcal{C}_{k}\right)\), together with the prior probabilities \(p\left(\mathcal{C}_{k}\right)\) for the classes, and then we compute the required posterior probabilities using Bayes' theorem

\begin{align*}
p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)=\frac{p\left(\mathbf{x} \mid \mathcal{C}_{k}\right) p\left(\mathcal{C}_{k}\right)}{p(\mathbf{x})}
\end{align*}

We shall discuss examples of all three approaches in this chapter.

In the linear regression models considered in Chapter 3, the model prediction \(y(\mathbf{x}, \mathbf{w})\) was given by a linear function of the parameters \(\mathbf{w}\). In the simplest case, the model is also linear in the input variables and therefore takes the form \(y(\mathbf{x})=\) \(\mathbf{w}^{\mathrm{T}} \mathbf{x}+w_{0}\), so that \(y\) is a real number. For classification problems, however, we wish to predict discrete class labels, or more generally posterior probabilities that lie in the range \((0,1)\). To achieve this, we consider a generalization of this model in which we transform the linear function of \(\mathbf{w}\) using a nonlinear function \(f(\cdot)\) so that

\begin{align*}
y(\mathbf{x})=f\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}+w_{0}\right) \tag{4.3}
\end{align*}

In the machine learning literature \(f(\cdot)\) is known as an activation function, whereas its inverse is called a link function in the statistics literature. The decision surfaces correspond to \(y(\mathbf{x})=\) constant, so that \(\mathbf{w}^{\mathrm{T}} \mathbf{x}+w_{0}=\) constant and hence the decision surfaces are linear functions of \(\mathbf{x}\), even if the function \(f(\cdot)\) is nonlinear. For this reason, the class of models described by (4.3) are called generalized linear models

(McCullagh and Nelder, 1989). Note, however, that in contrast to the models used for regression, they are no longer linear in the parameters due to the presence of the nonlinear function \(f(\cdot)\). This will lead to more complex analytical and computational properties than for linear regression models. Nevertheless, these models are still relatively simple compared to the more general nonlinear models that will be studied in subsequent chapters.

The algorithms discussed in this chapter will be equally applicable if we first make a fixed nonlinear transformation of the input variables using a vector of basis functions \(\phi(\mathbf{x})\) as we did for regression models in Chapter 3. We begin by considering classification directly in the original input space \(\mathbf{x}\), while in Section 4.3 we shall find it convenient to switch to a notation involving basis functions for consistency with later chapters.

[[id:43aa39fd-b16a-429b-b59a-408240ae3523][Discriminant functions]]
[[id:623b54a5-2318-4774-922b-b0e5e42959da][Probabilistic generative models]]
[[id:7ba4082f-5e4b-416b-915c-d42ea674f52c][Probabilistic discriminative models]]
[[id:fe110473-ca15-4c42-ad93-f45a4ea7c4a2][The Laplace approximation]]
[[id:d4e1b57b-5547-45aa-b134-f32593415c23][Bayesian logistic regression]]
