:PROPERTIES:
:ID:       81a6cc3d-f3f3-4953-bf21-f808e9d9a3d9
:END:
#+TITLE: Mixtures of Gaussians (Gaussian distribution)
#+FILETAGS: :literature:prml:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

While the Gaussian distribution has some important analytical properties, it suffers from significant limitations when it comes to modelling real data sets. Consider the example shown in Figure 2.21. This is known as the 'Old Faithful' data set, and comprises 272 measurements of the eruption of the Old Faithful geyser at Yel- Figure 2.22 Example of a Gaussian mixture distribution in one dimension showing three Gaussians (each scaled by a coefficient) in blue and their sum in red.

the eruption in minutes (horizontal axis) and the time in minutes to the next eruption (vertical axis). We see that the data set forms two dominant clumps, and that a simple Gaussian distribution is unable to capture this structure, whereas a linear superposition of two Gaussians gives a better characterization of the data set.

Such superpositions, formed by taking linear combinations of more basic distributions such as Gaussians, can be formulated as probabilistic models known as mixture distributions (McLachlan and Basford, 1988; McLachlan and Peel, 2000). In Figure 2.22 we see that a linear combination of Gaussians can give rise to very complex densities. By using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.

We therefore consider a superposition of \(K\) Gaussian densities of the form

\[p(\mathbf{x})=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\]

which is called a mixture of Gaussians. Each Gaussian density \(\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\) is called a component of the mixture and has its own mean \(\boldsymbol{\mu}_{k}\) and covariance \(\boldsymbol{\Sigma}_{k}\). Contour and surface plots for a Gaussian mixture having 3 components are shown in Figure 2.23.

In this section we shall consider Gaussian components to illustrate the framework of mixture models. More generally, mixture models can comprise linear combinations of other distributions. For instance, in Section 9.3.3 we shall consider mixtures of Bernoulli distributions as an example of a mixture model for discrete variables.

The parameters \(\pi_{k}\) in (2.188) are called mixing coefficients. If we integrate both sides of (2.188) with respect to \(\mathbf{x}\), and note that both \(p(\mathbf{x})\) and the individual Gaussian components are normalized, we obtain

\[\sum_{k=1}^{K} \pi_{k}=1 .\]

Also, the requirement that \(p(\mathbf{x}) \geqslant 0\), together with \(\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right) \geqslant 0\), implies \(\pi_{k} \geqslant 0\) for all \(k\). Combining this with the condition (2.189) we obtain

\[0 \leqslant \pi_{k} \leqslant 1 \text {. }\]

Figure 2.23 Illustration of a mixture of 3 Gaussians in a two-dimensional space. (a) Contours of constant density for each of the mixture components, in which the 3 components are denoted red, blue and green, and the values of the mixing coefficients are shown below each component. (b) Contours of the marginal probability density \(p(\mathbf{x})\) of the mixture distribution. (c) A surface plot of the distribution \(p(\mathbf{x})\).

We therefore see that the mixing coefficients satisfy the requirements to be probabilities.

From the sum and product rules, the marginal density is given by

\[p(\mathbf{x})=\sum_{k=1}^{K} p(k) p(\mathbf{x} \mid k)\]

which is equivalent to (2.188) in which we can view \(\pi_{k}=p(k)\) as the prior probability of picking the \(k^{\text {th }}\) component, and the density \(\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)=p(\mathbf{x} \mid k)\) as the probability of \(\mathbf{x}\) conditioned on \(k\). As we shall see in later chapters, an important role is played by the posterior probabilities \(p(k \mid \mathbf{x})\), which are also known as responsibilities. From Bayes' theorem these are given by

\[\begin{align*}
\gamma_{k}(\mathbf{x}) & \equiv p(k \mid \mathbf{x}) \\
& =\frac{p(k) p(\mathbf{x} \mid k)}{\sum_{l} p(l) p(\mathbf{x} \mid l)} \\
& =\frac{\pi_{k} \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}, \mathbf{\Sigma}_{k}\right)}{\sum_{l} \pi_{l} \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{l}, \mathbf{\Sigma}_{l}\right)}
\end{align*}\]

We shall discuss the probabilistic interpretation of the mixture distribution in greater detail in Chapter 9.

The form of the Gaussian mixture distribution is governed by the parameters \(\pi\), \(\boldsymbol{\mu}\) and \(\boldsymbol{\Sigma}\), where we have used the notation \(\boldsymbol{\pi} \equiv\left\{\pi_{1}, \ldots, \pi_{K}\right\}, \boldsymbol{\mu} \equiv\left\{\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{K}\right\}\) and \(\boldsymbol{\Sigma} \equiv\left\{\boldsymbol{\Sigma}_{1}, \ldots \boldsymbol{\Sigma}_{K}\right\}\). One way to set the values of these parameters is to use maximum likelihood. From (2.188) the log of the likelihood function is given by

\[\ln p(\mathbf{X} \mid \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})=\sum_{n=1}^{N} \ln \left\{\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)\right\}\]

where \(\mathbf{X}=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\right\}\). We immediately see that the situation is now much more complex than with a single Gaussian, due to the presence of the summation over \(k\) inside the logarithm. As a result, the maximum likelihood solution for the parameters no longer has a closed-form analytical solution. One approach to maximizing the likelihood function is to use iterative numerical optimization techniques (Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008). Alternatively we can employ a powerful framework called expectation maximization, which will be discussed at length in Chapter 9.