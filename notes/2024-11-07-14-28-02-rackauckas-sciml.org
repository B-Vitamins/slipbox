:PROPERTIES:
:ID:       31f5c8c4-0174-4480-b925-9142b8d7f81f
:END:
#+TITLE: Parallel Computing and Scientific Machine Learning by Chris Rackauckas
#+AUTHOR: Chris Rackauckas
#+SOURCE: https://github.com/SciML
#+FILETAGS: :literature:ml:sci:comp:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org
* Optimizing Serial Code
:LOGBOOK:
CLOCK: [2023-01-21 Sat 01:26]--[2023-01-21 Sat 04:22] =>  2:56
:END:
At the center of any fast parallel code is a fast serial code. Parallelism is made to be a performance multiplier, so if you start from a bad position it won't ever get much better. Thus the first thing that we need to do is understand what makes code slow and how to avoid the pitfalls. This discussion of serial code optimization will also directly motivate why we will be using Julia throughout this course.
** Mental Model of a Memory
:PROPERTIES:
:CUSTOM_ID: mental-model-of-a-memory
:END:
To start optimizing code you need a good mental model of a computer.
*** High Level View
:PROPERTIES:
:CUSTOM_ID: high-level-view
:END:
At the highest level you have a CPU's core memory which directly accesses a L1 cache. The L1 cache has the fastest access, so things which will be needed soon are kept there. However, it is filled from the L2 cache, which itself is filled from the L3 cache, which is filled from the main memory. This bring us to the first idea in optimizing code: using things that are already in a closer cache can help the code run faster because it doesn't have to be queried for and moved up this chain.

[[https://hackernoon.com/hn-images/1*nT3RAGnOAWmKmvOBnizNtw.png]]

When something needs to be pulled directly from main memory this is known as a /cache miss/. To understand the cost of a cache miss vs standard calculations, take a look at [[http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/][this classic chart]].

(Cache-aware and cache-oblivious algorithms are methods which change their indexing structure to optimize their use of the cache lines. We will return to this when talking about performance of linear algebra.)

*** Cache Lines and Row/Column-Major
:PROPERTIES:
:CUSTOM_ID: cache-lines-and-rowcolumn-major
:END:
Many algorithms in numerical linear algebra are designed to minimize cache misses. Because of this chain, many modern CPUs try to guess what you will want next in your cache. When dealing with arrays, it will speculate ahead and grab what is known as a /cache line/: the next chunk in the array. Thus, your algorithms will be faster if you iterate along the values that it is grabbing.

The values that it grabs are the next values in the contiguous order of the stored array. There are two common conventions: row major and column major. Row major means that the linear array of memory is formed by stacking the rows one after another, while column major puts the column vectors one after another.

[[https://eli.thegreenplace.net/images/2015/column-major-2D.png]]

/Julia, MATLAB, and Fortran are column major/. Python's numpy is row-major.

#+begin_src julia
A = rand(100,100)
B = rand(100,100)
C = rand(100,100)
using BenchmarkTools
function inner_rows!(C,A,B)
  for i in 1:100, j in 1:100
    C[i,j] = A[i,j] + B[i,j]
  end
end
@btime inner_rows!(C,A,B)
#+end_src

#+begin_src julia
function inner_cols!(C,A,B)
  for j in 1:100, i in 1:100
    C[i,j] = A[i,j] + B[i,j]
  end
end
@btime inner_cols!(C,A,B)
#+end_src

*** Lower Level View: The Stack and the Heap
:PROPERTIES:
:CUSTOM_ID: lower-level-view-the-stack-and-the-heap
:END:
Locally, the stack is composed of a /stack/ and a /heap/. The stack requires a static allocation: it is ordered. Because it's ordered, it is very clear where things are in the stack, and therefore accesses are very quick (think instantaneous). However, because this is static, it requires that the size of the variables is known at compile time (to determine all of the variable locations). Since that is not possible with all variables, there exists the heap. The heap is essentially a stack of pointers to objects in memory. When heap variables are needed, their values are pulled up the cache chain and accessed.

[[https://bayanbox.ir/view/581244719208138556/virtual-memory.jpg]] [[https://camo.githubusercontent.com/ca96d70d09ce694363e44b93fd975bb3033898c1/687474703a2f2f7475746f7269616c732e6a656e6b6f762e636f6d2f696d616765732f6a6176612d636f6e63757272656e63792f6a6176612d6d656d6f72792d6d6f64656c2d352e706e67]]

*** Heap Allocations and Speed
:PROPERTIES:
:CUSTOM_ID: heap-allocations-and-speed
:END:
Heap allocations are costly because they involve this pointer indirection, so stack allocation should be done when sensible (it's not helpful for really large arrays, but for small values like scalars it's essential!)

#+begin_src julia
function inner_alloc!(C,A,B)
  for j in 1:100, i in 1:100
    val = [A[i,j] + B[i,j]]
    C[i,j] = val[1]
  end
end
@btime inner_alloc!(C,A,B)
#+end_src

#+begin_src julia
function inner_noalloc!(C,A,B)
  for j in 1:100, i in 1:100
    val = A[i,j] + B[i,j]
    C[i,j] = val[1]
  end
end
@btime inner_noalloc!(C,A,B)
#+end_src

Why does the array here get heap-allocated? It isn't able to prove/guarantee at compile-time that the array's size will always be a given value, and thus it allocates it to the heap. =@btime= tells us this allocation occurred and shows us the total heap memory that was taken. Meanwhile, the size of a Float64 number is known at compile-time (64-bits), and so this is stored onto the stack and given a specific location that the compiler will be able to directly address.

Note that one can use the StaticArrays.jl library to get statically-sized arrays and thus arrays which are stack-allocated:

#+begin_src julia
using StaticArrays
function static_inner_alloc!(C,A,B)
  for j in 1:100, i in 1:100
    val = @SVector [A[i,j] + B[i,j]]
    C[i,j] = val[1]
  end
end
@btime static_inner_alloc!(C,A,B)
#+end_src

*** Mutation to Avoid Heap Allocations
:PROPERTIES:
:CUSTOM_ID: mutation-to-avoid-heap-allocations
:END:
Many times you do need to write into an array, so how can you write into an array without performing a heap allocation? The answer is mutation. Mutation is changing the values of an already existing array. In that case, no free memory has to be found to put the array (and no memory has to be freed by the garbage collector).

In Julia, functions which mutate the first value are conventionally noted by a =!=. See the difference between these two equivalent functions:

#+begin_src julia
function inner_noalloc!(C,A,B)
  for j in 1:100, i in 1:100
    val = A[i,j] + B[i,j]
    C[i,j] = val[1]
  end
end
@btime inner_noalloc!(C,A,B)
#+end_src

#+begin_src julia
function inner_alloc(A,B)
  C = similar(A)
  for j in 1:100, i in 1:100
    val = A[i,j] + B[i,j]
    C[i,j] = val[1]
  end
end
@btime inner_alloc(A,B)
#+end_src

To use this algorithm effectively, the =!= algorithm assumes that the caller already has allocated the output array to put as the output argument. If that is not true, then one would need to manually allocate. The goal of that interface is to give the caller control over the allocations to allow them to manually reduce the total number of heap allocations and thus increase the speed.

*** Julia's Broadcasting Mechanism
:PROPERTIES:
:CUSTOM_ID: julias-broadcasting-mechanism
:END:
Wouldn't it be nice to not have to write the loop there? In many high level languages this is simply called /vectorization/. In Julia, we will call it /array vectorization/ to distinguish it from the /SIMD vectorization/ which is common in lower level languages like C, Fortran, and Julia.

In Julia, if you use =.= on an operator it will transform it to the broadcasted form. Broadcast is /lazy/: it will build up an entire =.='d expression and then call =broadcast!= on composed expression. This is customizable and [[https://docs.julialang.org/en/v1/manual/interfaces/#man-interfaces-broadcasting-1][documented in detail]]. However, to a first approximation we can think of the broadcast mechanism as a mechanism for building /fused expressions/. For example, the Julia code:

#+begin_src julia
A .+ B .+ C;
#+end_src

under the hood lowers to something like:

#+begin_src julia
map((a,b,c)->a+b+c,A,B,C);
#+end_src

where =map= is a function that just loops over the values element-wise.

*Take a quick second to think about why loop fusion may be an optimization.*

This about what would happen if you did not fuse the operations. We can write that out as:

#+begin_src julia
tmp = A .+ B
tmp .+ C;
#+end_src

Notice that if we did not fuse the expressions, we would need some place to put the result of =A .+ B=, and that would have to be an array, which means it would cause a heap allocation. Thus broadcast fusion eliminates the /temporary variable/ (colloquially called just a /temporary/).

#+begin_src julia
function unfused(A,B,C)
  tmp = A .+ B
  tmp .+ C
end
@btime unfused(A,B,C);
#+end_src

#+begin_src julia
fused(A,B,C) = A .+ B .+ C
@btime fused(A,B,C);
#+end_src

Note that we can also fuse the output by using ~.=~. This is essentially the vectorized version of a =!= function:

#+begin_src julia
D = similar(A)
fused!(D,A,B,C) = (D .= A .+ B .+ C)
@btime fused!(D,A,B,C);
#+end_src

*** Note on Broadcasting Function Calls
:PROPERTIES:
:CUSTOM_ID: note-on-broadcasting-function-calls
:END:
Julia allows for broadcasting the call =()= operator as well. =.()= will call the function element-wise on all arguments, so =sin.(A)= will be the elementwise sine function. This will fuse Julia like the other operators.

*** Note on Vectorization and Speed
:PROPERTIES:
:CUSTOM_ID: note-on-vectorization-and-speed
:END:
In articles on MATLAB, Python, R, etc., this is where you will be told to vectorize your code. Notice from above that this isn't a performance difference between writing loops and using vectorized broadcasts. This is not abnormal! The reason why you are told to vectorize code in these other languages is because they have a high per-operation overhead (which will be discussed further down). This means that every call, like =+=, is costly in these languages. To get around this issue and make the language usable, someone wrote and compiled the loop for the C/Fortran function that does the broadcasted form (see numpy's Github repo). Thus =A .+ B='s MATLAB/Python/R equivalents are calling a single C function to generally avoid the cost of function calls and thus are faster.

But this is not an intrinsic property of vectorization. Vectorization isn't "fast" in these languages, it's just close to the correct speed. The reason vectorization is recommended is because looping is slow in these languages. Because looping isn't slow in Julia (or C, C++, Fortran, etc.), loops and vectorization generally have the same speed. So use the one that works best for your code without a care about performance.

(As a small side effect, these high level languages tend to allocate a lot of temporary variables since the individual C kernels are written for specific numbers of inputs and thus don't naturally fuse. Julia's broadcast mechanism is just generating and JIT compiling Julia functions on the fly, and thus it can accommodate the combinatorial explosion in the amount of choices just by only compiling the combinations that are necessary for a specific code)

*** Heap Allocations from Slicing
:PROPERTIES:
:CUSTOM_ID: heap-allocations-from-slicing
:END:
It's important to note that slices in Julia produce copies instead of views. Thus for example:

#+begin_src julia
A[50,50]
#+end_src

allocates a new output. This is for safety, since if it pointed to the same array then writing to it would change the original array. We can demonstrate this by asking for a /view/ instead of a copy.

#+begin_src julia
@show A[1]
E = @view A[1:5,1:5]
E[1] = 2.0
@show A[1]
#+end_src

However, this means that =@view A[1:5,1:5]= did not allocate an array (it does allocate a pointer if the escape analysis is unable to prove that it can be elided. This means that in small loops there will be no allocation, while if the view is returned from a function for example it will allocate the pointer, ~80 bytes, but not the memory of the array. This means that it is O(1) in cost but with a relatively small constant).

*** Asymptotic Cost of Heap Allocations
:PROPERTIES:
:CUSTOM_ID: asymptotic-cost-of-heap-allocations
:END:
Heap allocations have to locate and prepare a space in RAM that is proportional to the amount of memory that is calculated, which means that the cost of a heap allocation for an array is O(n), with a large constant. As RAM begins to fill up, this cost dramatically increases. If you run out of RAM, your computer may begin to use /swap/, which is essentially RAM simulated on your hard drive. Generally when you hit swap your performance is so dead that you may think that your computation froze, but if you check your resource use you will notice that it's actually just filled the RAM and starting to use the swap.

But think of it as O(n) with a large constant factor. This means that for operations which only touch the data once, heap allocations can dominate the computational cost:

#+begin_src julia
using LinearAlgebra, BenchmarkTools
function alloc_timer(n)
    A = rand(n,n)
    B = rand(n,n)
    C = rand(n,n)
    t1 = @belapsed $A .* $B
    t2 = @belapsed ($C .= $A .* $B)
    t1,t2
end
ns = 2 .^ (2:11)
res = [alloc_timer(n) for n in ns]
alloc   = [x[1] for x in res]
noalloc = [x[2] for x in res]

using Plots
plot(ns,alloc,label="=",xscale=:log10,yscale=:log10,legend=:bottomright,
     title="Micro-optimizations matter for BLAS1")
plot!(ns,noalloc,label=".=")
#+end_src

However, when the computation takes O(n^3), like in matrix multiplications, the high constant factor only comes into play when the matrices are sufficiently small:

#+begin_src julia
using LinearAlgebra, BenchmarkTools
function alloc_timer(n)
    A = rand(n,n)
    B = rand(n,n)
    C = rand(n,n)
    t1 = @belapsed $A*$B
    t2 = @belapsed mul!($C,$A,$B)
    t1,t2
end
ns = 2 .^ (2:7)
res = [alloc_timer(n) for n in ns]
alloc   = [x[1] for x in res]
noalloc = [x[2] for x in res]

using Plots
plot(ns,alloc,label="*",xscale=:log10,yscale=:log10,legend=:bottomright,
     title="Micro-optimizations only matter for small matmuls")
plot!(ns,noalloc,label="mul!")
#+end_src

Though using a mutating form is never bad and always is a little bit better.

*** Optimizing Memory Use Summary
:PROPERTIES:
:CUSTOM_ID: optimizing-memory-use-summary
:END:
- Avoid cache misses by reusing values
- Iterate along columns
- Avoid heap allocations in inner loops
- Heap allocations occur when the size of things is not proven at compile-time
- Use fused broadcasts (with mutated outputs) to avoid heap allocations
- Array vectorization confers no special benefit in Julia because Julia loops are as fast as C or Fortran
- Use views instead of slices when applicable
- Avoiding heap allocations is most necessary for O(n) algorithms or algorithms with small arrays
- Use StaticArrays.jl to avoid heap allocations of small arrays in inner loops

** Julia's Type Inference and the Compiler
:PROPERTIES:
:CUSTOM_ID: julias-type-inference-and-the-compiler
:END:
Many people think Julia is fast because it is JIT compiled. That is simply not true (we've already shown examples where Julia code isn't fast, but it's always JIT compiled!). Instead, the reason why Julia is fast is because the combination of two ideas:
- Type inference
- Type specialization in functions
These two features naturally give rise to Julia's core design feature: multiple dispatch. Let's break down these pieces.
*** Type Inference
:PROPERTIES:
:CUSTOM_ID: type-inference
:END:
At the core level of the computer, everything has a type. Some languages are more explicit about said types, while others try to hide the types from the user. A type tells the compiler how to to store and interpret the memory of a value. For example, if the compiled code knows that the value in the register is supposed to be interpreted as a 64-bit floating point number, then it understands that slab of memory like:

[[https://i.stack.imgur.com/ZUbLc.png]]

Importantly, it will know what to do for function calls. If the code tells it to add two floating point numbers, it will send them as inputs to the Floating Point Unit (FPU) which will give the output.

If the types are not known, then... ? So one cannot actually compute until the types are known, since otherwise it's impossible to interpret the memory. In languages like C, the programmer has to declare the types of variables in the program:

#+begin_example
void add(double *a, double *b, double *c, size_t n){
  size_t i;
  for(i = 0; i < n; ++i) {
    c[i] = a[i] + b[i];
  }
}
#+end_example

The types are known at compile time because the programmer set it in stone. In many interpreted languages Python, types are checked at runtime. For example,

#+begin_example
a = 2
b = 4
a + b
#+end_example

when the addition occurs, the Python interpreter will check the object holding the values and ask it for its types, and use those types to know how to compute the + function. For this reason, the add function in Python is rather complex since it needs to decode and have a version for all primitive types!

Not only is there runtime overhead checks in function calls due to to not being explicit about types, there is also a memory overhead since it is impossible to know how much memory a value with take since that's a property of its type. Thus the Python interpreter cannot statically guarantee exact unchanging values for the size that a value would take in the stack, meaning that the variables are not stack-allocated. This means that every number ends up heap-allocated, which hopefully begins to explain why this is not as fast as C.

The solution is Julia is somewhat of a hybrid. The Julia code looks like:

#+begin_src julia
a = 2
b = 4
a + b
#+end_src

However, before JIT compilation, Julia runs a type inference algorithm which finds out that =A= is an =Int=, and =B= is an =Int=. You can then understand that if it can prove that =A+B= is an =Int=, then it can propagate all of the types through.

*** Type Specialization in Functions
:PROPERTIES:
:CUSTOM_ID: type-specialization-in-functions
:END:
Julia is able to propagate type inference through functions because, even if a function is "untyped", Julia will interpret this as a /generic function/ over possible /methods/, where every method has a concrete type. This means that in Julia, the function:

#+begin_src julia
f(x,y) = x+y
#+end_src

is not what you may think of as a "single function", since given inputs of different types it will actually be a different function. We can see this by examining the /LLVM IR/ (LLVM is Julia's compiler, the IR is the /Intermediate Representation/, i.e. a platform-independent representation of assembly that lives in LLVM that it knows how to convert into assembly per architecture):

#+begin_src julia
using InteractiveUtils
@code_llvm f(2,5)
#+end_src

#+begin_src julia
@code_llvm f(2.0,5.0)
#+end_src

Notice that when =f= is the function that takes in two =Int=s, =Int=s add to give an =Int= and thus =f= outputs an =Int=. When =f= is the function that takes two =Float64=s, =f= returns a =Float64=. Thus in the code:

#+begin_src julia
function g(x,y)
  a = 4
  b = 2
  c = f(x,a)
  d = f(b,c)
  f(d,y)
end

@code_llvm g(2,5)
#+end_src

=g= on two =Int= inputs is a function that has =Int= s at every step along the way and spits out an =Int=. We can use the =@code_warntype= macro to better see the inference along the steps of the function:

#+begin_src julia
@code_warntype g(2,5)
#+end_src

What happens on mixtures?

#+begin_src julia
@code_llvm f(2.0,5)
#+end_src

When we add an =Int= to a =Float64=, we promote the =Int= to a =Float64= and then perform the =+= between two =Float64=s. When we go to the full function, we see that it can still infer:

#+begin_src julia
@code_warntype g(2.0,5)
#+end_src

and it uses this to build a very efficient assembly code because it knows exactly what the types will be at every step:

#+begin_src julia
@code_llvm g(2.0,5)
#+end_src

(notice how it handles the constant /literals/ 4 and 2: it converted them at compile time to reduce the algorithm to 3 floating point additions).

*** Type Stability
:PROPERTIES:
:CUSTOM_ID: type-stability
:END:
Why is the inference algorithm able to infer all of the types of =g=? It's because it knows the types coming out of =f= at compile time. Given an =Int= and a =Float64=, =f= will always output a =Float64=, and thus it can continue with inference knowing that =c=, =d=, and eventually the output is =Float64=. Thus in order for this to occur, we need that the type of the output on our function is directly inferred from the type of the input. This property is known as type-stability.

An example of breaking it is as follows:

#+begin_src julia
function h(x,y)
  out = x + y
  rand() < 0.5 ? out : Float64(out)
end
#+end_src

Here, on an integer input the output's type is randomly either Int or Float64, and thus the output is unknown:

#+begin_src julia
@code_warntype h(2,5)
#+end_src

This means that its output type is =Union{Int,Float64}= (Julia uses union types to keep the types still somewhat constrained). Once there are multiple choices, those need to get propagate through the compiler, and all subsequent calculations are the result of either being an =Int= or a =Float64=.

(Note that Julia has small union optimizations, so if this union is of size 4 or less then Julia will still be able to optimize it quite a bit.)

*** Multiple Dispatch
:PROPERTIES:
:CUSTOM_ID: multiple-dispatch
:END:
The =+= function on numbers was implemented in Julia, so how were these rules all written down? The answer is multiple dispatch. In Julia, you can tell a function how to act differently on different types by using type assertions on the input values. For example, let's make a function that computes =2x + y= on =Int= and =x/y= on =Float64=:

#+begin_src julia
ff(x::Int,y::Int) = 2x + y
ff(x::Float64,y::Float64) = x/y
@show ff(2,5)
@show ff(2.0,5.0)
#+end_src

The =+= function in Julia is just defined as =+(a,b)=, and we can actually point to that code in the Julia distribution:

#+begin_src julia
@which +(2.0,5)
#+end_src

To control at a higher level, Julia uses /abstract types/. For example, =Float64 <: AbstractFloat=, meaning =Float64=s are a subtype of =AbstractFloat=. We also have that =Int <: Integer=, while both =AbstractFloat <: Number= and =Integer <: Number=.

[[https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Type-hierarchy-for-julia-numbers.png/800px-Type-hierarchy-for-julia-numbers.png]]

Julia allows the user to define dispatches at a higher level, and the version that is called is the most strict version that is correct. For example, right now with =ff= we will get a =MethodError= if we call it between a =Int= and a =Float64= because no such method exists:

#+begin_src julia
ff(2.0,5)
#+end_src

However, we can add a /fallback method/ to the function =ff= for two numbers:

#+begin_src julia
ff(x::Number,y::Number) = x + y
ff(2.0,5)
#+end_src

Notice that the fallback method still specializes on the inputs:

#+begin_src julia
@code_llvm ff(2.0,5)
#+end_src

It's essentially just a template for what functions to possibly try and create given the types that are seen. When it sees =Float64= and =Int=, it knows it should try and create the function that does =x+y=, and once it knows it's =Float64= plus a =Int=, it knows it should create the function that converts the =Int= to a =Float64= and then does addition between two =Float64=s, and that is precisely the generated LLVM IR on this pair of input types.

And that's essentially Julia's secret sauce: since it's always specializing its types on each function, if those functions themselves can infer the output, then the entire function can be inferred and generate optimal code, which is then optimized by the compiler and out comes an efficient function. If types can't be inferred, Julia falls back to a slower "Python" mode (though with optimizations in cases like small unions). Users then get control over this specialization process through multiple dispatch, which is then Julia's core feature since it allows adding new options without any runtime cost.

*** Any Fallbacks
:PROPERTIES:
:CUSTOM_ID: any-fallbacks
:END:
Note that =f(x,y) = x+y= is equivalent to =f(x::Any,y::Any) = x+y=, where =Any= is the maximal supertype of every Julia type. Thus =f(x,y) = x+y= is essentially a fallback for all possible input values, telling it what to do in the case that no other dispatches exist. However, note that this dispatch itself is not slow, since it will be specialized on the input types.

*** Ambiguities
:PROPERTIES:
:CUSTOM_ID: ambiguities
:END:
The version that is called is the most strict version that is correct. What happens if it's impossible to define "the most strict version"? For example,

#+begin_src julia
ff(x::Float64,y::Number) = 5x + 2y
ff(x::Number,y::Int) = x - y
#+end_src

What should it call on =f(2.0,5)= now? =ff(x::Float64,y::Number)= and =ff(x::Number,y::Int)= are both more strict than =ff(x::Number,y::Number)=, so one of them should be called, but neither are more strict than each other, and thus you will end up with an ambiguity error:

#+begin_src julia
ff(2.0,5)
#+end_src

*** Untyped Containers
:PROPERTIES:
:CUSTOM_ID: untyped-containers
:END:
One way to ruin inference is to use an untyped container. For example, the array constructors use type inference themselves to know what their container type will be. Therefore,

#+begin_src julia
a = [1.0,2.0,3.0]
#+end_src

uses type inference on its inputs to know that it should be something that holds =Float64= values, and thus it is a 1-dimensional array of =Float64= values, or =Array{Float64,1}=. The accesses:

#+begin_src julia
a[1]
#+end_src

are then inferred, since this is just the function =getindex(a::Array{T},i) where T= which is a function that will produce something of type =T=, the element type of the array. However, if we tell Julia to make an array with element type =Any=:

#+begin_src julia
b = ["1.0",2,2.0]
#+end_src

(here, Julia falls back to =Any= because it cannot promote the values to the same type), then the best inference can do on the output is to say it could have any type:

#+begin_src julia
function bad_container(a)
  a[2]
end
@code_warntype bad_container(a)
#+end_src

#+begin_src julia
@code_warntype bad_container(b)
#+end_src

This is one common way that type inference can breakdown. For example, even if the array is all numbers, we can still break inference:

#+begin_src julia
x = Number[1.0,3]
function q(x)
  a = 4
  b = 2
  c = f(x[1],a)
  d = f(b,c)
  f(d,x[2])
end
@code_warntype q(x)
#+end_src

Here the type inference algorithm quickly gives up and infers to =Any=, losing all specialization and automatically switching to Python-style runtime type checking.

*** Type definitions
:PROPERTIES:
:CUSTOM_ID: type-definitions
:END:
*** Value types and isbits
:PROPERTIES:
:CUSTOM_ID: value-types-and-isbits
:END:
In Julia, types which can fully inferred and which are composed of primitive or =isbits= types are value types. This means that, inside of an array, their values are the values of the type itself, and not a pointer to the values.

You can check if the type is a value type through =isbits=:

#+begin_src julia
isbits(1.0)
#+end_src

Note that a Julia =struct= which holds =isbits= values is =isbits= as well, if it's fully inferred:

#+begin_src julia
struct MyComplex
  real::Float64
  imag::Float64
end
isbits(MyComplex(1.0,1.0))
#+end_src

We can see that the compiler knows how to use this efficiently since it knows that what comes out is always =Float64=:

#+begin_src julia
Base.:+(a::MyComplex,b::MyComplex) = MyComplex(a.real+b.real,a.imag+b.imag)
Base.:+(a::MyComplex,b::Int) = MyComplex(a.real+b,a.imag)
Base.:+(b::Int,a::MyComplex) = MyComplex(a.real+b,a.imag)
g(MyComplex(1.0,1.0),MyComplex(1.0,1.0))
#+end_src

#+begin_src julia
@code_warntype g(MyComplex(1.0,1.0),MyComplex(1.0,1.0))
#+end_src

#+begin_src julia
@code_llvm g(MyComplex(1.0,1.0),MyComplex(1.0,1.0))
#+end_src

Note that the compiled code simply works directly on the =double= pieces. We can also make this be concrete without pre-specifying that the values always have to be =Float64= by using a type parameter.

#+begin_src julia
struct MyParameterizedComplex{T}
  real::T
  imag::T
end
isbits(MyParameterizedComplex(1.0,1.0))
#+end_src

Note that =MyParameterizedComplex{T}= is a concrete type for every =T=: it is a shorthand form for defining a whole family of types.

#+begin_src julia
Base.:+(a::MyParameterizedComplex,b::MyParameterizedComplex) = MyParameterizedComplex(a.real+b.real,a.imag+b.imag)
Base.:+(a::MyParameterizedComplex,b::Int) = MyParameterizedComplex(a.real+b,a.imag)
Base.:+(b::Int,a::MyParameterizedComplex) = MyParameterizedComplex(a.real+b,a.imag)
g(MyParameterizedComplex(1.0,1.0),MyParameterizedComplex(1.0,1.0))
#+end_src

#+begin_src julia
@code_warntype g(MyParameterizedComplex(1.0,1.0),MyParameterizedComplex(1.0,1.0))
#+end_src

See that this code also automatically works and compiles efficiently for =Float32= as well:

#+begin_src julia
@code_warntype g(MyParameterizedComplex(1.0f0,1.0f0),MyParameterizedComplex(1.0f0,1.0f0))
#+end_src

#+begin_src julia
@code_llvm g(MyParameterizedComplex(1.0f0,1.0f0),MyParameterizedComplex(1.0f0,1.0f0))
#+end_src

It is important to know that if there is any piece of a type which doesn't contain type information, then it cannot be isbits because then it would have to be compiled in such a way that the size is not known in advance. For example:

#+begin_src julia
struct MySlowComplex
  real
  imag
end
isbits(MySlowComplex(1.0,1.0))
#+end_src

#+begin_src julia
Base.:+(a::MySlowComplex,b::MySlowComplex) = MySlowComplex(a.real+b.real,a.imag+b.imag)
Base.:+(a::MySlowComplex,b::Int) = MySlowComplex(a.real+b,a.imag)
Base.:+(b::Int,a::MySlowComplex) = MySlowComplex(a.real+b,a.imag)
g(MySlowComplex(1.0,1.0),MySlowComplex(1.0,1.0))
#+end_src

#+begin_src julia
@code_warntype g(MySlowComplex(1.0,1.0),MySlowComplex(1.0,1.0))
#+end_src

#+begin_src julia
@code_llvm g(MySlowComplex(1.0,1.0),MySlowComplex(1.0,1.0))
#+end_src

#+begin_src julia
struct MySlowComplex2
  real::AbstractFloat
  imag::AbstractFloat
end
isbits(MySlowComplex2(1.0,1.0))
#+end_src

#+begin_src julia
Base.:+(a::MySlowComplex2,b::MySlowComplex2) = MySlowComplex2(a.real+b.real,a.imag+b.imag)
Base.:+(a::MySlowComplex2,b::Int) = MySlowComplex2(a.real+b,a.imag)
Base.:+(b::Int,a::MySlowComplex2) = MySlowComplex2(a.real+b,a.imag)
g(MySlowComplex2(1.0,1.0),MySlowComplex2(1.0,1.0))
#+end_src

Here's the timings:

#+begin_src julia
a = MyComplex(1.0,1.0)
b = MyComplex(2.0,1.0)
@btime g(a,b)
#+end_src

#+begin_src julia
a = MyParameterizedComplex(1.0,1.0)
b = MyParameterizedComplex(2.0,1.0)
@btime g(a,b)
#+end_src

#+begin_src julia
a = MySlowComplex(1.0,1.0)
b = MySlowComplex(2.0,1.0)
@btime g(a,b)
#+end_src

#+begin_src julia
a = MySlowComplex2(1.0,1.0)
b = MySlowComplex2(2.0,1.0)
@btime g(a,b)
#+end_src

*** Note on Julia
:PROPERTIES:
:CUSTOM_ID: note-on-julia
:END:
Note that, because of these type specialization, value types, etc. properties, the number types, even ones such as =Int=, =Float64=, and =Complex=, are all themselves implemented in pure Julia! Thus even basic pieces can be implemented in Julia with full performance, given one uses the features correctly.

*** Note on isbits
:PROPERTIES:
:CUSTOM_ID: note-on-isbits
:END:
Note that a type which is =mutable struct= will not be isbits. This means that mutable structs will be a pointer to a heap allocated object, unless it's shortlived and the compiler can erase its construction. Also, note that =isbits= compiles down to bit operations from pure Julia, which means that these types can directly compile to GPU kernels through CUDAnative without modification.

*** Function Barriers
:PROPERTIES:
:CUSTOM_ID: function-barriers
:END:
Since functions automatically specialize on their input types in Julia, we can use this to our advantage in order to make an inner loop fully inferred. For example, take the code from above but with a loop:

#+begin_src julia
function r(x)
  a = 4
  b = 2
  for i in 1:100
    c = f(x[1],a)
    d = f(b,c)
    a = f(d,x[2])
  end
  a
end
@btime r(x)
#+end_src

In here, the loop variables are not inferred and thus this is really slow. However, we can force a function call in the middle to end up with specialization and in the inner loop be stable:

#+begin_src julia
s(x) = _s(x[1],x[2])
function _s(x1,x2)
  a = 4
  b = 2
  for i in 1:100
    c = f(x1,a)
    d = f(b,c)
    a = f(d,x2)
  end
  a
end
@btime s(x)
#+end_src

Notice that this algorithm still doesn't infer:

#+begin_src julia
@code_warntype s(x)
#+end_src

since the output of =_s= isn't inferred, but while it's in =_s= it will have specialized on the fact that =x[1]= is a =Float64= while =x[2]= is a =Int=, making that inner loop fast. In fact, it will only need to pay one /dynamic dispatch/, i.e. a multiple dispatch determination that happens at runtime. Notice that whenever functions are inferred, the dispatching is static since the choice of the dispatch is already made and compiled into the LLVM IR.

*** Specialization at Compile Time
:PROPERTIES:
:CUSTOM_ID: specialization-at-compile-time
:END:
Julia code will specialize at compile time if it can prove something about the result. For example:

#+begin_src julia
function fff(x)
  if x isa Int
    y = 2
  else
    y = 4.0
  end
  x + y
end
#+end_src

You might think this function has a branch, but in reality Julia can determine whether =x= is an =Int= or not at compile time, so it will actually compile it away and just turn it into the function =x+2= or =x+4.0=:

#+begin_src julia
@code_llvm fff(5)
#+end_src

#+begin_src julia
@code_llvm fff(2.0)
#+end_src

Thus one does not need to worry about over-optimizing since in the obvious cases the compiler will actually remove all of the extra pieces when it can!

*** Global Scope and Optimizations
:PROPERTIES:
:CUSTOM_ID: global-scope-and-optimizations
:END:
This discussion shows how Julia's optimizations all apply during function specialization times. Thus calling Julia functions is fast. But what about when doing something outside of the function, like directly in a module or in the REPL?

#+begin_src julia
@btime for j in 1:100, i in 1:100
  global A,B,C
  C[i,j] = A[i,j] + B[i,j]
end
#+end_src

This is very slow because the types of =A=, =B=, and =C= cannot be inferred. Why can't they be inferred? Well, at any time in the dynamic REPL scope I can do something like =C = "haha now a string!"=, and thus it cannot specialize on the types currently existing in the REPL (since asynchronous changes could also occur), and therefore it defaults back to doing a type check at every single function which slows it down. Moral of the story, Julia functions are fast but its global scope is too dynamic to be optimized.

*** Summary
:PROPERTIES:
:CUSTOM_ID: summary
:END:
- Julia is not fast because of its JIT, it's fast because of function specialization and type inference
- Type stable functions allow inference to fully occur
- Multiple dispatch works within the function specialization mechanism to create overhead-free compile time controls
- Julia will specialize the generic functions
- Making sure values are concretely typed in inner loops is essential for performance

** Overheads of Individual Operations
:PROPERTIES:
:CUSTOM_ID: overheads-of-individual-operations
:END:
Now let's dig even a little deeper. Everything the processor does has a cost. A great chart to keep in mind is [[http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/][this classic one]]. A few things should immediately jump out to you:
- Simple arithmetic, like floating point additions, are super cheap. ~1 clock cycle, or a few nanoseconds.
- Processors do /branch prediction/ on =if= statements. If the code goes down the predicted route, the =if= statement costs ~1-2 clock cycles. If it goes down the wrong route, then it will take ~10-20 clock cycles. This means that predictable branches, like ones with clear patterns or usually the same output, are much cheaper (almost free) than unpredictable branches.
- Function calls are expensive: 15-60 clock cycles!
- RAM reads are very expensive, with lower caches less expensive.
*** Bounds Checking
:PROPERTIES:
:CUSTOM_ID: bounds-checking
:END:
Let's check the LLVM IR on one of our earlier loops:

#+begin_src julia
function inner_noalloc!(C,A,B)
  for j in 1:100, i in 1:100
    val = A[i,j] + B[i,j]
    C[i,j] = val[1]
  end
end
@code_llvm inner_noalloc!(C,A,B)
#+end_src

Notice that this =getelementptr inbounds= stuff is bounds checking. Julia, like all other high level languages, enables bounds checking by default in order to not allow the user to index outside of an array. Indexing outside of an array is dangerous: it can quite easily segfault your system if you change some memory that is unknown beyond your actual array. Thus Julia throws an error:

#+begin_src julia
A[101,1]
#+end_src

In tight inner loops, we can remove this bounds checking process using the =@inbounds= macro:

#+begin_src julia
function inner_noalloc_ib!(C,A,B)
  @inbounds for j in 1:100, i in 1:100
    val = A[i,j] + B[i,j]
    C[i,j] = val[1]
  end
end
@btime inner_noalloc!(C,A,B)
#+end_src

#+begin_src julia
@btime inner_noalloc_ib!(C,A,B)
#+end_src

*** SIMD
:PROPERTIES:
:CUSTOM_ID: simd
:END:
Now let's inspect the LLVM IR again:

#+begin_src julia
@code_llvm inner_noalloc_ib!(C,A,B)
#+end_src

If you look closely, you will see things like:

#+begin_example
%wide.load24 = load <4 x double>, <4 x double> addrspac(13)* %46, align 8
; └
; ┌ @ float.jl:395 within `+'
%47 = fadd <4 x double> %wide.load, %wide.load24
#+end_example

What this is saying is that it's loading and adding 4 =Float64= s at a time! This feature of the processor is known as SIMD: single input multiple data. If certain primitive floating point operations, like =+= and =*=, are done in succession (i.e. no inbounds checks between them!), then the processor can lump them together and do multiples at once. Since clock cycles have stopped improving while transistors have gotten smaller, this "lumping" has been a big source of speedups in computational mathematics even though the actual =+= and =*= hasn't gotten faster. Thus to get full speed we want to make sure this is utilized whenever possible, which essentially just amounts to doing type inferred loops with no branches or bounds checks in the way.

*** FMA
:PROPERTIES:
:CUSTOM_ID: fma
:END:
Modern processors have a single operation that fuses the multiplication and the addition in the operation =x*y+z=, known as a /fused multiply-add/ or FMA. Note that FMA has less floating point roundoff error than the two operation form. We can see this intrinsic in the resulting LLVM IR:

#+begin_src julia
@code_llvm fma(2.0,5.0,3.0)
#+end_src

The Julia function =muladd= will automatically choose between FMA and the original form depending on the availability of the routine in the processor. The MuladdMacro.jl package has a macro =@muladd= which pulls apart statements to add =muladd= expressions. For example, =x1*y1 + x2*y2 + x3*y3= can be rewritten as:

#+begin_example
muladd(x1,y1,muladd(x2,y2,x3*y3))
#+end_example

Which reduces the linear combination to just 3 arithmetic operations. FMA operations can be SIMD'd.

*** Inlining
:PROPERTIES:
:CUSTOM_ID: inlining
:END:
All of this would go to waste if function call costs of 50 clock cycles were interrupting every single =+=. Fortunately these function calls disappear during the compilation process due to what's known as inlining. Essentially, if the function call is determined to be "cheap enough", the actual function call is removed and the code is basically pasted into the function caller. We can force a function call to occur by teling it to not inline:

#+begin_src julia
@noinline fnoinline(x,y) = x + y
finline(x,y) = x + y # Can add @inline, but this is automatic here
function qinline(x,y)
  a = 4
  b = 2
  c = finline(x,a)
  d = finline(b,c)
  finline(d,y)
end
function qnoinline(x,y)
  a = 4
  b = 2
  c = fnoinline(x,a)
  d = fnoinline(b,c)
  fnoinline(d,y)
end
#+end_src

#+begin_src julia
@code_llvm qinline(1.0,2.0)
#+end_src

#+begin_src julia
@code_llvm qnoinline(1.0,2.0)
#+end_src

We can see now that it keeps the function calls:

#+begin_example
%4 = call double @julia_fnoinline_21538(double %3, double %1)
#+end_example

and this is slower in comparison to what we had before (but it still infers).

#+begin_src julia
x = 1.0
y = 2.0
@btime qinline(x,y)
#+end_src

#+begin_src julia
@btime qnoinline(x,y)
#+end_src

Note that if we ever want to go the other direction and tell Julia to inline as much as possible, one can use the macro =@inline=.

*** Summary
:PROPERTIES:
:CUSTOM_ID: summary-1
:END:
- Scalar operations are super cheap, and if they are cache-aligned then more than one will occur in a clock cycle.
- Inlining a function will remove the high function call overhead.
- Branch prediction is pretty good these days, so keep them out of super tight inner loops but don't worry all too much about them.
- Cache misses are quite expensive the further out it goes.

** Note on Benchmarking
:PROPERTIES:
:CUSTOM_ID: note-on-benchmarking
:END:
Julia's compiler is smart. This means that if you don't try hard enough, Julia's compiler might get rid of your issues. For example, it can delete branches and directly compute the result if all of the values are known at compile time. So be very careful when benchmarking: your tests may have just compiled away!

Notice the following:

#+begin_src julia
@btime qinline(1.0,2.0)
#+end_src

Dang, that's much faster! But if you look into it, Julia's compiler is actually "cheating" on this benchmark:

#+begin_src julia
cheat() = qinline(1.0,2.0)
@code_llvm cheat()
#+end_src

It realized that =1.0= and =2.0= are constants, so it did what's known as /constant propagation/, and then used those constants inside of the function. It realized that the solution is always =9=, so it compiled the function that... spits out =9=! So it's fast because it's not computing anything. So be very careful about propagation of constants and literals. In general this is a very helpful feature, but when benchmarking this can cause some weird behavior. If a micro benchmark is taking less than a nanosecond, check and see if the compiler "fixed" your code!

** Conclusion
:PROPERTIES:
:CUSTOM_ID: conclusion
:END:
Optimize your serial code before you parallelize. There's a lot to think about.
** Discussion Questions
:PROPERTIES:
:CUSTOM_ID: discussion-questions
:END:
Here's a few discussion questions to think about performance engineering in scientific tasks:

1) What are the advantages of a =Vector{Array}= vs a =Matrix=? What are the disadvantage? (What's different?)
2) What is a good way to implement a data frame?
3) What are some good things that come out of generic functions for free? What are some things you should watch out for with generic functions?
* Introduction to Scientific Machine Learning Through Physics-Informed Neural Networks
:LOGBOOK:
CLOCK: [2023-01-21 Sat 04:23]--[2023-01-21 Sat 04:56] =>  0:33
:END:
Here we will start to dig into what scientific machine learning is all about by looking at physics-informed neural networks. Let's start by understanding what a neural network really is, why they are used, and what kinds of problems that they solve, and then we will use this understanding of a neural network to see how to solve ordinary differential equations with neural networks. For there, we will use this method to regularize neural networks with physical equations, the aforementioned physics-informed neural network, and see how to define neural network architectures that satisfy physical constraints to improve the training process.
** Getting Started with Machine Learning: Adding Flux
:PROPERTIES:
:CUSTOM_ID: getting-started-with-machine-learning-adding-flux
:END:
To add Flux.jl we would do:

#+begin_src julia;eval=false
]add Flux
#+end_src

To then use the package we will then use the =using= command:

#+begin_src julia
using Flux
#+end_src

If you prefer to namespace all commands (like is normally done in Python, i.e. =Flux.gradient= instead of =gradient=), you can use the command:

#+begin_src julia;eval=false
import Flux
#+end_src

Note that the installation and precompilation of these packages will occur at the =add= and first =using= phases, so they may take awhile (subsequent uses will utilize the precompiled form and take a lot less time!)

** What is a Neural Network?
:PROPERTIES:
:CUSTOM_ID: what-is-a-neural-network
:END:
A neural network is a function:

#+begin_src math
\text{NN}(x) = W_3\sigma_2(W_2\sigma_1(W_1x + b_1) + b_2) + b_3
#+end_src

where we can change the number of layers (=(W_i,b_i)=) as necessary. Let's assume we want to approximate some \(R^{10} \rightarrow R^5\) function. To do this we need to make sure that we start with 10 inputs and arrive at 5 outputs. If we want a bigger middle layer for example, we can do something like (10,32,32,5). Size changing occurs at the site of the matrix multiplication, which means that we want a 32x10 matrix, then a 32x32 matrix, and finally a 5x32 matrix. This neural network would look like:

#+begin_src julia
W = [randn(32,10),randn(32,32),randn(5,32)]
b = [zeros(32),zeros(32),zeros(5)]
#+end_src

#+begin_src julia
simpleNN(x) = W[3]*tanh.(W[2]*tanh.(W[1]*x + b[1]) + b[2]) + b[3]
simpleNN(rand(10))
#+end_src

This is our direct definition of a neural network. Notice that we choose to use =tanh= as our *activation function* between the layers.

*** Defining Neural Networks with Flux.jl
:PROPERTIES:
:CUSTOM_ID: defining-neural-networks-with-flux.jl
:END:
One of the main deep learning libraries in Julia is Flux.jl. Flux is an interesting library for scientific machine learning because it is built on top of language-wide *automatic differentiation* libraries, giving rise to a programming paradigm known as *differentiable programming*, which means that one can write a program in a manner that it has easily accessible fast derivatives. However, due to being built on a differentiable programming base, the underlying functionality is simply standard Julia code,

To learn how to use the library, consult the documentation. A Google search will bring up the [[https://github.com/FluxML/Flux.jl][Flux.jl Github repository]]. From there, the blue link on the README brings you to [[https://fluxml.ai/Flux.jl/stable/][the package documentation]]. This is common through Julia so it's a good habit to learn!

In the documentation you will find that the way a neural network is defined is through a =Chain= of layers. A =Dense= layer is the kind we defined above, which is given by an input size, an output size, and an activation function. For example, the following recreates the neural network that we had above:

#+begin_src julia
using Flux
NN2 = Chain(Dense(10 => 32,tanh),
           Dense(32 => 32,tanh),
           Dense(32 => 5))
NN2(rand(10))
#+end_src

Notice that Flux.jl as a library is written in pure Julia, which means that every piece of this syntax is just sugar over some Julia code that we can specialize ourselves (this is the advantage of having a language fast enough for the implementation of the library and the use of the library!)

For example, the activation function is just a scalar Julia function. If we wanted to replace it by something like the quadratic function, we can just use an *anonymous function* to define the scalar function we would like to use:

#+begin_src julia
NN3 = Chain(Dense(10 => 32,x->x^2),
            Dense(32 => 32,x->max(0,x)),
            Dense(32 => 5))
NN3(rand(10))
#+end_src

The second activation function there is what's known as a =relu=. A =relu= can be good to use because it's an exceptionally operation and satisfies a form of the UAT. However, a downside is that its derivative is not continuous, which could impact the numerical properties of some algorithms, and thus it's widely used throughout standard machine learning but we'll see reasons why it may be disadvantageous in some cases in scientific machine learning.

*** Digging into the Construction of a Neural Network Library
:PROPERTIES:
:CUSTOM_ID: digging-into-the-construction-of-a-neural-network-library
:END:
Again, as mentioned before, this neural network =NN2= is simply a function:

#+begin_src julia
simpleNN(x) = W[3]*tanh.(W[2]*tanh.(W[1]*x + b[1]) + b[2]) + b[3]
#+end_src

Let's dig into the library and see how that's represented and really understand the construction of a deep learning library. First, let's figure out where =Dense= comes from and what it does.

#+begin_src julia
using InteractiveUtils
@which Dense(10 => 32,tanh)
#+end_src

If we go to that spot of the documentation, we find the following.

#+begin_src julia;eval=false
struct Dense{F, M<:AbstractMatrix, B}
  weight::M
  bias::B
  σ::F
  function Dense(W::M, bias = true, σ::F = identity) where {M<:AbstractMatrix, F}
    b = create_bias(W, bias, size(W,1))
    new{F,M,typeof(b)}(W, b, σ)
  end
end

function Dense((in, out)::Pair{<:Integer, <:Integer}, σ = identity;
               init = glorot_uniform, bias = true)
  Dense(init(out, in), bias, σ)
end
#+end_src

First, =Dense= defines a struct in Julia. This struct just holds a weight matrix =W=, a bias vector =b=, and an activation function =σ=. The function called =Dense= is what's known as an *outer constructor* which defines how the =Dense= type is built. If you give it two integers (and optionally an activation function which defaults to =identity=), then what it will do is take random initial =W= and =b= matrices (according to the =glorot_uniform= distribution for =W= and =zeros= for =b=), and then it will build the type with those matrices.

The last portion might be new. This is known as a *callable struct*, or a functor. It defines the dispatch for how calls work on the struct. As a quick demonstration, let's define a type =MyCallableStruct= with a field =x=, and then make instances of =A= be the function =x+y=:

#+begin_src julia
struct MyCallableStruct
  x
end

(a::MyCallableStruct)(y) = a.x+y
a = MyCallableStruct(2)
a(3)
#+end_src

If you're familiar with object-oriented programming, this is similar to using an object in a way that references the =self=, though it's a bit more general due to allowing dispatching, i.e. this can then dependent on the input types as well.

So let's look at that =Dense= call with this in mind:

#+begin_src julia;eval=false
function (a::Dense)(x::AbstractArray)
  W, b, σ = a.W, a.b, a.σ
  σ.(W*x .+ b)
end
#+end_src

This means that =Dense= is a function that takes in an =x= and computes =σ.(W*x.+b)=, which is precisely how we defined the layer before! To see that this is just a function, let's call it directly:

#+begin_src julia
denselayer_f = Dense(32 => 32,tanh)
denselayer_f(rand(32))
#+end_src

So okay, =Dense= objects are just functions that have weight and bias matrices inside of them. Now what does =Chain= do?

#+begin_src julia
@which Chain(1,2,3)
#+end_src

gives us:

#+begin_src julia;eval=false
struct Chain{T<:Tuple}
  layers::T
  Chain(xs...) = new{typeof(xs)}(xs)
end

applychain(::Tuple{}, x) = x
applychain(fs::Tuple, x) = applychain(tail(fs), first(fs)(x))

(c::Chain)(x) = applychain(c.layers, x)
#+end_src

Let's now dig into this. The =...= is known that the *slurp operator*, which allows for "slurping up" multiple arguments into a single object =xs=. For example:

#+begin_src julia
slurper(xs...) = @show xs
slurper(1,2,3,4,5)
#+end_src

We see that slurps the inputs up into a =Tuple=, which is an immutable data store. (Note: Tuples are stack-allocated if inferred and is the internal data store of the compiler itself, and compiler inference can know exactly the size and the type of each individual object, so this does not have an overhead if fully inferred).

The function =Chain(xs...) = new{typeof(xs)}(xs)= is an *inner constructor* which builds a new instance of =Chain= where =layers= is a tuple of the inputs. This means that in our case where we put a bunch of =Dense= inside of there, =layers= is a tuple of functions. What does =Chain= do? Let's look at its call:

#+begin_src julia;eval=false
(c::Chain)(x) = applychain(c.layers, x)
#+end_src

This takes the tuple of functions and then does =applychain= on it.

#+begin_src julia;eval=false
applychain(::Tuple{}, x) = x
applychain(fs::Tuple, x) = applychain(tail(fs), first(fs)(x))
#+end_src

=applychain= is a recursive function which applies the first element of the tuple onto =x=, then it calls =applychain= to call the second function onto =x=, repeatedly until there are no more functions in which case it returns =x=. This means that =applychain= is simply doing =h(g(f(x)))= on the tuple of functions =(f,g,h)=! We can thus see that this library function is exactly equivalent to the neural network we defined by hand, just put together in a different form to give a nice user interface.

**** Detail: Recursion?
:PROPERTIES:
:CUSTOM_ID: detail-recursion
:END:
But there's one more detail... why recursion? If you define a function, look at its type:

#+begin_src julia
ff(x,y) = 2x+y
typeof(ff)
#+end_src

Notice that its type is simply =typeof(ff)= which is unique to the function, i.e. every single function is its own struct. In fact, given what we just learned, it wouldn't be a surprise to learn that is exactly what a function is in Julia! A function definition lowers at the parser level to something like:

#+begin_src julia;eval=false
struct ff2 <: Function end
(_::ff2)(x,y) = 2x + y
const ff = ff2()
#+end_src

This means that the primitive operation here that everything really comes down to is calls on structs. Why is this done with unique *singleton* types? (Singleton types are types where every instance is equivalent). Well, if we want the compiler to be able to optimize with respect to which function we are handling inside of another function, then we need "what function we are dealing with" as compile-time information, which necessitates being type information.

Tuples are contravariant and heterogeneously typed with a parameter per internal object. For example:

#+begin_src julia
tup = (1.0,1,"1")
typeof(tup)
#+end_src

This means that it is possible to infer outputs of a tuple even if it's heterogeneously typed by making good use of constant literals. For example, the expression =tup[1]= will be inferred to have the output =Float64=. However, note that if =i= is not a compile-time constant, then =tup[i]= cannot be inferred since, given what the compiler knows, the output could be either a =Float64=, an =Int64=, or a =String=.

So now let's think back to our tuple of functions. By what we described before, =tup = (f,g,h)= is going to have a different type for each of the functions and thus could not specialize on the inputs if we used =tup[i]=. Therefore:

#+begin_src julia;eval=false
for i in 1:length(tup)
  x = tup[i](x)
end
#+end_src

would be slow (if the function call cost is small compared to the dispatch cost of about 100ns. This is not always the case, but should be considered in many instances!). So how can you get around it? Well, if everything was constant literals then this would specialize:

#+begin_src julia;eval=false
tup[3](tup[2](tup[1](x)))
#+end_src

would fully specialize and infer, and the compiler would have full knowledge of the entire call chain as if it were written out as straightline code. Now if we look at the recursion again:

#+begin_src julia;eval=false
applychain(::Tuple{}, x) = x
applychain(fs::Tuple, x) = applychain(tail(fs), first(fs)(x))
#+end_src

we see that, at compile-time, we know that =typeof((f,g,h)) = Tuple{typeof(f),typeof(g),typeof(h)}=, and so we know that the first =first(fs)= will be =f=, and can specialize on this. We know then that =tail(fs)= has to be the =(g,h)= and so then we recurse and know that =g= is first and ... This means that this scheme is equivalent to have written out =xs[3](xs[2](xs[1](x)))= and is thus generating code perfectly specialized to the order and amount of functions we had put into the =Chain=. This kind of abstraction, an abstraction where all of the overhead compiles away, is known as a *zero-cost abstraction*.

(Note that technically, there is a cost since the compiler has to unravel this chain of events.)

*** Training Neural Networks
:PROPERTIES:
:CUSTOM_ID: training-neural-networks
:END:
Now let's get into training neural networks. "Training" a neural network is simply the process of finding weights that minimize a loss function. For example, let's say we wanted to make our neural network be the constant function =1= for any input =x \in [0,1]^{10}=. We can then write the loss function:

#+begin_src julia
NN = Chain(Dense(10 => 32,tanh),
           Dense(32 => 32,tanh),
           Dense(32 => 5))
loss() = sum(abs2,sum(abs2,NN(rand(10)).-1) for i in 1:100)
loss()
#+end_src

This loss function takes 100 random points in =[0,1]= and then computes the output of the neural network minus =1= on each of the values, and sums up the squared values (=abs2=). Why the squared values? This means that every computed loss value is positive, and so we know that by decreasing the loss this means that, on average our neural network outputs are closer to 1. What are the weights? Since we're using the Flux callable struct style from above, the weights are those inside of the =NN= chain object, which we can inspect:

#+begin_src julia
NN[1].weight # The W matrix of the first layer
#+end_src

Now let's grab all of the parameters together:

#+begin_src julia
p = Flux.params(NN)
#+end_src

That's a helper function on =Chain= which recursively gathers all of the defining parameters. Let's now find the optimal values =p= which cause the neural network to be the constant =1= function:

#+begin_src julia
Flux.train!(loss, p, Iterators.repeated((), 10000), ADAM(0.1))
#+end_src

Now let's check the loss:

#+begin_src julia
loss()
#+end_src

This means that =NN(x)= is now a very good function approximator to =f(x) = ones(5)=!

*** So Why Machine Learning? Why Neural Networks?
:PROPERTIES:
:CUSTOM_ID: so-why-machine-learning-why-neural-networks
:END:
All we did was find parameters that made =NN(x)= act like a function =f(x)=. How does that relate to machine learning? Well, in any case where one is acting on data =(x,y)=, the idea is to assume that there exists some underlying mathematical model =f(x) = y=. If we had perfect knowledge of what =f= is, then from only the information of =x= we can then predict what =y= would be. The inference problem is to then figure out what function =f= should be. Therefore, machine learning on data is simply this problem of finding an approximator to some unknown function!

So why neural networks? Neural networks satisfy two properties. The first of which is known as the Universal Approximation Theorem (UAT), which in simple non-mathematical language means that, for any ϵ of accuracy, if your neural network is large enough (has enough layers, the weight matrices are large enough), then it can approximate *any* (nice) function =f= within that ϵ. Therefore, we can reduce the problem of finding missing functions, the problem of machine learning, to a problem of finding the weights of neural networks, which is a well-defined mathematical optimization problem.

Why neural networks specifically? That's a fairly good question, since there are many other functions with this property. For example, you will have learned from analysis that =a_0 + a_1 x + a_2 x^2 + \ldots= arbitrary polynomials can be used to approximate any analytic function (this is the Taylor series). Similarly, a Fourier series

#+begin_src math
f(x) = a_0 + \sum_k b_k \cos(kx) + c_k \sin(kx)
#+end_src

can approximate any continuous function =f= (and discontinuous functions also can have convergence, etc. these are the details of a harmonic analysis course).

That's all for one dimension. How about two dimensional functions? It turns out it's not difficult to prove that tensor products of universal approximators will give higher dimensional universal approximators. So for example, tensoring together two polynomials:

#+begin_src math
a_0 + a_1 x + a_2 y + a_3 x y + a_4 x^2 y + a_5 x y^2 + a_6 x^2 y^2 + \ldots
#+end_src

will give a two-dimensional function approximator. But notice how we have to resolve every combination of terms. This means that if we used =n= coefficients in each dimension =d=, the total number of coefficients to build a =d=-dimensional universal approximator from one-dimensional objects would need =n^d= coefficients. This exponential growth is known as *the curse of dimensionality*.

The second property of neural networks that makes them applicable to machine learning is that they overcome the curse of dimensionality. The proofs in this area [[https://arxiv.org/abs/1908.10828][can be a little difficult to parse]], but what they boil down to is proving in many cases that the growth of neural networks to sufficiently approximate a =d=-dimensional function grows as a polynomial of =d=, rather than exponential. This means that there's some dimensional cutoff where for =d>cutoff= it is more efficient to use a neural network. This can be problem-specific, but generally it tends to be the case at least by 8 or 10 dimensions.

Neural networks have a few other properties to consider as well:

1. The assumptions of the neural network can be encoded into the neural architectures. A neural network where the last layer has an activation function =x->x^2= is a neural network where all outputs are positive. This means that if you want to find a positive function, you can make the optimization easier by enforcing this constraint. A lot of other constraints can be enforced, like =tanh= activation functions can make the neural network be a smooth (all derivatives finite) function, or other activations can cause finite numbers of learnable discontinuities.
2. Generating higher dimensional forms from one dimensional forms does not have good symmetry. For example, the two-dimensional tensor Fourier basis does not have a good way to represent =sin(xy)=. This property of the approximator is called (non)isotropy and more detail can be found in [[https://www.youtube.com/watch?v=JngdaWe3-gg][this wonderful talk about function approximation for multidimensional integration (cubature)]]. Neural networks are naturally not aligned to a basis.
3. Neural networks are "easy" to compute. There's good software for them, GPU-acceleration, and all other kinds of tooling that make them particularly simple to use.
4. There are proofs that in many scenarios for neural networks [[https://arxiv.org/abs/2006.05900][the local minima are the global minima]], meaning that local optimization is sufficient for training a neural network. Global optimization (which we will cover later in the course) is much more expensive than local methods like gradient descent, and thus this can be a good property to abuse for faster computation.

*** From Machine Learning to Scientific Machine Learning: Structure and Science
:PROPERTIES:
:CUSTOM_ID: from-machine-learning-to-scientific-machine-learning-structure-and-science
:END:
This understanding of a neural network and their libraries directly bridges to the understanding of scientific machine learning and the computation done in the field. In scientific machine learning, neural networks and machine learning are used as the basis to solve problems in scientific computing. [[https://en.wikipedia.org/wiki/Computational_science][Scientific computing, as a discipline also known as Computational Science, is a field of study which focuses on scientific simulation, using tools such as differential equations to investigate physical, biological, and other phenomena]].

What we wish to do in scientific machine learning is use these properties of neural networks to improve the way that we investigate our scientific models.

**** Aside: Why Differential Equations?
:PROPERTIES:
:CUSTOM_ID: aside-why-differential-equations
:END:
Why do differential equations come up so often in as the model in the scientific context? This is a deep question with quite a simple answer. Essentially, all scientific experiments always have to test how things change. For example, you take a system now, you change it, and your measurement is how the changes you made caused changes in the system. This boils down to gather information about how, for some arbitrary system =y = f(x)=, how =\Delta x= is related to =\Delta y=. Thus what you learn from scientific experiments, what is codified as scientific laws, is not "the answer", but the answer to how things change. This process of writing down equations by describing how they change precisely gives differential equations.

** Solving ODEs with Neural Networks: The Physics-Informed Neural Network
:PROPERTIES:
:CUSTOM_ID: solving-odes-with-neural-networks-the-physics-informed-neural-network
:END:
Now let's get to our first true SciML application: solving ordinary differential equations with neural networks. The process of solving a differential equation with a neural network, or using a differential equation as a regularizer in the loss function, is known as a *physics-informed neural network*, since this allows for physical equations to guide the training of the neural network in circumstances where data might be lacking.

*** Background: A Method for Solving Ordinary Differential Equations with Neural Networks
:PROPERTIES:
:CUSTOM_ID: background-a-method-for-solving-ordinary-differential-equations-with-neural-networks
:END:
[[https://arxiv.org/pdf/physics/9705023.pdf][This is a result first due to Lagaris et. al from 1998]]. The idea is to solve differential equations using neural networks by representing the solution by a neural network and training the resulting network to satisfy the conditions required by the differential equation.

Let's say we want to solve a system of ordinary differential equations

$$u' = f(u,t)$$

with $t \in [0,1]$ and a known initial condition $u(0)=u_0$. To solve this, we approximate the solution by a neural network:

$$NN(t) \approx u(t)$$

If $NN(t)$ was the true solution, then it would hold that $NN'(t) = f(NN(t),t)$ for all $t$. Thus we turn this condition into our loss function. This motivates the loss function:

$$L(p) = \sum_i \left(\frac{dNN(t_i)}{dt} - f(NN(t_i),t_i) \right)^2$$

The choice of $t_i$ could be done in many ways: it can be random, it can be a grid, etc. Anyways, when this loss function is minimized (gradients computed with standard reverse-mode automatic differentiation), then we have that $\frac{dNN(t_i)}{dt} \approx f(NN(t_i),t_i)$ and thus $NN(t)$ approximately solves the differential equation.

Note that we still have to handle the initial condition. One simple way to do this is to add an initial condition term to the cost function. This would look like:

$$L(p) = (NN(0) - u_0)^2 + \sum_i \left(\frac{dNN(t_i)}{dt} - f(NN(t_i),t_i) \right)^2$$

While that would work, it can be more efficient to encode the initial condition into the function itself so that it's trivially satisfied for any possible set of parameters. For example, instead of directly using a neural network, we can use:

$$g(t) = u_0 + tNN(t)$$

as our solution. Notice that $g(t)$ is thus a universal approximator for all continuous functions such that $g(0)=u_0$ (this is a property one should prove!). Since $g(t)$ will always satisfy the initial condition, we can train $g(t)$ to satisfy the derivative function then it will automatically be a solution to the derivative function. In this sense, we can use the loss function:

$$L(p) = \sum_i \left(\frac{dg(t_i)}{dt} - f(g(t_i),t_i) \right)^2$$

where =p= are the parameters that define =g=, which in turn are the parameters which define the neural network =NN= that define =g=. Thus this reduces down, once again, to simply finding weights which minimize a loss function!

*** Coding Up the Method
:PROPERTIES:
:CUSTOM_ID: coding-up-the-method
:END:
Now let's implement this method with Flux. Let's define a neural network to be the =NN(t)= above. To make the problem easier, let's look at the ODE:

\[u' = \cos 2\pi t\]

and approximate it with the neural network from a scalar to a scalar:

#+begin_src julia
using Flux
NNODE = Chain(x -> [x], # Take in a scalar and transform it into an array
           Dense(1 => 32,tanh),
           Dense(32 => 1),
           first) # Take first value, i.e. return a scalar
NNODE(1.0)
#+end_src

Instead of directly approximating the neural network, we will use the transformed equation that is forced to satisfy the boundary conditions. Using =u0=1.0=, we have the function:

#+begin_src julia
g(t) = t*NNODE(t) + 1f0
#+end_src

as our universal approximator. Thus, for this to be a function that satisfies

\[g' = \cos 2\pi t\]

we would need that:

#+begin_src julia
using Statistics
ϵ = sqrt(eps(Float32))
loss() = mean(abs2(((g(t+ϵ)-g(t))/ϵ) - cos(2π*t)) for t in 0:1f-2:1f0)
#+end_src

would be minimized.

#+begin_src julia
opt = Flux.Descent(0.01)
data = Iterators.repeated((), 5000)
iter = 0
cb = function () #callback function to observe training
  global iter += 1
  if iter % 500 == 0
    display(loss())
  end
end
display(loss())
Flux.train!(loss, Flux.params(NNODE), data, opt; cb=cb)
#+end_src

How well did this do? Well if we take the integral of both sides of our differential equation, we see it's fairly trivial:

#+begin_src math
\int g' = g = \int \cos 2\pi t = C + \frac{\sin 2\pi t}{2\pi}
#+end_src

where we defined =C = 1=. Let's take a bunch of (input,output) pairs from the neural network and plot it against the analytical solution to the differential equation:

#+begin_src julia
using Plots
t = 0:0.001:1.0
plot(t,g.(t),label="NN")
plot!(t,1.0 .+ sin.(2π.*t)/2π, label = "True Solution")
#+end_src

We see that it matches very well, and we can keep improving this fit by increasing the size of the neural network, using more training points, and training for more iterations.

*** Example: Harmonic Oscillator Informed Training
:PROPERTIES:
:CUSTOM_ID: example-harmonic-oscillator-informed-training
:END:
Using this idea, differential equations encoding physical laws can be utilized inside of loss functions for terms which we have some basis to believe should approximately follow some physical system. Let's investigate this last step by looking at how to inform the training of a neural network using the harmonic oscillator.

Let's assume that we are taking measurements of (position,force) in some real one-dimensional spring pushing and pulling against a wall.

[[https://thumbs.dreamstime.com/b/hookes-law-vector-illustration-physics-extend-spring-force-explanation-scheme-compress-mathematical-experiment-weight-177188357.jpg]]

But instead of the simple spring, let's assume we had a more complex spring, for example, let's say =F(x) = -kx + 0.1sin(x)= where this extra term is due to some deformities in the medal (assume mass=1). Then by Newton's law of motion we have a second order ordinary differential equation:

#+begin_src math
x'' = -kx + 0.1 \sin(x)
#+end_src

We can use the [[https://diffeq.sciml.ai/stable/][DifferentialEquations.jl package]] to solve this differential equation and see what this system looks like:

#+begin_src julia
using DifferentialEquations
k = 1.0
force(dx,x,k,t) = -k*x + 0.1sin(x)
prob = SecondOrderODEProblem(force,1.0,0.0,(0.0,10.0),k)
sol = solve(prob)
plot(sol,label=["Velocity" "Position"])
#+end_src

Don't worry if you don't understand this sytnax yet: we will go over differential equation solvers and DifferentialEquations.jl in a later lecture.

Let's say we want to learn how to predict the force applied on the spring at each point in space, =F(x)=. We want to learn a function, so this is the job for machine learning! However, we only have 6 measurements, which includes the information about (position,velocity,force) at evenly spaced times:

#+begin_src julia
plot_t = 0:0.01:10
data_plot = sol(plot_t)
positions_plot = [state[2] for state in data_plot]
force_plot = [force(state[1],state[2],k,t) for state in data_plot]

# Generate the dataset
t = 0:3.3:10
dataset = sol(t)
position_data = [state[2] for state in sol(t)]
force_data = [force(state[1],state[2],k,t) for state in sol(t)]

plot(plot_t,force_plot,xlabel="t",label="True Force")
scatter!(t,force_data,label="Force Measurements")
#+end_src

Can we train a neural network to approximate the expected force at any location for this spring? To see whether this is possible with a standard neural network, let's just do it. Let's define a neural network to be =F(x)= and see if we can learn the force function!

#+begin_src julia
NNForce = Chain(x -> [x],
           Dense(1 => 32,tanh),
           Dense(32 => 1),
           first)
#+end_src

Now our loss function will be to match the force at the (position,force) pairs in the dataset:

#+begin_src julia
loss() = sum(abs2,NNForce(position_data[i]) - force_data[i] for i in 1:length(position_data))
loss()
#+end_src

Our random parameters do not do so well, so let's train!

#+begin_src julia
opt = Flux.Descent(0.01)
data = Iterators.repeated((), 5000)
iter = 0
cb = function () #callback function to observe training
  global iter += 1
  if iter % 500 == 0
    display(loss())
  end
end
display(loss())
Flux.train!(loss, Flux.params(NNForce), data, opt; cb=cb)
#+end_src

The neural network almost exactly matched the dataset, but how well did it actually learn the real force function? Let's plot it to see:

#+begin_src julia
learned_force_plot = NNForce.(positions_plot)

plot(plot_t,force_plot,xlabel="t",label="True Force")
plot!(plot_t,learned_force_plot,label="Predicted Force")
scatter!(t,force_data,label="Force Measurements")
#+end_src

Ouch. The problem is that a neural network can approximate any function, so it approximated /a/ function that fits the data, but not /the correct/ function. We somehow need to have more data... but where can we get more data?

Well, even a first year undergrad in physics will know Hooke's law, which is that the idealized spring should satisfy =F(x) = -kx=. This is a decent assumption for the evolution of the system:

#+begin_src julia
force2(dx,x,k,t) = -k*x
prob_simplified = SecondOrderODEProblem(force2,1.0,0.0,(0.0,10.0),k)
sol_simplified = solve(prob_simplified)
plot(sol,label=["Velocity" "Position"])
plot!(sol_simplified,label=["Velocity Simplified" "Position Simplified"])
#+end_src

While it's not quite correct, and it definitely drifts near the end, it should be a useful non-data assumption that we can add to improve the fitting. So, assuming we know =k= (this lab you probably have done before!), we can regularize this fitting by having a term that states our neural network should be the solution to the differential equation.

This term looks like what we had done before:

#+begin_src julia
random_positions = [2rand()-1 for i in 1:100] # random values in [-1,1]
loss_ode() = sum(abs2,NNForce(x) - (-k*x) for x in random_positions)
loss_ode()
#+end_src

If this term is zero, then =F(x) = -kx=, which is approximately true. So now let's put these together:

#+begin_src julia
λ = 0.1
composed_loss() = loss() + λ*loss_ode()
#+end_src

where =λ= is some weight factor to control the regularization against the physics assumption. Now we can train the physics-informed neural network:

#+begin_src julia
opt = Flux.Descent(0.01)
data = Iterators.repeated((), 5000)
iter = 0
cb = function () #callback function to observe training
  global iter += 1
  if iter % 500 == 0
    display(composed_loss())
  end
end
display(composed_loss())
Flux.train!(composed_loss, Flux.params(NNForce), data, opt; cb=cb)

learned_force_plot = NNForce.(positions_plot)

plot(plot_t,force_plot,xlabel="t",label="True Force")
plot!(plot_t,learned_force_plot,label="Predicted Force")
scatter!(t,force_data,label="Force Measurements")
#+end_src

And there we go: we have used knowledge of physics to help inform our neural network training process!

** Conclusion
:PROPERTIES:
:CUSTOM_ID: conclusion
:END:
In this lecture we motivated machine learning not as a process of predicting from data but as a process for learning arbitrary nonlinear functions. Neural networks were just one choice of possible function. We then demonstrated how differential equations could be solved using this function approximation technique and then put together these two domains, solving differential equations and approximating data, into a single process to allow for physical knowledge to be embedded into the training process of a neural network, thus arriving at a physics-informed neural network. This is just one method in scientific machine learning which we will be exploring in more detail, demonstrating how we can utilize scientific knowledge to improve fits and allow for data-efficient machine learning.
* How Loops Work, An Introduction to Discrete Dynamics
As we saw with the physics-informed neural networks, the basics of most scientific models are dynamical systems. Thus if we want to start to dig into deeper methods, we will need to start looking into the theory and practice of nonlinear dynamical systems. In this lecture we will go over the basic properties of dynamical systems and understand their general behavior through code. We will also learn the idea of stability as an asymptotic property of a mapping, and understand when a system is stable.
** Discrete Dynamical Systems
:PROPERTIES:
:CUSTOM_ID: discrete-dynamical-systems
:END:
A discrete dynamical system is a system which updates through discrete updates:

\[u_{n+1} = model(u_n,n)\]

There are many examples of a discrete dynamical system found throughout the scientific literature. For example, many ecological models are discrete dynamical systems, with the most famous being the logistic map:

\[u_{n+1} = r u_n (1 - u_n)\]

describing the growth of a population with a carrying capacity of 1 and a growth rate of =r=. Another way in which discrete dynamical systems are often encountered is through time series models. These are generally seen in financial forecasting and For example, the autoregressive model AR1 is the following linear dynamical system:

\[u_{n+1} = \alpha u_n + \epsilon_n\]

where \(\epsilon\) is a standard normal random number. The AR(k) model allows itself to update using delays as well:

\[u_{n+1} = \sum_{j=0}^{k-1} \alpha_j u_{n-j} + \epsilon_n\]

The ARMA model is one that allows using delays on the randomness as well:

\[u_{n+1} = \sum_{j=0}^{k-1} (\alpha_j u_{n-j}  + \beta_j \epsilon_{n-j})\]

Another embodiment of a discrete dynamical system is a Recurrent Neural Network (RNN). In its simplest form, a RNN is a system of the form:

\[u_{n+1} = u_n + f(u_n,\theta)\]

where \(f\) is a neural network parameterized by \(\theta\).

Note that discrete dynamical systems are even more fundamental than just the ones shown. In any case where a continuous model is discretized to loop on the computer, the resulting algorithm is a discrete dynamical system and thus evolves according to its properties. This fact will be revisited later.

** Properties of Linear Dynamical Systems
:PROPERTIES:
:CUSTOM_ID: properties-of-linear-dynamical-systems
:END:
First let's take a look at the scalar linear dynamical system:

\[u_{n+1} = \alpha u_{n}\]

We want to ask what the global or geometric behavior of this system is. We can do this by expanding out the system. Notice that if =u_0= is known, then

\[u_{n+1} = \alpha^n u_0\]

The global behavior can then be categorized as:

- If \(\Vert \alpha \Vert < 1\), then \(u_n \rightarrow 0\)
- If \(\Vert \alpha \Vert > 1\), then \(u_n \rightarrow \infty\)

If \(\Vert \alpha \Vert = 1\), then \(u_n \rightarrow u_0\) if everything is in the real numbers, but more complex dynamics can occur on the complex plane.

** Nonlinear Geometric Dynamics
:PROPERTIES:
:CUSTOM_ID: nonlinear-geometric-dynamics
:END:
The Geometric Theory of Dynamical Systems is the investigation of their long-term properties and the geometry of the phase space which they occupy. Let's start looking at this in practical terms: how do nonlinear update equations act as time goes to infinity?

**** Banach Fixed Point Theorem
:PROPERTIES:
:CUSTOM_ID: banach-fixed-point-theorem
:END:
There are surprisingly simple results that we can prove. First let's recall the Banach Fixed Point Theorem (also known as the Contraction Mapping Theorem). Let \((X,d)\) be a metric space (\(X\) is the set of points we are thinking of, here the real numbers. \(d\) is a distance function). \(f\) is a contraction mapping if

\[d(f(x),f(y)) \leq q d(x,y)\]

where \(q < 1\), that is, if applying \(f\) always decreases the distance. The theorem then states that if \(f\) is a contraction mapping, then there is a unique fixed point (point \(x^\ast\) where \(f(x^\ast)=x^\ast\)) and a sequence such that \(x_0 \rightarrow x^\ast\) where

\[x_{n+1} = f(x_n)\]

The proof is by induction, showing that the sequence is Cauchy. For some \(m>n\) we do by the Triangle Inequality

\[d(x_m,x_n) \leq d(x_{m},x_{m-1}) + \ldots + d(x_{n+1},x_n)\]

then apply the contraction relation down to the bottom:

\[d(x_m,x_n) \leq q^{m-1} d(x_{1},x_{0}) + \ldots + q^{n} d(x_{1},x_0)\]

\[d(x_m,x_n) \leq q^n d(x_{1},x_{0}) \sum_{k=0}^{m-n-1} q^k\]

and since adding more never hurts:

\[d(x_m,x_n) \leq q^n d(x_{1},x_{0}) \sum_{k=0}^{\infty} q^k\]

But that summation is just a geometric series now, and since \(q<1\) we know it converges to \(1/(1-q)\), and so we get:

\[d(x_m,x_n) \leq \frac{q^n}{1-q} d(x_{1},x_{0})\]

The coefficient converges to zero as \(n\) increases, and so the sequence must be Cauchy, which implies there's a unique fixed point.

**** Stability of Linear Discrete Dynamical Systems
:PROPERTIES:
:CUSTOM_ID: stability-of-linear-discrete-dynamical-systems
:END:
Now let's take a mapping \(f\) which is sufficiently nice (\(f \in C^1\), i.e. the derivative of \(f\) exists and is continuous), where

\[x_{n+1} = f(x_n)\]

Assume that \(\Vert f^\prime (x^\ast) \Vert < 1\) at some point where \(f(x)=x\). Then by continuity of the second derivative, it follows that there is a neighborhood where \(\Vert f^\prime (x) \Vert < 1\) (). Now recall that this means

\[\frac{df}{dx} \leq 1\]

which means that, for any \(x\) and \(y\) in the neighborhood,

\[\Vert \frac{f(y)-f(x)}{y-x} \Vert \leq 1\]

or

\[\Vert f(y)-f(x) \Vert \leq \Vert y-x \Vert\]

This is essentially another way of saying that a function that is differentiable is Lipschitz, where we can use the derivative as the Lipschitz bound. But notice this means that, in this neighborhood, a function with a derivative less than 1 is a contraction mapping, and thus there is a limiting sequence which goes to the fixed point by the Banach Fixed Point Theorem. Furthermore, the uniqueness guarantees that there is only one fixed point in a sufficiently small neighborhood where the derivative is all less than 1.

A way to interpret this result is that, any nice enough function \(f\) is locally linear. Thus we can understand the global properties of \(f\) by looking the linearization of its dynamics, where the best linear approximation is the linear function \(f^\prime (x) x\). This means that we can think of

\[x_{n+1} = f(x_n)\]

locally as being approximated by

\[x_{n+1} = f^\prime (x) x_n\]

and so if the derivative is less than 1 in some neighborhood of a fixed point, then we locally have a linear dynamical system which looks like the simple \(x_{n+1} = \alpha x_n\) where \(\alpha <1\), and so we get the same convergence property.

This is termed "stability" since, if you are a little bit off from the fixed point, you will tend to go right back to it. An unstable fixed point is one where you fall away. And what happens when the derivative is one? There are various forms of semi-stability that can be proved which go beyond the topic of this course.

**** Update Form
:PROPERTIES:
:CUSTOM_ID: update-form
:END:
Now let's look at another form:

\[x_{n+1} = x_n + f(x_n)\]

For example, this is what we generally see with the recurrent neural network (or, as we will find out later, this is how discretizations of continuous systems tend to look!). In this case, we can say that this is a dynamical system

\[x_{n+1} = g(x_n)\]

and so if \(-2 < f^\prime < 0\), then \(\Vert g^\prime \Vert = \Vert 1 + f^\prime \Vert < 1\) and so we have the same stability idea except now with a condition shifted to zero instead of one.

** Multivariable Systems
:PROPERTIES:
:CUSTOM_ID: multivariable-systems
:END:
Now let \(x \in R^k\) be a vector, and define discrete mappings:

\[x_{n+1} = f(x_n)\]

To visualize this, let's write out the version for \(x \in R^3\):

\[x_{n+1}=\left[\begin{array}{c}
a_{n+1}\\
b_{n+1}\\
c_{n+1}
\end{array}\right]=\left[\begin{array}{c}
f_{1}(a_{n},b_{n},c_{n})\\
f_{2}(a_{n},b_{n},c_{n})\\
f_{3}(a_{n},b_{n},c_{n})
\end{array}\right]=f(x_{n})\]

The linear multidimensional discrete dynamical system is:

\[x_{n+1} = A x_n\]

The easiest way to analyze a multidimensional system is to turn it into a bunch of single dimension systems. To do this, assume that \(A\) is diagonalizable. This means that there exists a diagonalization \(A =P^{-1}DP\) where \(P\) is the matrix of eigenvectors and \(D\) is the diagonal matrix of eigenvalues. We can then decompose the system as follows:

\[Px_{n+1} = DPx_n\]

and now define new variables \(z_n = Px_n\). In these variables,

\[z_{n+1} = D z_n\]

but \(D\) is diagonal, so this is a system of \(k\) independent linear dynamical systems. We know that the linear dynamical system will converge to zero if \(\Vert D_i \Vert < 1\), and so this means that \(z_n\) converges to zero if all of the eigenvalues are within the unit circle. Since \(P0 = 0\), this implies that if all of the eigenvalues of \(A\) are in the unit circle, then \(x_n \rightarrow 0\).

A multidimensional version of the contraction mapping theorem is then proven exactly in this manner, meaning that if \(f(x) = x\) and all eigenvalues of the Jacobian matrix (the linearization of \(f\)) are in the unit circle, then \(x\) is a unique fixed point in some neighborhood.

**** Understanding Delayed Systems
:PROPERTIES:
:CUSTOM_ID: understanding-delayed-systems
:END:
A similar property holds in linear dynamical systems with delays. Take

\[x_{n+1} = \sum_{j=0}^{k-1} \alpha_j x_{n-j}\]

Notice that we can write this as a multidimensional non-delayed system. Let \(x_n^i\) be the \(i\)th term in the vector of the \(n\) time. Then we have:

\[x_{n+1}^1 = \sum_{j=1}^{k-1} \alpha_{j-1} x_{n}^{j}\]

as an equivalent way to write this, where

\[x_{n+1}^j = x_n^{j-1}\]

for all of the other terms. Essentially, instead of a system with a delay, we store the memory in other terms of the vector, and keep shifting them down. However, this makes our system much easier to analyze. Instead of a linear delayed dynamical system, this is now a linear multidimensional dynamical system. Its characteristic polynomial is

\[\varphi(x) = 1 - \sum_{j=0}^{k-1} \alpha_j x^j\]

and so if all of the roots are in the unit circle then this system is stable.

**** Stochastic Dynamical Systems
:PROPERTIES:
:CUSTOM_ID: stochastic-dynamical-systems
:END:
Now let's take a look again at the autoregressive process from time series analysis:

\[u_{n+1} = \sum_{j=0}^{k-1} \alpha_j u_{n-j} + \epsilon_n\]

In a very quick handwavy way, we can understand such a system by seeing how the perturbations propagate. If \(u_0 = 0\), then the starting is just \(\epsilon_0\). If we assume all other \(\epsilon_i = 0\), then this system is the same as a linear dynamical system with delays. If all of the roots are in the unit circle, then it goes to zero, meaning the perturbation is forgotten or squashed over time.

We can analyze this more by using the moments. Notice that, by the linearity of the expected value,

\[\mathbb{E}[u_{n+1}] = \sum_{j=0}^{k-1} \alpha_j \mathbb{E}[u_{n-j}]\]

is a deterministic linear dynamical system which converges if the roots are in the unit circle. This means that the mean stabilizes over time if all of the roots are in the unit circle. In time series analysis, this is called stationarity of the time series.

We can then also look at the stability of the variance as well. Recall that

\[\mathbb{V}[x] = \mathbb{E}[x^2] - \mathbb{E}[x]^2\]

and so therefore

\[\mathbb{E}[u_{n+1}^2] = \mathbb{E}[\sum_{j=0}^{k-1} \alpha_j u_{n-j}^2]\]

and with a bunch of analysis here, working in the same way with the same basic ideas, we can determine conditions on which the variance goes to zero.

** Periodicity and Chaos
:PROPERTIES:
:CUSTOM_ID: periodicity-and-chaos
:END:
Stability is the simplest geometric dynamical property, but there are many others. For example, maps can also have periodic orbits, like:

\[u_{n+1} = -u_n\]

will bounce back and forth between two values. These periodic orbits themselves have geometric properties, such as whether it's a stable periodic orbit (points nearby are attracted to the periodic orbit). Periodic orbits have a length as well: this was a periodic orbit of length 2.

Chaos is another interesting property of a discrete dynamical system. It can be interpreted as a periodic orbit where the length is infinity. This can happen if, by changing a parameter, a period 2 orbit becomes a period 4, then a period 8, etc. (a phenomenon known as period doubling), and when it goes beyond the accumulation point the "infinite period orbit" is reached and chaos is found. A homework problem will delve into the properties of chaos as an example of a simple embarrassingly data-parallel problem.

** Efficient Implmentation of Dynamical Systems
:PROPERTIES:
:CUSTOM_ID: efficient-implmentation-of-dynamical-systems
:END:
Dynamical systems are just loops, so the implementation is easy to understand. However, there are a few things that one must keep in mind in order to allow for efficient implementations.

**** Higher order functions
:PROPERTIES:
:CUSTOM_ID: higher-order-functions
:END:
Functions which compute the solutions to dynamical systems are inherently /higher order functions/, which means it's a function which takes in a function as an argument. The following is a quick implementation:

#+begin_src julia
"""
`solve_system(f,u0,n)`

Solves the dynamical system

``u_{n+1} = f(u_n)``

for N steps. Returns the solution at step `n` with parameters `p`.

"""
function solve_system(f,u0,p,n)
  u = u0
  for i in 1:n-1
    u = f(u,p)
  end
  u
end
#+end_src

Notice the ="""= before the function: this is a docstring. When the Julia REPL is queried with =?solve_system= this will be the description that is displayed.

Now, is this function going to be efficient? Recall from the earlier discussion that:

- Type-stability is necessary for inference to carry forward type information.
- Julia auto-specializes on input types.
- Inlining can occur automatically for sufficiently small functions.

From this information, we know that in order for this to be efficient, we require that the type of =f(u)= is inferred. But if =f= is a variable, how can that be inferred? In order for that to occur, we would have to know what =f= is, since not all functions will give the same output type. Additionally, in order to inline the function, we will have to know what the function is at compile-time. So, is it possible to make this implementation efficient?

It turns out that this does optimize due to one fact: every function is given by its own type. We can verify this by defining a function and checking its type:

#+begin_src julia
f(u,p) = u^2 - p*u
typeof(f)
#+end_src

It displays =typeof(f)= to indicate that the function =f= is its own type, and thus at automatic specialization time the compiler knows all of the information about the function and thus inlines and performs inference correctly.

Note that this does mean that the function will need to recompile for every new =f=. This is similar to statically compiling a function for use in a C/Fortran library. What is the equivalent to using a function like a shared library or a shared object? This is given by FunctionWrappers.jl. This directly stores the function pointer in an object that can have shared type information in order to keep every function as the same type. However, the wrapped function has more information about the pointer... what's necessary?

The answer is that FunctionWrappers.jl allows for specifying the input and output types, in order for the wrapper to do the right assertions for inference to carry forward type stability, since in this case inference is not able to step through the function pointer.

**** Quick Check
:PROPERTIES:
:CUSTOM_ID: quick-check
:END:
What will approximately be the value of this dynamical system after 1000 steps if you start at =1.0= with parameter =p=0.25=? Can you guess without solving the system? Think about steady states and stabiltiy.

#+begin_src julia
solve_system(f,1.0,0.25,1000)
#+end_src

The answer is that it goes to zero. The steady states are the zeros of the polynomial, which are =0= and =p+1=. It's reasonable to believe that it either goes to one of those 2 values or infinity. In the first step, =1^2 - 0.25 = 0.75 < 1= which suggests (but doesn't confirm!) that it's a contraction. Notice that the derivative is =2u-p=, and so =u=p+1=1.25= is not a stable steady state, and thus we go to zero. In fact, we can check a few values:

#+begin_src julia
solve_system(f,1.1,0.25,1000)
#+end_src

#+begin_src julia
solve_system(f,1.22,0.25,1000)
#+end_src

#+begin_src julia
solve_system(f,1.25,0.25,1000)
#+end_src

#+begin_src julia
solve_system(f,1.251,0.25,20)
#+end_src

Notice that the moment we go above the steady state =p+1=, we exponentially grow to infinity.

Just to double check the implementation:

#+begin_src julia
solve_system(f,1.251,0.25,10)
solve_system(f,1.251,0.25,100)
solve_system(f,1.251,0.25,1000)
#+end_src

Those allocations are just the output, and notice it's independent of the loop count.

**** Multidimensional System Implementations
:PROPERTIES:
:CUSTOM_ID: multidimensional-system-implementations
:END:
When we go to multidimensional systems, some care needs to be taken to decrease the number of allocations which are occurring. One of the ways to do this is to utilize statically sized arrays. For example, let's look at a discretization of the Lorenz system:

#+begin_src julia
function lorenz(u,p)
  α,σ,ρ,β = p
  du1 = u[1] + α*(σ*(u[2]-u[1]))
  du2 = u[2] + α*(u[1]*(ρ-u[3]) - u[2])
  du3 = u[3] + α*(u[1]*u[2] - β*u[3])
  [du1,du2,du3]
end
p = (0.02,10.0,28.0,8/3)
solve_system(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

Let's see what this gives us by saving:

#+begin_src julia
function solve_system_save(f,u0,p,n)
  u = Vector{typeof(u0)}(undef,n)
  u[1] = u0
  for i in 1:n-1
    u[i+1] = f(u[i],p)
  end
  u
end
to_plot = solve_system_save(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

#+begin_src julia
using Plots
x = [to_plot[i][1] for i in 1:length(to_plot)]
y = [to_plot[i][2] for i in 1:length(to_plot)]
z = [to_plot[i][3] for i in 1:length(to_plot)]
plot(x,y,z)
#+end_src

This is the chaotic Lorenz attractor plotted in phase space, i.e. the values of the variables against each other.

Let's look at the implementation a little bit more. =u = Vector{typeof(u0)}(undef,n)= is type-generic, meaning any =u0= can be used with that code. However, as a vector of vectors, it is a vector of pointers to contiguous memory, instead of being contiguous itself. Note that that means there is not much of a cost by not pre-specifying the size up front:

#+begin_src julia
function solve_system_save_push(f,u0,p,n)
  u = Vector{typeof(u0)}(undef,1)
  u[1] = u0
  for i in 1:n-1
    push!(u,f(u[i],p))
  end
  u
end
@time solve_system_save(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

#+begin_src julia
@time solve_system_save_push(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

The first time Julia compiles the function, and the second is a straight call.

#+begin_src julia
@time solve_system_save_push(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

or we can use =@btime=:

#+begin_src julia
using BenchmarkTools
@btime solve_system_save(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

#+begin_src julia
@btime solve_system_save_push(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

This is because growth costs are amortized, meaning that when pushing, the size isn't increasing by one each time, but rather it's doing something like doubling, so that it's averaging O(1) cost to keep growing (in theory the best is to do Golden ratio resizing, and long discussions can be had on this topic).

We can also look at what happens if we use matrices:

#+begin_src julia
function solve_system_save_matrix(f,u0,p,n)
  u = Matrix{eltype(u0)}(undef,length(u0),n)
  u[:,1] = u0
  for i in 1:n-1
    u[:,i+1] = f(u[:,i],p)
  end
  u
end
@btime solve_system_save_matrix(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

Where is this cost coming from? A large portion of the cost is due to the slicing on the =u=, which we can fix with a =view=:

#+begin_src julia
function solve_system_save_matrix_view(f,u0,p,n)
  u = Matrix{eltype(u0)}(undef,length(u0),n)
  u[:,1] = u0
  for i in 1:n-1
    u[:,i+1] = f(@view(u[:,i]),p)
  end
  u
end
@btime solve_system_save_matrix_view(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

Since we are only ever using single columns as a unit, notice that there isn't any benefit to keeping the whole thing contiguous, and in fact there are some downsides (cache is harder to optimize because the longer cache lines are unnecessary, the views need to be used). Also, growing the matrix adaptively is not a very good idea since every growth requires both allocating memory and copying over the old values:

#+begin_src julia
function solve_system_save_matrix_resize(f,u0,p,n)
  u = Matrix{eltype(u0)}(undef,length(u0),1)
  u[:,1] = u0
  for i in 1:n-1
    u = hcat(u,f(@view(u[:,i]),p))
  end
  u
end
@btime solve_system_save_matrix_resize(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

So for now let's go back to the Vector of Arrays approach. One way to reduce the number of allocations is to require that the user provides an in-place non-allocating function. For example:

#+begin_src julia
function lorenz(du,u,p)
  α,σ,ρ,β = p
  du[1] = u[1] + α*(σ*(u[2]-u[1]))
  du[2] = u[2] + α*(u[1]*(ρ-u[3]) - u[2])
  du[3] = u[3] + α*(u[1]*u[2] - β*u[3])
end
p = (0.02,10.0,28.0,8/3)
function solve_system_save(f,u0,p,n)
  u = Vector{typeof(u0)}(undef,n)
  du = similar(u0)
  u[1] = u0
  for i in 1:n-1
    f(du,u[i],p)
    u[i+1] = du
  end
  u
end
solve_system_save(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

Oh no, all of the outputs are the same! What happened? The problem is in the line =u[i+1] = du=. What we had done is set the save vector to the same pointer as =du=, effectively linking all of the pointers. The moral of the story is, you cannot get around allocating for all of these outputs if you're going to give the user all of the outputs! It's impossible to not make all of these arrays, so if this is the case then you'd have to:

#+begin_src julia
function solve_system_save_copy(f,u0,p,n)
  u = Vector{typeof(u0)}(undef,n)
  du = similar(u0)
  u[1] = u0
  for i in 1:n-1
    f(du,u[i],p)
    u[i+1] = copy(du)
  end
  u
end
solve_system_save_copy(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

which nullifies the advantage of the non-allocating approach. However, if only the end point is necessary, then the reduced allocation approach is helpful:

#+begin_src julia
function solve_system_mutate(f,u0,p,n)
  # create work buffers
  du = similar(u0); u = copy(u0)
  # non-allocating loop
  for i in 1:n-1
    f(du,u,p)
    u,du = du,u
  end
  u
end
solve_system_mutate(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

Here we see a little trick: the line =u,du = du,u= is swapping the pointer of =u= with the pointer of =du= (since the value of the array is its reference). An alternative way to write the loop is:

#+begin_src julia;eval=false
for i in 1:n-1
  f(du,u,p)
  u .= du
end
#+end_src

which would compute =f= and then take the values of =du= and update =u= with them, but that's 3 extra operations than required, whereas =u,du = du,u= will change =u= to be a pointer to the updated memory and now =du= is an "empty" cache array that we can refill (this decreases the computational cost by ~33%). Let's see what the cost is with this newest version:

#+begin_src julia
@btime solve_system(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

#+begin_src julia
@btime solve_system_mutate(lorenz,[1.0,0.0,0.0],p,1000)
#+end_src

One last change that we could do is make use of StaticArrays. To do this, we need to go back to non-mutating, like:

#+begin_src julia
using StaticArrays
function lorenz(u,p)
  α,σ,ρ,β = p
  du1 = u[1] + α*(σ*(u[2]-u[1]))
  du2 = u[2] + α*(u[1]*(ρ-u[3]) - u[2])
  du3 = u[3] + α*(u[1]*u[2] - β*u[3])
  @SVector [du1,du2,du3]
end
p = (0.02,10.0,28.0,8/3)
function solve_system_save(f,u0,p,n)
  u = Vector{typeof(u0)}(undef,n)
  u[1] = u0
  for i in 1:n-1
    u[i+1] = f(u[i],p)
  end
  u
end
solve_system_save(lorenz,@SVector[1.0,0.0,0.0],p,1000)
#+end_src

#+begin_src julia
@btime solve_system_save(lorenz,@SVector[1.0,0.0,0.0],p,1000)
#+end_src

This is utilizing a lot more optimizations, like SIMD, automatically, which is helpful. Let's also remove the bounds checks:

#+begin_src julia
function lorenz(u,p)
  α,σ,ρ,β = p
  @inbounds begin
    du1 = u[1] + α*(σ*(u[2]-u[1]))
    du2 = u[2] + α*(u[1]*(ρ-u[3]) - u[2])
    du3 = u[3] + α*(u[1]*u[2] - β*u[3])
  end
  @SVector [du1,du2,du3]
end
function solve_system_save(f,u0,p,n)
  u = Vector{typeof(u0)}(undef,n)
  @inbounds u[1] = u0
  @inbounds for i in 1:n-1
    u[i+1] = f(u[i],p)
  end
  u
end
solve_system_save(lorenz,@SVector[1.0,0.0,0.0],p,1000)
#+end_src

#+begin_src julia
@btime solve_system_save(lorenz,@SVector[1.0,0.0,0.0],p,1000)
#+end_src

And we can get down to non-allocating for the loop:

#+begin_src julia
@btime solve_system(lorenz,@SVector([1.0,0.0,0.0]),p,1000)
#+end_src

Notice that the single allocation is the output.

We can lastly make the saving version completely non-allocating if we hoist the allocation out to the higher level:

#+begin_src julia
function solve_system_save!(u,f,u0,p,n)
  @inbounds u[1] = u0
  @inbounds for i in 1:length(u)-1
    u[i+1] = f(u[i],p)
  end
  u
end
u = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)
@btime solve_system_save!(u,lorenz,@SVector([1.0,0.0,0.0]),p,1000)
#+end_src

It is important to note that this single allocation does not seem to effect the timing of the result in this case, when run serially. However, when parallelism or embedded applications get involved, this can be a significant effect.

** Discussion Questions
:PROPERTIES:
:CUSTOM_ID: discussion-questions
:END:
1. What are some ways to compute steady states? Periodic orbits?
2. When using the mutating algorithms, what are the data dependencies between different solves if they were to happen simultaneously?
3. We saw that there is a connection between delayed systems and multivariable systems. How deep does that go? Is every delayed system also a multivariable system and vice versa? Is this a useful idea to explore?
* The Basics of Single Node Parallel Computing
Moore's law was the idea that computers double in efficiency at fixed time points, leading to exponentially more computing power over time. This was true for a very long time.
[[https://assets.weforum.org/editor/large_SOupdi6_TD1Lyud4kWEHmsB5rcslL0q2BB6UCRCEZKE.png]]
However, sometime in the last decade, computer cores have stopped getting faster.
#+begin_quote
The technology that promises to keep Moore's Law going after 2013 is known as extreme ultraviolet (EUV) lithography. It uses light to write a pattern into a chemical layer on top of a silicon wafer, which is then chemically etched into the silicon to make chip components. EUV lithography uses very high energy ultraviolet light rays that are closer to X-rays than visible light. That's attractive because EUV light has a short wavelength---around 13 nanometers---which allows for making smaller details than the 193-nanometer ultraviolet light used in lithography today. But EUV has proved surprisingly difficult to perfect.
#+end_quote
-MIT Technology Review
The answer to the "end of Moore's Law" is Parallel Computing. However, programs need to be specifically designed in order to adequately use parallelism. This lecture will describe at a very high level the forms of parallelism and when they are appropriate. We will then proceed to use shared-memory multithreading to parallelize the simulation of the discrete dynamical system.
** Managing Threads
:PROPERTIES:
:CUSTOM_ID: managing-threads
:END:
*** Concurrency vs Parallelism and Green Threads
:PROPERTIES:
:CUSTOM_ID: concurrency-vs-parallelism-and-green-threads
:END:
There is a difference between concurrency and parallelism. In a nutshell:

- Concurrency: Interruptability
- Parallelism: Independentability

[[http://tutorials.jenkov.com/images/java-concurrency/concurrency-vs-parallelism-1.png]] [[http://tutorials.jenkov.com/images/java-concurrency/concurrency-vs-parallelism-2.png]]

To start thinking about concurrency, we need to distinguish between a process and a thread. A process is discrete running instance of a computer program. It has allocated memory for the program's code, its data, a heap, etc. Each process can have many compute threads. These threads are the unit of execution that needs to be done. On each task is its own stack and a virtual CPU (virtual CPU since it's not the true CPU, since that would require that the task is ON the CPU, which it might not be because the task can be temporarily haulted). The kernel of the operating systems then /schedules/ tasks, which runs them. In order to keep the computer running smooth, /context switching/, i.e. changing the task that is actually running, happens all the time. This is independent of whether tasks are actually scheduled in parallel or not.

[[https://blog-assets.risingstack.com/2017/02/kernel-processes-and-threads-1.png]]

[[https://dave.cheney.net/wp-content/uploads/2015/08/process.png]]

[[https://dave.cheney.net/wp-content/uploads/2015/08/guard-page.png]]

Each thread has its own stack associated with it.

[[https://dave.cheney.net/wp-content/uploads/2015/08/threads.png]]

[[https://dave.cheney.net/wp-content/uploads/2015/08/stack-growth.png]]

This is an important distinction because many tasks may need to be ran concurrently but without parallelism. Examples of this are input/output (I/O). For example, in a game you may want to be updating the graphics, but the moment a user clicks you want to handle that event. You do not necessarily need to have these running in parallel, but you need the event handling task to be running concurrently to the processing of the game.

[[https://assets.weforum.org/editor/large_MbM-fLOQDkOW_Gvmj_X5ZO9ys6dDF4EMrtiVQG-Fy4Y.png]]

Data handling is the key area of scientific computing where green threads (concurrent non-parallel threads) show up. For data handling, one may need to send a signal that causes a message to start being passed. Alternative hardware take over at that point. This alternative hardware is a processor specific for an I/O bus, like the controller for the SSD, modem, GPU, or Infiniband. It will be polled, then it will execute the command, and give the result. There are two variants:

- Non-Blocking vs Blocking: Whether the thread will periodically poll for whether that task is complete, or whether it should wait for the task to complete before doing anything else
- Synchronous vs Asynchronus: Whether to execute the operation as initiated by the program or as a response to an event from the kernel.

I/O operations cause a /privileged context switch/, allowing the task which is handling the I/O to directly be switched to in order to continue actions.

**** The Main Event Loop
:PROPERTIES:
:CUSTOM_ID: the-main-event-loop
:END:
Julia, along with other languages with a runtime (Javascript, Go, etc.) at its core is a single process running an event loop. This event loop has is the main thread, and "Julia program" or "script" that one is running is actually ran in a green thread that is controlled by the main event loop. The event loop takes over to look for other work whenever the program hits a /yield point/. More yield points allows for more aggressive task switching, while it also means more switches to the event loop which /suspends/ the numerical task, i.e. making it slower. Thus yielding shouldn't interrupt the main loop!

This is one area where languages can wildly differ in implementation. Languages structured for lots of I/O and input handling, like Javascript, have yield points at every line (it's an interpreted language and therefore the interpreter can always take control). In Julia, the yield points are minimized. The common yield points are allocations and I/O (=println=). This means that a tight non-allocating inner loop will not have any yield points and will be a thread that is not interruptible. While this is great for numerical performance, it is something to be aware of.

Side effect: if you run a long tight loop and wish to exit it, you may try =Ctrl + C= and see that it doesn't work. This is because interrupts are handled by the event loop. The event loop is never re-entered until after your tight numerical loop, and therefore you have the waiting occur. If you hit =Ctrl + C= multiple times, you will escalate the interruption until the OS takes over and then this is handled by the signal handling of the OS's event loop, which sends a higher level interrupt which Julia handles the moment the safety locks says it's okay (these locks occur during memory allocations to ensure that memory is not corrupted).

**** Asynchronus Calling Example
:PROPERTIES:
:CUSTOM_ID: asynchronus-calling-example
:END:
This example will become more clear when we get to distributed computing, but for think of =remotecall_fetch= as a way to run a command on a different computer. What we want to do is start all of the commands at once, and then wait for all the results before finishing the loop. We will use =@async= to make the call to =remotecall_fetch= be non-blocking, i.e. it'll start the job and only poll infrequently to find out when the other machine has completed the job and returned the result. We then add =@sync= to the loop, which will only continue the loop after all of the green threads have fetched the result. Otherwise, it's possible that =a[idx]= may not be filled yet, since the thread may not have fetched the result!

#+begin_src julia;eval=false
@time begin
    a = Vector{Any}(undef,nworkers())
    @sync for (idx, pid) in enumerate(workers())
        @async a[idx] = remotecall_fetch(sleep, pid, 2)
    end
end
#+end_src

The same can be done for writing to the disk. =@async= is a quick shorthand for spawning a green thread which will handle that I/O operation, and the main event loop will keep switching between them until they are all handled. =@sync= encodes that the program will not continue until all green threads are handled. This could be done more manually with =Task= and =Channel=s, which will be something we touch on in the future.

*** Examples of the Differences
:PROPERTIES:
:CUSTOM_ID: examples-of-the-differences
:END:
Synchronous = Thread will complete an action

Blocking = Thread will wait until action is completed

- Asynchronous + Non-Blocking: I/O
- Asynchronous + Blocking: Threaded atomics (demonstrated next lecture)
- Synchronous + Blocking: Standard computing, =@sync=
- Synchronous + Non-Blocking: Webservers where an I/O operation can be performed, but one never checks if the operation is completed.

*** Multithreading
:PROPERTIES:
:CUSTOM_ID: multithreading
:END:
If your threads are independent, then it may make sense to run them in parallel. This is the form of parallelism known as multithreading. To understand the data that is available in a multithreaded setup, let's look at the picture of threads again:

[[https://blog-assets.risingstack.com/2017/02/kernel-processes-and-threads-1.png]]

Each thread has its own call stack, but it's the process that holds the heap. This means that dynamically-sized heap allocated objects are shared between threads with no cost, a setup known as shared-memory computing.

**** Loop-Based Multithreading with [cite/t:@threads]
:PROPERTIES:
:CUSTOM_ID: loop-based-multithreading-with-threads
:END:
Let's look back at our Lorenz dynamical system from before:

#+begin_src julia
using StaticArrays, BenchmarkTools
function lorenz(u,p)
  α,σ,ρ,β = p
  @inbounds begin
    du1 = u[1] + α*(σ*(u[2]-u[1]))
    du2 = u[2] + α*(u[1]*(ρ-u[3]) - u[2])
    du3 = u[3] + α*(u[1]*u[2] - β*u[3])
  end
  @SVector [du1,du2,du3]
end
function solve_system_save!(u,f,u0,p,n)
  @inbounds u[1] = u0
  @inbounds for i in 1:length(u)-1
    u[i+1] = f(u[i],p)
  end
  u
end
p = (0.02,10.0,28.0,8/3)
u = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)
@btime solve_system_save!(u,lorenz,@SVector([1.0,0.0,0.0]),p,1000)
#+end_src

In order to use multithreading on this code, we need to take a look at the dependency graph and see what items can be calculated independently of each other. Notice that

#+begin_example
σ*(u[2]-u[1])
ρ-u[3]
u[1]*u[2]
β*u[3]
#+end_example

are all independent operations, so in theory we could split those off to different threads, move up, etc.

Or we can have three threads:

#+begin_example
u[1] + α*(σ*(u[2]-u[1]))
u[2] + α*(u[1]*(ρ-u[3]) - u[2])
u[3] + α*(u[1]*u[2] - β*u[3])
#+end_example

all don't depend on the output of each other, so these tasks can be run in parallel. We can do this by using Julia's =Threads.@threads= macro which puts each of the computations of a loop in a different thread. The threaded loops do not allow you to return a value, so how do you build up the values for the =@SVector=?

...?

...?

...?

It's not possible! To understand why, let's look at the picture again:

[[https://blog-assets.risingstack.com/2017/02/kernel-processes-and-threads-1.png]]

There is a shared heap, but the stacks are thread local. This means that a value cannot be stack allocated in one thread and magically appear when re-entering the main thread: it needs to go on the heap somewhere. But if it needs to go onto the heap, then it makes sense for us to have preallocated its location. But if we want to preallocate =du[1]=, =du[2]=, and =du[3]=, then it makes sense to use the fully non-allocating update form:

#+begin_src julia
function lorenz!(du,u,p)
  α,σ,ρ,β = p
  @inbounds begin
    du[1] = u[1] + α*(σ*(u[2]-u[1]))
    du[2] = u[2] + α*(u[1]*(ρ-u[3]) - u[2])
    du[3] = u[3] + α*(u[1]*u[2] - β*u[3])
  end
end
function solve_system_save_iip!(u,f,u0,p,n)
  @inbounds u[1] = u0
  @inbounds for i in 1:length(u)-1
    f(u[i+1],u[i],p)
  end
  u
end
p = (0.02,10.0,28.0,8/3)
u = [Vector{Float64}(undef,3) for i in 1:1000]
@btime solve_system_save_iip!(u,lorenz!,[1.0,0.0,0.0],p,1000)
#+end_src

and now we multithread:

#+begin_src julia
using Base.Threads
function lorenz_mt!(du,u,p)
  α,σ,ρ,β = p
  let du=du, u=u, p=p
    Threads.@threads for i in 1:3
      @inbounds begin
        if i == 1
          du[1] = u[1] + α*(σ*(u[2]-u[1]))
        elseif i == 2
          du[2] = u[2] + α*(u[1]*(ρ-u[3]) - u[2])
        else
          du[3] = u[3] + α*(u[1]*u[2] - β*u[3])
        end
        nothing
      end
    end
  end
  nothing
end
function solve_system_save_iip!(u,f,u0,p,n)
  @inbounds u[1] = u0
  @inbounds for i in 1:length(u)-1
    f(u[i+1],u[i],p)
  end
  u
end
p = (0.02,10.0,28.0,8/3)
u = [Vector{Float64}(undef,3) for i in 1:1000]
@btime solve_system_save_iip!(u,lorenz_mt!,[1.0,0.0,0.0],p,1000);
#+end_src

*Parallelism doesn't always make things faster*. There are two costs associated with this code. For one, we had to go to the slower heap+mutation version, so its implementation starting point is slower. But secondly, and more importantly, the cost of spinning a new thread is non-negligible. In fact, here we can see that it even needs to make a small allocation for the new context. The total cost is on the order of It's on the order of 50ns: not huge, but something to take note of. So what we've done is taken almost free calculations and made them ~50ns by making each in a different thread, instead of just having it be one thread with one call stack.

The moral of the story is that you need to make sure that there's enough work per thread in order to effectively accelerate a program with parallelism.

*** Data-Parallel Problems
:PROPERTIES:
:CUSTOM_ID: data-parallel-problems
:END:
So not every setup is amenable to parallelism. Dynamical systems are notorious for being quite difficult to parallelize because the dependency of the future time step on the previous time step is clear, meaning that one cannot easily "parallelize through time" (though it is possible, which we will study later).

However, one common way that these systems are generally parallelized is in their inputs. The following questions allow for independent simulations:

- What steady state does an input =u0= go to for some list/region of initial conditions?
- How does the solution very when I use a different =p=?

The problem has a few descriptions. For one, it's called an /embarrassingly parallel/ problem since the problem can remain largely intact to solve the parallelism problem. To solve this, we can use the exact same =solve_system_save_iip!=, and just change how we are calling it. Secondly, this is called a /data parallel/ problem, since it parallelized by splitting up the input data (here, the possible =u0= or =p=s) and acting on them independently.

**** Multithreaded Parameter Searches
:PROPERTIES:
:CUSTOM_ID: multithreaded-parameter-searches
:END:
Now let's multithread our parameter search. Let's say we wanted to compute the mean of the values in the trajectory. For a single input pair, we can compute that like:

#+begin_src julia
using Statistics
function compute_trajectory_mean(u0,p)
  u = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)
  solve_system_save!(u,lorenz,u0,p,1000);
  mean(u)
end
@btime compute_trajectory_mean(@SVector([1.0,0.0,0.0]),p)
#+end_src

We can make this faster by preallocating the /cache/ vector =u=. For example, we can globalize it:

#+begin_src julia
u = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)
function compute_trajectory_mean2(u0,p)
  # u is automatically captured
  solve_system_save!(u,lorenz,u0,p,1000);
  mean(u)
end
@btime compute_trajectory_mean2(@SVector([1.0,0.0,0.0]),p)
#+end_src

But this is still allocating? The issue with this code is that =u= is a global, and captured globals cannot be inferred because their type can change at any time. Thus what we can do instead is capture a constant:

#+begin_src julia
const _u_cache = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)
function compute_trajectory_mean3(u0,p)
  # u is automatically captured
  solve_system_save!(_u_cache,lorenz,u0,p,1000);
  mean(_u_cache)
end
@btime compute_trajectory_mean3(@SVector([1.0,0.0,0.0]),p)
#+end_src

Now it's just allocating the output. The other way to do this is to use a /closure/ which encapsulates the cache data:

#+begin_src julia
function _compute_trajectory_mean4(u,u0,p)
  solve_system_save!(u,lorenz,u0,p,1000);
  mean(u)
end
compute_trajectory_mean4(u0,p) = _compute_trajectory_mean4(_u_cache,u0,p)
@btime compute_trajectory_mean4(@SVector([1.0,0.0,0.0]),p)
#+end_src

This is the same, but a bit more explicit. Now let's create our parameter search function. Let's take a sample of parameters:

#+begin_src julia
ps = [(0.02,10.0,28.0,8/3) .* (1.0,rand(3)...) for i in 1:1000]
#+end_src

And let's get the mean of the trajectory for each of the parameters.

#+begin_src julia
serial_out = map(p -> compute_trajectory_mean4(@SVector([1.0,0.0,0.0]),p),ps)
#+end_src

Now let's do this with multithreading:

#+begin_src julia
function tmap(f,ps)
  out = Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000)
  Threads.@threads for i in 1:1000
    # each loop part is using a different part of the data
    out[i] = f(ps[i])
  end
  out
end
threaded_out = tmap(p -> compute_trajectory_mean4(@SVector([1.0,0.0,0.0]),p),ps)
#+end_src

Let's check the output:

#+begin_src julia
serial_out - threaded_out
#+end_src

Oh no, we don't get the same answer! What happened?

The answer is the caching. Every single thread is using =_u_cache= as the cache, and so while one is writing into it the other is reading out of it, and thus is getting the value written to it from the wrong cache!

To fix this, what we need is a different heap per thread:

#+begin_src julia
const _u_cache_threads = [Vector{typeof(@SVector([1.0,0.0,0.0]))}(undef,1000) for i in 1:Threads.nthreads()]
function compute_trajectory_mean5(u0,p)
  # u is automatically captured
  solve_system_save!(_u_cache_threads[Threads.threadid()],lorenz,u0,p,1000);
  mean(_u_cache_threads[Threads.threadid()])
end
@btime compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p)
#+end_src

#+begin_src julia
serial_out = map(p -> compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)
threaded_out = tmap(p -> compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)
serial_out - threaded_out
#+end_src

#+begin_src julia
@btime serial_out = map(p -> compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)
#+end_src

#+begin_src julia
@btime threaded_out = tmap(p -> compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)
#+end_src

*** Hierarchical Task-Based Multithreading and Dynamic Scheduling
:PROPERTIES:
:CUSTOM_ID: hierarchical-task-based-multithreading-and-dynamic-scheduling
:END:
The major change in Julia v1.3 is that Julia's =Task=s, which are traditionally its green threads interface, are now the basis of its multithreading infrastructure. This means that all independent threads are parallelized, and a new interface for multithreading will exist that works by spawning threads.

This implementation follows Go's goroutines and the classic multithreading interface of Cilk. There is a Julia-level scheduler that handles the multithreading to put different tasks on different vCPU threads. A benefit from this is hierarchical multithreading. Since Julia's tasks can spawn tasks, what can happen is a task can create tasks which create tasks which etc. In Julia (/Go/Cilk), this is then seen as a single pool of tasks which it can schedule, and thus it will still make sure only =N= are running at a time (as opposed to the naive implementation where the total number of running threads is equal then multiplied). This is essential for numerical performance because running multiple compute threads on a single CPU thread requires constant context switching between the threads, which will slow down the computations.

To directly use the task-based interface, simply use =Threads.@spawn= to spawn new tasks. For example:

#+begin_src julia
function tmap2(f,ps)
  tasks = [Threads.@spawn f(ps[i]) for i in 1:1000]
  out = [fetch(t) for t in tasks]
end
threaded_out = tmap2(p -> compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)
#+end_src

However, if we check the timing we see:

#+begin_src julia
@btime tmap2(p -> compute_trajectory_mean5(@SVector([1.0,0.0,0.0]),p),ps)
#+end_src

=Threads.@threads= is built on the same multithreading infrastructure, so why is this so much slower? The reason is because =Threads.@threads= employs *static scheduling* while =Threads.@spawn= is using *dynamic scheduling*. Dynamic scheduling is the model of allowing the runtime to determine the ordering and scheduling of processes, i.e. what tasks will run run where and when. Julia's task-based multithreading system has a thread scheduler which will automatically do this for you in the background, but because this is done at runtime it will have overhead. Static scheduling is the model of pre-determining where and when tasks will run, instead of allowing this to be determined at runtime. =Threads.@threads= is "quasi-static" in the sense that it cuts the loop so that it spawns only as many tasks as there are threads, essentially assigning one thread for even chunks of the input data.

Does this lack of runtime overhead mean that static scheduling is "better"? No, it simply has trade-offs. Static scheduling assumes that the runtime of each block is the same. For this specific case where there are fixed number of loop iterations for the dynamical systems, we know that every =compute_trajectory_mean5= costs exactly the same, and thus this will be more efficient. However, There are many cases where this might not be efficient. For example:

#+begin_src julia
function sleepmap_static()
  out = Vector{Int}(undef,24)
  Threads.@threads for i in 1:24
    sleep(i/10)
    out[i] = i
  end
  out
end
isleep(i) = (sleep(i/10);i)
function sleepmap_spawn()
  tasks = [Threads.@spawn(isleep(i)) for i in 1:24]
  out = [fetch(t) for t in tasks]
end

@btime sleepmap_static()
@btime sleepmap_spawn()
#+end_src

The reason why this occurs is because of how the static scheduling had chunked my calculation. On my computer:

#+begin_src julia
Threads.nthreads()
#+end_src

This means that there are 6 tasks that are created by =Threads.@threads=. The first takes:

#+begin_src julia
sum(i/10 for i in 1:4)
#+end_src

1 second, while the next group takes longer, then the next, etc. while the last takes:

#+begin_src julia
sum(i/10 for i in 21:24)
#+end_src

9 seconds (which is precisely the result!). Thus by unevenly distributing the runtime, we run as fast as the slowest thread. However, dynamic scheduling allows new tasks to immediately run when another is finished, meaning that the in that case the shorter tasks tend to be piled together, causing a faster execution. Thus whether dynamic or static scheduling is beneficial is dependent on the problem and the implementation of the static schedule.

**** Possible Project
:PROPERTIES:
:CUSTOM_ID: possible-project
:END:
Note that this can extend to external library calls as well. [[https://github.com/JuliaMath/FFTW.jl/pull/105][FFTW.jl recently gained support for this]]. A possible final project would be to do a similar change [[https://github.com/JuliaLang/julia/issues/32786][to OpenBLAS]].

** A Teaser for Alternative Parallelism Models
:PROPERTIES:
:CUSTOM_ID: a-teaser-for-alternative-parallelism-models
:END:
*** Simplest Parallel Code
:PROPERTIES:
:CUSTOM_ID: simplest-parallel-code
:END:
#+begin_src julia
A = rand(10000,10000)
B = rand(10000,10000)
A*B
#+end_src

If you are using a computer that has N cores, then this will use N cores. Try it and look at your resource usage!

*** Array-Based Parallelism
:PROPERTIES:
:CUSTOM_ID: array-based-parallelism
:END:
The simplest form of parallelism is array-based parallelism. The idea is that you use some construction of an array whose operations are already designed to be parallel under the hood. In Julia, some examples of this are:

- DistributedArrays (Distributed Computing)
- Elemental
- MPIArrays
- CuArrays (GPUs)

This is not a Julia specific idea either.

*** BLAS and Standard Libraries
:PROPERTIES:
:CUSTOM_ID: blas-and-standard-libraries
:END:
The basic linear algebra calls are all handled by a set of libraries which follow the same interface known as BLAS (Basic Linear Algebra Subroutines). It's divided into 3 portions:

- BLAS1: Element-wise operations (O(n))
- BLAS2: Matrix-vector operations (O(n^2))
- BLAS3: Matrix-matrix operations (O(n^3))

BLAS implementations are highly optimized, like OpenBLAS and Intel MKL, so every numerical language and library essentially uses similar underlying BLAS implementations. Extensions to these, known as LAPACK, include operations like factorizations, and are included in these standard libraries. These are all multithreaded. The reason why this is a location to target is because the operation count is high enough that parallelism can be made efficient even when only targeting this level: a matrix multiplication can take on the order of seconds, minutes, hours, or even days, and these are all highly parallel operations. This means you can get away with a bunch just by parallelizing at this level, which happens to be a bottleneck for a lot scientific computing codes.

This is also commonly the level at which GPU computing occurs in machine learning libraries for reasons which we will explain later.

*** MPI
:PROPERTIES:
:CUSTOM_ID: mpi
:END:
Well, this is a big topic and we'll address this one later!

** Conclusion
:PROPERTIES:
:CUSTOM_ID: conclusion
:END:
The easiest forms of parallelism are:

- Embarrassingly parallel
- Array-level parallelism (built into linear algebra)

Exploit these when possible.
* The Different Flavors of Parallelism
Now that you are aware of the basics of parallel computing, let's give a high level overview of the differences between different modes of parallelism.
** Lowest Level: SIMD
:PROPERTIES:
:CUSTOM_ID: lowest-level-simd
:END:
Recall SIMD, the idea that processors can run multiple commands simultaneously on specially structured data. "Single Instruction Multiple Data". SIMD is parallelism within a single core.

*** High Level Idea of SIMD
:PROPERTIES:
:CUSTOM_ID: high-level-idea-of-simd
:END:
Calculations can occur in parallel in the processor if there is sufficient structure in the computation.

*** How to do SIMD
:PROPERTIES:
:CUSTOM_ID: how-to-do-simd
:END:
The simplest way to do SIMD is simply to make sure that your values are aligned. If they are, then great, LLVM's autovectorizer pass has a good chance of automatic vectorization (in the world of computing, "SIMD" is synonymous with vectorization since it is taking specific values and instead computing on small vectors. That is not to be confused with "vectorization" in the sense of Python/R/MATLAB, which is a programming style which prefers using C-defined primitive functions, like broadcast or matrix multiplication).

You can check for auto-vectorization inside of the LLVM IR by looking for statements like:

#+begin_example
%wide.load24 = load <4 x double>, <4 x double> addrspac(13)* %46, align 8
; └
; ┌ @ float.jl:395 within `+'
%47 = fadd <4 x double> %wide.load, %wide.load24
#+end_example

which means that 4 additions are happening simultaneously. The amount of vectorization is heavily dependent on your architecture. The ancient form of SIMD, the SSE(2) instructions, required that your data was aligned. Now there's a bit more leeway, but generally it holds that making your the data you're trying to SIMD over is aligned. Thus there can be major differences in computing using a /struct of array/ format instead of an /arrays of structs/ format. For example:

#+begin_src julia
struct MyComplex
  real::Float64
  imag::Float64
end
arr = [MyComplex(rand(),rand()) for i in 1:100]
#+end_src

is represented in memory as

#+begin_example
[real1,imag1,real2,imag2,...]
#+end_example

while the struct of array formats are

#+begin_src julia
struct MyComplexes
  real::Vector{Float64}
  imag::Vector{Float64}
end
arr2 = MyComplexes(rand(100),rand(100))
#+end_src

Now let's check what happens when we perform a reduction:

#+begin_src julia
using InteractiveUtils
Base.:+(x::MyComplex,y::MyComplex) = MyComplex(x.real+y.real,x.imag+y.imag)
Base.:/(x::MyComplex,y::Int) = MyComplex(x.real/y,x.imag/y)
average(x::Vector{MyComplex}) = sum(x)/length(x)
@code_llvm average(arr)
#+end_src

What this is doing is creating small little vectors and then parallelizing the operations of those vectors by calling specific vector-parallel instructions. Keep this in mind.

*** Explicit SIMD
:PROPERTIES:
:CUSTOM_ID: explicit-simd
:END:
The following was all a form of *loop-level parallelism* known as loop vectorization. It's simply easier for compilers to reason at the array level, prove iterates are independent, and automatically generate SIMD code from that. This is not necessary, and compilers can produce SIMD code from non-looping code through a process known as *SLP supervectorization*, but the results are far from optimal and the compiler requires a lot of time to do this calculation, meaning that it's usually not a pass used by default.

If you want to pack the vectors yourself, then primitives for doing so from within Julia are available in SIMD.jl. This is for "real" performance warriors. This looks like for example:

#+begin_src julia
using SIMD
v = Vec{4,Float64}((1,2,3,4))
@show v+v # basic arithmetic is supported
@show sum(v) # basic reductions are supported
#+end_src

Using this you can pull apart code and force the usage of SIMD vectors. One library which makes great use of this is LoopVectorization.jl. However, one word of "caution":

*Most performance optimization is not trying to do something really good for performance. Most performance optimization is trying to not do something that is actively bad for performance.*

*** Summary of SIMD
:PROPERTIES:
:CUSTOM_ID: summary-of-simd
:END:
- Communication in SIMD is due to locality: if things are local the processor can automatically setup the operations.
- There's no real worry about "getting it wrong": you cannot overwrite pieces from different parts of the arithmetic unit, and if SIMD is unsafe then it just won't auto-vectorize.
- Suitable for operations measured in ns.

** Next Level Up: Multithreading
:PROPERTIES:
:CUSTOM_ID: next-level-up-multithreading
:END:
Last time we briefly went over multithreading and described how every process has multiple threads which share a single heap, and when multiple threads are executed simultaneously we have multithreaded parallelism. Note that you can have multiple threads which aren't executed simultaneously, like in the case of I/O operations, and this is an example of concurrency without parallelism and is commonly referred to as green threads.

[[https://blog-assets.risingstack.com/2017/02/kernel-processes-and-threads-1.png]]

Last time we described a simple multithreaded program and noticed that multithreading has an overhead cost of around 50ns-100ns. This is due to the construction of the new stack (among other things) each time a new computational thread is spun up. This means that, unlike SIMD, some thought needs to be put in as to when to perform multithreading: it's not always a good idea. It needs to be high enough on the cost for this to be counter-balanced.

One abstraction that was glossed over was the memory access style. Before, we were considering a single heap, or an UMA style:

[[https://software.intel.com/sites/default/files/m/2/0/4/e/d/39352-figure-1.jpg]]

However, this is the case for all shared memory devices. For example, compute nodes on the HPC tend to be "dual Xeon" or "quad Xeon", where each Xeon processor is itself a multicore processor. But each processor on its own accesses its own local caches, and thus one has to be aware that this is setup in a NUMA (non-uniform memory access) manner:

[[https://software.intel.com/sites/default/files/m/2/d/c/b/2/39353-figure-2.jpg]]

where there is a cache that is closer to the processor and a cache that is further away. Care should be taken in this to localize the computation per thread, otherwise a cost associated with the memory sharing will be hit (but all sharing will still be automatic).

In this sense, interthread communication is naturally done through the heap: if you want other threads to be able to touch a value, then you can simply place it on the heap and then it'll be available. We saw this last time by how overlapping computations can re-use the same heap-based caches, meaning that care needs to be taken with how one writes into a dynamically-allocated array.

A simple example that demonstrates this is. First, let's make sure we have multithreading enabled:

#+begin_src julia
using Base.Threads
Threads.nthreads() # should not be 1
#+end_src

#+begin_src julia
using BenchmarkTools
acc = 0
@threads for i in 1:10_000
    global acc
    acc += 1
end
acc
#+end_src

The reason for this behavior is that there is a difference between the reading and the writing step to an array. Here, values are being read while other threads are writing, meaning that they see a lower value than when they are attempting to write into it. The result is that the total summation is lower than the true value because of this clashing. We can prevent this by only allowing one thread to utilize the heap-allocated variable at a time. One abstraction for doing this is /atomics/:

#+begin_src julia
acc = Atomic{Int64}(0)
@threads for i in 1:10_000
    atomic_add!(acc, 1)
end
acc
#+end_src

When an atomic add is being done, all other threads wishing to do the same computation are blocked. This of course can have a massive effect on performance since atomic computations are not parallel.

Julia also exposes a lower level of heap control in threading using /locks/

#+begin_src julia
const acc_lock = Ref{Int64}(0)
const splock = SpinLock()
function f1()
    @threads for i in 1:10_000
        lock(splock)
        acc_lock[] += 1
        unlock(splock)
    end
end
const rsplock = ReentrantLock()
function f2()
    @threads for i in 1:10_000
        lock(rsplock)
        acc_lock[] += 1
        unlock(rsplock)
    end
end
acc2 = Atomic{Int64}(0)
function g()
  @threads for i in 1:10_000
      atomic_add!(acc2, 1)
  end
end
const acc_s = Ref{Int64}(0)
function h()
  global acc_s
  for i in 1:10_000
      acc_s[] += 1
  end
end
@btime f1()
#+end_src

=SpinLock= is non-reentrant, i.e. it will block itself if a thread that calls a =lock= does another =lock=. Therefore it has to be used with caution (every =lock= goes with one =unlock=), but it's fast. =ReentrantLock= alleviates those concerns, but trades off a bit of performance:

#+begin_src julia
@btime f2()
#+end_src

But if you can use atomics, they will be faster:

#+begin_src julia
@btime g()
#+end_src

and if your computation is actually serial, then use serial code:

#+begin_src julia
@btime h()
#+end_src

Why is this so fast? Check the code:

#+begin_src julia
@code_llvm h()
#+end_src

It just knows to add 10000. So to get a proper timing let's make the size mutable:

#+begin_src julia
const len = Ref{Int}(10_000)
function h2()
  global acc_s
  global len
  for i in 1:len[]
      acc_s[] += 1
  end
end
@btime h2()
#+end_src

#+begin_src julia
@code_llvm h2()
#+end_src

It's still optimizing it!

#+begin_src julia
non_const_len = 10000
function h3()
  global acc_s
  global non_const_len
  len2::Int = non_const_len
  for i in 1:len2
      acc_s[] += 1
  end
end
@btime h3()
#+end_src

Note that what is shown here is a type-declaration. =a::T = ...= forces =a= to be of type =T= throughout the whole function. By giving the compiler this information, I am able to use the non-constant global in a type-stable manner.

One last thing to note about multithreaded computations, and parallel computations, is that one cannot assume that the parallelized computation is computed in any given order. For example, the following will has a quasi-random ordering:

#+begin_src julia
const a2 = zeros(nthreads()*10)
const acc_lock2 = Ref{Int64}(0)
const splock2 = SpinLock()
function f_order()
    @threads for i in 1:length(a2)
        lock(splock2)
        acc_lock2[] += 1
        a2[i] = acc_lock2[]
        unlock(splock2)
    end
end
f_order()
a2
#+end_src

Note that here we can see that Julia 1.5 is dividing up the work into groups of 10 for each thread, and then one thread dominates the computation at a time, but which thread dominates is random.

*** The Dining Philosophers Problem
:PROPERTIES:
:CUSTOM_ID: the-dining-philosophers-problem
:END:
A classic tale in parallel computing is the dining philosophers problem. In this case, there are N philosophers at a table who all want to eat at the same time, following all of the same rules. Each philosopher must alternatively think and then eat. They need both their left and right fork to start eating, but cannot start eating until they have both forks. The problem is how to setup a concurrent algorithm that will not cause any philosophers to starve.

The difficulty is a situation known as /deadlock/. For example, if each philosopher was told to grab the right fork when it's available, and then the left fork, and put down the fork after eating, then they will all grab the right fork and none will ever eat because they will all be waiting on the left fork. This is analogous to two blocked computations which are waiting on the other to finish. Thus, when using blocking structures, one needs to be careful about deadlock!

*** Two Programming Models: Loop-Level Parallelism and Task-Based Parallelism
:PROPERTIES:
:CUSTOM_ID: two-programming-models-loop-level-parallelism-and-task-based-parallelism
:END:
As described in the previous lecture, one can also use =Threads.@spawn= to do multithreading in Julia v1.3+. The same factors all applay: how to do locks and Mutex etc. This is a case of a parallelism construct having two alternative *programming models*. =Threads.@spawn= represents task-based parallelism, while =Threads.@threads= represents Loop-Level Parallelism or a parallel iterator model. Loop-based parallelization models are very high level and, assuming every iteration is independent, almost requires no code change. Task-based parallelism is a more expressive parallelism model, but usually requires modifying the code to be explicitly written as a set of parallelizable tasks. Note that in the case of Julia, =Threads.@threads= is implemented using =Threads.@spawn='s model.

*** Summary of Multithreading
:PROPERTIES:
:CUSTOM_ID: summary-of-multithreading
:END:
- Communication in multithreading is done on the heap. Locks and atomics allow for a form of safe message passing.
- 50ns-100ns of overhead. Suitable for 1μs calculations.
- Be careful of ordering and heap-allocated values.

** GPU Computing
:PROPERTIES:
:CUSTOM_ID: gpu-computing
:END:
GPUs are not fast. In fact, the problem with GPUs is that each processor is slow. However, GPUs have a lot of cores... like thousands.

[[https://miro.medium.com/max/832/0*xzPjWMqXC0NB6D69.jpg]]

An RTX2080, a standard "gaming" GPU (not even the ones in the cluster), has 2944 cores. However, not only are GPUs slow, but they also need to be programmed in a style that is /SPMD/, which standard for Single Program Multiple Data. This means that every single thread must be running the same program but on different pieces of data. Exactly the same program. If you have

#+begin_src julia;eval=false
if a > 1
  # Do something
else
  # Do something else
end
#+end_src

where some of the data goes on one branch and other data goes on the other branch, every single thread will run both branches (performing "fake" computations while on the other branch). This means that GPU tasks should be "very parallel" with as few conditionals as possible.

*** GPU Memory
:PROPERTIES:
:CUSTOM_ID: gpu-memory
:END:
GPUs themselves are shared memory devices, meaning they have a heap that is shared amongst all threads. However, GPUs are heavily in the NUMA camp, where different blocks of the GPU have much faster access to certain parts of the memory. Additionally, this heap is disconnected from the standard processor, so data must be passed to the GPU and data must be returned.

GPU memory size is relatively small compared to CPUs. Example: the RTX2080Ti has 8GB of RAM. Thus one needs to be doing computations that are memory compact (such as matrix multiplications, which are O(n^3) making the computation time scale quicker than the memory cost).

*** Note on GPU Hardware
:PROPERTIES:
:CUSTOM_ID: note-on-gpu-hardware
:END:
Standard GPU hardware "for gaming", like RTX2070, is just as fast as higher end GPU hardware for Float32. Higher end hardware, like the Tesla, add more memory, memory safety, and Float64 support. However, these require being in a server since they have alternative cooling strategies, making them a higher end product.

*** SPMD Kernel Generation GPU Computing Models
:PROPERTIES:
:CUSTOM_ID: spmd-kernel-generation-gpu-computing-models
:END:
The core programming models for GPU computing are SPMD kernel compilers, of which the most well-known is CUDA. CUDA is a C++-like programming language which compiles to .ptx kernels, and GPU execution on NVIDIA GPUs is done by "all steams" of a GPU doing concurrent execution of the kernel (generally, without going into more details, you can of "all streams" as just meaning "all cores". More detailed views of GPU execution will come later).

.ptx CUDA kernels can be compiled from LLVM IR, and thus since Julia is a programming language which emits LLVM IR for all of its operations, native Julia programs are compatible with compilation to CUDA. The helper functions to enable this separate compilation path is CUDA.jl. Let's take a look at a basic CUDA.jl kernel generating example:

#+begin_src julia
using CUDA

N = 2^20
x_d = CUDA.fill(1.0f0, N)  # a vector stored on the GPU filled with 1.0 (Float32)
y_d = CUDA.fill(2.0f0, N)  # a vector stored on the GPU filled with 2.0

function gpu_add2!(y, x)
    index = threadIdx().x    # this example only requires linear indexing, so just use `x`
    stride = blockDim().x
    for i = index:stride:length(y)
        @inbounds y[i] += x[i]
    end
    return nothing
end

fill!(y_d, 2)
@cuda threads=256 gpu_add2!(y_d, x_d)
all(Array(y_d) .== 3.0f0)
#+end_src

The key to understanding the SPMD kernel approach is the =index = threadIdx().x= and =stride = blockDim().x= portions.

[[https://juliagpu.gitlab.io/CUDA.jl/tutorials/intro1.png]]

The way kernels are expected to run in parallel is that they are given a specific block of the computation and are expected to write a kernel which only on that small block of the input. This kernel is then called on every separate thread on the GPU, making each CUDA core simultaneously compute each block. Thus as a user in such a SPMD programming model, you never specify the computation globally but instead simply specify how chunks should behave, giving the compiler the leeway to determine the optimal global execution.

*** Array-Based GPU Computing Models
:PROPERTIES:
:CUSTOM_ID: array-based-gpu-computing-models
:END:
The simplest version of GPU computing is the array-based programming model.

#+begin_src julia
A = rand(100,100); B = rand(100,100)
using CUDA
# Pass to the GPU
cuA = cu(A); cuB = cu(B)
cuC = cuA*cuB
# Pass to the CPU
C = Array(cuC)
#+end_src

Let's see the transfer times:

#+begin_src julia
@btime cu(A)
#+end_src

#+begin_src julia
@btime Array(cuC)
#+end_src

The cost transferring is about 20μs-50μs in each direction, meaning that one needs to be doing operations that cost at least 200μs for GPUs to break even. A good rule of thumb is that GPU computations should take at least a millisecond, or GPU memory should be re-used.

*** Summary of GPUs
:PROPERTIES:
:CUSTOM_ID: summary-of-gpus
:END:
- GPUs cores are slow
- GPUs are SPMD
- GPUs are generally used for linear algebra
- Suitable for SPMD 1ms computations

** Xeon Phi Accelerators and OpenCL
:PROPERTIES:
:CUSTOM_ID: xeon-phi-accelerators-and-opencl
:END:
Other architectures exist to keep in mind. Xeon Phis are a now-defunct accelerator that used X86 (standard processors) as the base, using hundreds of them. For example, the Knights Landing series had 256 core accelerator cards. These were all clocked down, meaning they were still slower than a standard CPU, but there were less restrictions on SPMD (though SPMD-like computations were still preferred in order to heavily make use of SIMD). However, because machine learning essentially only needs linear algebra, and linear algebra is faster when restricting to SPMD-architectures, this failed. These devices can still be found on many high end clusters.

One alternative to CUDA is OpenCL which supports alternative architectures such as the Xeon Phi at the same time that it supports GPUs. However, one of the issues with OpenCL is that its BLAS implementation currently does not match the speed of CuBLAS, which makes NVIDIA-specific libraries still the king of machine learning and most scientific computing.

** TPU Computing
:PROPERTIES:
:CUSTOM_ID: tpu-computing
:END:
TPUs are tensor processing units, which is Google's newest accelerator technology. They are essentially just "tensor operation compilers", which in computer science speak is simply higher dimensional linear algebra. To do this, they internally utilize a BFloat16 type, which is a 16-bit floating point number with the same exponent size as a Float32 with an 8-bit significant. This means that computations are highly prone to /catastrophic cancellation/. This computational device only works because BFloat16 has primitive operations for FMA which allows 32-bit-like accuracy of multiply-add operations, and thus computations which are only dot products (linear algebra) end up okay. Thus this is simply a GPU-like device which has gone further to completely specialize in linear algebra.

** Multiprocessing (Distributed Computing)
:PROPERTIES:
:CUSTOM_ID: multiprocessing-distributed-computing
:END:
While multithreading computes with multiple threads, multiprocessing computes with multiple independent processes. Note that processes do not share any memory, not heap or data, and thus this mode of computing also allows for /distributed computations/, which is the case where processes may be on separate computing hardware. However, even if they are on the same hardware, the lack of a shared address space means that multiprocessing has to do /message passing/, i.e. send data from one process to the other.

*** Distributed Tasks with Explicit Memory Handling: The Master-Worker Model
:PROPERTIES:
:CUSTOM_ID: distributed-tasks-with-explicit-memory-handling-the-master-worker-model
:END:
Given the amount of control over data handling, there are many different models for distributed computing. The simplest, the one that Julia's Distributed Standard Library defaults to, is the /master-worker model/. The master-worker model has one process, deemed the master, which controls the worker processes.

Here we can start by adding some new worker processes:

#+begin_src julia;eval=false
using Distributed
addprocs(4)
#+end_src

This adds 4 worker processes for the master to control. The simplest computations are those where the master process gives the worker process a job which returns the value afterwards. For example, a =pmap= operation or =@distributed= loop gives the worker a function to execute, along with the data, and the worker then computes and returns the result.

At a lower level, this is done by =Distributed.@spawn=ing jobs, or using a =remotecall= and =fetch=ing the result. [[https://github.com/ChrisRackauckas/ParallelDataTransfer.jl][ParallelDataTransfer.jl]] gives an extended set of primitive message passing operations. For example, we can explicitly tell it to compute a function =f= on the remote process like:

#+begin_src julia;eval=false
@everywhere f(x) = x.^2 # Define this function on all processes
t = remotecall(f,2,randn(10))
#+end_src

=remotecall= is a non-blocking operation that returns a =Future=. To access the data, one should use the blocking operation =fetch= to receive the data:

#+begin_src julia;eval=false
xsq = fetch(t)
#+end_src

*** Distributed Tasks with Implicit Memory Handling: Distributed Task-Based Parallelism
:PROPERTIES:
:CUSTOM_ID: distributed-tasks-with-implicit-memory-handling-distributed-task-based-parallelism
:END:
Another popular programming model for distributed computation is task-based parallelism but where all of the memory handling is implicit. Since, unlike the shared memory parallelism case, data transfers are required for given processes to share heap allocated values, distributed task-based parallelism libraries tend to want a global view of the whole computation in order to build a sophisticated schedule that includes where certain data lives and when transfers will occur. Because of this, distributed task-based parallelism libraries tend to want the entire *computational graph* of the computation, to be able to restructure the graph as necessary with their own data transfer portions spliced into the compute. Examples of this kind of framework are:

- Tensorflow
- dask ("distributed tasks")
- Dagger.jl

Using these kinds of libraries requires building a directed acyclic graph (DAG). For example, the following showcases how to use Dagger.jl to represent a bunch of summations:

#+begin_src julia;eval=false
using Dagger

add1(value) = value + 1
add2(value) = value + 2
combine(a...) = sum(a)

p = delayed(add1)(4)
q = delayed(add2)(p)
r = delayed(add1)(3)
s = delayed(combine)(p, q, r)

@assert collect(s) == 16
#+end_src

Once the global computation is specified, commands like =collect= are used to instantiate the graph on given input data, which then run the computation in a (potentially) distributed manner, depending on internal scheduler heuristics.

*** Distributed Array-Based Parallelism: SharedArrays, Elemental, and DArrays
:PROPERTIES:
:CUSTOM_ID: distributed-array-based-parallelism-sharedarrays-elemental-and-darrays
:END:
Because array operations are a standard way to compute in scientific computing, there are higher level primitives to help with message passing. A =SharedArray= is an array which acts like a shared memory device. This means that every change to a =SharedArray= causes message passing to keep them in sync, and thus this should be used with a performance caution. [[https://github.com/JuliaParallel/DistributedArrays.jl][DistributedArrays.jl]] is a parallel array type which has local blocks and can be used for writing higher level abstractions with explicit message passing. Because it is currently missing high-level parallel linear algebra, currently the recommended tool for distributed linear algebra is [[https://github.com/JuliaParallel/Elemental.jl][Elemental.jl]].

*** MapReduce, Hadoop, and Spark: The Map-Reduce Model
:PROPERTIES:
:CUSTOM_ID: mapreduce-hadoop-and-spark-the-map-reduce-model
:END:
Many data-parallel operations work by mapping a function =f= onto each piece of data and then reducing it. For example, the sum of squares maps the function =x -> x^2= onto each value, and then these values are reduced by performing a summation. MapReduce was a Google framework in the 2000's built around this as the parallel computing concept, and current data-handling frameworks, like Hadoop and Spark, continue this as the core distributed programming model.

In Julia, there exists the =mapreduce= function for performing serial mapreduce operations. It also work on GPUs. However, it does not auto-distribute. For distributed map-reduce programming, the =@distributed= for-loop macro can be used. For example, sum of squares of random numbers is:

#+begin_src julia;eval=false
@distributed (+) for i in 1:1000
  rand()^2
end
#+end_src

One can see that computing summary statistics is easily done in this framework which is why it was majorly adopted among "big data" communities.

=@distributed= uses a static scheduler. The dynamic scheduling equivalent is =pmap=:

#+begin_src julia;eval=false
pmap(i->rand()^2,1:100)
#+end_src

which will dynamically allocate jobs to processes as they declare they have finished jobs. This thus has the same performance difference behavior as =Threads.@threads= vs =Threads.@spawn=.

*** MPI: The Distributed SPMD Model
:PROPERTIES:
:CUSTOM_ID: mpi-the-distributed-spmd-model
:END:
The main way to do high-performance multiprocessing is /MPI/, which is an old distributed computing interface from the C/Fortran days. Julia has access to the MPI programming model through MPI.jl. The programming model for MPI is that every computer is running the same program, and synchronization is performed by blocking communication. For example, let's look at the following:

#+begin_src julia;eval=false
using MPI
MPI.Init()

comm = MPI.COMM_WORLD
rank = MPI.Comm_rank(comm)
size = MPI.Comm_size(comm)

dst = mod(rank+1, size)
src = mod(rank-1, size)

N = 4

send_mesg = Array{Float64}(undef, N)
recv_mesg = Array{Float64}(undef, N)

fill!(send_mesg, Float64(rank))

rreq = MPI.Irecv!(recv_mesg, src,  src+32, comm)

print("$rank: Sending   $rank -> $dst = $send_mesg\n")
sreq = MPI.Isend(send_mesg, dst, rank+32, comm)

stats = MPI.Waitall!([rreq, sreq])

print("$rank: Received $src -> $rank = $recv_mesg\n")

MPI.Barrier(comm)
#+end_src

#+begin_src julia;eval=false
> mpiexecjl -n 3 julia examples/04-sendrecv.jl
1: Sending   1 -> 2 = [1.0, 1.0, 1.0, 1.0]
0: Sending   0 -> 1 = [0.0, 0.0, 0.0, 0.0]
1: Received 0 -> 1 = [0.0, 0.0, 0.0, 0.0]
2: Sending   2 -> 0 = [2.0, 2.0, 2.0, 2.0]
0: Received 2 -> 0 = [2.0, 2.0, 2.0, 2.0]
2: Received 1 -> 2 = [1.0, 1.0, 1.0, 1.0]
#+end_src

Let's investigate this a little bit. Think about having two computers run this line-by-line side by side. They will both locally build arrays, and then call =MPI.Irecv!=, which is an asynchronous non-blocking call to listen for a message from a given =rank= (a rank is the ID for a given process). Then they call their =sreq = MPI.Isend= function, which is an asynchronous non-blocking call to send a message =send_mesg= to the chosen =rank=. When the expected message is found, =MPI.Irecv!= will then run on its green thread and finish, updating the =recv_mesg= with the information from the message. However, in order to make sure all of the messages are received, we have added in a blocking operation =MPI.Waitall!([rreq, sreq])=, which will block all further execution on the given rank until both its =rreq= and =sreq= tasks are completed. After that is done, each given rank will have its updated data, and the script will continue on all ranks.

This model is thus very asynchronous and allows for many different computers to run one highly parallelized program, managing the data transmissions in a sparse way without a single computer in charge of managing the whole computation. However, it can be prone to deadlock, since errors in the program may for example require rank 1 to receive a message from rank 2 before continuing the program, but rank 2 won't continue to program until it receives a message from rank 1. For this reason, while MPI has been the most successful large-scale distributed computing model and almost all major high-performance computing (HPC) cluster competitions have been won by codes utilizing the MPI model, the MPI model is nowadays considered a last resort due to these safety issues.

*** Summary of Multiprocessing
:PROPERTIES:
:CUSTOM_ID: summary-of-multiprocessing
:END:
- Cost is hardware dependent: only suitable for 1ms or higher depending on the connections through which the messages are being passed and the topology of the network.
- The Master-worker programming model is Julia's =Distributed= model
- The Map-reduce programming model is a common data-handling model
- Array-based distributed computations are another abstraction, used in all forms of parallelism.
- MPI is a SPMD model of distributed computing, where each process is completely independent and one just controls the memory handling.

** The Bait-and-switch: Parallelism is about Programming Models
:PROPERTIES:
:CUSTOM_ID: the-bait-and-switch-parallelism-is-about-programming-models
:END:
While this looked like a lecture about parallel programming at the different levels and types of hardware, this wide overview showcases that the real underlying commonality within parallel program is in the *parallel programming models*, of which there are not too many. There are:

- Map-reduce parallelism models. =pmap=, MapReduce (Hadoop/Spark)
  - Pros: Easy to use
  - Cons: Requires that your program is specifically only mapping functions =f= and reducing them. That said, many data science operations like =mean=, =variance=, =maximum=, etc. can be represented as map-reduce calls, which lead to the popularity of these approaches for "big data" operations.
- Array-based parallelism models. SIMD (at the compiler level), =CuArray=, =DistributedArray=, =PyTorch.torch=, ...
  - Pros: Easy to use, can have very fast library implementations for specific functions
  - Cons: Less control and restricted to specific functions implemented by the library. Parallelism matches the data structure, so it requires the user to be careful and know the best way to split the data.
- Loop-based parallelism models. =Threads.@threads=, =@distributed=, OpenMP, MATLAB's =parfor=, Chapel's iterator parallelism
  - Pros: Easy to use, almost no code change can make existing loops parallelized
  - Cons: Refined operations, like locking and sharing data, can be awkward to write. Less control over fine details like scheduling, meaning less opportunities to optimize.
- Task-based parallelism models with implicit distributed data handling. =Threads.@spawn=, Dagger.jl, TensorFlow, dask
  - Pros: Relatively high level, low risk of errors since parallelism is mostly handled for the user. User simply describes which functions to call in what order.
  - Cons: When used on distributed systems, implicit data handling is hard, meaning it's generally not as efficient if you don't optimize the code yourself or help the optimizer, and these require specific programming constructs for building the computational graph. Note this is only a downside for distributed data parallelism, whereas when applied to shared memory systems these aspects no longer require handling by the task scheduler.
- Task-based parallelism models with explicit data handling. =Distributed.@spawn=
  - Pros: Allows for control over what compute hardware will have specific pieces of data and allows for transferring data manually.
  - Cons: Requires transferring data manually. All computations are managed by a single process/computer/node and thus it can have some issues scaling to extreme (1000+ node) computing situations.
- SPMD kernel parallelism models. CUDA, MPI, KernelAbstractions.jl
  - Pros: Reduces the problem for the user to only specify what happens in small chunks of the problem. Works on accelerator hardware like GPUs, TPUs, and beyond.
  - Cons: Only works for computations that be represented block-wise, and relies on the compiler to generate good code.

In this sense, the different parallel programming "languages" and features are much more similar than they are all different, falling into similar categories.
* Ordinary Differential Equations, Applications and Discretizations
Now that we have a sense of parallelism, let's return back to our thread on scientific machine learning to start constructing parallel algorithms for integration of scientific models. We previously introduced discrete dynamical systems and their asymptotic behavior. However, many physical systems are not discrete and are in fact continuous. In this discussion we will understand how to numerically compute ordinary differential equations by transforming them into discrete dynamical systems, and use this to come up with simulation techniques for physical systems.
** What is an Ordinary Differential Equation?
:PROPERTIES:
:CUSTOM_ID: what-is-an-ordinary-differential-equation
:END:
An ordinary differential equation is an equation defined by a relationship on the derivative. In its general form we have that

\[u' = f(u,p,t)\]

describes the evolution of some variable \(u(t)\) which we would like to solve for. In its simplest sense, the solution to the ordinary differential equation is just the integral, since by taking the integral of both sides and applying the Fundamental Theorem of Calculus we have that

\[u = \int_{t_0}^{t_f} f(u,p,t)dt\]

The difficulty of this equation is that the variable \(u(t)\) is unknown and dependent on \(t\), meaning that the integral cannot readily be solved by simple calculus. In fact, in almost all cases there exists no analytical solution for \(u\) which is readily available. However, we can understand the behavior by looking at some simple cases.

** Solving Ordinary Differential Equations in Julia
:PROPERTIES:
:CUSTOM_ID: solving-ordinary-differential-equations-in-julia
:END:
To solve an ordinary differential equation in Julia, one can use the [[http://docs.juliadiffeq.org/latest/][DifferentialEquations.jl]] package to define the differential equation you'd like to solve. Let's say we want to solve the Lorenz equations:

#+begin_src math
\begin{align}
\frac{dx}{dt} &= σ(y-x) \\
\frac{dy}{dt} &= x(ρ-z) - y \\
\frac{dz}{dt} &= xy - βz \\
\end{align}
#+end_src

which was the system used in our investigation of discrete dynamics. The first thing we need to do is give it this differential equation. We can either write it in an in-place form =f(du,u,p,t)= or an out-of-place form =f(u,p,t)=. Let's write it in the in-place form:

#+begin_src julia
function lorenz(du,u,p,t)
 du[1] = p[1]*(u[2]-u[1])
 du[2] = u[1]*(p[2]-u[3]) - u[2]
 du[3] = u[1]*u[2] - p[3]*u[3]
end
#+end_src

*Question: How could I maybe speed this up a little?*

Next we give an /initial condition/. Here, this is a vector of equations, so our initial condition has to be a vector. Let's choose the following initial condition:

#+begin_src julia
u0 = [1.0,0.0,0.0]
#+end_src

Notice that I made sure to use =Float64= values in the initial condition. The Julia library's functions are generic and internally use the corresponding types that you give it. Integer types do not bode well for continuous problems.

Next, we have to tell it the timespan to solve on. Here, let's some from time 0 to 100. This means that we would use:

#+begin_src julia
tspan = (0.0,100.0)
#+end_src

Now we need to define our parameters. We will use the same ones as from our discrete dynamical system investigation.

#+begin_src julia
p = (10.0,28.0,8/3)
#+end_src

These describe an =ODEProblem=. Let's bring in DifferentialEquations.jl and define the ODE:

#+begin_src julia
using DifferentialEquations
prob = ODEProblem(lorenz,u0,tspan,p)
#+end_src

Now we can solve it by calling =solve=:

#+begin_src julia
sol = solve(prob)
#+end_src

To see what the solution looks like, we can call =plot=:

#+begin_src julia
using Plots
plot(sol)
#+end_src

We can also plot phase space diagrams by telling it which =vars= to compare on which axis. Let's plot this in the =(x,y,z)= plane:

#+begin_src julia
plot(sol,vars=(1,2,3))
#+end_src

Note that the sentinal to time is =0=, so we can also do =(t,y,z)= with:

#+begin_src julia
plot(sol,vars=(0,2,3))
#+end_src

The equation is continuous and therefore the solution is continuous. We can see this by checking how it is at any random time value:

#+begin_src julia
sol(0.5)
#+end_src

which gives the current evolution at that time point.

** Differential Equations from Scientific Contexts
:PROPERTIES:
:CUSTOM_ID: differential-equations-from-scientific-contexts
:END:
*** N-Body Problems and Astronomy
:PROPERTIES:
:CUSTOM_ID: n-body-problems-and-astronomy
:END:
There are many different contexts in which differential equations show up. In fact, it's not a stretch to say that the laws in all fields of science are encoded in differential equations. The starting point for physics is Newton's laws of gravity, which define an N-body ordinary differential equation system by describing the force between two particles as:

\[F = G \frac{m_1m_2}{r^2}\]

where \(r^2\) is the Euclidian distance between the two particles. From here, we use the fact that

\[F = ma\]

to receive differential equations in terms of the accelerations of each particle. The differential equation is a system, where we know the change in position is due to the current velocity:

\[x' = v\]

and the change in velocity is the acceleration:

\[v' = F/m = G \frac{m_i}{r_i^2}\]

where \(i\) runs over the other particles. Thus we have a vector of position derivatives and a vector of velocity derivatives that evolve over time to give the evolving positions and velocity.

An example of this is the [[https://archimede.dm.uniba.it/~testset/report/plei.pdf][Pleiades problem]], which is an approximation to a 7-star chaotic system. It can be written as:

#+begin_src julia
using OrdinaryDiffEq

function pleiades(du,u,p,t)
  @inbounds begin
  x = view(u,1:7)   # x
  y = view(u,8:14)  # y
  v = view(u,15:21) # x′
  w = view(u,22:28) # y′
  du[1:7] .= v
  du[8:14].= w
  for i in 15:28
    du[i] = zero(u[1])
  end
  for i=1:7,j=1:7
    if i != j
      r = ((x[i]-x[j])^2 + (y[i] - y[j])^2)^(3/2)
      du[14+i] += j*(x[j] - x[i])/r
      du[21+i] += j*(y[j] - y[i])/r
    end
  end
  end
end
tspan = (0.0,3.0)
prob = ODEProblem(pleiades,[3.0,3.0,-1.0,-3.0,2.0,-2.0,2.0,3.0,-3.0,2.0,0,0,-4.0,4.0,0,0,0,0,0,1.75,-1.5,0,0,0,-1.25,1,0,0],tspan)
#+end_src

where we assume \(m_i = i\). When we solve this equation we receive the following:

#+begin_src julia
sol = solve(prob,Vern8(),abstol=1e-10,reltol=1e-10)
plot(sol)
#+end_src

#+begin_src julia
tspan = (0.0,200.0)
prob = ODEProblem(pleiades,[3.0,3.0,-1.0,-3.0,2.0,-2.0,2.0,3.0,-3.0,2.0,0,0,-4.0,4.0,0,0,0,0,0,1.75,-1.5,0,0,0,-1.25,1,0,0],tspan)
sol = solve(prob,Vern8(),abstol=1e-10,reltol=1e-10)
plot(sol,vars=((1:7),(8:14)))
#+end_src

*** Population Ecology: Lotka-Volterra
:PROPERTIES:
:CUSTOM_ID: population-ecology-lotka-volterra
:END:
Population ecology's starting point is the Lotka-Volterra equations which describes the interactions between a predator and a prey. In this case, the prey grows at an exponential rate but has a term that reduces its population by being eaten by the predator. The predator's growth is dependent on the available food (the amount of prey) and has a decay rate due to old age. This model is then written as follows:

#+begin_src julia
function lotka(du,u,p,t)
  du[1] = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = -p[3]*u[2] + p[4]*u[1]*u[2]
end

p = [1.5,1.0,3.0,1.0]
prob = ODEProblem(lotka,[1.0,1.0],(0.0,10.0),p)
sol = solve(prob)
plot(sol)
#+end_src

*** Biochemistry: Robertson Equations
:PROPERTIES:
:CUSTOM_ID: biochemistry-robertson-equations
:END:
Biochemical equations commonly display large separation of timescales which lead to a stiffness phenomena that will be investigated later. The classic "hard" equations for ODE integration thus tend to come from biology (not physics!) due to this property. One of the standard models is the Robertson model, which can be described as:

#+begin_src julia
using Sundials, ParameterizedFunctions
function rober(du,u,p,t)
  y₁,y₂,y₃ = u
  k₁,k₂,k₃ = p
  du[1] = -k₁*y₁+k₃*y₂*y₃
  du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃
  du[3] =  k₂*y₂^2
end
prob = ODEProblem(rober,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))
sol = solve(prob,Rosenbrock23())
plot(sol)
#+end_src

#+begin_src julia
plot(sol, xscale=:log10, tspan=(1e-6, 1e5), layout=(3,1))
#+end_src

*** Chemical Physics: Pollution Models
:PROPERTIES:
:CUSTOM_ID: chemical-physics-pollution-models
:END:
Chemical reactions in physical models are also described as differential equation systems. The following is a classic model of dynamics between different species of pollutants:

#+begin_src julia
k1=.35e0
k2=.266e2
k3=.123e5
k4=.86e-3
k5=.82e-3
k6=.15e5
k7=.13e-3
k8=.24e5
k9=.165e5
k10=.9e4
k11=.22e-1
k12=.12e5
k13=.188e1
k14=.163e5
k15=.48e7
k16=.35e-3
k17=.175e-1
k18=.1e9
k19=.444e12
k20=.124e4
k21=.21e1
k22=.578e1
k23=.474e-1
k24=.178e4
k25=.312e1
p = (k1,k2,k3,k4,k5,k6,k7,k8,k9,k10,k11,k12,k13,k14,k15,k16,k17,k18,k19,k20,k21,k22,k23,k24,k25)
function f(dy,y,p,t)
 k1,k2,k3,k4,k5,k6,k7,k8,k9,k10,k11,k12,k13,k14,k15,k16,k17,k18,k19,k20,k21,k22,k23,k24,k25 = p
 r1  = k1 *y[1]
 r2  = k2 *y[2]*y[4]
 r3  = k3 *y[5]*y[2]
 r4  = k4 *y[7]
 r5  = k5 *y[7]
 r6  = k6 *y[7]*y[6]
 r7  = k7 *y[9]
 r8  = k8 *y[9]*y[6]
 r9  = k9 *y[11]*y[2]
 r10 = k10*y[11]*y[1]
 r11 = k11*y[13]
 r12 = k12*y[10]*y[2]
 r13 = k13*y[14]
 r14 = k14*y[1]*y[6]
 r15 = k15*y[3]
 r16 = k16*y[4]
 r17 = k17*y[4]
 r18 = k18*y[16]
 r19 = k19*y[16]
 r20 = k20*y[17]*y[6]
 r21 = k21*y[19]
 r22 = k22*y[19]
 r23 = k23*y[1]*y[4]
 r24 = k24*y[19]*y[1]
 r25 = k25*y[20]

 dy[1]  = -r1-r10-r14-r23-r24+
          r2+r3+r9+r11+r12+r22+r25
 dy[2]  = -r2-r3-r9-r12+r1+r21
 dy[3]  = -r15+r1+r17+r19+r22
 dy[4]  = -r2-r16-r17-r23+r15
 dy[5]  = -r3+r4+r4+r6+r7+r13+r20
 dy[6]  = -r6-r8-r14-r20+r3+r18+r18
 dy[7]  = -r4-r5-r6+r13
 dy[8]  = r4+r5+r6+r7
 dy[9]  = -r7-r8
 dy[10] = -r12+r7+r9
 dy[11] = -r9-r10+r8+r11
 dy[12] = r9
 dy[13] = -r11+r10
 dy[14] = -r13+r12
 dy[15] = r14
 dy[16] = -r18-r19+r16
 dy[17] = -r20
 dy[18] = r20
 dy[19] = -r21-r22-r24+r23+r25
 dy[20] = -r25+r24
end
#+end_src

#+begin_src julia
u0 = zeros(20)
u0[2]  = 0.2
u0[4]  = 0.04
u0[7]  = 0.1
u0[8]  = 0.3
u0[9]  = 0.01
u0[17] = 0.007
prob = ODEProblem(f,u0,(0.0,60.0),p)
sol = solve(prob,Rodas5())
#+end_src

#+begin_src julia
plot(sol)
#+end_src

#+begin_src julia
plot(sol, xscale=:log10, tspan=(1e-6, 60), layout=(3,1))
#+end_src

** Geometric Properties
:PROPERTIES:
:CUSTOM_ID: geometric-properties
:END:
*** Linear Ordinary Differential Equations
:PROPERTIES:
:CUSTOM_ID: linear-ordinary-differential-equations
:END:
The simplest ordinary differential equation is the scalar linear ODE, which is given in the form

\[u' = \alpha u\]

We can solve this by noticing that \((e^{\alpha t})^\prime = \alpha e^{\alpha t}\) satisfies the differential equation and thus the general solution is:

\[u(t) = u(0)e^{\alpha t}\]

From the analytical solution we have that:

- If \(Re(\alpha) > 0\) then \(u(t) \rightarrow \infty\) as \(t \rightarrow \infty\)
- If \(Re(\alpha) < 0\) then \(u(t) \rightarrow 0\) as \(t \rightarrow \infty\)
- If \(Re(\alpha) = 0\) then \(u(t)\) has a constant or periodic solution.

This theory can then be extended to multivariable systems in the same way as the discrete dynamics case. Let \(u\) be a vector and have

\[u' = Au\]

be a linear ordinary differential equation. Assuming \(A\) is diagonalizable, we diagonalize \(A = P^{-1}DP\) to get

\[Pu' = DPu\]

and change coordinates \(z = Pu\) so that we have

\[z' = Dz\]

which decouples the equation into a system of linear ordinary differential equations which we solve individually. Thus we see that, similarly to the discrete dynamical system, we have that:

- If all of the eigenvalues negative, then \(u(t) \rightarrow 0\) as \(t \rightarrow \infty\)
- If any eigenvalue is positive, then \(u(t) \rightarrow \infty\) as \(t \rightarrow \infty\)

*** Nonlinear Ordinary Differential Equations
:PROPERTIES:
:CUSTOM_ID: nonlinear-ordinary-differential-equations
:END:
As with discrete dynamical systems, the geometric properties extend locally to the linearization of the continuous dynamical system as defined by:

\[u' = \frac{df}{du} u\]

where \(\frac{df}{du}\) is the Jacobian of the system. This is a consequence of the Hartman-Grubman Theorem.

** Numerically Solving Ordinary Differential Equations
:PROPERTIES:
:CUSTOM_ID: numerically-solving-ordinary-differential-equations
:END:
*** Euler's Method
:PROPERTIES:
:CUSTOM_ID: eulers-method
:END:
To numerically solve an ordinary differential equation, one turns the continuous equation into a discrete equation by /discretizing/ it. The simplest discretization is the /Euler method/. The Euler method can be thought of as a simple approximation replacing \(dt\) with a small non-infinitesimal \(\Delta t\). Thus we can approximate

\[f(u,p,t) = u' = \frac{du}{dt} \approx \frac{\Delta u}{\Delta t}\]

and now since \(\Delta u = u_{n+1} - u_n\) we have that

\[\Delta t f(u,p,t) = u_{n+1} - u_n\]

We need to make a choice as to where we evaluate \(f\) at. The simplest approximation is to evaluate it at \(t_n\) with \(u_n\) where we already have the data, and thus we re-arrange to get

\[u_{n+1} = u_n + \Delta t f(u,p,t)\]

This is the Euler method.

We can interpret it more rigorously by looking at the Taylor series expansion. First write out the Taylor series for the ODE's solution in the near future:

\[u(t+\Delta t) = u(t) + \Delta t u'(t) + \frac{\Delta t^2}{2} u''(t) + \ldots\]

Recall that \(u' = f(u,p,t)\) by the definition of the ODE system, and thus we have that

\[u(t+\Delta t) = u(t) + \Delta t f(u,p,t) + \mathcal{O}(\Delta t^2)\]

This is a first order approximation because the error in our step can be expresed as an error in the derivative, i.e.

\[\frac{u(t + \Delta t) - u(t)}{\Delta t} = f(u,p,t) + \mathcal{O}(\Delta t)\]

*** Higher Order Methods
:PROPERTIES:
:CUSTOM_ID: higher-order-methods
:END:
We can use this analysis to extend our methods to higher order approximation by simply matching the Taylor series to a higher order. Intuitively, when we developed the Euler method we had to make a choice:

\[u_{n+1} = u_n + \Delta t f(u,p,t)\]

where do we evaluate \(f\)? One may think that the best derivative approximation my come from the middle of the interval, in which case we might want to evaluate it at \(t + \frac{\Delta t}{2}\). To do so, we can use the Euler method to approximate the value at \(t + \frac{\Delta t}{2}\) and then use that value to approximate the derivative at \(t + \frac{\Delta t}{2}\). This looks like:

#+begin_src math
k_1 = f(u_n,p,t)\\
k_2 = f(u_n + \frac{\Delta t}{2} k_1,p,t + \frac{\Delta t}{2})\\
u_{n+1} = u_n + \Delta t k_2
#+end_src

which we can also write as:

#+begin_src math
u_{n+1} = u_n + \Delta t f(u_n + \frac{\Delta t}{2} f_n,p,t + \frac{\Delta t}{2})
#+end_src

where \(f_n = f(u_n,p,t)\). If we do the two-dimensional Taylor expansion we get:

#+begin_src math
u_{n+1} = u_n + \Delta t f_n + \frac{\Delta t^2}{2}(f_t + f_u f)(u_n,p,t)\\
+ \frac{\Delta t^3}{6} (f_{tt} + 2f_{tu}f + f_{uu}f^2)(u_n,p,t)
#+end_src

which when we compare against the true Taylor series:

#+begin_src math
u(t+\Delta t) = u_n + \Delta t f(u_n,p,t) + \frac{\Delta t^2}{2}(f_t + f_u f)(u_n,p,t)
+ \frac{\Delta t^3}{6}(f_{tt} + 2f_{tu} + f_{uu}f^2 + f_t f_u + f_u^2 f)(u_n,p,t)
#+end_src

and thus we see that

#+begin_src math
u(t + \Delta t) - u_n = \mathcal{O}(\Delta t^3)
#+end_src

*** Runge-Kutta Methods
:PROPERTIES:
:CUSTOM_ID: runge-kutta-methods
:END:
More generally, Runge-Kutta methods are of the form:

#+begin_src math
k_1 = f(u_n,p,t)\\
k_2 = f(u_n + \Delta t (a_{21} k_1),p,t + \Delta t c_2)\\
k_3 = f(u_n + \Delta t (a_{31} k_1 + a_{32} k_2),p,t + \Delta t c_3)\\
\vdots \\
u_{n+1} = u_n + \Delta t (b_1 k_1 + \ldots + b_s k_s)
#+end_src

where \(s\) is the number of stages. These can be expressed as a tableau:

[[https://en.wikipedia.org/wiki/List_of_Runge%E2%80%93Kutta_methods]]

The order of the Runge-Kutta method is simply the number of terms in the Taylor series that ends up being matched by the resulting expansion. For example, for the 4th order you can expand out and see that the following equations need to be satisfied:

[[https://user-images.githubusercontent.com/1814174/95117136-105ae780-0716-11eb-9f6a-49fecf7adbeb.PNG]]

The classic Runge-Kutta method is also known as RK4 and is the following 4th order method:

#+begin_src math
k_1 = f(u_n,p,t)\\
k_2 = f(u_n + \frac{\Delta t}{2} k_1,p,t + \frac{\Delta t}{2})\\
k_3 = f(u_n + \frac{\Delta t}{2} k_2,p,t + \frac{\Delta t}{2})\\
k_4 = f(u_n + \Delta t k_3,p,t + \Delta t)\\
u_{n+1} = u_n + \frac{\Delta t}{6}(k_1 + 2 k_2 + 2 k_3 + k_4)\\
#+end_src

While it's widely known and simple to remember, it's not necessarily good. The way to judge a Runge-Kutta method is by looking at the size of the coefficient of the next term in the Taylor series: if it's large then the true error can be larger, even if it matches another one asymptotically.

** What Makes a Good Method?
:PROPERTIES:
:CUSTOM_ID: what-makes-a-good-method
:END:
*** Leading Truncation Coeffcients
:PROPERTIES:
:CUSTOM_ID: leading-truncation-coeffcients
:END:
For given orders of explicit Runge-Kutta methods, lower bounds for the number of =f= evaluations (stages) required to receive a given order are known:

[[https://user-images.githubusercontent.com/1814174/95117078-f8836380-0715-11eb-9acf-0626338307d1.PNG]]

While unintuitive, using the method is not necessarily the one that reduces the coefficient the most. The reason is because what is attempted in ODE solving is precisely the opposite of the analysis. In the ODE analysis, we're looking at behavior as \(\Delta t \rightarrow 0\). However, when efficiently solving ODEs, we want to use the largest \(\Delta t\) which satisfies error tolerances.

The most widely used method is the Dormand-Prince 5th order Runge-Kutta method, whose tableau is represented as:

[[http://rotordynamics.files.wordpress.com/2014/05/new-picture6.png]]

Notice that this method takes 7 calls to =f= for 5th order. The key to this method is that it has optimized leading truncation error coefficients, under some extra assumptions which allow for the analysis to be simplified.

*** Looking at the Effects of RK Method Choices and Code Optimizations
:PROPERTIES:
:CUSTOM_ID: looking-at-the-effects-of-rk-method-choices-and-code-optimizations
:END:
Pulling from the [[https://github.com/SciML/SciMLBenchmarks.jl][SciML Benchmarks]], we can see the general effect of these different properties on a given set of Runge-Kutta methods:

[[https://user-images.githubusercontent.com/1814174/95118000-7c8a1b00-0717-11eb-8080-2179da500cd2.PNG]]

Here, the order of the method is given in the name. We can see one immediate factor is that, as the requested error in the calculation decreases, the higher order methods become more efficient. This is because to decrease error, you decrease \(\Delta t\), and thus the exponent difference with respect to \(\Delta t\) has more of a chance to pay off for the extra calls to =f=. Additionally, we can see that order is not the only determining factor for efficiency: the Vern8 method seems to have a clear approximate 2.5x performance advantage over the whole span of the benchmark compared to the DP8 method, even though both are 8th order methods. This is because of the leading truncation terms: with a small enough \(\Delta t\), the more optimized method (Vern8) will generally have low error in a step for the same \(\Delta t\) because the coefficients in the expansion are generally smaller.

This is a factor which is generally ignored in high level discussions of numerical differential equations, but can lead to orders of magnitude differences! This is highlighted in the following plot:

[[https://user-images.githubusercontent.com/1814174/95118457-544eec00-0718-11eb-8c19-f402e2cb8842.PNG]]

Here we see ODEInterface.jl's ODEInterfaceDiffEq.jl wrapper into the SciML common interface for the standard =dopri= method from Fortran, and ODE.jl, the original ODE solvers in Julia, have a performance disadvantage compared to the DifferentialEquations.jl methods due in part to some of the coding performance pieces that we discussed in the first few lectures.

Specifically, a large part of this can be attributed to inlining of the higher order functions, i.e. ODEs are defined by a user function and then have to be called from the solver. If the solver code is compiled as a shared library ahead of time, like is commonly done in C++ or Fortran, then there can be a function call overhead that is eliminated by JIT compilation optimizing across the function call barriers (known as interprocedural optimization). This is one way which a JIT system can outperform an AOT (ahead of time) compiled system in real-world code (for completeness, two other ways are by doing full function specialization, which is something that is [[https://scalac.io/specialized-generics-object-instantiation/][not generally possible in AOT languages given that you cannot know all types ahead of time for a fully generic function]], and [[https://github.com/dyu/ffi-overhead#results-500m-calls][calling C itself, i.e. c-ffi (foreign function interface), can be optimized using the runtime information of the JIT compiler to outperform C!]]).

The other performance difference being shown here is due to optimization of the method. While a slightly different order, we can see a clear difference in the performance of RK4 vs the coefficient optimized methods. It's about the same order of magnitude as "highly optimized code differences", showing that both the Runge-Kutta coefficients and the code implementation can have a significant impact on performance.

Taking a look at what happens when interpreted languages get involved highlights some of the code challenges in this domain. Let's take a look at for example the results when simulating 3 ODE systems with the various RK methods:

[[https://user-images.githubusercontent.com/1814174/95131785-b1549d00-072c-11eb-8d2a-490f69a4b99f.PNG]]

We see that using interpreted languages introduces around a 50x-100x performance penalty. If you recall in your previous lecture, the discrete dynamical system that was being simulated was the 3-dimensional Lorenz equation discretized by Euler's method, meaning that the performance of that implementation is a good proxy for understanding the performance differences in this graph. Recall that in previous lectures we saw an approximately 5x performance advantage when specializing on the system function and size and around 10x by reducing allocations: these features account for the performance differences noticed between library implementations, which are then compounded by the use of different RK methods (note that R uses "call by copy" which even further increases the memory usages and makes standard usage of the language incompatible with mutating function calls!).

*** Stability of a Method
:PROPERTIES:
:CUSTOM_ID: stability-of-a-method
:END:
Simply having an order on the truncation error does not imply convergence of the method. The disconnect is that the errors at a given time point may not dissipate. What also needs to be checked is the asymptotic behavior of a disturbance. To see this, one can utilize the linear test problem:

\[u' = \alpha u\]

and ask the question, does the discrete dynamical system defined by the discretized ODE end up going to zero? You would hope that the discretized dynamical system and the continuous dynamical system have the same properties in this simple case, and this is known as linear stability analysis of the method.

As an example, take a look at the Euler method. Recall that the Euler method was given by:

\[u_{n+1} = u_n + \Delta t f(u_n,p,t)\]

When we plug in the linear test equation, we get that

\[u_{n+1} = u_n + \Delta t \alpha u_n\]

If we let \(z = \Delta t \alpha\), then we get the following:

\[u_{n+1} = u_n + z u_n = (1+z)u_n\]

which is stable when \(z\) is in the shifted unit circle. This means that, as a necessary condition, the step size \(\Delta t\) needs to be small enough that \(z\) satisfies this condition, placing a stepsize limit on the method.

[[https://user-images.githubusercontent.com/1814174/95117231-3c766880-0716-11eb-9069-039253bcebda.PNG]]

If \(\Delta t\) is ever too large, it will cause the equation to overshoot zero, which then causes oscillations that spiral out to infinity.

[[https://user-images.githubusercontent.com/1814174/95132604-0d6bf100-072e-11eb-8af5-663512a0db14.PNG]]

[[https://user-images.githubusercontent.com/1814174/95132963-9125dd80-072e-11eb-878e-61f77a20d03e.gif]]

Thus the stability condition places a hard constraint on the allowed \(\Delta t\) which will result in a realistic simulation.

For reference, the stability regions of the 2nd and 4th order Runge-Kutta methods that we discussed are as follows:

[[https://user-images.githubusercontent.com/1814174/95117286-56b04680-0716-11eb-9c6a-07fc4d190a09.PNG]]

*** Interpretation of the Linear Stability Condition
:PROPERTIES:
:CUSTOM_ID: interpretation-of-the-linear-stability-condition
:END:
To interpret the linear stability condition, recall that the linearization of a system interprets the dynamics as locally being due to the Jacobian of the system. Thus

\[u' = f(u,p,t)\]

is locally equivalent to

\[u' = \frac{df}{du}u\]

You can understand the local behavior through diagonalizing this matrix. Therefore, the scalar for the linear stability analysis is performing an analysis on the eigenvalues of the Jacobian. The method will be stable if the largest eigenvalues of df/du are all within the stability limit. This means that stability effects are different throughout the solution of a nonlinear equation and are generally understood locally (though different more comprehensive stability conditions exist!).

*** Implicit Methods
:PROPERTIES:
:CUSTOM_ID: implicit-methods
:END:
If instead of the Euler method we defined \(f\) to be evaluated at the future point, we would receive a method like:

\[u_{n+1} = u_n + \Delta t f(u_{n+1},p,t+\Delta t)\]

in which case, for the stability calculation we would have that

\[u_{n+1} = u_n + \Delta t \alpha u_n\]

or

\[(1-z) u_{n+1} = u_n\]

which means that

\[u_{n+1} = \frac{1}{1-z} u_n\]

which is stable for all \(Re(z) < 0\) a property which is known as A-stability. It is also stable as \(z \rightarrow \infty\), a property known as L-stability. This means that for equations with very ill-conditioned Jacobians, this method is still able to be use reasonably large stepsizes and can thus be efficient.

[[https://user-images.githubusercontent.com/1814174/95117191-28326b80-0716-11eb-8e17-889308bdff53.PNG]]

*** Stiffness and Timescale Separation
:PROPERTIES:
:CUSTOM_ID: stiffness-and-timescale-separation
:END:
From this we see that there is a maximal stepsize whenever the eigenvalues of the Jacobian are sufficiently large. It turns out that's not an issue if the phenomena we see are fast, since then the total integration time tends to be small. However, if we have some equations with both fast modes and slow modes, like the Robertson equation, then it is very difficult because in order to resolve the slow dynamics over a long timespan, one needs to ensure that the fast dynamics do not diverge. This is a property known as stiffness. Stiffness can thus be approximated in some sense by the condition number of the Jacobian. The condition number of a matrix is its maximal eigenvalue divided by its minimal eigenvalue and gives a rough measure of the local timescale separations. If this value is large and one wants to resolve the slow dynamics, then explict integrators, like the explicit Runge-Kutta methods described before, have issues with stability. In this case implicit integrators (or other forms of stabilized stepping) are required in order to efficiently reach the end time step.

[[https://user-images.githubusercontent.com/1814174/95132552-f6c59a00-072d-11eb-881e-24364b7b728f.PNG]]

** Exploiting Continuity
:PROPERTIES:
:CUSTOM_ID: exploiting-continuity
:END:
So far, we have looked at ordinary differential equations as a \(\Delta t \rightarrow 0\) formulation of a discrete dynamical system. However, continuous dynamics and discrete dynamics have very different characteristics which can be utilized in order to arrive at simpler models and faster computations.

*** Geometric Properties: No Jumping and the Poincaré--Bendixson theorem
:PROPERTIES:
:CUSTOM_ID: geometric-properties-no-jumping-and-the-poincarébendixson-theorem
:END:
In terms of geometric properties, continuity places a large constraint on the possible dynamics. This is because of the physical constraint on "jumping", i.e. flows of differential equations cannot jump over each other. If you are ever at some point in phase space and \(f\) is not explicitly time-dependent, then the direction of \(u'\) is uniquely determined (given reasonable assumptions on \(f\)), meaning that flow lines (solutions to the differential equation) can never cross.

A result from this is the Poincaré--Bendixson theorem, which states that, with any arbitrary (but nice) two dimensional continuous system, you can only have 3 behaviors:

- Steady state behavior
- Divergence
- Periodic orbits

A simple proof by picture shows this.
* Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras
** Machine Epsilon and Roundoff Error
:PROPERTIES:
:CUSTOM_ID: machine-epsilon-and-roundoff-error
:END:
Floating point arithmetic is relatively scaled, which means that the precision that you get from calculations is relative to the size of the floating point numbers. Generally, you have 16 digits of accuracy in (64-bit) floating point operations. To measure this, we define /machine epsilon/ as the value by which =1 + E = 1=. For floating point numbers, this is:

#+begin_src julia
eps(Float64)
#+end_src

However, since it's relative, this value changes as we change our reference value:

#+begin_src julia
@show eps(1.0)
@show eps(0.1)
@show eps(0.01)
#+end_src

Thus issues with /roundoff error/ come when one subtracts out the higher digits. For example, \((x + \epsilon) - x\) should just be \(\epsilon\) if there was no roundoff error, but if \(\epsilon\) is small then this kicks in. If \(x = 1\) and \(\epsilon\) is of size around \(10^{-10}\), then \(x+ \epsilon\) is correct for 10 digits, dropping off the smallest 6 due to error in the addition to \(1\). But when you subtract off \(x\), you don't get those digits back, and thus you only have 6 digits of \(\epsilon\) correct.

Let's see this in action:

#+begin_src julia
ϵ = 1e-10rand()
@show ϵ
@show (1+ϵ)
ϵ2 = (1+ϵ) - 1
(ϵ - ϵ2)
#+end_src

See how \(\epsilon\) is only rebuilt at accuracy around \(10^{-16}\) and thus we only keep around 6 digits of accuracy when it's generated at the size of around \(10^{-10}\)!

*** Finite Differencing and Numerical Stability
:PROPERTIES:
:CUSTOM_ID: finite-differencing-and-numerical-stability
:END:
To start understanding how to compute derivatives on a computer, we start with /finite differencing/. For finite differencing, recall that the definition of the derivative is:

\[f'(x) = \lim_{\epsilon \rightarrow 0} \frac{f(x+\epsilon)-f(x)}{\epsilon}\]

Finite differencing directly follows from this definition by choosing a small \(\epsilon\). However, choosing a good \(\epsilon\) is very difficult. If \(\epsilon\) is too large than there is error since this definition is asymptotic. However, if \(\epsilon\) is too small, you receive roundoff error. To understand why you would get roundoff error, recall that floating point error is relative, and can essentially store 16 digits of accuracy. So let's say we choose \(\epsilon = 10^{-6}\). Then \(f(x+\epsilon) - f(x)\) is roughly the same in the first 6 digits, meaning that after the subtraction there is only 10 digits of accuracy, and then dividing by \(10^{-6}\) simply brings those 10 digits back up to the correct relative size.

[[https://www.researchgate.net/profile/Jongrae_Kim/publication/267216155/figure/fig1/AS:651888458493955@1532433728729/Finite-Difference-Error-Versus-Step-Size.png]]

This means that we want to choose \(\epsilon\) small enough that the \(\mathcal{O}(\epsilon^2)\) error of the truncation is balanced by the \(O(1/\epsilon)\) roundoff error. Under some minor assumptions, one can argue that the average best point is \(\sqrt(E)\), where E is machine epsilon

#+begin_src julia
@show eps(Float64)
@show sqrt(eps(Float64))
#+end_src

This means we should not expect better than 8 digits of accuracy, even when things are good with finite differencing.

[[http://degenerateconic.com/wp-content/uploads/2014/11/complex_step1.png]]

The centered difference formula is a little bit better, but this picture suggests something much better...

*** Differencing in a Different Dimension: Complex Step Differentiation
:PROPERTIES:
:CUSTOM_ID: differencing-in-a-different-dimension-complex-step-differentiation
:END:
The problem with finite differencing is that we are mixing our really small number with the really large number, and so when we do the subtract we lose accuracy. Instead, we want to keep the small perturbation completely separate.

To see how to do this, assume that \(x \in \mathbb{R}\) and assume that \(f\) is complex analytic. You want to calculate a real derivative, but your function just happens to also be complex analytic when extended to the complex plane. Thus it has a Taylor series, and let's see what happens when we expand out this Taylor series purely in the complex direction:

\[f(x+ih) = f(x) + f'(x)ih - \frac{1}{2}f''(x)h^2 + \mathcal{O}(h^3)\]

which we can re-arrange as:

\[if'(x) = \frac{f(x+ih) - f(x)}{h} + \frac{1}{2}f''(x)h + \mathcal{O}(h^2)\]

Since \(x\) is real and \(f\) is real-valued on the reals, \(if'\) is purely imaginary. So let's take the imaginary parts of both sides:

\[f'(x) = \frac{Im(f(x+ih))}{h} + \mathcal{O}(h^2)\]

since \(Im(f(x)) = 0\) (since it's real valued, the next order term cancels for the same reason). Thus with a sufficiently small choice of \(h\), this is the /complex step differentiation/ formula for calculating the derivative.

But to understand the computational advantage, recall that \(x\) is pure real, and thus \(x+ih\) is a complex number where *the \(h\) never directly interacts with \(x\)* since a complex number is a two dimensional number where you keep the two pieces separate. Thus there is no numerical cancellation by using a small value of \(h\), and thus, due to the relative precision of floating point numbers, both the real and imaginary parts will be computed to (approximately) 16 digits of accuracy for any choice of \(h\).

*** Derivatives as nilpotent sensitivities
:PROPERTIES:
:CUSTOM_ID: derivatives-as-nilpotent-sensitivities
:END:
The derivative measures the *sensitivity* of a function, i.e. how much the function output changes when the input changes by a small amount \(\epsilon\):

\[f(a + \epsilon) = f(a) + f'(a) \epsilon + o(\epsilon).\]

In the following we will ignore higher-order terms; formally we set \(\epsilon^2 = 0\). This form of analysis can be made rigorous through a form of non-standard analysis called /Smooth Infinitesimal Analysis/ [1], though note that nilpotent infinitesimal requires /constructive logic/, and thus proof by contradiction is not allowed in this logic due to a lack of the /law of the excluded middle/.

A function \(f\) will be represented by its value \(f(a)\) and derivative \(f'(a)\), encoded as the coefficients of a degree-1 (Taylor) polynomial in \(\epsilon\):

\[f \rightsquigarrow f(a) + \epsilon f'(a)\]

Conversely, if we have such an expansion in \(\epsilon\) for a given function \(f\), then we can identify the coefficient of \(\epsilon\) as the derivative of \(f\).

*** Dual numbers
:PROPERTIES:
:CUSTOM_ID: dual-numbers
:END:
Thus, to extend the idea of complex step differentiation beyond complex analytic functions, we define a new number type, the /dual number/. A dual number is a multidimensional number where the sensitivity of the function is propagated along the dual portion.

Here we will now start to use \(\epsilon\) as a dimensional signifier, like \(i\), \(j\), or \(k\) for quaternion numbers. In order for this to work out, we need to derive an appropriate algebra for our numbers. To do this, we will look at Taylor series to make our algebra reconstruct differentiation.

Note that the chain rule has been explicitly encoded in the derivative part.

\[f(a + \epsilon) = f(a) + \epsilon f'(a)\]

to first order. If we have two functions

\[f \rightsquigarrow f(a) + \epsilon f'(a)\] \[g \rightsquigarrow g(a) + \epsilon g'(a)\]

then we can manipulate these Taylor expansions to calculate combinations of these functions as follows. Using the nilpotent algebra, we have that:

\[(f + g) = [f(a) + g(a)] + \epsilon[f'(a) + g'(a)]\]

\[(f \cdot g) = [f(a) \cdot g(a)] + \epsilon[f(a) \cdot g'(a) + g(a) \cdot f'(a) ]\]

From these we can /infer/ the derivatives by taking the component of \(\epsilon\). These also tell us the way to implement these in the computer.

*** Computer representation
:PROPERTIES:
:CUSTOM_ID: computer-representation
:END:
Setup (not necessary from the REPL):

#+begin_src julia
using InteractiveUtils  # only needed when using Weave
#+end_src

Each function requires two pieces of information and some particular "behavior", so we store these in a =struct=. It's common to call this a "dual number":

#+begin_src julia
struct Dual{T}
    val::T   # value
    der::T  # derivative
end
#+end_src

Each =Dual= object represents a function. We define arithmetic operations to mirror performing those operations on the corresponding functions.

We must first import the operations from =Base=:

#+begin_src julia
Base.:+(f::Dual, g::Dual) = Dual(f.val + g.val, f.der + g.der)
Base.:+(f::Dual, α::Number) = Dual(f.val + α, f.der)
Base.:+(α::Number, f::Dual) = f + α

#=
You can also write:
import Base: +
f::Dual + g::Dual = Dual(f.val + g.val, f.der + g.der)
=#

Base.:-(f::Dual, g::Dual) = Dual(f.val - g.val, f.der - g.der)

# Product Rule
Base.:*(f::Dual, g::Dual) = Dual(f.val*g.val, f.der*g.val + f.val*g.der)
Base.:*(α::Number, f::Dual) = Dual(f.val * α, f.der * α)
Base.:*(f::Dual, α::Number) = α * f

# Quotient Rule
Base.:/(f::Dual, g::Dual) = Dual(f.val/g.val, (f.der*g.val - f.val*g.der)/(g.val^2))
Base.:/(α::Number, f::Dual) = Dual(α/f.val, -α*f.der/f.val^2)
Base.:/(f::Dual, α::Number) = f * inv(α) # Dual(f.val/α, f.der * (1/α))

Base.:^(f::Dual, n::Integer) = Base.power_by_squaring(f, n)  # use repeated squaring for integer powers
#+end_src

We can now define =Dual=s and manipulate them:

#+begin_src julia
fd = Dual(3, 4)
gd = Dual(5, 6)

fd + gd
#+end_src

#+begin_src julia
fd * gd
#+end_src

#+begin_src julia
fd * (gd + gd)
#+end_src

*** Performance
:PROPERTIES:
:CUSTOM_ID: performance
:END:
It seems like we may have introduced significant computational overhead by creating a new data structure, and associated methods. Let's see how the performance is:

#+begin_src julia
add(a1, a2, b1, b2) = (a1+b1, a2+b2)
#+end_src

#+begin_src julia
add(1, 2, 3, 4)

using BenchmarkTools
a, b, c, d = 1, 2, 3, 4
@btime add($(Ref(a))[], $(Ref(b))[], $(Ref(c))[], $(Ref(d))[])
#+end_src

#+begin_src julia
a = Dual(1, 2)
b = Dual(3, 4)

add(j1, j2) = j1 + j2
add(a, b)
@btime add($(Ref(a))[], $(Ref(b))[])
#+end_src

It seems like we have lost /no/ performance.

#+begin_src julia
@code_native add(1, 2, 3, 4)
#+end_src

#+begin_src julia
@code_native add(a, b)
#+end_src

We see that the data structure itself has disappeared, and we basically have a standard Julia tuple.

*** Defining Higher Order Primitives
:PROPERTIES:
:CUSTOM_ID: defining-higher-order-primitives
:END:
We can also define functions of =Dual= objects, using the chain rule. To speed up our derivative function, we can directly hardcode the derivative of known functions which we call /primitives/. If =f= is a =Dual= representing the function \(f\), then =exp(f)= should be a =Dual= representing the function \(\exp \circ f\), i.e. with value \(\exp(f(a))\) and derivative \((\exp \circ f)'(a) = \exp(f(a)) \, f'(a)\):

#+begin_src julia
import Base: exp
#+end_src

#+begin_src julia
exp(f::Dual) = Dual(exp(f.val), exp(f.val) * f.der)
#+end_src

#+begin_src julia
fd
#+end_src

#+begin_src julia
exp(fd)
#+end_src

** Differentiating arbitrary functions
:PROPERTIES:
:CUSTOM_ID: differentiating-arbitrary-functions
:END:
For functions where we don't have a rule, we can recursively do dual number arithmetic within the function until we hit primitives where we know the derivative, and then use the chain rule to propagate the information back up. Under this algebra, we can represent \(a + \epsilon\) as =Dual(a, 1)=. Thus, applying =f= to =Dual(a, 1)= should give =Dual(f(a), f'(a))=. This is thus a 2-dimensional number for calculating the derivative without floating point error, *using the compiler to transform our equations into dual number arithmetic*. To differentiate an arbitrary function, we define a generic function and then change the algebra.

#+begin_src julia
hf(x) = x^2 + 2
a = 3
xx = Dual(a, 1)
#+end_src

Now we simply evaluate the function =h= at the =Dual= number =xx=:

#+begin_src julia
hf(xx)
#+end_src

The first component of the resulting =Dual= is the value \(h(a)\), and the second component is the derivative, \(h'(a)\)!

We can codify this into a function as follows:

#+begin_src julia
derivative(f, x) = f(Dual(x, one(x))).der
#+end_src

Here, =one= is the function that gives the value \(1\) with the same type as that of =x=.

Finally we can now calculate derivatives such as

#+begin_src julia
derivative(x -> 3x^5 + 2, 2)
#+end_src

As a bigger example, we can take a pure Julia =sqrt= function and differentiate it by changing the internal algebra:

#+begin_src julia
function newtons(x)
   a = x
   for i in 1:300
       a = 0.5 * (a + x/a)
   end
   a
end
@show newtons(2.0)
@show (newtons(2.0+sqrt(eps())) - newtons(2.0))/ sqrt(eps())
newtons(Dual(2.0,1.0))
#+end_src

*** Higher dimensions
:PROPERTIES:
:CUSTOM_ID: higher-dimensions
:END:
How can we extend this to higher dimensional functions? For example, we wish to differentiate the following function \(f: \mathbb{R}^2 \to \mathbb{R}\):

#+begin_src julia
fquad(x, y) = x^2 + x*y
#+end_src

Recall that the *partial derivative* \(\partial f/\partial x\) is defined by fixing \(y\) and differentiating the resulting function of \(x\):

#+begin_src julia
a, b = 3.0, 4.0

fquad_1(x) = fquad(x, b)  # single-variable function
#+end_src

Since we now have a single-variable function, we can differentiate it:

#+begin_src julia
derivative(fquad_1, a)
#+end_src

Under the hood this is doing

#+begin_src julia
fquad(Dual(a, one(a)), b)
#+end_src

Similarly, we can differentiate with respect to \(y\) by doing

#+begin_src julia
fquad_2(y) = fquad(a, y)  # single-variable function

derivative(fquad_2, b)
#+end_src

Note that we must do *two separate calculations* to get the two partial derivatives; in general, calculating the gradient \(\nabla\) of a function \(f:\mathbb{R}^n \to \mathbb{R}\) requires \(n\) separate calculations.

*** Implementation of higher-dimensional forward-mode AD
:PROPERTIES:
:CUSTOM_ID: implementation-of-higher-dimensional-forward-mode-ad
:END:
We can implement derivatives of functions \(f: \mathbb{R}^n \to \mathbb{R}\) by adding several independent partial derivative components to our dual numbers.

We can think of these as \(\epsilon\) perturbations in different directions, which satisfy \(\epsilon_i^2 = \epsilon_i \epsilon_j = 0\), and we will call \(\epsilon\) the vector of all perturbations. Then we have

\[f(a + \epsilon) = f(a) + \nabla f(a) \cdot \epsilon + \mathcal{O}(\epsilon^2),\]

where \(a \in \mathbb{R}^n\) and \(\nabla f(a)\) is the *gradient* of \(f\) at \(a\), i.e. the vector of partial derivatives in each direction. \(\nabla f(a) \cdot \epsilon\) is the *directional derivative* of \(f\) in the direction \(\epsilon\).

We now proceed similarly to the univariate case:

\[(f + g)(a + \epsilon) = [f(a) + g(a)] + [\nabla f(a) + \nabla g(a)] \cdot \epsilon\]

\[\begin{align}
(f \cdot g)(a + \epsilon) &= [f(a) + \nabla f(a) \cdot \epsilon ] \, [g(a) + \nabla g(a) \cdot \epsilon ] \\
&= f(a) g(a) + [f(a) \nabla g(a) + g(a) \nabla f(a)] \cdot \epsilon.
\end{align}\]

We will use the =StaticArrays.jl= package for efficient small vectors:

#+begin_src julia
using StaticArrays

struct MultiDual{N,T}
    val::T
    derivs::SVector{N,T}
end

import Base: +, *

function +(f::MultiDual{N,T}, g::MultiDual{N,T}) where {N,T}
    return MultiDual{N,T}(f.val + g.val, f.derivs + g.derivs)
end

function *(f::MultiDual{N,T}, g::MultiDual{N,T}) where {N,T}
    return MultiDual{N,T}(f.val * g.val, f.val .* g.derivs + g.val .* f.derivs)
end
#+end_src

#+begin_src julia
gcubic(x, y) = x*x*y + x + y

(a, b) = (1.0, 2.0)

xx = MultiDual(a, SVector(1.0, 0.0))
yy = MultiDual(b, SVector(0.0, 1.0))

gcubic(xx, yy)
#+end_src

We can calculate the Jacobian of a function \(\mathbb{R}^n \to \mathbb{R}^m\) by applying this to each component function:

#+begin_src julia
fsvec(x, y) = SVector(x*x + y*y , x + y)

fsvec(xx, yy)
#+end_src

It would be possible (and better for performance in many cases) to store all of the partials in a matrix instead.

Forward-mode AD is implemented in a clean and efficient way in the =ForwardDiff.jl= package:

#+begin_src julia
using ForwardDiff, StaticArrays

ForwardDiff.gradient( xx -> ( (x, y) = xx; x^2 * y + x*y ), [1, 2])
#+end_src

*** Directional derivative and gradient of functions \(f: \mathbb{R}^n \to \mathbb{R}\)
:PROPERTIES:
:CUSTOM_ID: directional-derivative-and-gradient-of-functions-f-mathbbrn-to-mathbbr
:END:
For a function \(f: \mathbb{R}^n \to \mathbb{R}\) the basic operation is the *directional derivative*:

\[\lim_{\epsilon \to 0} \frac{f(\mathbf{x} + \epsilon \mathbf{v}) - f(\mathbf{x})}{\epsilon} =
[\nabla f(\mathbf{x})] \cdot \mathbf{v},\]

where \(\epsilon\) is still a single dimension and \(\nabla f(\mathbf{x})\) is the direction in which we calculate.

We can directly do this using the same simple =Dual= numbers as above, using the /same/ \(\epsilon\), e.g.

\[f(x, y) = x^2  \sin(y)\]

\[\begin{align}
f(x_0 + a\epsilon, y_0 + b\epsilon) &= (x_0 + a\epsilon)^2  \sin(y_0 + b\epsilon) \\
&= x_0^2  \sin(y_0) + \epsilon[2ax_0  \sin(y_0) + x_0^2 b \cos(y_0)] + o(\epsilon)
\end{align}\]

so we have indeed calculated \(\nabla f(x_0, y_0) \cdot \mathbf{v},\) where \(\mathbf{v} = (a, b)\) are the components that we put into the derivative component of the =Dual= numbers.

If we wish to calculate the directional derivative in another direction, we could repeat the calculation with a different \(\mathbf{v}\). A better solution is to use another independent epsilon \(\epsilon\), expanding \[x = x_0 + a_1 \epsilon_1 + a_2 \epsilon_2\] and putting \(\epsilon_1 \epsilon_2 = 0\).

In particular, if we wish to calculate the gradient itself, \(\nabla f(x_0, y_0)\), we need to calculate both partial derivatives, which corresponds to two directional derivatives, in the directions \((1, 0)\) and \((0, 1)\), respectively.

*** Forward-Mode AD as jvp
:PROPERTIES:
:CUSTOM_ID: forward-mode-ad-as-jvp
:END:
Note that another representation of the directional derivative is \(f'(x)v\), where \(f'(x)\) is the Jacobian or total derivative of \(f\) at \(x\). To see the equivalence of this to a directional derivative, write it out in the standard basis:

\[w_i = \sum_{j}^{m} J_{ij} v_{j}\]

Now write out what \(J\) means and we see that:

\[w_i = \sum_j^{m} \frac{df_i}{dx_j} v_j = \nabla f_i(x) \cdot v\]

*The primitive action of forward-mode AD is \(f'(x)v\)!*

This is also known as a /Jacobian-vector product/, or /jvp/ for short.

We can thus represent vector calculus with multidimensional dual numbers as follows. Let \(d =[x,y]\), the vector of dual numbers. We can instead represent this as:

\[d = d_0 + v_1 \epsilon_1 + v_2 \epsilon_2\]

where \(d_0\) is the /primal/ vector \([x_0,y_0]\) and the \(v_i\) are the vectors for the /dual/ directions. If you work out this algebra, then note that a single application of \(f\) to a multidimensional dual number calculates:

\[f(d) = f(d_0) + f'(d_0)v_1 \epsilon_1 + f'(d_0)v_2 \epsilon_2\]

i.e. it calculates the result of \(f(x,y)\) and two separate directional derivatives. Note that because the information about \(f(d_0)\) is shared between the calculations, this is more efficient than doing multiple applications of \(f\). And of course, this is then generalized to \(m\) many directional derivatives at once by:

\[d = d_0 + v_1 \epsilon_1 + v_2 \epsilon_2 + \ldots + v_m \epsilon_m\]

*** Jacobian
:PROPERTIES:
:CUSTOM_ID: jacobian
:END:
For a function \(f: \mathbb{R}^n \to \mathbb{R}^m\), we reduce (conceptually, although not necessarily in code) to its component functions \(f_i: \mathbb{R}^n \to \mathbb{R}\), where \(f(x) = (f_1(x), f_2(x), \ldots, f_m(x))\).

Then

\[\begin{align}
f(x + \epsilon v) &= (f_1(x + \epsilon v), \ldots, f_m(x + \epsilon v)) \\
&= (f_1(x) + \epsilon[\nabla f_1(x) \cdot v], \dots, f_m(x) + \epsilon[\nabla f_m(x) \cdot v] \\
&= f(x) + [f'(x) \cdot v] \epsilon,
\end{align}\]

To calculate the complete Jacobian, we calculate these directional derivatives in the \(n\) different directions of the basis vectors, i.e. if

\(d = d_0 + e_1 \epsilon_1 + \ldots + e_n \epsilon_n\)

for \(e_i\) the \(i\)th basis vector, then

\(f(d) = f(d_0) + Je_1 \epsilon_1 + \ldots + Je_n \epsilon_n\)

computes all columns of the Jacobian simultaniously.

*** Array of Structs Representation
:PROPERTIES:
:CUSTOM_ID: array-of-structs-representation
:END:
Instead of thinking about a vector of dual numbers, thus we can instead think of dual numbers with vectors for the components. But if there are vectors for the components, then we can think of the grouping of dual components as a matrix. Thus define our multidimensional multi-partial dual number as:

\[D_0 = [d_1,d_2,d_3,\ldots,d_n]\]

\[\Sigma = \begin{bmatrix}
        d_{11} & d_{12} & \cdots & d_{1n} \\
        d_{21} & d_{22} &  & \vdots \\
        \vdots & & \ddots & \vdots \\
        d_{m1} & \hdots & \hdots & d_{mn}
    \end{bmatrix}\]

\[\epsilon=[\epsilon_1,\epsilon_2,\ldots,\epsilon_m]\]

\[D = D_0 + \Sigma \epsilon\]

where \(D_0\) is a vector in \(\mathbb{R}^n\), \(\epsilon\) is a vector of dimensional signifiers and \(\Sigma\) is a matrix in \(\mathbb{R}^{n \times m}\) where \(m\) is the number of concurrent differentiation dimensions. Each row of this is a dual number, but now we can use this to easily define higher dimensional primitives.

For example, let \(f(x) = Ax\), matrix multiplication. Then, we can show with our dual number arithmetic that:

\[f(D) = A*D_0 + A*\Sigma*\epsilon\]

is how one would compute the value of \(f(D_0)\) and the derivative \(f'(D_0)\) in all directions signified by the columns of \(\Sigma\) simultaneously. Using multidimensional Taylor series expansions and doing the manipulations like before indeed implies that the arithmetic on this object should follow:

\[f(D) = f(D_0) + f'(D_0)\Sigma \epsilon\]

where \(f'\) is the total derivative or the Jacobian of \(f\). This then allows our system to be highly efficient by allowing the definition of multidimensional functions, like linear algebra, to be primitives of multi-directional derivatives.

*** Higher derivatives
:PROPERTIES:
:CUSTOM_ID: higher-derivatives
:END:
The above techniques can be extended to higher derivatives by /adding more terms to the Taylor polynomial/, e.g.

\[f(a + \epsilon) = f(a) + \epsilon f'(a) + \frac{1}{2} \epsilon^2 f''(a) + o(\epsilon^2).\]

We treat this as a degree-2 (or degree-\(n\), in general) polynomial and do polynomial arithmetic to calculate the new polynomials. The coefficients of powers of \(\epsilon\) then give the higher-order derivatives.

For example, for a function \(f: \mathbb{R}^n \to \mathbb{R}\) we have

\[f(x + \epsilon v) = f(x) + \epsilon \left[ \sum_i (\partial_i f)(x) v_i \right] + \frac{1}{2}\epsilon^2 \left[ \sum_i \sum_j (\partial_{i,j} f) v_i v_j \right]\]

using =Dual= numbers with a single \(\epsilon\) component. In this way we can compute coefficients of the (symmetric) Hessian matrix.

** Application: solving nonlinear equations using the Newton method
:PROPERTIES:
:CUSTOM_ID: application-solving-nonlinear-equations-using-the-newton-method
:END:
As an application, we will see how to solve nonlinear equations of the form \[f(x) = 0\] for functions \(f: \mathbb{R}^n \to \mathbb{R}^n\).

Since in general we cannot do anything with nonlinearity, we try to reduce it (approximate it) with something linear. Furthermore, in general we know that it is not possible to solve nonlinear equations in closed form (even for polynomials of degree \(\ge 5\)), so we will need some kind of iterative method.

We start from an initial guess \(x_0\). The idea of the *Newton method* is to follow the tangent line to the function \(f\) at the point \(x_0\) and find where it intersects the \(x\)-axis; this will give the next iterate \(x_1\).

Algebraically, we want to solve \(f(x_1) = 0\). Suppose that \(x_1 = x_0 + \delta\) for some \(\delta\) that is currently unknown and which we wish to calculate.

Assuming \(\delta\) is small, we can expand:

\[f(x_1) = f(x_0 + \delta) = f(x_0) + Df(x_0) \cdot \delta + \mathcal{O}(\| \delta \|^2).\]

Since we wish to solve

\[f(x_0 + \delta) \simeq 0,\]

we put

\[f(x_0) + Df(x_0) \cdot \delta = 0,\]

so that /mathematically/ we have

\[\delta = -[Df(x_0)]^{-1} \cdot f(x_0).\]

Computationally we prefer to solve the matrix equation

\[J \delta = -f(x_0),\]

where \(J := Df(x_0)\) is the Jacobian of the function; Julia uses the syntax =\= ("backslash") for solving linear systems in an efficient way:

#+begin_src julia
using ForwardDiff, StaticArrays

function newton_step(f, x0)
    J = ForwardDiff.jacobian(f, x0)
    δ = J \ f(x0)

    return x0 - δ
end

function newton(f, x0)
    x = x0

    for i in 1:10
        x = newton_step(f, x)
        @show x
    end

    return x
end

fsvec2(xx) = ( (x, y) = xx;  SVector(x^2 + y^2 - 1, x - y) )

x0 = SVector(3.0, 5.0)

x = newton(fsvec2, x0)
#+end_src

** Conclusion
:PROPERTIES:
:CUSTOM_ID: conclusion
:END:
To make derivative calculations efficient and correct, we can move to higher dimensional numbers. In multiple dimensions, these then allow for multiple directional derivatives to be computed simultaneously, giving a method for computing the Jacobian of a function \(f\) on a single input. This is a direct application of using the compiler as part of a mathematical framework.

*** References
:PROPERTIES:
:CUSTOM_ID: references
:END:
- John L. Bell, /An Invitation to Smooth Infinitesimal Analysis/, http://publish.uwo.ca/~jbell/invitation%20to%20SIA.pdf
- Bell, John L. /A Primer of Infinitesimal Analysis/
- Nocedal & Wright, /Numerical Optimization/, Chapter 8
- Griewank & Walther, /Evaluating Derivatives/

Many thanks to David Sanders for helping make these lecture notes.
* Solving Stiff Ordinary Differential Equations
We have previously shown how to solve non-stiff ODEs via optimized Runge-Kutta methods, but we ended by showing that there is a fundamental limitation of these methods when attempting to solve stiff ordinary differential equations. However, we can get around these limitations by using different types of methods, like implicit Euler. Let's now go down the path of understanding how to efficiently implement stiff ordinary differential equation solvers, and its interaction with other domains like automatic differentiation.

When one is solving a large-scale scientific computing problem with MPI, this is almost always the piece of code where all of the time is spent, so let's understand how what it's doing.
** Newton's Method and Jacobians
:PROPERTIES:
:CUSTOM_ID: newtons-method-and-jacobians
:END:
Recall that the implicit Euler method is the following:

\[u_{n+1} = u_n + \Delta t f(u_{n+1},p,t + \Delta t)\]

If we wanted to use this method, we would need to find out how to get the value \(u_{n+1}\) when only knowing the value \(u_n\). To do so, we can move everything to one side:

\[u_{n+1} - \Delta t f(u_{n+1},p,t + \Delta t) - u_n = 0\]

and now we have a problem

\[g(u_{n+1}) = 0\]

This is the classic rootfinding problem \[g(x)=0\], find \(x\). The way that we solve the rootfinding problem is, once again, by replacing this problem about a continuous function \(g\) with a discrete dynamical system whose steady state is the solution to the \[g(x)=0\]. There are many methods for this, but some choices of the rootfinding method effect the stability of the ODE solver itself since we need to make sure that the steady state solution is a stable steady state of the iteration process, otherwise the rootfinding method will diverge (will be explored in the homework).

Thus for example, fixed point iteration is not appropriate for stiff differential equations. Methods which are used in the stiff case are either Anderson Acceleration or Newton's method. Newton's is by far the most common (and generally performs the best), so we can go down this route.

Let's use the syntax \[g(x)=0\]. Here we need some starting value \(x_0\) as our first guess for \(u_{n+1}\). The easiest guess is \(u_{n}\), though additional information about the equation can be used to compute a better starting value (known as a /step predictor/). Once we have a starting value, we run the iteration:

\[x_{k+1} = x_k - J(x_k)^{-1}g(x_k)\]

where \(J(x_k)\) is the Jacobian of \(g\) at the point \(x_k\). However, the mathematical formulation is never the syntax that you should use for the actual application! Instead, numerically this is two stages:

- Solve \(Ja=g(x_k)\) for \(a\)
- Update \(x_{k+1} = x_k - a\)

By doing this, we can turn the matrix inversion into a problem of a linear solve and then an update. The reason this is done is manyfold, but one major reason is because the inverse of a sparse matrix can be dense, and this Jacobian is in many cases (PDEs) a large and dense matrix.

Now let's break this down step by step.

*** Some Quick Notes
:PROPERTIES:
:CUSTOM_ID: some-quick-notes
:END:
The Jacobian of \(g\) can also be written as \(J = I - \gamma \frac{df}{du}\) for the ODE \(u' = f(u,p,t)\), where \(\gamma = \Delta t\) for the implicit Euler method. This general form holds for all other (SDIRK) implicit methods, changing the value of \(\gamma\). Additionally, the class of Rosenbrock methods solves a linear system with exactly the same \(J\), meaning that essentially all implicit and semi-implicit ODE solvers have to do the same Newton iteration process on the same structure. This is the portion of the code that is generally the bottleneck.

Additionally, if one is solving a mass matrix ODE: \(Mu' = f(u,p,t)\), exactly the same treatment can be had with \(J = M - \gamma \frac{df}{du}\). This works even if \(M\) is singular, a case known as a /differential-algebraic equation/ or a DAE. A DAE for example can be an ODE with constraint equations, and these structures can be represented as an ODE where these constraints lead to a singularity in the mass matrix (a row of all zeros is a term that is only the right hand side equals zero!).

** Generation of the Jacobian
:PROPERTIES:
:CUSTOM_ID: generation-of-the-jacobian
:END:
*** Dense Finite Differences and Forward-Mode AD
:PROPERTIES:
:CUSTOM_ID: dense-finite-differences-and-forward-mode-ad
:END:
Recall that the Jacobian is the matrix of \(\frac{df_i}{dx_j}\) for \(f\) a vector-valued function. The simplest way to generate the Jacobian is through finite differences. For each \(h_j = h e_j\) for \(e_j\) the basis vector of the \(j\)th axis and some sufficiently small \(h\), then we can compute column \(j\) of the Jacobian by:

\[\frac{f(x+h_j)-f(x)}{h}\]

Thus \(m+1\) applications of \(f\) are required to compute the full Jacobian.

This can be improved by using forward-mode automatic differentiation. Recall that we can formulate a multidimensional duel number of the form

\[d = x + v_1 \epsilon_1 + \ldots + v_m \epsilon_m\]

We can then seed the vectors \(v_j = h_j\) so that the differentiation directions are along the basis vectors, and then the output dual is the result:

\[f(d) = f(x) + J_1 \epsilon_1 + \ldots + J_m \epsilon_m\]

where \(J_j\) is the \(j\)th column of the Jacobian. And thus with one calculation of the /primal/ (f(x)) we have calculated the entire Jacobian.

*** Sparse Differentiation and Matrix Coloring
:PROPERTIES:
:CUSTOM_ID: sparse-differentiation-and-matrix-coloring
:END:
However, when the Jacobian is sparse we can compute it much faster. We can understand this by looking at the following system:

\[f(x)=\left[\begin{array}{c}
x_{1}+x_{3}\\
x_{2}x_{3}\\
x_{1}
\end{array}\right]\]

Notice that in 3 differencing steps we can calculate:

\[f(x+\epsilon e_{1})=\left[\begin{array}{c}
x_{1}+x_{3}+\epsilon\\
x_{2}x_{3}\\
x_{1}+\epsilon
\end{array}\right]\]

\[f(x+\epsilon e_{2})=\left[\begin{array}{c}
x_{1}+x_{3}\\
x_{2}x_{3}+\epsilon x_{3}\\
x_{1}
\end{array}\right]\]

\[f(x+\epsilon e_{3})=\left[\begin{array}{c}
x_{1}+x_{3}+\epsilon\\
x_{2}x_{3}+\epsilon x_{2}\\
x_{1}
\end{array}\right]\]

and thus:

\[\frac{f(x+\epsilon e_{1})-f(x)}{\epsilon}=\left[\begin{array}{c}
1\\
0\\
1
\end{array}\right]\]

\[\frac{f(x+\epsilon e_{2})-f(x)}{\epsilon}=\left[\begin{array}{c}
0\\
x_{3}\\
0
\end{array}\right]\]

\[\frac{f(x+\epsilon e_{3})-f(x)}{\epsilon}=\left[\begin{array}{c}
1\\
x_{2}\\
0
\end{array}\right]\]

But notice that the calculation of \(e_1\) and \(e_2\) do not interact. If we had done:

\[\frac{f(x+\epsilon e_{1}+\epsilon e_{2})-f(x)}{\epsilon}=\left[\begin{array}{c}
1\\
x_{3}\\
1
\end{array}\right]\]

we would still get the correct value for every row because the \(\epsilon\) terms do not collide (a situation known as /perturbation confusion/). If we knew the sparsity pattern of the Jacobian included a 0 at (2,1), (1,2), and (3,2), then we would know that the vectors would have to be \([1 0 1]\) and \([0 x_3 0]\), meaning that columns 1 and 2 can be computed simultaneously and decompressed. This is the key to sparse differentiation.

[[https://user-images.githubusercontent.com/1814174/66027457-efd7cc00-e4c8-11e9-8346-accf468541fb.PNG]]

With forward-mode automatic differentiation, recall that we calculate multiple dimensions simultaneously by using a multidimensional dual number seeded by the vectors of the differentiation directions, that is:

\[d = x + v_1 \epsilon_1 + \ldots + v_m \epsilon_m\]

Instead of using the primitive differentiation directions \(e_j\), we can instead replace this with the mixed values. For example, the Jacobian of the example function can be computed in one function call to \(f\) with the dual number input:

\[d = x + (e_1 + e_2) \epsilon_1 + e_3 \epsilon_2\]

and performing the decompression via the sparsity pattern. Thus the sparsity pattern gives a direct way to optimize the construction of the Jacobian.

This idea of independent directions can be formalized as a /matrix coloring/. Take \(S_{ij}\) the sparsity pattern of some Jacobian matrix \(J_{ij}\). Define a graph on the nodes 1 through m where there is an edge between \(i\) and \(j\) if there is a row where \(i\) and \(j\) are non-zero. This graph is the column connectivity graph of the Jacobian. What we wish to do is find the smallest set of differentiation directions such that differentiating in the direction of \(e_i\) does not collide with differentiation in the direction of \(e_j\). The connectivity graph is setup so that way this cannot be done if the two nodes are adjacent. If we let the subset of nodes differentiated together be a /color/, the question is, what is the smallest number of colors s.t. no adjacent nodes are the same color. This is the classic /distance-1 coloring problem/ from graph theory. It is well-known that the problem of finding the /chromatic number/, the minimal number of colors for a graph, is generally NP-complete. However, there are heuristic methods for performing a distance-1 coloring quite quickly. For example, a greedy algorithm is as follows:

- Pick a node at random to be color 1.
- Make all nodes adjacent to that be the lowest color that they can be (in this step that will be 2).
- Now look at all nodes adjacent to that. Make all nodes be the lowest color that they can be (either 1 or 3).
- Repeat by looking at the next set of adjacent nodes and color as conservatively as possible.

This can be visualized as follows:

[[https://user-images.githubusercontent.com/1814174/66027433-e189b000-e4c8-11e9-8c2e-3999954cda28.PNG]]

The result will color the entire connected component. While not giving an optimal result, it will still give a result that is a sufficient reduction in the number of differentiation directions (without solving an NP-complete problem) and thus can lead to a large computational saving.

At the end, let \(c_i\) be the vector of 1's and 0's, where it's 1 for every node that is color \(i\) and 0 otherwise. Sparse automatic differentiation of the Jacobian is then computed with:

\[d = x + c_1 \epsilon_1 + \ldots + c_k \epsilon_k\]

that is, the full Jacobian is computed with one dual number which consists of the primal calculation along with \(k\) dual dimensions, where \(k\) is the computed chromatic number of the connectivity graph on the Jacobian. Once this calculation is complete, the colored columns can be decompressed into the full Jacobian using the sparsity information, generating the original quantity that we wanted to compute.

For more information on the graph coloring aspects, find the paper titled "What Color Is Your Jacobian? Graph Coloring for Computing Derivatives" by Gebremedhin.

**** Note on Sparse Reverse-Mode AD
:PROPERTIES:
:CUSTOM_ID: note-on-sparse-reverse-mode-ad
:END:
Reverse-mode automatic differentiation can be though of as a method for computing one row of a Jacobian per seed, as opposed to one column per seed given by forward-mode AD. Thus sparse reverse-mode automatic differentiation can be done by looking at the connectivity graph of the column and using the resulting color vectors to seed the reverse accumulation process.

** Linear Solving
:PROPERTIES:
:CUSTOM_ID: linear-solving
:END:
After the Jacobian has been computed, we need to solve a linear equation \(Ja=b\). While mathematically you can solve this by computing the inverse \(J^{-1}\), this is not a good way to perform the calculation because even if \(J\) is sparse, then \(J^{-1}\) is in general dense and thus may not fit into memory (remember, this is \(N^2\) as many terms, where \(N\) is the size of the ordinary differential equation that is being solved, so if it's a large equation it is very feasible and common that the ODE is representable but its full Jacobian is not able to fit into RAM). Note that some may say that this is done for numerical stability reasons: that is incorrect. In fact, under reasonable assumptions for how the inverse is computed, it will be as numerically stable as other techniques we will mention.

Thus instead of generating the inverse, we can instead perform a /matrix factorization/. A matrix factorization is a transformation of the matrix into a form that is more amenable to certain analyses. For our purposes, a general Jacobian within a Newton iteration can be transformed via the /LU-factorization/ or (/LU-decomposition/), i.e.

\[J = LU\]

where \(L\) is lower triangular and \(U\) is upper triangular. If we write the linear equation in this form:

\[LUa = b\]

then we see that we can solve it by first solving \(L(Ua) = b\). Since \(L\) is lower triangular, this is done by the backsubstitution algorithm. That is, in a lower triangular form, we can solve for the first value since we have:

\[L_{11} a_1 = b_1\]

and thus by dividing we solve. For the next term, we have that

\[L_{21} a_1 + L_{22} a_2 = b_2\]

and thus we plug in the solution to \(a_1\) and solve to get \(a_2\). The lower triangular form allows this to continue. This occurs in 1+2+3+...+n operations, and is thus O(n^2). Next, we solve \(Ua = b\), which once again is done by a backsubstitution algorithm but in the reverse direction. Together those two operations are O(n^2) and complete the inversion of \(LU\).

So is this an O(n^2) algorithm for computing the solution of a linear system? No, because the computation of \(LU\) itself is an O(n^3) calculation, and thus the true complexity of solving a linear system is still O(n^3). However, if we have already factorized \(J\), then we can repeatedly use the same \(LU\) factors to solve additional linear problems \(Jv = u\) with different vectors. We can exploit this to accelerate the Newton method. Instead of doing the calculation:

\[x_{k+1} = x_k - J(x_k)^{-1}g(x_k)\]

we can instead do:

\[x_{k+1} = x_k - J(x_0)^{-1}g(x_k)\]

so that all of the Jacobians are the same. This means that a single O(n^3) factorization can be done, with multiple O(n^2) calculations using the same factorization. This is known as a Quasi-Newton method. While this makes the Newton method no longer quadratically convergent, it minimizes the large constant factor on the computational cost while retaining the same dynamical properties, i.e. the same steady state and thus the same overall solution. This makes sense for sufficiently large \(n\), but requires sufficiently large \(n\) because the loss of quadratic convergence means that it will take more steps to converge than before, and thus more \(O(n^2)\) backsolves are required, meaning that the difference between factorizations and backsolves needs to be large enough in order to offset the cost of extra steps.

**** Note on Sparse Factorization
:PROPERTIES:
:CUSTOM_ID: note-on-sparse-factorization
:END:
Note that LU-factorization, and other factorizations, have generalizations to sparse matrices where a /symbolic factorization/ is utilized to compute a sparse storage of the values which then allow for a fast backsubstitution. More details are outside the scope of this course, but note that Julia and MATLAB will both use the library SuiteSparse in the background when =lu= is called on a sparse matrix.

** Jacobian-Free Newton Krylov (JFNK)
:PROPERTIES:
:CUSTOM_ID: jacobian-free-newton-krylov-jfnk
:END:
An alternative method for solving the linear system is the Jacobian-Free Newton Krylov technique. This technique is broken into two pieces: the /jvp/ calculation and the Krylov subspace iterative linear solver.

*** Jacobian-Vector Products as Directional Derivatives
:PROPERTIES:
:CUSTOM_ID: jacobian-vector-products-as-directional-derivatives
:END:
We don't actually need to compute \(J\) itself, since all that we actually need is the =v = J*w=. Is it possible to compute the /Jacobian-Vector Product/, or the jvp, without producing the Jacobian?

To see how this is done let's take a look at what is actually calculated. Written out in the standard basis, we have that:

\[w_i = \sum_{j}^{m} J_{ij} v_{j}\]

Now write out what \(J\) means and we see that:

\[w_i = \sum_j^{m} \frac{df_i}{dx_j} v_j = \nabla f_i(x) \cdot v\]

that is, the \(i\)th component of \(Jv\) is the directional derivative of \(f_i\) in the direction \(v\). This means that in general, the jvp \(Jv\) is actually just the directional derivative in the direction of \(v\), that is:

\(Jv = \nabla f \cdot v\)

and therefore it has another mathematical representation, that is:

\[Jv = \lim_{\epsilon \rightarrow 0} \frac{f(x+v \epsilon) - f(x)}{\epsilon}\]

From this alternative form it is clear that *we can always compute a jvp with a single computation*. Using finite differences, a simple approximation is the following:

\[Jv \approx \frac{f(x+v \epsilon) - f(x)}{\epsilon}\]

for non-zero \(\epsilon\). Similarly, recall that in forward-mode automatic differentiation we can choose directions by seeding the dual part. Therefore, using the dual number with one partial component:

\[d = x + v \epsilon\]

we get that

\[f(d) = f(x) + Jv \epsilon\]

and thus a single application with a single partial gives the jvp.

**** Note on Reverse-Mode Automatic Differentiation
:PROPERTIES:
:CUSTOM_ID: note-on-reverse-mode-automatic-differentiation
:END:
As noted earlier, reverse-mode automatic differentiation has its primitives compute rows of the Jacobian in the seeded direction. This means that the seeded reverse-mode call with the vector \(v\) computes \(v^T J\), that is the /vector (transpose) Jacobian transpose/, or /vjp/ for short. When discussing parameter estimation and adjoints, this shorthand will be introduced as a way for using a traditionally machine learning tool to accelerate traditionally scientific computing tasks.

*** Krylov Subspace Methods For Solving Linear Systems
:PROPERTIES:
:CUSTOM_ID: krylov-subspace-methods-for-solving-linear-systems
:END:
**** Basic Iterative Solver Methods
:PROPERTIES:
:CUSTOM_ID: basic-iterative-solver-methods
:END:
Now that we have direct access to quick calculations of \(Jv\), how would we use this to solve the linear system \(Jw = v\) quickly? This is done through /iterative linear solvers/. These methods replace the process of solving for a factorization with, you may have guessed it, a discrete dynamical system whose solution is \(w\). To do this, what we want is some iterative process so that

\[Jw - b = 0\]

So now let's split \(J = A - B\), then if we are iterating the vectors \(w_k\) such that \(w_k \rightarrow w\), then if we plug this into the previous (residual) equation we get

\[A w_{k+1} = Bw_k + b\]

since when we plug in \(w\) we get zero (the sequence must be Cauchy so the difference \(w_{k+1} - w_k \rightarrow 0\)). Thus if we can split our matrix \(J\) into a component \(A\) which is easy to invert and a part \(B\) that is just everything else, then we would have a bunch of easy linear systems to solve. There are many different choices that we can do. If we let \(J = L + D + U\), where \(L\) is the lower portion of \(J\), \(D\) is the diagonal, and \(U\) is the upper portion, then the following are well-known methods:

- Richardson: \(A = \omega I\) for some \(\omega\)
- Jacobi: \(A = D\)
- Damped Jacobi: \(A = \omega D\)
- Gauss-Seidel: \(A = D-L\)
- Successive Over Relaxation: \(A = \omega D - L\)
- Symmetric Successive Over Relaxation: \(A = \frac{1}{\omega (2 - \omega)}(D-\omega L)D^{-1}(D-\omega U)\)

These decompositions are chosen since a diagonal matrix is easy to invert (it's just the inversion of the scalars of the diagonal) and it's easy to solve an upper or lower triangular linear system (once again, it's backsubstitution).

Since these methods give a a linear dynamical system, we know that there is a unique steady state solution, which happens to be \(Aw - Bw = Jw = b\). Thus we will converge to it as long as the steady state is stable. To see if it's stable, take the update equation

\[w_{k+1} = A^{-1}(Bw_k + b)\]

and check the eigenvalues of the system: if they are within the unit circle then you have stability. Notice that this can always occur by bringing the eigenvalues of \(A^{-1}\) closer to zero, which can be done by multiplying \(A\) by a significantly large value, hence the \(\omega\) quantities. While that always works, this essentially amounts to decreasing the stepsize of the iterative process and thus requiring more steps, thus making it take more computations. Thus the game is to pick the largest stepsize (\(\omega\)) for which the steady state is stable. We will leave that as outside the topic of this course.

**** Krylov Subspace Methods
:PROPERTIES:
:CUSTOM_ID: krylov-subspace-methods
:END:
While the classical iterative solver methods give the background for understanding an alternative to direct inversion or factorization of a matrix, the problem with that approach is that it requires the ability to split the matrix \(J\), which we would like to avoid computing. Instead, we would like to develop an iterative solver technique which instead just uses the solution to \(Jv\). Indeed there are such methods, and these are the Krylov subspace methods. A Krylov subspace is the space spanned by:

\[\mathcal{K}_k = \text{span} \{v,Jv,J^2 v, \ldots, J^k v\}\]

There are a few nice properties about Krylov subspaces that can be exploited. For one, it is known that there is a finite maximum dimension of the Krylov subspace, that is there is a value \(r\) such that \(J^{r+1} v \in \mathcal{K}_r\), which means that the complete Krylov subspace can be computed in finitely many jvp, since \(J^2 v\) is just the jvp where the vector is the jvp. Indeed, one can show that \(J^i v\) is linearly independent for each \(i\), and thus that maximal value is \(m\), the dimension of the Jacobian. Therefore in \(m\) jvps the solution is guaranteed to live in the Krylov subspace, giving a maximal computational cost and a proof of convergence if the vector in there is the "optimal in the space".

The most common method in the Krylov subspace family of methods is the GMRES method. Essentially, in step \(i\) one computes \(\mathcal{K}_i\), and finds the \(x\) that is the closest to the Krylov subspace, i.e. finds the \(x \in \mathcal{K}_i\) such that \(\Vert Jx-v \Vert\) is minimized. At each step, it adds the new vector to the Krylov subspace after orthgonalizing it against the other vectors via Arnoldi iterations, leading to an orthogonal basis of \(\mathcal{K}_i\) which makes it easy to express \(x\).

While one has a guaranteed bound on the number of possible jvps in GMRES which is simply the number of ODEs (since that is what determines the size of the Jacobian and thus the total dimension of the problem), that bound is not necessarily a good one. For a large sparse matrix, it may be computationally impractical to ever compute 100,000 jvps. Thus one does not typically run the algorithm to conclusion, and instead stops when \(\Vert Jx-v \Vert\) is sufficiently below some user-defined error tolerance.

** Intermediate Conclusion
:PROPERTIES:
:CUSTOM_ID: intermediate-conclusion
:END:
Let's take a step back and see what our intermediate conclusion is. In order to solve for the implicit step, it just boils down to doing Newton's method on some \(g(x)=0\). If the Jacobian is small enough, one factorizes the Jacobian and uses Quasi-Newton iterations in order to utilize the stored LU-decomposition in multiple steps to reduce the computation cost. If the Jacobian is sparse, sparse automatic differentiation through matrix coloring is employed to directly fill the sparse matrix with less applications of \(g\), and then this sparse matrix is factorized using a sparse LU factorization.

When the matrix is too large, then one resorts to using a Krylov subspace method, since this only requires being able to do \(Jv\) calculations. In general, \(Jv\) can be done matrix-free because it is simply the directional derivative in the direction of the vector \(v\), which can be computed through either numerical or forward-mode automatic differentiation. This is then used in the GMRES iterative process to find the solution in the Krylov subspace which is closest to the solution, exiting early when the residual error is small enough. If this is converging too slow, then preconditioning is used.

That's the basic algorithm, but what are the other important details for getting this right?

** The Need for Speed
:PROPERTIES:
:CUSTOM_ID: the-need-for-speed
:END:
*** Preconditioning
:PROPERTIES:
:CUSTOM_ID: preconditioning
:END:
However, the speed at GMRES convergences is dependent on the correlations between the vectors, which can be shown to be related to the condition number of the Jacobian matrix. A high condition number makes convergence slower (this is the case for the traditional iterative methods as well), which in turn is an issue because it is the high condition number on the Jacobian which leads to stiffness and causes one to have to use an implicit integrator in the first place!

To help speed up the convergence, a common technique is known as /preconditioning/. Preconditioning is the process of using a semi-inverse to the matrix in order to split the matrix so that the iterative problem that is being solved is one that has a smaller condition number. Mathematically, it involves decomposing \(J = P_l A P_r\) where \(P_l\) and \(P_r\) are the left and right preconditioners which have simple inverses, and thus instead of solving \(Jx=v\), we would solve:

\[P_l A P_r x = v\]

or

\[A P_r x = P_l^{-1}v\]

which then means that the Krylov subpace that needs to be solved for is that defined by \(A\): \(\mathcal{K} = \text{span}\{v,Av,A^2 v, \ldots\}\). There are many possible choices for these preconditioners, but they are usually problem dependent. For example, for ODEs which come from parabolic and elliptic PDE discretizations, the /multigrid method/, such as a geometric multigrid or an algebraic multigrid, is a preconditioner that can accelerate the iterative solving process. One generic preconditioner that can generally be used is to divide by the norm of the vector \(v\), which is a scaling employed by both SUNDIALS CVODE and by DifferentialEquations.jl and can be shown to be almost always advantageous.

*** Jacobian Re-use
:PROPERTIES:
:CUSTOM_ID: jacobian-re-use
:END:
If the problem is small enough such that the factorization is used and a Quasi-Newton technique is employed, it then holds that for most steps \(J\) is only approximate since it can be using an old LU-factorization. To push it even further, high performance codes allow for /jacobian reuse/, which is allowing the same Jacobian to be reused between different timesteps. If the Jacobian is too incorrect, it can cause the Newton iterations to diverge, which is then when one would calculate a new Jacobian and compute a new LU-factorization.

*** Adaptive Timestepping
:PROPERTIES:
:CUSTOM_ID: adaptive-timestepping
:END:
In simple cases, like partial differential equation discretizations of physical problems, the resulting ODEs are not too stiff and thus Newton's iteration generally works. However, in cases like stiff biological models, Newton's iteration can itself not always be stable enough to allow convergence. In fact, with many of the stiff biological models commonly used in benchmarks, no method is stable enough to pass without using adaptive timestepping! Thus one may need to adapt the timestep in order to improve the ability for the Newton method to converge (smaller timesteps increase the stability of the Newton stepping, see the homework).

This needs to be mixed with the Jacobian re-use strategy, since \(J = I - \gamma \frac{df}{du}\) where \(\gamma\) is dependent on \(\Delta t\) (and \(\gamma = \Delta t\) for implicit Euler) means that the Jacobian of the Newton method changes as \(\Delta t\) changes. Thus one usually has a tiered algorithm for determining when to update the factorizations of \(J\) vs when to compute a new \(\frac{df}{du}\) and then refactorize. This is generally dependent on estimates of convergence rates to heuristically guess how far off \(\frac{df}{du}\) is from the current true value.

So how does one perform adaptivity? This is generally done through a rejection sampling technique. First one needs some estimate of the error in a step. This is calculated through an /embedded method/, which is a method that is able to be calculated without any extra \(f\) evaluations that is (usually) one order different from the true method. The difference between the true and the embedded method is then an error estimate. If this is greater than a user chosen tolerance, the step is rejected and re-ran with a smaller \(\Delta t\) (possibly refactorizing, etc.). If this is less than the user tolerance, the step is accepted and \(\Delta t\) is changed.

There are many schemes for how one can change \(\Delta t\). One of the most common is known as the /P-control/, which stands for the proportional controller which is used throughout control theory. In this case, the control is to change \(\Delta t\) in proportion to the current error ratio from the desired tolerance. If we let

\[q = \frac{\text{E}}{\max(u_k,u_{k+1}) \tau_r + \tau_a}\]

where \(\tau_r\) is the relative tolerance and \(\tau_a\) is the absolute tolerance, then \(q\) is the ratio of the current error to the current tolerance. If \(q<1\), then the error is less than the tolerance and the step is accepted, and vice versa for \(q>1\). In either case, we let \(\Delta t_{new} = q \Delta t\) be the proportional update.

However, proportional error control has many known features that are undesirable. For example, it happens to work in a "bang bang" manner, meaning that it can drastically change its behavior from step to step. One step may multiply the step size by 10x, then the next by 2x. This is an issue because it effects the stability of the ODE solver method (since the stability is not a property of a single step, but rather it's a property of the global behavior over time)! Thus to smooth it out, one can use a /PI-control/, which modifies the control factor by a history value, i.e. the error in one step in the past. This of course also means that one can utilize a PID-controller for time stepping. And there are many other techniques that can be used, but many of the most optimized codes tend to use a PI-control mechanism.

** Methodological Summary
:PROPERTIES:
:CUSTOM_ID: methodological-summary
:END:
Here's a quick summary of the methodologies in a hierarchical sense:

- At the lowest level is the linear solve, either done by JFNK or (sparse) factorization. For large enough systems, this is the brunt of the work. This is thus the piece to computationally optimize as much as possible, and parallelize. For sparse factorizations, this can be done with a distributed sparse library implementation. For JFNK, the efficiency is simply due to the efficiency of your ODE function =f=.
- An optional level for JFNK is the preconditioning level, where preconditioners can be used to decrease the total number of iterations required for Krylov subspace methods like GMRES to converge, and thus reduce the total number of =f= calls.
- At the nonlinear solver level, different Newton-like techniques are utilized to minimize the number of factorizations/linear solves required, and maximize the stability of the Newton method.
- At the ODE solver level, more efficient integrators and adaptive methods for stiff ODEs are used to reduce the cost by affecting the linear solves. Most of these calculations are dominated by the linear solve portion when it's in the regime of large stiff systems. Jacobian reuse techniques, partial factorizations, and IMEX methods come into play as ways to reduce the cost per factorization and reduce the total number of factorizations.
* Basic Parameter Estimation, Reverse-Mode AD, and Inverse Problems
Have a model. Have data. Fit model to data.

This is a problem that goes under many different names: /parameter estimation/, /inverse problems/, /training/, etc. In this lecture we will go through the methods for how that's done, starting with the basics and bringing in the recent techniques from machine learning that can be used to improve the basic implementations.
** The Shooting Method for Parameter Fitting
:PROPERTIES:
:CUSTOM_ID: the-shooting-method-for-parameter-fitting
:END:
Assume that we have some model \(u = f(p)\) where \(p\) is our parameters, where we put in some parameters and receive our simulated data \(u\). How should you choose \(p\) such that \(u\) best fits that data? The /shooting method/ directly uses this high level definition of the model by putting a cost function on the output \(C(p)\). This cost function is dependent on a user-choice and it's model-dependent. However, a common one is the L2-loss. If \(y\) is our expected data, then the L2-loss function against the data is simply:

\[C(p) = \Vert f(p) - y \Vert\]

where \(C(p): \mathbb{R}^n \rightarrow \mathbb{R}\) is a function that returns a scalar. The shooting method then directly optimizes this cost function by having the optimizer generate a data given new choices of \(p\).

*** Methods for Optimization
:PROPERTIES:
:CUSTOM_ID: methods-for-optimization
:END:
There are many different nonlinear optimization methods which can be used for this purpose, and for a full survey one should look at packages like [[https://github.com/JuliaOpt/JuMP.jl][JuMP]], [[https://github.com/JuliaNLSolvers/Optim.jl][Optim.jl]], and [[https://github.com/JuliaOpt/NLopt.jl][NLopt.jl]].

There are generally two sets of methods: global and local optimization methods. Local optimization methods attempt to find the best nearby extrema by finding a point where the gradient \(\frac{dC}{dp} = 0\). Global optimization methods attempt to explore the whole space and find the best of the extrema. Global methods tend to employ a lot more heuristics and are extremely computationally difficult, and thus many studies focus on local optimization. We will focus strictly on local optimization, but one may want to look into global optimization for many applications of parameter estimation.

Most local optimizers make use of derivative information in order to accelerate the solver. The simplest of which is the method of /gradient descent/. In this method, given a set of parameters \(p_i\), the next step of parameters one will try is:

\[p_{i+1} = p_i - \alpha \frac{dC}{dP}\]

that is, update \(p_i\) by walking in the downward direction of the gradient. Instead of using just first order information, one may want to directly solve the rootfinding problem \(\frac{dC}{dp} = 0\) using Newton's method. Newton's method in this case looks like:

\[p_{i+1} = p_i - (\frac{d}{dp}\frac{dC}{dp})^{-1} \frac{dC}{dp}\]

But notice that the Jacobian of the gradient is the Hessian, and thus we can rewrite this as:

\[p_{i+1} = p_i - H(p_i)^{-1} \frac{dC(p_i)}{dp}\]

where \(H(p)\) is the Hessian matrix \(H_{ij} = \frac{dC}{dx_i dx_j}\). However, solving a system of equations which involves the Hessian can be difficult (just like the Jacobian, but now with another layer of differentiation!), and thus many optimization techniques attempt to avoid the Hessian. A commonly used technique that is somewhat in the middle is the /BFGS/ technique, which is a gradient-based optimization method that attempts to approximate the Hessian along the way to modify its stepping behavior. It uses the history of previously calculated points in order to build this quick Hessian approximate. If one keeps only a constant length history, say 5 time points, then one arrives at the /l-BFGS/ technique, which is one of the most common large-scale optimization techniques.

*** Connection Between Optimization and Differential Equations
:PROPERTIES:
:CUSTOM_ID: connection-between-optimization-and-differential-equations
:END:
There is actually a strong connection between optimization and differential equations. Let's say we wanted to follow the gradient of the solution towards a local minimum. That would mean that the flow that we would wish to follow is given by an ODE, specifically the ODE:

\[p' = -\frac{dC}{dp}\]

If we apply the Euler method to this ODE, then we receive

\[p_{n+1} = p_n - \alpha \frac{dC(p_n)}{dp}\]

and we thus recover the gradient descent method. Now assume that you want to use implicit Euler. Then we would have the system

\[p_{n+1} = p_n - \alpha \frac{dC(p_{n+1})}{dp}\]

which we would then move to one side:

\[p_{n+1} - p_n + \alpha \frac{dC(p_{n+1})}{dp} = 0\]

and solve each step via a Newton method. For this Newton method, we need to take the Jacobian of this gradient function, and once again the Hessian arrives as the fundamental quantity.

*** Neural Network Training as a Shooting Method for Functions
:PROPERTIES:
:CUSTOM_ID: neural-network-training-as-a-shooting-method-for-functions
:END:
A one layer dense neuron is traditionally written as the function:

\[layer(x) = \sigma.(Wx + b)\]

where \(x \in \mathbb{R}^n\), \(W \in \mathbb{R}^{m \times n}\), \(b \in \mathbb{R}^{m}\) and \(\sigma\) is some choice of \(\mathbb{R}\rightarrow\mathbb{R}\) nonlinear function, where the =.= is the Julia dot to signify element-wise operation.

A traditional /neural network/, /feed-forward network/, or /multi-layer perceptron/ is a 3 layer function, i.e.

\[NN(x) = W_3 \sigma_2.(W_2\sigma_1.(W_1x + b_1) + b_2) + b_3\]

where the first layer is called the input layer, the second is called the hidden layer, and the final is called the output layer. This specific function was seen as desirable because of the /Universal Approximation Theorem/, which is formally stated as follows:

Let \(\sigma\) be a nonconstant, bounded, and continuous function. Let \(I_m = [0,1]^m\). The space of real-valued continuous functions on \(I_m\) is denoted by \(C(I_m)\). For any \(\epsilon >0\) and any \(f\in C(I_m)\), there exists an integer \(N\), real constants \(W_i\) and \(b_i\) s.t.

\[\Vert NN(x) - f(x) \Vert < \epsilon\]

for all \(x \in I_m\). Equivalently, \(NN\) given parameters is dense in \(C(I_m)\).

However, it turns out that using only one hidden layer can require exponential growth in the size of said hidden layer, where the size is given by the number of columns in \(W_1\). To counteract this, /deep neural networks/ were developed to be in the form of the recurrence relation:

\[v_{i+1} = \sigma_i.(W_i v_{i} + b_i)\] \[v_1 = x\] \[DNN(x) = v_{n}\]

for some \(n\) where \(n\) is the number of /layers/. Given a sufficient size of the hidden layers, this kind of function is a universal approximator (2017). Although it's not quite known yet, some results have shown that this kind of function is able to fit high dimensional functions without the /curse of dimensionality/, i.e. the number of parameters does not grow exponentially with the input size. More mathematical results in this direction are still being investigated.

However, this theory gives a direct way to transform the fitting of an arbitrary function into a parameter shooting problem. Given an unknown function \(f\) one wishes to fit, one can place the cost function

\[C(p) = \Vert DNN(x;p) - f(x) \Vert\]

where \(DNN(x;p)\) signifies the deep neural network given by the parameters \(p\), where the full set of parameters is the \(W_i\) and \(b_i\). To make the evaluation of that function be practical, we can instead say we wish to evaluate the difference at finitely many points:

\[C(p) = \sum_k^N \Vert DNN(x_k;p) - f(x_k) \Vert\]

/Training/ a neural network is machine learning speak for finding the \(p\) which minimizes this cost function. Notice that this is then a shooting method problem, where a cost function is defined by direct evaluations of the model with some choice of parameters.

*** Recurrent Neural Networks
:PROPERTIES:
:CUSTOM_ID: recurrent-neural-networks
:END:
Recurrent neural networks are networks which are given by the recurrence relation:

\[x_{k+1} = x_k + DNN(x_k,k;p)\]

Given our machinery, we can see this is equivalent to the Euler discretization with \(\Delta t = 1\) on the /neural ordinary differential equation/ defined by:

\[x' = DNN(x,t;p)\]

Thus a recurrent neural network is a sequence of applications of a neural network (or possibly a neural network indexed by integer time).

** Computing Gradients
:PROPERTIES:
:CUSTOM_ID: computing-gradients
:END:
This shows that many different problems, from training neural networks to fitting differential equations, all have the same underlying mathematical structure which requires the ability to compute the gradient of a cost function given model evaluations. However, this simply reduces to computing the gradient of the model's output given the parameters. To see this, let's take for example the L2 loss function, i.e.

\[C(p) = \sum_i^N \Vert f(x_i;p) - y_i \Vert\]

for some finite data points \(y_i\). In the ODE model, \(y_i\) are time series points. In the general neural network, \(y_i = d(x_i)\) for the function we wish to fit \(d\). In data science applications of machine learning, \(y_i = d_i\) the discrete data points we wish to fit. In any of these cases, we see that by the chain rule we have

\[\frac{dC}{dp} = \sum_i^N 2 \left(f(x_i;p) - y_i \right) \frac{df(x_i)}{dp}\]

and therefore, knowing how to efficiently compute \[\frac{df(x_i)}{dp}\] is the essential question for shooting-based parameter fitting.

*** Forward-Mode Automatic Differentiation for Gradients
:PROPERTIES:
:CUSTOM_ID: forward-mode-automatic-differentiation-for-gradients
:END:
Let's recall the forward-mode method for computing gradients. For an arbitrary nonlinear function \(f\) with scalar output, we can compute derivatives by putting a dual number in. For example, with

\[d = d_0 + v_1 \epsilon_1 + \ldots + v_m \epsilon_m\]

we have that

\[f(d) = f(d_0) + f'(d_0)v_1 \epsilon_1 + \ldots + f'(d_0)v_m \epsilon_m\]

where \(f'(d_0)v_i\) is the direction derivative in the direction of \(v_i\). To compute the gradient with respond to the input, we thus need to make \(v_i = e_i\).

However, in this case we now do not want to compute the derivative with respect to the input! Instead, now we have \(f(x;p)\) and want to compute the derivatives with respect to \(p\). This simply means that we want to take derivatives in the directions of the parameters. To do this, let:

\[x = x_0 + 0 \epsilon_1 + \ldots + 0 \epsilon_k\] \[P = p + e_1 \epsilon_1 + \ldots + e_k \epsilon_k\]

where there are \(k\) parameters. We then have that

\[f(x;P) = f(x;p) + \frac{df}{dp_1} \epsilon_1 + \ldots + \frac{df}{dp_k} \epsilon_k\]

as the output, and thus a \(k+1\)-dimensional number computes the gradient of the function with respect to \(k\) parameters.

Can we do better?

*** The Adjoint Technique and Reverse Accumulation
:PROPERTIES:
:CUSTOM_ID: the-adjoint-technique-and-reverse-accumulation
:END:
The fast method for computing gradients goes under many times. The /adjoint technique/, /backpropagation/, and /reverse-mode automatic differentiation/ are in some sense all equivalent phrases given to this method from different disciplines. To understand the adjoint technique, we will look at the multivariate chain rule on a /computation graph/. Recall that for \(f(x(t),y(t))\) that we have:

\[\frac{df}{dt} = \frac{df}{dx}\frac{dx}{dt} + \frac{df}{dy}\frac{dy}{dt}\]

We can visualize our direct dependences as the computation graph:

[[https://user-images.githubusercontent.com/1814174/66461367-e3162380-ea46-11e9-8e80-09b32e138269.PNG]]

i.e. \(t\) directly determines \(x\) and \(y\) which then determines \(f\). To calculate Assume you've already evaluated \(f(t)\). If this has been done, then you've already had to calculate \(x\) and \(y\). Thus given the function \(f\), we can now calculate \(\frac{df}{dx}\) and \(\frac{df}{dy}\), and then calculate \(\frac{dx}{dt}\) and \(\frac{dy}{dt}\).

Now let's put another layer in the computation. Let's make \(f(x(v(t),w(t)),y(v(t),w(t))\). We can write out the full expression for the derivative. Notice that even with this additional layer, the statement we wrote above still holds:

\[\frac{df}{dt} = \frac{df}{dx}\frac{dx}{dt} + \frac{df}{dy}\frac{dy}{dt}\]

So given an evaluation of \(f\), we can (still) directly calculate \(\frac{df}{dx}\) and \(\frac{df}{dy}\). But now, to calculate \(\frac{dx}{dt}\) and \(\frac{dy}{dt}\), we do the next step of the chain rule:

\[\frac{dx}{dt} = \frac{dx}{dv}\frac{dv}{dt} + \frac{dx}{dw}\frac{dw}{dt}\]

and similar for \(y\). So plug it all in, and you see that our equations will grow wild if we actually try to plug it in! But it's clear that, to calculate \[\frac{df}{dt}\], we can first calculate \(\frac{df}{dx}\), and then multiply that to \(\frac{dx}{dt}\). If we had more layers, we could calculate the /sensitivity/ (the derivative) of the output to the last layer, then and then the sensitivity to the second layer back is the sensitivity of the last layer multiplied to that, and the third layer back has the sensitivity of the second layer multiplied to it!

*** Logistic Regression Example
:PROPERTIES:
:CUSTOM_ID: logistic-regression-example
:END:
To better see this structure, let's write out a simple example. Let our /forward pass/ through our function be:

\[\begin{align}
z &= wx + b\\
y &= \sigma(z)\\
\mathcal{L} &= \frac{1}{2}(y-t)^2\\
\mathcal{R} &= \frac{1}{2}w^2\\
\mathcal{L}_{reg} &= \mathcal{L} + \lambda \mathcal{R}\end{align}\]

[[https://user-images.githubusercontent.com/1814174/66462825-e2cb5780-ea49-11e9-9804-240037fb6b56.PNG]]

The formulation of the program here is called a /Wengert list, tape, or graph/. In this, \(x\) and \(t\) are inputs, \(b\) and \(W\) are parameters, \(z\), \(y\), \(\mathcal{L}\), and \(\mathcal{R}\) are intermediates, and \(\mathcal{L}_{reg}\) is our output.

This is a simple univariate logistic regression model. To do logistic regression, we wish to find the parameters \(w\) and \(b\) which minimize the distance of \(\mathcal{L}_{reg}\) from a desired output, which is done by computing derivatives.

Let's calculate the derivatives with respect to each quantity in reverse order. If our program is \(f(x) = \mathcal{L}_{reg}\), then we have that

\[\frac{df}{d\mathcal{L}_{reg}} = 1\]

as the derivatives of the last layer. To computerize our notation, let's write

\[\overline{\mathcal{L}_{reg}} = \frac{df}{d\mathcal{L}_{reg}}\]

for our computed values. For the derivatives of the second to last layer, we have that:

\[\begin{align}
  \overline{\mathcal{R}} &= \frac{df}{d\mathcal{L}_{reg}} \frac{d\mathcal{L}_{reg}}{d\mathcal{R}}\\
                         &= \overline{\mathcal{L}_{reg}} \lambda \end{align}\]

\[\begin{align}
 \overline{\mathcal{L}} &= \frac{df}{d\mathcal{L}_{reg}} \frac{d\mathcal{L}_{reg}}{d\mathcal{L}}\\
                        &= \overline{\mathcal{L}_{reg}} \end{align}\]

This was our observation from before that the derivative of the second layer is the partial derivative of the current values times the sensitivity of the final layer. And then we keep multiplying, so now for our next layer we have that:

\[\begin{align}
  \overline{y} &= \overline{\mathcal{L}} \frac{d\mathcal{L}}{dy}\\
               &= \overline{\mathcal{L}} (y-t) \end{align}\]

And notice that the chain rule holds since \(\overline{\mathcal{L}}\) implicitly already has the multiplication by \(\overline{\mathcal{L}_{reg}}\) inside of it. Then the next layer is:

\[\begin{align}
 \frac{df}{z} &= \overline{y} \frac{dy}{dz}\\
              &= \overline{y} \sigma^\prime(z) \end{align}\]

Then the next layer. Notice that here, by the chain rule on \(w\) we have that:

\[\begin{align}
  \overline{w} &= \overline{z} \frac{\partial z}{\partial w} + \overline{\mathcal{R}} \frac{d \mathcal{R}}{dw}\\
               &= \overline{z} x + \overline{\mathcal{R}} w\end{align}\]

\[\begin{align}
 \overline{b} &= \overline{z} \frac{\partial z}{\partial b}\\
              &= \overline{z} \end{align}\]

This completely calculates all derivatives. In conclusion, the rule is:

- You sum terms from each outward arrow
- Each arrow has the derivative term of the end times the partial of the current term.
- Recurse backwards to build simple linear combination expressions.

You can thus think of the relations as a message passing relation in reverse to the forward pass:

[[https://user-images.githubusercontent.com/1814174/66466679-1b226400-ea51-11e9-9e3c-5cc7939c243b.PNG]]

Note that the reverse-pass has the values of the forward pass, like \(x\) and \(t\), embedded within it.

*** Backpropagation of a Neural Network
:PROPERTIES:
:CUSTOM_ID: backpropagation-of-a-neural-network
:END:
Now let's look at backpropagation of a deep neural network. Before getting to it in the linear algebraic sense, let's write everything in terms of scalars. This means we can write a simple neural network as:

\[\begin{align}
  z_i &= \sum_j W_{ij}^1 x_j + b_i^1\\
  h_i &= \sigma(z_i)\\
  y_i &= \sum_j W_{ij}^2 h_j + b_i^2\\
  \mathcal{L} &= \frac{1}{2} \sum_k \left(y_k - t_k \right)^2 \end{align}\]

where I have chosen the L2 loss function. This is visualized by the computational graph:

[[https://user-images.githubusercontent.com/1814174/66464817-ad286d80-ea4d-11e9-9a4c-f7bcf1b34475.PNG]]

Then we can do the same process as before to get:

\[\begin{align}
  \overline{\mathcal{L}} &= 1\\
  \overline{y_i} &= \overline{\mathcal{L}} (y_i - t_i)\\
  \overline{w_{ij}^2} &= \overline{y_i} h_j\\
  \overline{b_i^2} &= \overline{y_i}\\
  \overline{h_i} &= \sum_k (\overline{y_k}w_{ki}^2)\\
  \overline{z_i} &= \overline{h_i}\sigma^\prime(z_i)\\
  \overline{w_{ij}^1} &= \overline{z_i} x_j\\
  \overline{b_i^1} &= \overline{z_i}\end{align}\]

just by examining the computation graph. Now let's write this in linear algebraic form.

[[https://user-images.githubusercontent.com/1814174/66465741-69366800-ea4f-11e9-9c20-07806214008b.PNG]]

The forward pass for this simple neural network was:

\[\begin{align}
  z &= W_1 x + b_1\\
  h &= \sigma(z)\\
  y &= W_2 h + b_2\\
  \mathcal{L} = \frac{1}{2} \Vert y-t \Vert^2 \end{align}\]

If we carefully decode our scalar expression, we see that we get the following:

\[\begin{align}
  \overline{\mathcal{L}} &= 1\\
  \overline{y} &= \overline{\mathcal{L}}(y-t)\\
  \overline{W_2} &= \overline{y}h^{T}\\
  \overline{b_2} &= \overline{y}\\
  \overline{h} &= W_2^T \overline{y}\\
  \overline{z} &= \overline{h} .* \sigma^\prime(z)\\
  \overline{W_1} &= \overline{z} x^T\\
  \overline{b_1} &= \overline{z} \end{align}\]

We can thus decode the rules as:

- Multiplying by the matrix going forwards means multiplying by the transpose going backwards. A term on the left stays on the left, and a term on the right stays on the right.
- Element-wise operations give element-wise multiplication

Notice that the summation is then easily encoded into this rule by the transpose operation.

We can write it in the general DNN form of:

\[r_i = W_i v_{i} + b_i\] \[v_{i+1} = \sigma_i.(r_i)\] \[v_1 = x\] \[\mathcal{L} = \frac{1}{2} \Vert v_{n} - t \Vert\]

\[\begin{align}
  \overline{\mathcal{L}} &= 1\\
  \overline{v_n} &= \overline{\mathcal{L}}(y-t)\\
  \overline{r_i} &= \overline{v_i} .* \sigma_i^\prime (r_i)\\
  \overline{W_i} &= \overline{v_i}r_{i-1}^{T}\\
  \overline{b_i} &= \overline{v_i}\\
  \overline{v_{i-1}} &= W_{i}^{T} \overline{v_i} \end{align}\]

*** Reverse-Mode Automatic Differentiation and vjps
:PROPERTIES:
:CUSTOM_ID: reverse-mode-automatic-differentiation-and-vjps
:END:
Backpropagation of a neural network is thus a different way of accumulating derivatives. If \(f\) is a composition of \(L\) functions:

\[f = f^L \circ f^{L-1} \circ \ldots \circ f^1\]

Then the Jacobian matrix satisfies:

\[J = J_L J_{L-1} \ldots J_1\]

A program is essentially a nice way of writing a function in composition form. Forward-mode automatic differentiation worked by propagating forward the actions of the Jacobians at every step of the program:

\[Jv = J_L (J_{L-1} (\ldots (J_1 v) \ldots ))\]

effectively calculating the Jacobian of the program by multiplying by the Jacobians from left to right at each step of the way. This means doing primitive \(Jv\) calculations on each underlying problem, and pushing that calculation through.

But what about reverse accumulation? This can be isolated to the simple expression graph:

[[https://user-images.githubusercontent.com/1814174/66471491-650f4800-ea59-11e9-9b42-4b32d0d0d76f.PNG]]

In backpropagation, we just showed that when doing reverse accumulation, the rule is that multiplication forwards is multiplication by the transpose backwards. So if the forward way to compute the Jacobian in reverse is to replace the matrix by its transpose:

[[https://user-images.githubusercontent.com/1814174/66471687-c6cfb200-ea59-11e9-9b80-f9206ffda87f.PNG]]

We can either look at it as \(J^T v\), or by transposing the equation \(v^T J\). It's right there that we have a vector-transpose Jacobian product, or a /vjp/.

We can thus think of this as a different direction for the Jacobian accumulation. Reverse-mode automatic differentiation moves backwards through our composed Jacobian. For a value \(v\) at the end, we can push it backwards:

\[v^T J = (\ldots ((v^T J_L) J_{L-1}) \ldots ) J_1\]

doing a vjp at every step of the way, which is simply doing reverse-mode AD of that function (and if it's linear, then simply doing the matrix multiplication). Thus reverse-mode AD is just a grouping of vjps into a single larger expression, instead of linearizing every single step.

*** Primitives of Reverse Mode
:PROPERTIES:
:CUSTOM_ID: primitives-of-reverse-mode
:END:
For forward-mode AD, we saw that we could define primitives in order to accelerate the calculation. For example, knowing that

\[exp(x+\epsilon) = exp(x) + exp(x)\epsilon\]

allows the program to skip autodifferentiating through the code for =exp=. This was simple with forward-mode since we could represent the operation on a Dual number. What's the equivalent for reverse-mode AD? The answer is the /pullback/ function. If \(y = [y_1,y_2,\ldots] = f(x_1,x_2, \ldots)\), then \([\overline{x_1},\overline{x_2},\ldots]=\mathcal{B}_f^x(\overline{y})\) is the pullback of \(f\) at the point \(x\), defined for a scalar loss function \(L(y)\) as:

\[\overline{x_i} = \frac{\partial L}{\partial x_i} = \sum_j \frac{\partial L}{\partial y_j} \frac{\partial y_j}{\partial x_i}\]

Using the notation from earlier, \(\overline{y} = \frac{\partial L}{\partial y}\) is the derivative of the some intermediate w.r.t. the cost function, and thus

\[\overline{x_i} = \sum_j \overline{y_j} \frac{\partial y_j}{\partial x_i} = \mathcal{B}_f^x(\overline{y})\]

Note that \(\mathcal{B}_f^x(\overline{y})\) is a function of \(x\) because the reverse pass that is use embeds values from the forward pass, and the values from the forward pass to use are those calculated during the evaluation of \(f(x)\).

By the chain rule, if we don't have a primitive defined for \(y_i(x)\), we can compute that by \(\mathcal{B}_{y_i}(\overline{y})\), and recursively apply this process until we hit rules that we know. The rules to start with are the scalar derivative rules with follow quite simply, and the multivariate rules which we derived above. For example, if \(y=f(x)=Ax\), then

\[\mathcal{B}_{f}^x(\overline{y}) = \overline{y}^T A\]

which is simply saying that the Jacobian of \(f\) at \(x\) is \(A\), and so the vjp is to multiply the vector transpose by \(A\).

Likewise, for element-wise operations, the Jacobian is diagonal, and thus the vjp is multiplying once again by a diagonal matrix against the derivative, deriving the same pullback as we had for backpropagation in a neural network. This then is a quicker encoding and derivation of backpropagation.

*** Multivariate Derivatives from Reverse Mode
:PROPERTIES:
:CUSTOM_ID: multivariate-derivatives-from-reverse-mode
:END:
Since the primitive of reverse mode is the vjp, we can understand its behavior by looking at a large primitive. In our simplest case, the function \(f(x)=Ax\) outputs a vector value, which we apply our loss function \(L(y) = \Vert y-t \Vert\) to get a scalar. Thus we seed the scalar output \(v=1\), and in the first step backwards we have a vector to scalar function, so the first pullback transforms from \(1\) to the vector \(v_2 = 2|y-t|\). Then we take that vector and multiply it like \(v_2^T A\) to get the derivatives w.r.t. \(x\).

Now let \(L(y)\) be a vector function, i.e. we output a vector instead of a scalar from our loss function. Then \(v\) is the /seed/ to this process. Let's assume that \(v = e_i\), one of the basis vectors. Then

\[v_i^T J = e_i^T J\]

pulls computes a row of the Jacobian. There, if we had a vector function \(y=f(x)\), the pullback \(\mathcal{B}_f^x(e_i)\) is the row of the Jacobian \(f'(x)\). Concatenating these is thus a way to build a full Jacobian. The gradient is thus a special case where \(y\) is scalar, and thus the resulting Jacobian is just a single row, and therefore we set the seed equal to \(1\) to compute the unscaled gradient.

*** Multi-Seeding
:PROPERTIES:
:CUSTOM_ID: multi-seeding
:END:
Similarly to forward-mode having a dual number with multiple simultanious derivatives through partials \(d = x + v_1 \epsilon_1 + \ldots + v_m \epsilon_m\), one can see that multi-seeding is an option in reverse-mode AD by, instead of pulling back a matrix instead of a row vector, where each row is a direction. Thus the matrix \(A = [v_1 v_2 \ldots v_n]^T\) evaluated as \(\mathcal{B}_f^x(A)\) is the equivalent operation to the forward-mode \(f(d)\) for generalized multivariate multiseeded reverse-mode automatic differentiation. One should take care to recognize the Jacobian as a generalized linear operator in this case and ensure that the shapes in the program correctly handle this storage of the reverse seed. When linear, this will automatically make use of BLAS3 operations, making it an efficient form for neural networks.

*** Sparse Reverse Mode AD
:PROPERTIES:
:CUSTOM_ID: sparse-reverse-mode-ad
:END:
Since the Jacobian is built row-by-row with reverse mode AD, the sparse differentiation discussion from forward-mode AD applies similarly but to the transpose. Therefore, in order to perform sparse reverse mode automatic differentiation, one would build up a connectivity graph of the columns, and perform a coloring algorithm on this graph. The seeds of the reverse call, \(v_i\), would then be the color vectors, which would compute compressed rows, that are then decompressed similarly to the forward-mode case.

*** Forward Mode vs Reverse Mode
:PROPERTIES:
:CUSTOM_ID: forward-mode-vs-reverse-mode
:END:
Notice that a pullback of a single scalar gives the gradient of a function, while the /pushforward/ using forward-mode of a dual gives a directional derivative. Forward mode computes columns of a Jacobian, while reverse mode computes gradients (rows of a Jacobian). Therefore, the relative efficiency of the two approaches is based on the size of the Jacobian. If \(f:\mathbb{R}^n \rightarrow \mathbb{R}^m\), then the Jacobian is of size \[m \times n\]. If \(m\) is much smaller than \(n\), then computing by each row will be faster, and thus use reverse mode. In the case of a gradient, \(m=1\) while \(n\) can be large, leading to this phenomena. Likewise, if \(n\) is much smaller than \(m\), then computing by each column will be faster. We will see shortly the reverse mode AD has a high overhead with respect to forward mode, and thus if the values are relatively equal (or \(n\) and \(m\) are small), forward mode is more efficient.

However, since optimization needs gradients, reverse-mode definitely has a place in the standard toolchain which is why backpropagation is so central to machine learning.

*** Side Note on Mixed Mode
:PROPERTIES:
:CUSTOM_ID: side-note-on-mixed-mode
:END:
Interestingly, one can find cases where mixing the forward and reverse mode results would give an asymptotically better result. For example, if a Jacobian was non-zero in only the first 3 rows and first 3 columns, then sparse forward mode would still require N partials and reverse mode would require M seeds. However, one forward mode call of 3 partials and one reverse mode call of 3 seeds would calculate all three rows and columns with \(\mathcal{O}(1)\) work, as opposed to \(\mathcal{O}(N)\) or \(\mathcal{O}(M)\). Exactly how to make use of this insight in an automated manner is an open research question.

*** Forward-Over-Reverse and Hessian-Free Products
:PROPERTIES:
:CUSTOM_ID: forward-over-reverse-and-hessian-free-products
:END:
Using this knowledge, we can also develop quick ways for computing the Hessian. Recall from earlier in the discussion that Hessians are the Jacobian of the gradient. So let's say for a scalar function \(f\) we want to compute the Hessian. To compute the gradient, we use the reverse-mode AD pullback \(\nabla f(x) = \mathcal{B}_f^x(1)\). Recall that the pullback is a function of \(x\) since that is the value at which the values from the forward pass are taken. Then since the Jacobian of the gradient vector is \(n \times n\) (as many terms in the gradient as there are inputs!), it holds that we want to use forward-mode AD for this Jacobian. Therefore, using the dual number \(x = x_0 + e_1 \epsilon_1 + \ldots + e_n \epsilon_n\) the reverse mode gradient function computes the full Hessian in one forward pass. What this amounts to is pushing forward the dual number forward sensitivities when building the pullback, and then when doing the pullback the dual portions, will be holding vectors for the columns of the Hessian.

Similarly, Hessian-vector products without computing the Hessian can be computed using the Jacobian-vector product trick on the function defined by the gradient. Here, \(Hv\) is equivalent to the dual part of

\(\nabla f(x+v\epsilon) = \mathcal{B}_f^{x+v\epsilon}(1)\)

This means that our Newton method for optimization:

\[p_{i+1} = p_i - H(p_i)^{-1} \frac{dC(p_i)}{dp}\]

can be treated similarly to that for the nonlinear solving problem, where the linear system can be solved using Hessian-free vector products to build a Krylov subspace, giving rise to the /Hessian-free Newton Krylov/ method for optimization.

*** References
:PROPERTIES:
:CUSTOM_ID: references
:END:
We thank [[https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec06.pdf][Roger Grosse's lecture notes]] for the amazing tikz graphs.
* Differentiable Programming and Neural Differential Equations
Our last discussion focused on how, at a high mathematical level, one could in theory build programs which compute gradients in a fast manner by looking at the computational graph and performing reverse-mode automatic differentiation. Within the context of parameter identification, we saw many advantages to this approach because it did not scale multiplicatively in the number of parameters, and thus it is an efficient way to calculate Jacobians of objects where there are less rows than columns (think of the gradient as 1 row).

More precisely, this is seen to be more about sparsity patterns, with reverse-mode as being more efficient if there are "enough" less row seeds required than column partials (with mixed mode approaches sometimes being much better). However, to make reverse-mode AD realistically usable inside of a programming language instead of a compute graph, we need to do three things:

1. We need to have a way of implementing reverse-mode AD on a language.
2. We need a systematic way to derive "adjoint" relationships (pullbacks).
3. We need to see if there are better ways to fit parameters to data, rather than performing reverse-mode AD through entire programs!

** Implementation of Reverse-Mode AD
:PROPERTIES:
:CUSTOM_ID: implementation-of-reverse-mode-ad
:END:
Forward-mode AD was implementable through operator overloading and dual number arithmetic. However, reverse-mode AD requires reversing a program through its computational structure, which is a much more difficult operation. This begs the question, how does one actually build a reverse-mode AD implementation?

*** Static Graph AD
:PROPERTIES:
:CUSTOM_ID: static-graph-ad
:END:
The most obvious solution is to use a static compute graph, since how we defined our differentiation structure was on a compute graph. Tensorflow is a modern example of this approach, where a user must define variables and operations in a graph language (that's embedded into Python, R, Julia, etc.), and then execution on the graph is easy to differentiate. This has the advantage of being a simplified and controlled form, which means that not only differentiation transformations are possible, but also things like automatic parallelization. However, many see directly writing a (static) computation graph as a barrier for practical use since it requires completely rewriting all existing programs to this structure.

*** Tracing-Based AD and Wengert Lists
:PROPERTIES:
:CUSTOM_ID: tracing-based-ad-and-wengert-lists
:END:
Recall that an alternative formulation of reverse-mode AD for composed functions

\[f = f^L \circ f^{L-1} \circ \ldots \circ f^1\]

is through pullbacks on the Jacobians:

\[v^T J = (\ldots ((v^T J_L) J_{L-1}) \ldots ) J_1\]

Therefore, if one can transform the program structure into a list of composed functions, then reverse-mode AD is the successive application of pullbacks going in the reverse direction:

\[\mathcal{B}_{f}^{x}(A)=\mathcal{B}_{f^{1}}^{x}\left(\ldots\left(\mathcal{\mathcal{B}}_{f^{L-1}}^{f^{L-2}(f^{L-3}(\ldots f^{1}(x)\ldots))}\left(\mathcal{B}_{f^{L}}^{f^{L-1}(f^{L-2}(\ldots f^{1}(x)\ldots))}(A)\right)\right)\ldots\right)\]

Recall that the pullback \(\mathcal{B}_f^x(\overline{y})\) requires knowing:

1. The operation being performed
2. The value \(x\) of the forward pass

The idea is to then build a /Wengert list/ that is from exactly the forward pass of a specific \(x\), also known as a /trace/, and thus giving rise to /tracing-based reverse-mode AD/. This is the basis of many reverse-mode implementations, such as Julia's Tracker.jl (Flux.jl's old AD), ReverseDiff.jl, PyTorch, Tensorflow Eager, Autograd, and Autograd.jl. It is widely adopted due to its simplicity in implementation.

**** Inspecting Tracker.jl
:PROPERTIES:
:CUSTOM_ID: inspecting-tracker.jl
:END:
Tracker.jl is a very simple implementation to inspect. The definition of its number and array types are as follows:

#+begin_src julia;eval=false
struct Call{F,As<:Tuple}
  func::F
  args::As
end

mutable struct Tracked{T}
  ref::UInt32
  f::Call
  isleaf::Bool
  grad::T
  Tracked{T}(f::Call) where T = new(0, f, false)
  Tracked{T}(f::Call, grad::T) where T = new(0, f, false, grad)
  Tracked{T}(f::Call{Nothing}, grad::T) where T = new(0, f, true, grad)
end

mutable struct TrackedReal{T<:Real} <: Real
  data::T
  tracker::Tracked{T}
end

struct TrackedArray{T,N,A<:AbstractArray{T,N}} <: AbstractArray{T,N}
  tracker::Tracked{A}
  data::A
  grad::A
  TrackedArray{T,N,A}(t::Tracked{A}, data::A) where {T,N,A} = new(t, data)
  TrackedArray{T,N,A}(t::Tracked{A}, data::A, grad::A) where {T,N,A} = new(t, data, grad)
end
#+end_src

As expected, it replaces every single number and array with a value that will store not just perform the operation, but also build up a list of operations along with the values at every stage. Then pullback rules are implemented for primitives via the =@grad= macro. For example, the pullback for the dot product is implemented as:

#+begin_src julia;eval=false
@grad dot(xs, ys) = dot(data(xs), data(ys)), Δ -> (Δ .* ys, Δ .* xs)
#+end_src

This is read as: the value going forward is computed by using the Julia =dot= function on the arrays, and the pullback embeds the backs of the forward pass and uses =Δ .* ys= as the derivative with respect to =x=, and =Δ .* xs= as the derivative with respect to =y=. This element-wise nature makes sense given the diagonal-ness of the Jacobian.

Note that this also allows utilizing intermediates of the forward pass within the reverse pass. This is seen in the definition of the pullback of =meanpool=:

#+begin_src julia;eval=false
@grad function meanpool(x, pdims::PoolDims; kw...)
  y = meanpool(data(x), pdims; kw...)
  y, Δ -> (nobacksies(:meanpool, NNlib.∇meanpool(data.((Δ, y, x))..., pdims; kw...)), nothing)
end
#+end_src

where the derivative makes use of not only =x=, but also =y= so that the =meanpool= does not need to be re-calculated.

Using this style, Tracker.jl moves forward, building up the value and closures for the backpass and then recursively pulls back the input =Δ= to receive the derivatve.

*** Source-to-Source AD
:PROPERTIES:
:CUSTOM_ID: source-to-source-ad
:END:
Given our previous discussions on performance, you should be horrified with how this approach handles scalar values. Each =TrackedReal= holds as =Tracked{T}= which holds a =Call=, not a =Call{F,As<:Tuple}=, and thus it's not strictly typed. Because it's not strictly typed, this implies that every single operation is going to cause heap allocations. If you measure this in PyTorch, TensorFlow Eager, Tracker, etc. you get around 500ns-2ms of overhead. This means that a 2ns =+= operation becomes... >500ns! Oh my!

This is not the only issue with tracing. Another issue is that the trace is value-dependent, meaning that every new value can build a new trace. Thus one cannot easily JIT compile a trace because it'll be different for every gradient calculation (you can compile it, but you better make sure the compile times are short!). Lastly, the Wengert list can be much larger than the code itself. For example, if you trace through a loop that is =for i in 1:100000=, then the trace will be huge, even if the function is relatively simple. This is directly demonstrated in the JAX "how it works" slide:

[[https://iaml.it/blog/jax-intro-english/images/lifecycle.png]]

To avoid these issues, another version of reverse-mode automatic differentiation is /source-to-source/ transformations. In order to do source code transformations, you need to know how to transform all language constructs via the reverse pass. This can be quite difficult (what is the "adjoint" of =lock=?), but when worked out this has a few benefits. First of all, you do not have to track values, meaning stack-allocated values can stay on the stack. Additionally, you can JIT compile one backpass because you have a single function used for all backpasses. Lastly, you don't need to unroll your loops! Instead, which each branch you'd need to insert some data structure to recall the values used from the forward pass (in order to invert in the right directions). However, that can be much more lightweight than a tracking pass.

This can be a difficult problem to do on a general programming language. In general it needs a strong programmatic representation to use as a compute graph. Google's engineers did an analysis [[https://github.com/tensorflow/swift/blob/master/docs/WhySwiftForTensorFlow.md][when choosing Swift for TensorFlow]] and narrowed it down to either Swift or Julia due to their internal graph structures. Thus, it should be no surprise that the modern source-to-source AD systems are Zygote.jl for Julia, and Swift for TensorFlow in Swift. Additionally, older AD systems, like Tampenade, ADIFOR, and TAF, all for Fortran, were source-to-source AD systems.

** Derivation of Reverse Mode Rules: Adjoints and Implicit Function Theorem
:PROPERTIES:
:CUSTOM_ID: derivation-of-reverse-mode-rules-adjoints-and-implicit-function-theorem
:END:
In order to require the least amount of work from our AD system, we need to be able to derive the adjoint rules at the highest level possible. Here are a few well-known cases to start understanding. These next examples are from [[https://math.mit.edu/~stevenj/18.336/adjoint.pdf][Steven Johnson's resource]].

*** Adjoint of Linear Solve
:PROPERTIES:
:CUSTOM_ID: adjoint-of-linear-solve
:END:
Let's say we have the function \(A(p)x=b(p)\), i.e. this is the function that is given by the linear solving process, and we want to calculate the gradients of a cost function \(g(x,p)\). To evaluate the gradient directly, we'd calculate:

\[\frac{dg}{dp} = g_p + g_x x_p\]

where \(x_p\) is the derivative of each value of \(x\) with respect to each parameter \(p\), and thus it's an \(M \times P\) matrix (a Jacobian). Since \(g\) is a small cost function, \(g_p\) and \(g_x\) are easy to compute, but \(x_p\) is given by:

\[x_{p_i} = A^{-1}(b_{p_i}-A_{p_i}x)\]

and so this is \(P\) \(M \times M\) linear solves, which is expensive! However, if we multiply by

\[\lambda^{T} = g_x A^{-1}\]

then we obtain

\[\frac{dg}{dp}\vert_{f=0} = g_p - \lambda^T f_p = g_p - \lambda^T (A_p x - b_p)\]

which is an alternative formulation of the derivative at the solution value. However, in this case there is no computational benefit to this reformulation.

*** Adjoint of Nonlinear Solve
:PROPERTIES:
:CUSTOM_ID: adjoint-of-nonlinear-solve
:END:
Now let's look at some \(f(x,p)=0\) nonlinear solving. Differentiating by \(p\) gives us:

\[f_x x_p + f_p = 0\]

and thus \(x_p = -f_x^{-1}f_p\). Therefore, using our cost function we write:

\[\frac{dg}{dp} = g_p + g_x x_p = g_p - g_x \left(f_x^{-1} f_p \right)\]

or

\[\frac{dg}{dp} = g_p - \left(g_x f_x^{-1} \right) f_p\]

Since \(g_x\) is \(1 \times M\), \(f_x^{-1}\) is \(M \times M\), and \(f_p\) is \(M \times P\), this grouping changes the problem gets rid of the size \(MP\) term.

As is normal with backpasses, we solve for \(x\) through the forward pass however we like, and then for the backpass solve for

\[f_x^T \lambda = g_x^T\]

to obtain

\[\frac{dg}{dp}\vert_{f=0} = g_p - \lambda^T f_p\]

which does the calculation without ever building the size \(M \times MP\) term.

*** Adjoint of Ordinary Differential Equations
:PROPERTIES:
:CUSTOM_ID: adjoint-of-ordinary-differential-equations
:END:
We with to solve for some cost function \(G(u,p)\) evaluated throughout the differential equation, i.e.:

\[G(u,p) = G(u(p)) = \int_{t_0}^T g(u(t,p))dt\]

To derive this adjoint, introduce the Lagrange multiplier \(\lambda\) to form:

\[I(p) = G(p) - \int_{t_0}^T \lambda^\ast (u^\prime - f(u,p,t))dt\]

Since \(u^\prime = f(u,p,t)\), this is the mathematician's trick of adding zero, so then we have that

\[\frac{dG}{dp} = \frac{dI}{dp} = \int_{t_0}^T (g_p + g_u s)dt - \int_{t_0}^T \lambda^\ast (s^\prime - f_u s - f_p)dt\]

for \(s\) being the sensitivity, \(s = \frac{du}{dp}\). After applying integration by parts to \(\lambda^\ast s^\prime\), we get that:

\[\int_{t_{0}}^{T}\lambda^{\ast}\left(s^{\prime}-f_{u}s-f_{p}\right)dt	=\int_{t_{0}}^{T}\lambda^{\ast}s^{\prime}dt-\int_{t_{0}}^{T}\lambda^{\ast}\left(f_{u}s-f_{p}\right)dt\] \[=|\lambda^{\ast}(t)s(t)|_{t_{0}}^{T}-\int_{t_{0}}^{T}\lambda^{\ast\prime}sdt-\int_{t_{0}}^{T}\lambda^{\ast}\left(f_{u}s-f_{p}\right)dt\]

To see where we ended up, let's re-arrange the full expression now:

\[\frac{dG}{dp}	=\int_{t_{0}}^{T}(g_{p}+g_{u}s)dt+|\lambda^{\ast}(t)s(t)|_{t_{0}}^{T}-\int_{t_{0}}^{T}\lambda^{\ast\prime}sdt-\int_{t_{0}}^{T}\lambda^{\ast}\left(f_{u}s-f_{p}\right)dt\] \[=\int_{t_{0}}^{T}(g_{p}+\lambda^{\ast}f_{p})dt+|\lambda^{\ast}(t)s(t)|_{t_{0}}^{T}-\int_{t_{0}}^{T}\left(\lambda^{\ast\prime}+\lambda^\ast f_{u}-g_{u}\right)sdt\]

That was just a re-arrangement. Now, let's require that

\[\lambda^\prime = -\frac{df}{du}^\ast \lambda + \left(\frac{dg}{du} \right)^\ast\] \[\lambda(T) = 0\]

This means that the boundary term of the integration by parts is zero, and also one of those integral terms are perfectly zero. Thus, if \(\lambda\) satisfies that equation, then we get:

\[\frac{dG}{dp} = \lambda^\ast(t_0)\frac{dG}{du}(t_0) + \int_{t_0}^T \left(g_p + \lambda^\ast f_p \right)dt\]

which gives us our adjoint derivative relation.

If \(G\) is discrete, then it can be represented via the Dirac delta:

\[G(u,p) = \int_{t_0}^T \sum_{i=1}^N \Vert d_i - u(t_i,p)\Vert^2 \delta(t_i - t)dt\]

in which case

\[g_u(t_i) = 2(d_i - u(t_i,p))\]

at the data points \((t_i,d_i)\). Therefore, the derivative of an ODE solution with respect to a cost function is given by solving for \(\lambda^\ast\) using an ODE for \(\lambda^T\) in reverse time, and then using that to calculate \(\frac{dG}{dp}\). Note that \(\frac{dG}{dp}\) can be calculated simultaneously by appending a single value to the reverse ODE, since we can simply define the new ODE term as \(g_p + \lambda^\ast f_p\), which would then calculate the integral on the fly (ODE integration is just... integration!).

*** Complexities of Implementing ODE Adjoints
:PROPERTIES:
:CUSTOM_ID: complexities-of-implementing-ode-adjoints
:END:
The image below explains the dilemma:

[[https://user-images.githubusercontent.com/1814174/66882662-4d741a00-ef99-11e9-9233-4d6804fec2ec.PNG]]

Essentially, the whole problem is that we need to solve the ODE

\[\lambda^\prime = -\frac{df}{du}^\ast \lambda - \left(\frac{dg}{du} \right)^\ast\] \[\lambda(T) = 0\]

in reverse, but \(\frac{df}{du}\) is defined by \(u(t)\) which is a value only computed in the forward pass (the forward pass is embedded within the backpass!). Thus we need to be able to retrieve the value of \(u(t)\) to get the Jacobian on-demand. There are three ways which this can be done:

1. If you solve the reverse ODE \(u^\prime = f(u,p,t)\) backwards in time, mathematically it'll give equivalent values. Computation-wise, this means that you can append \(u(t)\) to \(\lambda(t)\) (to \(\frac{dG}{dp}\)) to calculate all terms at the same time with a single reverse pass ODE. However, numerically this is unstable and thus not always recommended (ODEs are reversible, but ODE solver methods are not necessarily going to generate the same exact values or trajectories in reverse!)
2. If you solve the forward ODE and receive a continuous solution \(u(t)\), you can interpolate it to retrieve the values at any given the time reverse pass needs the \(\frac{df}{du}\) Jacobian. This is fast but memory-intensive.
3. Every time you need a value \(u(t)\) during the backpass, you re-solve the forward ODE to \(u(t)\). This is expensive! Thus one can instead use /checkpoints/, i.e. save at finitely many time points during the forward pass, and use those as starting points for the \(u(t)\) calculation.

Alternative strategies can be investigated, such as an interpolation which stores values in a compressed form.

*** The vjp and Neural Ordinary Differential Equations
:PROPERTIES:
:CUSTOM_ID: the-vjp-and-neural-ordinary-differential-equations
:END:
It is here that we can note that, if \(f\) is a function defined by a neural network, we arrive at the /neural ordinary differential equation/. This adjoint method is thus the backpropagation method for the neural ODE. However, the backpass

\[\lambda^\prime = -\frac{df}{du}^\ast \lambda - \left(\frac{dg}{du} \right)^\ast\] \[\lambda(T) = 0\]

can be improved by noticing \(\frac{df}{du}^\ast \lambda\) is a vjp, and thus it can be calculated using \(\mathcal{B}_f^{u(t)}(\lambda^\ast)\), i.e. reverse-mode AD on the function \(f\). If \(f\) is a neural network, this means that the reverse ODE is defined through successive backpropagation passes of that neural network. The result is a derivative with respect to the cost function of the parameters defining \(f\) (either a model or a neural network), which can then be used to fit the data ("train").

** Alternative "Training" Strategies
:PROPERTIES:
:CUSTOM_ID: alternative-training-strategies
:END:
Those are the "brute force" training methods which simply use \(u(t,p)\) evaluations to calculate the cost. However, it is worth noting that there are a few better strategies that one can employ in the case of dynamical models.

*** Multiple Shooting Techniques
:PROPERTIES:
:CUSTOM_ID: multiple-shooting-techniques
:END:
Instead of shooting just from the beginning, one can instead shoot from multiple points in time:

[[https://user-images.githubusercontent.com/1814174/66883548-561a1f80-ef9c-11e9-9ce1-0b6b55c950f9.PNG]]

Of course, one won't know what the "initial condition in the future" is, but one can instead make that a parameter. By doing so, each interval can be solved independently, and one can then add to the cost function that the end of one interval must match up with the beginning of the other. This can make the integration more robust, since shooting with incorrect parameters over long time spans can give massive gradients which makes it hard to hone in on the correct values.

*** Collocation Methods
:PROPERTIES:
:CUSTOM_ID: collocation-methods
:END:
If the data is dense enough, one can fit a curve through the points, such as a spline:

[[https://user-images.githubusercontent.com/1814174/66883762-fc662500-ef9c-11e9-91c7-c445e32d120f.PNG]]

If that's the case, one can use the fit spline in order to estimate the derivative at each point. Since the ODE is defined as \(u^\prime = f(u,p,t)\), one then then use the cost function

\[C(p) = \sum_{i=1}^N \Vert\tilde{u}^{\prime}(t_i) - f(u(t_i),p,t)\Vert\]

where \(\tilde{u}^{\prime}(t_i)\) is the estimated derivative at the time point \(t_i\). Then one can fit the parameters to ensure this holds. This method can be extremely fast since the ODE doesn't ever have to be solved! However, note that this is not able to compensate for error accumulation, and thus early errors are not accounted for in the later parts of the data. This means that the integration won't necessarily match the data even if this fit is "good" if the data points are too far apart, a property that is not true with fitting. Thus, this is usually done as part of a /two-stage method/, where the starting stage uses collocation to get initial parameters which is then completed with a shooting method.
* GPU Programming
Our last discussion focused on how, at a high mathematical level, one could in theory build programs which compute gradients in a fast manner by looking at the computational graph and performing reverse-mode automatic differentiation. Within the context of parameter identification, we saw many advantages to this approach because it did not scale multiplicatively in the number of parameters, and thus it is an efficient way to calculate Jacobians of objects where there are less rows than columns (think of the gradient as 1 row).

More precisely, this is seen to be more about sparsity patterns, with reverse-mode as being more efficient if there are "enough" less row seeds required than column partials (with mixed mode approaches sometimes being much better). However, to make reverse-mode AD realistically usable inside of a programming language instead of a compute graph, we need to do three things:

1. We need to have a way of implementing reverse-mode AD on a language.
2. We need a systematic way to derive "adjoint" relationships (pullbacks).
3. We need to see if there are better ways to fit parameters to data, rather than performing reverse-mode AD through entire programs!

** Implementation of Reverse-Mode AD
:PROPERTIES:
:CUSTOM_ID: implementation-of-reverse-mode-ad
:END:
Forward-mode AD was implementable through operator overloading and dual number arithmetic. However, reverse-mode AD requires reversing a program through its computational structure, which is a much more difficult operation. This begs the question, how does one actually build a reverse-mode AD implementation?

*** Static Graph AD
:PROPERTIES:
:CUSTOM_ID: static-graph-ad
:END:
The most obvious solution is to use a static compute graph, since how we defined our differentiation structure was on a compute graph. Tensorflow is a modern example of this approach, where a user must define variables and operations in a graph language (that's embedded into Python, R, Julia, etc.), and then execution on the graph is easy to differentiate. This has the advantage of being a simplified and controlled form, which means that not only differentiation transformations are possible, but also things like automatic parallelization. However, many see directly writing a (static) computation graph as a barrier for practical use since it requires completely rewriting all existing programs to this structure.

*** Tracing-Based AD and Wengert Lists
:PROPERTIES:
:CUSTOM_ID: tracing-based-ad-and-wengert-lists
:END:
Recall that an alternative formulation of reverse-mode AD for composed functions

\[f = f^L \circ f^{L-1} \circ \ldots \circ f^1\]

is through pullbacks on the Jacobians:

\[v^T J = (\ldots ((v^T J_L) J_{L-1}) \ldots ) J_1\]

Therefore, if one can transform the program structure into a list of composed functions, then reverse-mode AD is the successive application of pullbacks going in the reverse direction:

\[\mathcal{B}_{f}^{x}(A)=\mathcal{B}_{f^{1}}^{x}\left(\ldots\left(\mathcal{\mathcal{B}}_{f^{L-1}}^{f^{L-2}(f^{L-3}(\ldots f^{1}(x)\ldots))}\left(\mathcal{B}_{f^{L}}^{f^{L-1}(f^{L-2}(\ldots f^{1}(x)\ldots))}(A)\right)\right)\ldots\right)\]

Recall that the pullback \(\mathcal{B}_f^x(\overline{y})\) requires knowing:

1. The operation being performed
2. The value \(x\) of the forward pass

The idea is to then build a /Wengert list/ that is from exactly the forward pass of a specific \(x\), also known as a /trace/, and thus giving rise to /tracing-based reverse-mode AD/. This is the basis of many reverse-mode implementations, such as Julia's Tracker.jl (Flux.jl's old AD), ReverseDiff.jl, PyTorch, Tensorflow Eager, Autograd, and Autograd.jl. It is widely adopted due to its simplicity in implementation.

**** Inspecting Tracker.jl
:PROPERTIES:
:CUSTOM_ID: inspecting-tracker.jl
:END:
Tracker.jl is a very simple implementation to inspect. The definition of its number and array types are as follows:

#+begin_src julia;eval=false
struct Call{F,As<:Tuple}
  func::F
  args::As
end

mutable struct Tracked{T}
  ref::UInt32
  f::Call
  isleaf::Bool
  grad::T
  Tracked{T}(f::Call) where T = new(0, f, false)
  Tracked{T}(f::Call, grad::T) where T = new(0, f, false, grad)
  Tracked{T}(f::Call{Nothing}, grad::T) where T = new(0, f, true, grad)
end

mutable struct TrackedReal{T<:Real} <: Real
  data::T
  tracker::Tracked{T}
end

struct TrackedArray{T,N,A<:AbstractArray{T,N}} <: AbstractArray{T,N}
  tracker::Tracked{A}
  data::A
  grad::A
  TrackedArray{T,N,A}(t::Tracked{A}, data::A) where {T,N,A} = new(t, data)
  TrackedArray{T,N,A}(t::Tracked{A}, data::A, grad::A) where {T,N,A} = new(t, data, grad)
end
#+end_src

As expected, it replaces every single number and array with a value that will store not just perform the operation, but also build up a list of operations along with the values at every stage. Then pullback rules are implemented for primitives via the =@grad= macro. For example, the pullback for the dot product is implemented as:

#+begin_src julia;eval=false
@grad dot(xs, ys) = dot(data(xs), data(ys)), Δ -> (Δ .* ys, Δ .* xs)
#+end_src

This is read as: the value going forward is computed by using the Julia =dot= function on the arrays, and the pullback embeds the backs of the forward pass and uses =Δ .* ys= as the derivative with respect to =x=, and =Δ .* xs= as the derivative with respect to =y=. This element-wise nature makes sense given the diagonal-ness of the Jacobian.

Note that this also allows utilizing intermediates of the forward pass within the reverse pass. This is seen in the definition of the pullback of =meanpool=:

#+begin_src julia;eval=false
@grad function meanpool(x, pdims::PoolDims; kw...)
  y = meanpool(data(x), pdims; kw...)
  y, Δ -> (nobacksies(:meanpool, NNlib.∇meanpool(data.((Δ, y, x))..., pdims; kw...)), nothing)
end
#+end_src

where the derivative makes use of not only =x=, but also =y= so that the =meanpool= does not need to be re-calculated.

Using this style, Tracker.jl moves forward, building up the value and closures for the backpass and then recursively pulls back the input =Δ= to receive the derivatve.

*** Source-to-Source AD
:PROPERTIES:
:CUSTOM_ID: source-to-source-ad
:END:
Given our previous discussions on performance, you should be horrified with how this approach handles scalar values. Each =TrackedReal= holds as =Tracked{T}= which holds a =Call=, not a =Call{F,As<:Tuple}=, and thus it's not strictly typed. Because it's not strictly typed, this implies that every single operation is going to cause heap allocations. If you measure this in PyTorch, TensorFlow Eager, Tracker, etc. you get around 500ns-2ms of overhead. This means that a 2ns =+= operation becomes... >500ns! Oh my!

This is not the only issue with tracing. Another issue is that the trace is value-dependent, meaning that every new value can build a new trace. Thus one cannot easily JIT compile a trace because it'll be different for every gradient calculation (you can compile it, but you better make sure the compile times are short!). Lastly, the Wengert list can be much larger than the code itself. For example, if you trace through a loop that is =for i in 1:100000=, then the trace will be huge, even if the function is relatively simple. This is directly demonstrated in the JAX "how it works" slide:

[[https://iaml.it/blog/jax-intro-english/images/lifecycle.png]]

To avoid these issues, another version of reverse-mode automatic differentiation is /source-to-source/ transformations. In order to do source code transformations, you need to know how to transform all language constructs via the reverse pass. This can be quite difficult (what is the "adjoint" of =lock=?), but when worked out this has a few benefits. First of all, you do not have to track values, meaning stack-allocated values can stay on the stack. Additionally, you can JIT compile one backpass because you have a single function used for all backpasses. Lastly, you don't need to unroll your loops! Instead, which each branch you'd need to insert some data structure to recall the values used from the forward pass (in order to invert in the right directions). However, that can be much more lightweight than a tracking pass.

This can be a difficult problem to do on a general programming language. In general it needs a strong programmatic representation to use as a compute graph. Google's engineers did an analysis [[https://github.com/tensorflow/swift/blob/master/docs/WhySwiftForTensorFlow.md][when choosing Swift for TensorFlow]] and narrowed it down to either Swift or Julia due to their internal graph structures. Thus, it should be no surprise that the modern source-to-source AD systems are Zygote.jl for Julia, and Swift for TensorFlow in Swift. Additionally, older AD systems, like Tampenade, ADIFOR, and TAF, all for Fortran, were source-to-source AD systems.

** Derivation of Reverse Mode Rules: Adjoints and Implicit Function Theorem
:PROPERTIES:
:CUSTOM_ID: derivation-of-reverse-mode-rules-adjoints-and-implicit-function-theorem
:END:
In order to require the least amount of work from our AD system, we need to be able to derive the adjoint rules at the highest level possible. Here are a few well-known cases to start understanding. These next examples are from [[https://math.mit.edu/~stevenj/18.336/adjoint.pdf][Steven Johnson's resource]].

*** Adjoint of Linear Solve
:PROPERTIES:
:CUSTOM_ID: adjoint-of-linear-solve
:END:
Let's say we have the function \(A(p)x=b(p)\), i.e. this is the function that is given by the linear solving process, and we want to calculate the gradients of a cost function \(g(x,p)\). To evaluate the gradient directly, we'd calculate:

\[\frac{dg}{dp} = g_p + g_x x_p\]

where \(x_p\) is the derivative of each value of \(x\) with respect to each parameter \(p\), and thus it's an \(M \times P\) matrix (a Jacobian). Since \(g\) is a small cost function, \(g_p\) and \(g_x\) are easy to compute, but \(x_p\) is given by:

\[x_{p_i} = A^{-1}(b_{p_i}-A_{p_i}x)\]

and so this is \(P\) \(M \times M\) linear solves, which is expensive! However, if we multiply by

\[\lambda^{T} = g_x A^{-1}\]

then we obtain

\[\frac{dg}{dp}\vert_{f=0} = g_p - \lambda^T f_p = g_p - \lambda^T (A_p x - b_p)\]

which is an alternative formulation of the derivative at the solution value. However, in this case there is no computational benefit to this reformulation.

*** Adjoint of Nonlinear Solve
:PROPERTIES:
:CUSTOM_ID: adjoint-of-nonlinear-solve
:END:
Now let's look at some \(f(x,p)=0\) nonlinear solving. Differentiating by \(p\) gives us:

\[f_x x_p + f_p = 0\]

and thus \(x_p = -f_x^{-1}f_p\). Therefore, using our cost function we write:

\[\frac{dg}{dp} = g_p + g_x x_p = g_p - g_x \left(f_x^{-1} f_p \right)\]

or

\[\frac{dg}{dp} = g_p - \left(g_x f_x^{-1} \right) f_p\]

Since \(g_x\) is \(1 \times M\), \(f_x^{-1}\) is \(M \times M\), and \(f_p\) is \(M \times P\), this grouping changes the problem gets rid of the size \(MP\) term.

As is normal with backpasses, we solve for \(x\) through the forward pass however we like, and then for the backpass solve for

\[f_x^T \lambda = g_x^T\]

to obtain

\[\frac{dg}{dp}\vert_{f=0} = g_p - \lambda^T f_p\]

which does the calculation without ever building the size \(M \times MP\) term.

*** Adjoint of Ordinary Differential Equations
:PROPERTIES:
:CUSTOM_ID: adjoint-of-ordinary-differential-equations
:END:
We with to solve for some cost function \(G(u,p)\) evaluated throughout the differential equation, i.e.:

\[G(u,p) = G(u(p)) = \int_{t_0}^T g(u(t,p))dt\]

To derive this adjoint, introduce the Lagrange multiplier \(\lambda\) to form:

\[I(p) = G(p) - \int_{t_0}^T \lambda^\ast (u^\prime - f(u,p,t))dt\]

Since \(u^\prime = f(u,p,t)\), this is the mathematician's trick of adding zero, so then we have that

\[\frac{dG}{dp} = \frac{dI}{dp} = \int_{t_0}^T (g_p + g_u s)dt - \int_{t_0}^T \lambda^\ast (s^\prime - f_u s - f_p)dt\]

for \(s\) being the sensitivity, \(s = \frac{du}{dp}\). After applying integration by parts to \(\lambda^\ast s^\prime\), we get that:

\[\int_{t_{0}}^{T}\lambda^{\ast}\left(s^{\prime}-f_{u}s-f_{p}\right)dt	=\int_{t_{0}}^{T}\lambda^{\ast}s^{\prime}dt-\int_{t_{0}}^{T}\lambda^{\ast}\left(f_{u}s-f_{p}\right)dt\] \[=|\lambda^{\ast}(t)s(t)|_{t_{0}}^{T}-\int_{t_{0}}^{T}\lambda^{\ast\prime}sdt-\int_{t_{0}}^{T}\lambda^{\ast}\left(f_{u}s-f_{p}\right)dt\]

To see where we ended up, let's re-arrange the full expression now:

\[\frac{dG}{dp}	=\int_{t_{0}}^{T}(g_{p}+g_{u}s)dt+|\lambda^{\ast}(t)s(t)|_{t_{0}}^{T}-\int_{t_{0}}^{T}\lambda^{\ast\prime}sdt-\int_{t_{0}}^{T}\lambda^{\ast}\left(f_{u}s-f_{p}\right)dt\] \[=\int_{t_{0}}^{T}(g_{p}+\lambda^{\ast}f_{p})dt+|\lambda^{\ast}(t)s(t)|_{t_{0}}^{T}-\int_{t_{0}}^{T}\left(\lambda^{\ast\prime}+\lambda^\ast f_{u}-g_{u}\right)sdt\]

That was just a re-arrangement. Now, let's require that

\[\lambda^\prime = -\frac{df}{du}^\ast \lambda + \left(\frac{dg}{du} \right)^\ast\] \[\lambda(T) = 0\]

This means that the boundary term of the integration by parts is zero, and also one of those integral terms are perfectly zero. Thus, if \(\lambda\) satisfies that equation, then we get:

\[\frac{dG}{dp} = \lambda^\ast(t_0)\frac{dG}{du}(t_0) + \int_{t_0}^T \left(g_p + \lambda^\ast f_p \right)dt\]

which gives us our adjoint derivative relation.

If \(G\) is discrete, then it can be represented via the Dirac delta:

\[G(u,p) = \int_{t_0}^T \sum_{i=1}^N \Vert d_i - u(t_i,p)\Vert^2 \delta(t_i - t)dt\]

in which case

\[g_u(t_i) = 2(d_i - u(t_i,p))\]

at the data points \((t_i,d_i)\). Therefore, the derivative of an ODE solution with respect to a cost function is given by solving for \(\lambda^\ast\) using an ODE for \(\lambda^T\) in reverse time, and then using that to calculate \(\frac{dG}{dp}\). Note that \(\frac{dG}{dp}\) can be calculated simultaneously by appending a single value to the reverse ODE, since we can simply define the new ODE term as \(g_p + \lambda^\ast f_p\), which would then calculate the integral on the fly (ODE integration is just... integration!).

*** Complexities of Implementing ODE Adjoints
:PROPERTIES:
:CUSTOM_ID: complexities-of-implementing-ode-adjoints
:END:
The image below explains the dilemma:

[[https://user-images.githubusercontent.com/1814174/66882662-4d741a00-ef99-11e9-9233-4d6804fec2ec.PNG]]

Essentially, the whole problem is that we need to solve the ODE

\[\lambda^\prime = -\frac{df}{du}^\ast \lambda - \left(\frac{dg}{du} \right)^\ast\] \[\lambda(T) = 0\]

in reverse, but \(\frac{df}{du}\) is defined by \(u(t)\) which is a value only computed in the forward pass (the forward pass is embedded within the backpass!). Thus we need to be able to retrieve the value of \(u(t)\) to get the Jacobian on-demand. There are three ways which this can be done:

1. If you solve the reverse ODE \(u^\prime = f(u,p,t)\) backwards in time, mathematically it'll give equivalent values. Computation-wise, this means that you can append \(u(t)\) to \(\lambda(t)\) (to \(\frac{dG}{dp}\)) to calculate all terms at the same time with a single reverse pass ODE. However, numerically this is unstable and thus not always recommended (ODEs are reversible, but ODE solver methods are not necessarily going to generate the same exact values or trajectories in reverse!)
2. If you solve the forward ODE and receive a continuous solution \(u(t)\), you can interpolate it to retrieve the values at any given the time reverse pass needs the \(\frac{df}{du}\) Jacobian. This is fast but memory-intensive.
3. Every time you need a value \(u(t)\) during the backpass, you re-solve the forward ODE to \(u(t)\). This is expensive! Thus one can instead use /checkpoints/, i.e. save at finitely many time points during the forward pass, and use those as starting points for the \(u(t)\) calculation.

Alternative strategies can be investigated, such as an interpolation which stores values in a compressed form.

*** The vjp and Neural Ordinary Differential Equations
:PROPERTIES:
:CUSTOM_ID: the-vjp-and-neural-ordinary-differential-equations
:END:
It is here that we can note that, if \(f\) is a function defined by a neural network, we arrive at the /neural ordinary differential equation/. This adjoint method is thus the backpropagation method for the neural ODE. However, the backpass

\[\lambda^\prime = -\frac{df}{du}^\ast \lambda - \left(\frac{dg}{du} \right)^\ast\] \[\lambda(T) = 0\]

can be improved by noticing \(\frac{df}{du}^\ast \lambda\) is a vjp, and thus it can be calculated using \(\mathcal{B}_f^{u(t)}(\lambda^\ast)\), i.e. reverse-mode AD on the function \(f\). If \(f\) is a neural network, this means that the reverse ODE is defined through successive backpropagation passes of that neural network. The result is a derivative with respect to the cost function of the parameters defining \(f\) (either a model or a neural network), which can then be used to fit the data ("train").

** Alternative "Training" Strategies
:PROPERTIES:
:CUSTOM_ID: alternative-training-strategies
:END:
Those are the "brute force" training methods which simply use \(u(t,p)\) evaluations to calculate the cost. However, it is worth noting that there are a few better strategies that one can employ in the case of dynamical models.

*** Multiple Shooting Techniques
:PROPERTIES:
:CUSTOM_ID: multiple-shooting-techniques
:END:
Instead of shooting just from the beginning, one can instead shoot from multiple points in time:

[[https://user-images.githubusercontent.com/1814174/66883548-561a1f80-ef9c-11e9-9ce1-0b6b55c950f9.PNG]]

Of course, one won't know what the "initial condition in the future" is, but one can instead make that a parameter. By doing so, each interval can be solved independently, and one can then add to the cost function that the end of one interval must match up with the beginning of the other. This can make the integration more robust, since shooting with incorrect parameters over long time spans can give massive gradients which makes it hard to hone in on the correct values.

*** Collocation Methods
:PROPERTIES:
:CUSTOM_ID: collocation-methods
:END:
If the data is dense enough, one can fit a curve through the points, such as a spline:

[[https://user-images.githubusercontent.com/1814174/66883762-fc662500-ef9c-11e9-91c7-c445e32d120f.PNG]]

If that's the case, one can use the fit spline in order to estimate the derivative at each point. Since the ODE is defined as \(u^\prime = f(u,p,t)\), one then then use the cost function

\[C(p) = \sum_{i=1}^N \Vert\tilde{u}^{\prime}(t_i) - f(u(t_i),p,t)\Vert\]

where \(\tilde{u}^{\prime}(t_i)\) is the estimated derivative at the time point \(t_i\). Then one can fit the parameters to ensure this holds. This method can be extremely fast since the ODE doesn't ever have to be solved! However, note that this is not able to compensate for error accumulation, and thus early errors are not accounted for in the later parts of the data. This means that the integration won't necessarily match the data even if this fit is "good" if the data points are too far apart, a property that is not true with fitting. Thus, this is usually done as part of a /two-stage method/, where the starting stage uses collocation to get initial parameters which is then completed with a shooting method.
* PDEs, Convolutions, and the Mathematics of Locality
At this point we have identified how the worlds of machine learning and scientific computing collide by looking at the parameter estimation problem. Training neural networks is parameter estimation of a function =f= where =f= is a neural network. Backpropogation of a neural network is simply the adjoint problem for =f=, and it falls under the class of methods used in reverse-mode automatic differentiation. But this story also extends to structure. Recurrent neural networks are the Euler discretization of a continuous recurrent neural network, also known as a neural ordinary differential equation.

Given all of these relations, our next focus will be on the other class of commonly used neural networks: the /convolutional neural network/ (CNN). It turns out that in this case there is also a clear analogue to convolutional neural networks in traditional scientific computing, and this is seen in discretizations of partial differential equations. To see this, we will first describe the convolution operation that is central to the CNN and see how this object naturally arises in numerical partial differential equations.

** Convolutional Neural Networks
:PROPERTIES:
:CUSTOM_ID: convolutional-neural-networks
:END:
The purpose of a convolutional neural network is to be a network which makes use of the spatial structure of an image. An image is a 3-dimensional object: width, height, and 3 color channels. The convolutional operations keeps this structure intact and acts against this object is a 3-tensor. A convolutional layer is a function that applies a /stencil/ to each point. This is illustrated by the following animation:

#+caption: convolution
[[https://miro.medium.com/max/526/1*GcI7G-JLAQiEoCON7xFbhg.gif]]

This is the 2D stencil:

#+begin_example
1  0  1
0  1  0
1  0  1
#+end_example

which is then applied to the matrix at each inner point to go from an NxNx3 matrix to an (N-2)x(N-2)x3 matrix.

Another operation used with convolutions is the /pooling layer/. For example, the /maxpool/ layer is stencil which takes the maximum of the the value and its neighbor, and the /meanpool/ takes the mean over the nearby values, i.e. it is equivalent to the stencil:

#+begin_example
1/9 1/9 1/9
1/9 1/9 1/9
1/9 1/9 1/9
#+end_example

A convolutional neural network is then composed of layers of this form. We can express this mathematically by letting \(conv(x;S)\) as the convolution of \(x\) given a stencil \(S\). If we let \(dense(x;W,b,σ) = σ(W*x + b)\) as a layer from a standard neural network, then deep convolutional neural networks are of forms like:

\[CNN(x) = dense(conv(maxpool(conv(x))))\]

which can be expressed in Flux.jl syntax as:

#+begin_src julia;eval=false
m = Chain(
  Conv((2,2), 1=>16, relu),
  x -> maxpool(x, (2,2)),
  Conv((2,2), 16=>8, relu),
  x -> maxpool(x, (2,2)),
  x -> reshape(x, :, size(x, 4)),
  Dense(288, 10), softmax) |> gpu
#+end_src

** Discretizations of Partial Differential Equations
:PROPERTIES:
:CUSTOM_ID: discretizations-of-partial-differential-equations
:END:
Now let's investigate discertizations of partial differential equations. A canonical differential equation to start with is the Poisson equation. This is the equation:

\[u_{xx} = f(x)\]

where here we have that subscripts correspond to partial derivatives, i.e. this syntax stands for the partial differential equation:

\[\frac{d^2u}{dx^2} = f(x)\]

In this case, \(f\) is some given data and the goal is to find the \(u\) that satisfies this equation. There are two ways this is generally done:

1. Expand out the derivative in terms of Taylor series approximations.
2. Expand out \(u\) in terms of some function basis.

*** Finite Difference Discretizations
:PROPERTIES:
:CUSTOM_ID: finite-difference-discretizations
:END:
Let's start by looking at Taylor series approximations to the derivative. In this case, we will use what's known as finite differences. The simplest finite difference approximation is known as the first order forward difference. This is commonly denoted as

\[\delta_{+}u=\frac{u(x+\Delta x)-u(x)}{\Delta x}\]

This looks like a derivative, and we think it's a derivative as \(\Delta x\rightarrow 0\), but let's show that this approximation is meaningful. Assume that \(u\) is sufficiently nice. Then from a Taylor series we have that

\[u(x+\Delta x)=u(x)+\Delta xu^{\prime}(x)+\mathcal{O}(\Delta x^{2})\]

(here I write \(\left(\Delta x\right)^{2}\) as \(\Delta x^{2}\) out of convenience, note that those two terms are not necessarily the same). That term on the end is called "Big-O Notation". What is means is that those terms are asymptotically like \(\Delta x^{2}\). If \(\Delta x\) is small, then \(\Delta x^{2}\ll\Delta x\) and so we can think of those terms as smaller than any of the terms we show in the expansion. By simplification notice that we get

\[\frac{u(x+\Delta x)-u(x)}{\Delta x}=u^{\prime}(x)+\mathcal{O}(\Delta x)\] This means that \(\delta_{+}\) is correct up to first order, where the \(\mathcal{O}(\Delta x)\) portion that we dropped is the error. Thus \(\delta_{+}\) is a first order approximation.

Notice that the same proof shows that the backwards difference,

\[\delta_{-}u=\frac{u(x)-u(x-\Delta x)}{\Delta x}\]

is first order.

**** Second Order Approximations to the First Derivative
:PROPERTIES:
:CUSTOM_ID: second-order-approximations-to-the-first-derivative
:END:
Now let's look at the following:

\[\delta_{0}u=\frac{u(x+\Delta x)-u(x-\Delta x)}{2\Delta x}.\]

The claim is this differencing scheme is second order. To show this, we once again turn to Taylor Series. Let's do this for both terms:

\[u(x+\Delta x)	=u(x)+\Delta xu^{\prime}(x)+\frac{\Delta x^{2}}{2}u^{\prime\prime}(x)+\mathcal{O}(\Delta x^{3})\] \[u(x-\Delta x)	=u(x)-\Delta xu^{\prime}(x)+\frac{\Delta x^{2}}{2}u^{\prime\prime}(x)+\mathcal{O}(\Delta x^{3})\]

Now we subtract the two:

\[u(x+\Delta x)-u(x-\Delta x)=2\Delta xu^{\prime}(x)+\mathcal{O}(\Delta x^{3})\]

and thus we move tems around to get

\[\delta_{0}u=\frac{u(x+\Delta x)-u(x-\Delta x)}{2\Delta x}=u^{\prime}(x)+\mathcal{O}\left(\Delta x^{2}\right)\]

What does this improvement mean? Let's say we go from \(\Delta x\) to \(\frac{\Delta x}{2}\). Then while the error from the first order method is around \(\frac{1}{2}\) the original error, the error from the central differencing method is \(\frac{1}{4}\) the original error! When trying to get an accurate solution, this quadratic reduction can make quite a difference in the number of required points.

**** Second Derivative Central Difference
:PROPERTIES:
:CUSTOM_ID: second-derivative-central-difference
:END:
Now we want a second derivative approximation. Let's show the classic central difference formula for the second derivative:

\[\delta_{0}^{2}u=\frac{u(x+\Delta x)-2u(x)+u(x-\Delta x)}{\Delta x^{2}}\]

is second order. To do so, we expand out the two terms:

\[u(x+\Delta x)	=u(x)+\Delta xu^{\prime}(x)+\frac{\Delta x^{2}}{2}u^{\prime\prime}(x)+\frac{\Delta x^{3}}{6}u^{\prime\prime\prime}(x)+\mathcal{O}\left(\Delta x^{4}\right)\] \[u(x-\Delta x)	=u(x)-\Delta xu^{\prime}(x)+\frac{\Delta x^{2}}{2}u^{\prime\prime}(x)-\frac{\Delta x^{3}}{6}u^{\prime\prime\prime}(x)+\mathcal{O}\left(\Delta x^{4}\right)\]

and now plug it in. It's clear the \(u(x)\) cancels out. The opposite signs makes \(u^{\prime}(x)\) cancel out, and then the same signs and cancellation makes the \(u^{\prime\prime}\) term have a coefficient of 1. But, the opposite signs makes the \(u^{\prime\prime\prime}\) term cancel out. Thus when we simplify and divide by \(\Delta x^{2}\) we get

\[\frac{u(x+\Delta x)-2u(x)+u(x-\Delta x)}{\Delta x^{2}}=u^{\prime\prime}(x)+\mathcal{O}\left(\Delta x^{2}\right).\]

**** Finite Differencing from Polynomial Interpolation
:PROPERTIES:
:CUSTOM_ID: finite-differencing-from-polynomial-interpolation
:END:
Finite differencing can also be derived from polynomial interpolation. Draw a line between two points. What is the approximation for the first derivative?

\[\delta_{+}u=\frac{u(x+\Delta x)-u(x)}{\Delta x}\]

Now draw a quadratic through three points. i.e., given \(u_{1}\), \(u_{2}\), and \(u_{3}\) at \(x=0\), \(\Delta x\), \(2\Delta x\), we want to find the interpolating polynomial

\[g(x)=a_{1}x^{2}+a_{2}x+a_{3}\].

Setting \(g(0)=u_{1}\), \(g(\Delta x)=u_{2}\), and \(g(2\Delta x)=u_{3}\), we get the following relations:

\[u_{1}	=g(0)=a_{3}\] \[u_{2}	=g(\Delta x)=a_{1}\Delta x^{2}+a_{2}\Delta x+a_{3}\] \[u_{3}	=g(2\Delta x)=4a_{1}\Delta x^{2}+2a_{2}\Delta x+a_{3}\]

which when we write in matrix form is:

\[\left(\begin{array}{ccc}
0 & 0 & 1\\
\Delta x^{2} & \Delta x & 1\\
4\Delta x^{2} & 2\Delta x & 1
\end{array}\right)\left(\begin{array}{c}
a_{1}\\
a_{2}\\
a_{3}
\end{array}\right)=\left(\begin{array}{c}
u_{1}\\
u_{2}\\
u_{3}
\end{array}\right)\]

and thus we can invert the matrix to get the a's:

\[a_{1}	=\frac{u_{3}-2u_{2}+u_{1}}{2\Delta x^{2}}\] \[a_{2}	=\frac{-u_{3}+4u_{2}-3u_{1}}{2\Delta x}\] \[a_{3}	=u_{1}\text{ or }g(x)=\frac{u_{3}-2u_{2}-u_{1}}{2\Delta x^{2}}x^{2}+\frac{-u_{3}+4u_{2}-3u_{1}}{2\Delta x}x+u_{1}\]

Now we can get derivative approximations from this. Notice for example that

\[g^{\prime}(x)=\frac{u_{3}-2u_{2}+u_{1}}{\Delta x^{2}}x+\frac{-u_{3}+4u_{2}-3u_{1}}{2\Delta x}\]

Now what's the derivative at the middle point?

\[g^{\prime}\left(\Delta x\right)=\frac{u_{3}-2u_{2}+u_{1}}{\Delta x}+\frac{-u_{3}+4u_{2}-3u_{1}}{2\Delta x}=\frac{u_{3}-u_{1}}{2\Delta x}.\]

And now check

\[g^{\prime\prime}(\Delta x)=\frac{u_{3}-2u_{2}+u_{1}}{\Delta x^{2}}\] which is the central derivative formula. This gives a systematic way of deriving higher order finite differencing formulas. In fact, this formulation allows one to derive finite difference formulae for non-evenly spaced grids as well! The algorithm which automatically generates stencils from the interpolating polynomial forms is the Fornberg algorithm.

**** Multidimensional Finite Difference Operations
:PROPERTIES:
:CUSTOM_ID: multidimensional-finite-difference-operations
:END:
Now let's look at the multidimensional Poisson equation, commonly written as:

\[\Delta u = f(x,y)\]

where \(\Delta u = u_{xx} + u_{yy}\). Using the logic of the previous sections, we can approximate the two derivatives to have:

\[\frac{u(x+\Delta x,y)-2u(x,y)+u(x-\Delta x,y)}{\Delta x^{2}} + \frac{u(x,y+\Delta y)-2u(x,y)+u(x-x,y-\Delta y)}{\Delta y^{2}}=u^{\prime\prime}(x)+\mathcal{O}\left(\Delta x^{2}\right) + \mathcal{O}\left(\Delta y^{2}\right).\]

Notice that this is the stencil operation:

#+begin_example
0  1 0
1 -4 1
0  1 0
#+end_example

This means that *derivative discretizations are stencil or convolutional operations*.

** Representation and Implementation of Stencil Operations
:PROPERTIES:
:CUSTOM_ID: representation-and-implementation-of-stencil-operations
:END:
*** Stencil Operations as Sparse Matrices
:PROPERTIES:
:CUSTOM_ID: stencil-operations-as-sparse-matrices
:END:
Stencil operations are linear operators, i.e. \(S[x+\alpha y] = S[x] + \alpha S[y]\) for any sufficiently nice stencil operation \(S\) (note "sufficiently nice": there is a "stencil" operation mentioned in the convolutional neural networks section which was not linear: which operation was it?). Now we write these operators as matrices. Notice that for the vector:

\[U=\left(\begin{array}{c}
u_{1}\\
\vdots\\
u_{n}
\end{array}\right),\] we have that

\[\delta_{+}U=\left(\begin{array}{c}
u_{2}-u_{1}\\
\vdots\\
u_{n}-u_{n-1}
\end{array}\right)\]

and so

\[\delta_{+}=\left(\begin{array}{ccccc}
-1 & 1\\
 & -1 & 1\\
 &  & \ddots & \ddots\\
 &  &  & -1 & 1
\end{array}\right)\]

We can do the same to understand the other operators. But notice something: this operator isn't square! In order for this to be square, in order to know what happens at the endpoint, we need to know the boundary conditions. I.e., an assumption on the value or derivative at \(u(0)\) or \(u(1)\) is required in order to get the first/last rows of the matrix!

Similarly, \(\delta_{0}^{2}\) can be represented as the tridiagonal matrix of =[1 -2 1]=, also known as the Strang matrix.

Now let's think about the higher dimensional forms as a vector, i.e. =vec(u)=. In this case, what is the matrix =A= for which =reshape(A*vec(u),size(u)...)= performs the higher dimensional Laplacian, i.e. \(u_{xx} + u_{yy}\)? The answer is that it discretizes via Kronecker products to:

\[A=I_{y}\otimes A_{x}+A_{y}\otimes I_{x}\]

or:

\[\frac{\partial^{2}}{\partial x^{2}}=\left(\begin{array}{cccc}
A_{x}\\
 & A_{x}\\
 &  & \ddots\\
 &  &  & A_{x}
\end{array}\right)\]

and

\[\frac{\partial^{2}}{\partial y^{2}}=\left(\begin{array}{cccc}
-2I_{x} & I_{x}\\
I_{x} & -2I_{x} & I_{x}\\
 &  & \ddots\\
\\
\end{array}\right)\]

To see why this is the case, understand it again as the stencil operation

#+begin_example
0  1 0
1 -4 1
0  1 0
#+end_example

In this operation, at a point you still use the up and down neighbors, and thus this has a tridiagonal form since those are the immediate neighbors, but the next \(y\) value is \(N\) over, so this is where the block tridiagonal form comes for the stencil in the \(y\) terms. When these are added together one receives the appropriate matrix. The Kronecker product effectively encodes this "N over" behavior. It also readily generalizes to \(N\) dimensions. To see this for 3-dimensional Laplacians, \(u_{xx} + u_{yy} + u_{zz}\), notice that

\[A=I_z \otimes I_{y}\otimes A_{x} + I_z \otimes A_{y}\otimes I_{x} + A_z \otimes I_y \otimes I_x\]

using the same reasoning about "N" over and "N^2 over", and from this formulation it's clear how to generalize to arbitrary dimensions.

We note that there is an alternative representation as well for 2D forms. We can represent them with left and right matrix operations. When \(u\) is represented as a matrix, notice that

\[A(u) = A_y u + u A_x\]

where \(A_y\) and \(A_x\) are both the =[1 -2 1]= 1D tridiagonal stencil matrix, but by right multiplying it's occurring along the columns and left multiplying occurs along the rows. This then gives a semi-dense formulation of the stencil operation.

*** Implementation via Stencil Compilers
:PROPERTIES:
:CUSTOM_ID: implementation-via-stencil-compilers
:END:
Sparse matrix implementations of stencils are fairly inefficient given the way that sparse matrices are represented (lists of (i,j,v) pairs, which are then compressed into CSR or CSC formats). However, it moves in the right direction by noticing that the operation

#+begin_example
u[i+1,j] + u[i,j+1] + u[i-1,j] + u[i,j-1] - 4u[i,j]
#+end_example

is an inefficient way to walk through the data. The reason is because =u[i,j+1]= is using values that are far away from =u[i,j]=, and thus they may not necessarily be in the cache.

Thus what is generally used is a /stencil compiler/ which generates functions for stencil operations. These work by dividing the tensor into blocks on which the stencil is applied, where the blocks are small enough to allow the cache lines to fit the future points. This is a very deep computational topic that is beyond the scope of this course. Note one of the main reasons why NVIDA's CUDA dominates machine learning is because of its =cudnn= library, which is a very efficient GPU stencil computation library that is specifically tuned to NVIDIA's GPUs.

** Cross-Discipline Learning
:PROPERTIES:
:CUSTOM_ID: cross-discipline-learning
:END:
Given these relations, there is a lot each of the disciplines can learn from one another about stencil computations.

*** What ML can learning from SciComp: Stability of Convolutional RNNs
:PROPERTIES:
:CUSTOM_ID: what-ml-can-learning-from-scicomp-stability-of-convolutional-rnns
:END:
Stability of time-dependent partial differential equations is a long-known problem. Stability of an RNN defined by stencil computations is then stability of Euler discretizations of PDEs. Let's take a look at Von Neumann analysis of PDE stability.

Let's look at the error update equation. Write

\[e_{i}^{n}=u(x_{j},t_{n})-u_{j}^{n}\]

For \(e_{i}^{n}\), as before, plug it in, add and subtract \(u(x_{j},t^{n})=u_{j}^{n}\), and then we get

\[e_{i}^{n+1}=e_{i}^{n}+\mu\left(e_{i+1}^{n}-2e_{i}^{n}+e_{i-1}^{n}\right)+\Delta t\tau_{i}^{n}\]

where

\[\tau_{i}^{n}\sim\mathcal{O}(\Delta t)+\mathcal{O}(\Delta x^{2}).\]

Stability requires that the homogenous equation goes to zero. Another way of saying that is that the propagation of errors has errors decrease their influence over time. Thus we look at:

\[e_{i}^{n+1}	=e_{i}^{n}+\mu\left(e_{i+1}^{n}-2e_{i}^{n}+e_{i-1}^{n}\right) =\left(1-2\mu\right)e_{i}^{n}+\mu e_{i+1}^{n}+\mu e_{i-1}^{n}\]

A necessary condition for decreasing is then for all coefficients to be positive

\[1-2\mu\geq0\] or

\[\mu\leq\frac{1}{2}\]

A more satisfying way may be to look at the generated ODE

\[u^{\prime}=Au\]

where A is the matrix \(\left[\mu,1-2\mu,\mu\right].\)

But finding the maximum eigenvalue is non-trivial. But for linear PDEs, one nice way to analyze the stability directly is to use the Fourier mode decomposition. This is known as Van Neumann stability analysis. To do this, decompose \(U\) into the Fourier modes:

\[U(x,t)=\sum_{k}\hat{U}(t)e^{ikx}\]

Since

\[x_{j}=j\Delta x,\]

we can write this out as

\[U_{j}^{n}=\hat{U}^{n}e^{ikj\Delta x}\]

and then plugging this into the FTCS scheme we get

\[\frac{\hat{U}^{n+1}e^{ikj\Delta x}-\hat{U}^{n}e^{ikj\Delta x}}{\Delta t}=\frac{\hat{U}^{n}e^{ik(j+1)\Delta x}-2\hat{U}^{n}e^{ikj\Delta x}+\hat{U}^{n}e^{ik(j-1)\Delta x}}{\Delta x^{2}}\]

Let G be the growth factor, defined as

\[G=\frac{\hat{U}^{n+1}}{\hat{U}^{n}}\]

and thus after cancelling we get

\[\frac{G-1}{\Delta t}=\frac{e^{ik\Delta x}-2+e^{-ik\Delta x}}{\Delta x^{2}}\]

Since

\[e^{ik\Delta x}+e^{-ik\Delta x}=2\cos\left(k\Delta x\right),\]

then we get

\[G=1-\mu\left(2\cos\left(k\Delta x\right)-2\right)\]

and using the half angle formula

\[G=1-4\mu\sin^{2}\left(\frac{k\Delta x}{2}\right)\]

In order to be stable, we require

\[\left|G\right|\leq1,\]

which means

\[-1\leq1-4\mu\sin^{2}\left(\frac{k\Delta x}{2}\right)\leq1 \mu>0\]

and so \(\leq1\) is simple. Since \(\sin^{2}(x)\leq1\), then we can simplify this to

\[-1\leq1-4\mu\]

and thus \(\mu\leq\frac{1}{2}\). With backwards Euler we get

\[\frac{G-1}{\Delta t}=\frac{G}{\Delta x^{2}}\left(e^{ik\Delta x}-2+e^{-ik\Delta x}\right)\]

and thus get

\[G+4G\mu\sin^{2}\left(\frac{k\Delta x}{2}\right)=1\]

and thus

\[G=\frac{1}{1+4\mu\sin^{2}\left(\frac{k\Delta x}{2}\right)}\leq1.\]

*** What SciComp can learn from ML: Moderate Generalizations to Partial Differential Equation Models
:PROPERTIES:
:CUSTOM_ID: what-scicomp-can-learn-from-ml-moderate-generalizations-to-partial-differential-equation-models
:END:
Instead of using

\[\Delta u = f\]

we can start with

\[S[u] = f\]

a stencil computation, predefined to match a known partial differential equation operator, and then /transfer learn/ the stencil to better match data. This is an approach which is starting to move down the lines of /physics-informed machine learning/ that will be further explored in future lectures.
* Mixing Differential Equations and Neural Networks for Physics-Informed Learning
Given this background in both neural network and differential equation modeling, let's take a moment to survey some methods which integrate the two ideas. In this course we have fully described how Physics-Informed Neural Networks (PINNs) and neural ordinary differential equations are both trained and used. There are many other methods which utilize the composition of these ideas.

Julia codes for these methods are being developed, optimized, and tested in the [[file:sciml.ai][SciML]] organization. Some packages to note are

- [[https://github.com/SciML/NeuralPDE.jl][NeuralPDE.jl]]
- [[https://github.com/SciML/DiffEqFlux.jl][DiffEqFlux.jl]]
- [[https://github.com/SciML/DataDrivenDiffEq.jl][DataDrivenDiffEq.jl]]
- [[https://github.com/SciML/Surrogates.jl][Surrogates.jl]]
- [[https://github.com/SciML/ReservoirComputing.jl][ReservoirComputing.jl]]

and many more collaborations with scientists around the world (too many to note). And there are some scattered packages in other languages to note too, such as:

- [[https://github.com/lululxvi/deepxde][deepxde]]
- [[https://github.com/dynamicslab/pysindy][pysindy]]
- [[https://github.com/kailaix/ADCME.jl][ADCME.jl]]

and many more. This lecture is a quick survey on different directions that people have taken so far in this field. It is by no means comprehensive.

** The Augmented Neural Ordinary Differential Equation
:PROPERTIES:
:CUSTOM_ID: the-augmented-neural-ordinary-differential-equation
:END:
Note that not every function can be represented by an ordinary differential equation. Specifically, \(u(t)\) is an \(\mathbb{R} \rightarrow \mathbb{R}^n\) function which cannot loop over itself except when the solution is cyclic. The reason is because the flow of the ODE's solution is unique from every time point, and for it to have "two directions" at a point \(u_i\) in phase space would have two solutions to the problem

\[u' = f(u,p,t)\]

where \(u(0)=u_i\), and thus this cannot happen (with \(f\) sufficiently nice). However, if we have another degree of freedom we can ensure that the ODE does not overlap with itself. This is the [[https://arxiv.org/abs/1904.01681][augmented neural ordinary differential equation]].

We only need one degree of freedom in order to not collide, so we can do the following. We can add a fake state to the ODE which is zero at every single data point. This then allows this extra dimension to "bump around" as necessary to let the function be a universal approximator. In code this looks like:

#+begin_src julia;eval=false
dudt = Chain(...) # Flux neural network
p,re = Flux.destructure(dudt)
dudt_(u,p,t) = re(p)(u)
prob = ODEProblem(dudt_,[u0,0f0],tspan,p)
augmented_data = vcat(ode_data,zeros(1,size(ode_data,2)))
#+end_src

** Extensions to other Differential Equations
:PROPERTIES:
:CUSTOM_ID: extensions-to-other-differential-equations
:END:
While our previous lectures focused on ordinary differential equations, the larger classes of differential equations can also have neural networks, for example:

- [[https://en.wikipedia.org/wiki/Stochastic_differential_equation][stochastic differential equations]]
- [[https://en.wikipedia.org/wiki/Delay_differential_equation][delay differential equations]]
- [[https://en.wikipedia.org/wiki/Partial_differential_equation][partial differential equations]]
- [[https://en.wikipedia.org/wiki/Jump_diffusion][jump stochastic differential equations]]
- [[http://diffeq.sciml.ai/latest/features/callback_functions/][Hybrid differential equations]] (DEs with event handling)

For each of these equations, one can come up with an adjoint definition in order to define a backpropogation, or perform direct automatic differentiation of the solver code. One such paper in this area includes [[https://arxiv.org/abs/1905.09883][neural stochastic differential equations]]

*** The Universal Ordinary Differential Equation
:PROPERTIES:
:CUSTOM_ID: the-universal-ordinary-differential-equation
:END:
This formulation of the neural differential equation in terms of a "knowledge-embedded" structure is leading. If we already knew something about the differential equation, could we use that information in the differential equation definition itself? This leads us to the idea of the [[https://arxiv.org/abs/2001.04385][universal differential equation]], which is a differential equation that embeds universal approximators in its definition to allow for learning arbitrary functions as pieces of the differential equation.

The best way to describe this object is to code up an example. As our example, let's say that we have a two-state system and know that the second state is defined by a linear ODE. This means we want to write:

\[x' = NN(x,y)\] \[y' = p_1 x + p_2 y\]

We can code this up as follows:

#+begin_src julia;eval=false
u0 = Float32[0.8; 0.8]
tspan = (0.0f0,25.0f0)

ann = Chain(Dense(2,10,tanh), Dense(10,1))

p1,re = Flux.destructure(ann)
p2 = Float32[-2.0,1.1]
p3 = [p1;p2]
ps = Flux.params(p3)

function dudt_(du,u,p,t)
    x, y = u
    du[1] = re(p[1:41])(u)[1]
    du[2] = p[end-1]*y + p[end]*x
end
prob = ODEProblem(dudt_,u0,tspan,p3)
concrete_solve(prob,Tsit5(),u0,p3,abstol=1e-8,reltol=1e-6)
#+end_src

and we can train the system to be stable at 1 as follows:

#+begin_src julia;eval=false
function predict_adjoint()
  Array(concrete_solve(prob,Tsit5(),u0,p3,saveat=0.0:0.1:25.0))
end
loss_adjoint() = sum(abs2,x-1 for x in predict_adjoint())
loss_adjoint()

data = Iterators.repeated((), 300)
opt = ADAM(0.01)
iter = 0
cb = function ()
  global iter += 1
  if iter % 50 == 0
    display(loss_adjoint())
    display(plot(solve(remake(prob,p=p3,u0=u0),Tsit5(),saveat=0.1),ylim=(0,6)))
  end
end

# Display the ODE with the current parameter values.
cb()

Flux.train!(loss_adjoint, ps, data, opt, cb = cb)
#+end_src

DiffEqFlux.jl supports the wide gambit of possible universal differential equations with combinations of stiffness, delays, stochasticity, etc. It does so by using Julia's language-wide AD tooling, such as ReverseDiff.jl, Tracker.jl, ForwardDiff.jl, and Zygote.jl, along with specializations available whenever adjoint methods are known (and the choice between the two is given to the user).

Many of the methods below can be encapsulated as a choice of a universal differential equation and trained with higher order, adaptive, and more efficient methods with DiffEqFlux.jl.

** Deep BSDE Methods for High Dimensional Partial Differential Equations
:PROPERTIES:
:CUSTOM_ID: deep-bsde-methods-for-high-dimensional-partial-differential-equations
:END:
The key paper on deep BSDE methods is [[https://www.pnas.org/content/115/34/8505][this article from PNAS]] by Jiequn Han, Arnulf Jentzen, and Weinan E. Follow up papers [[https://arxiv.org/pdf/1804.07010.pdf][like this one]] have identified a larger context in the sense of forward-backwards SDEs for a large class of partial differential equations.

*** Understanding the Setup for Terminal PDEs
:PROPERTIES:
:CUSTOM_ID: understanding-the-setup-for-terminal-pdes
:END:
While this setup may seem a bit contrived given the "very specific" partial differential equation form (you know the end value? You have some parabolic form?), it turns out that there is a large class of problems in economics and finance that satisfy this form. The reason is because in these problems you may know the value of something at the end, when you're going to sell it, and you want to evaluate it right now. The classic example is in options pricing. An option is a contract to be able to solve a stock at a given value. The simplest case is a contract that can only be executed at a pre-determined time in the future. Let's say we have an option to sell a stock at 100 no matter what. This means that, if the stock at the strike time (the time the option can be sold) is 70, we will make 30 from this option, and thus the option itself is worth 30. The question is, if I have this option today, the strike time is 3 months in the future, and the stock price is currently 70, how much should I value the option *today*?

To solve this, we need to put a model on how we think the stock price will evolve. One simple version is a linear stochastic differential equation, i.e. the stock price will evolve with a constant interest rate \(r\) with some volatility (randomness) \(\sigma\), in which case:

\[dX_t = r X_t dt + \sigma X_t dW_t.\]

From this model, we can evaluate the probability that the stock is going to be at given values, which then gives us the probability that the option is worth a given value, which then gives us the expected (or average) value of the option. This is the Black-Scholes problem. However, a more direct way of calculating this result is writing down a partial differential equation for the evolution of the value of the option \(V\) as a function of time \(t\) and the current stock price \(x\). At the final time point, if we know the stock price then we know the value of the option, and thus we have a terminal condition \(V(T,x) = g(x)\) for some known value function \(g(x)\). The question is, given this value at time \(T\), what is the value of the option at time \(t=0\) given that the stock currently has a value \(x = \zeta\). Why is this interesting? This will tell you what you think the option is currently valued at, and thus if it's cheaper than that, you can gain money by buying the option right now! This means that the "solution" to the PDE is the value \(V(0,\zeta)\), where we know the final points \(V(T,x) = g(x)\). This is precisely the type of problem that is solved by the deep BSDE method.

*** The Deep BSDE Method
:PROPERTIES:
:CUSTOM_ID: the-deep-bsde-method
:END:
Consider the class of semilinear parabolic PDEs, in finite time \(t\in[0, T]\) and \(d\)-dimensional space \(x\in\mathbb R^d\), that have the form

\[\begin{align}
  \frac{\partial u}{\partial t}(t,x) 	&+\frac{1}{2}\text{trace}\left(\sigma\sigma^{T}(t,x)\left(\text{Hess}_{x}u\right)(t,x)\right)\\
	&+\nabla u(t,x)\cdot\mu(t,x) \\
	&+f\left(t,x,u(t,x),\sigma^{T}(t,x)\nabla u(t,x)\right)=0,\end{align}\]

with a terminal condition \(u(T,x)=g(x)\). In this equation, \(\text{trace}\) is the trace of a matrix, \(\sigma^T\) is the transpose of \(\sigma\), \(\nabla u\) is the gradient of \(u\), and \(\text{Hess}_x u\) is the Hessian of \(u\) with respect to \(x\). Furthermore, \(\mu\) is a vector-valued function, \(\sigma\) is a \(d \times d\) matrix-valued function and \(f\) is a nonlinear function. We assume that \(\mu\), \(\sigma\), and \(f\) are known. We wish to find the solution at initial time, \(t=0\), at some starting point, \(x = \zeta\).

Let \(W_{t}\) be a Brownian motion and take \(X_t\) to be the solution to the stochastic differential equation

\[dX_t = \mu(t,X_t) dt + \sigma (t,X_t) dW_t\]

with initial condition \(X(0)=\zeta\). Previous work has shown that the solution satisfies the following BSDE:

\[\begin{align}
u(t, &X_t) - u(0,\zeta) = \\
& -\int_0^t f(s,X_s,u(s,X_s),\sigma^T(s,X_s)\nabla u(s,X_s)) ds \\
& + \int_0^t \left[\nabla u(s,X_s) \right]^T \sigma (s,X_s) dW_s,\end{align}\]

with terminating condition \(g(X_T) = u(X_T,W_T)\).

At this point, the authors approximate \(\left[\nabla u(s,X_s) \right]^T \sigma (s,X_s)\) and \(u(0,\zeta)\) as neural networks. Using the Euler-Maruyama discretization of the stochastic differential equation system, one arrives at a recurrent neural network:

#+caption: Deep BSDE
[[https://user-images.githubusercontent.com/1814174/69241180-357d5080-0b6c-11ea-926d-6e27d0a1b26b.PNG]]

*** Julia Implementation
:PROPERTIES:
:CUSTOM_ID: julia-implementation
:END:
A Julia implementation for the deep BSDE method can be found at [[https://github.com/SciML/NeuralPDE.jl][NeuralPDE.jl]]. The examples considered below are part of the [[https://github.com/SciML/NeuralPDE.jl/blob/master/test/NNPDEHan_tests.jl][standard test suite]].

*** Financial Applications of Deep BSDEs: Nonlinear Black-Scholes
:PROPERTIES:
:CUSTOM_ID: financial-applications-of-deep-bsdes-nonlinear-black-scholes
:END:
Now let's look at a few applications which have PDEs that are solved by this method. One set of problems that are solved, given our setup, are Black-Scholes types of equations. Unlike a lot of previous literature, this works for a wide class of nonlinear extensions to Black-Scholes with large portfolios. Here, the dimension of the PDE for \(V(t,x)\) is the dimension of \(x\), where the dimension is the number of stocks in the portfolio that we want to consider. If we want to track 1000 stocks, this means our PDE is 1000 dimensional! Traditional PDE solvers would need around \(N^{1000}\) points evolving over time in order to arrive at the solution, which is completely impractical.

One example of a nonlinear Black-Scholes equation in this form is the Black-Scholes equation with default risk. Here we are adding to the standard model the idea that the companies that we are buying stocks for can default, and thus our valuation has to take into account this default probability as the option will thus become value-less. The PDE that is arrived at is:

\[\frac{\partial u}{\partial t}(t,x) + \bar{\mu}\cdot \nabla u(t, x) + \frac{\bar{\sigma}^{2}}{2} \sum_{i=1}^{d} \left |x_{i}  \right |^{2} \frac{\partial^2 u}{\partial {x_{i}}^2}(t,x) \\ - (1 -\delta )Q(u(t,x))u(t,x) - Ru(t,x) = 0\]

with terminating condition \(g(x) = \min_{i} x_i\) for \(x = (x_{1}, . . . , x_{100}) \in R^{100}\), where \(\delta \in [0, 1)\), \(R\) is the interest rate of the risk-free asset, and Q is a piecewise linear function of the current value with three regions \((v^{h} < v ^{l}, \gamma^{h} > \gamma^{l})\),

\[\begin{align}
Q(y) &= \mathbb{1}_{(-\infty,\upsilon^{h})}(y)\gamma ^{h}
+ \mathbb{1}_{[\upsilon^{l},\infty)}(y)\gamma ^{l}
\\ &+ \mathbb{1}_{[\upsilon^{h},\upsilon^{l}]}(y)
\left[ \frac{(\gamma ^{h} - \gamma ^{l})}{(\upsilon ^{h}- \upsilon ^{l})}
(y - \upsilon ^{h}) + \gamma ^{h}  \right  ].
\end{align}\]

This PDE can be cast into the form of the deep BSDE method by setting:

\[\begin{align}
    \mu &= \overline{\mu} X_{t} \\
    \sigma &= \overline{\sigma} \text{diag}(X_{t}) \\
    f &= -(1 -\delta )Q(u(t,x))u(t,x) - R u(t,x)
\end{align}\]

The Julia code for this exact problem in 100 dimensions can be found [[https://github.com/JuliaDiffEq/NeuralNetDiffEq.jl/blob/79225699412bee6590af0a365d6ae2393a1c1af8/test/NNPDEHan_tests.jl#L213-L270][here]]

*** Stochastic Optimal Control as a Deep BSDE Application
:PROPERTIES:
:CUSTOM_ID: stochastic-optimal-control-as-a-deep-bsde-application
:END:
Another type of problem that fits into this terminal PDE form is the /stochastic optimal control problem/. The problem is a generalized context to what motivated us before. In this case, there are a set of agents which undergo some known stochastic model. What we want to do is apply some control (push them in some direction) at every single timepoint towards some goal. For example, we have the physics for the dynamics of drone flight, but there's randomness in the wind condition, and so we want to control the engine speeds to move in a certain direction. However, there is a cost associated with controlling, and thus the question is how to best balance the use of controls with the natural stochastic evolution.

It turns out this is in the same form as the Black-Scholes problem. There is a model evolving forwards, and when we get to the end we know how much everything "cost" because we know if the drone got to the right location and how much energy it took. So in the same sense as Black-Scholes, we can know the value at the end and try and propagate it backwards given the current state of the system \(x\), to find out \(u(0,\zeta)\), i.e. how should we control right now given the current system is in the state \(x = \zeta\). It turns out that the solution of \(u(t,x)\) where \(u(T,x)=g(x)\) and we want to find \(u(0,\zeta)\) is given by a partial differential equation which is known as the Hamilton-Jacobi-Bellman equation, which is one of these terminal PDEs that is representable by the deep BSDE method.

Take the classical linear-quadratic Gaussian (LQG) control problem in 100 dimensions

\[dX_t = 2\sqrt{\lambda} c_t dt + \sqrt{2} dW_t\]

with \(t\in [0,T]\), \(X_0 = x\), and with a cost function

\[C(c_t) = \mathbb{E}\left[\int_0^T \Vert c_t \Vert^2 dt + g(X_t) \right]\]

where \(X_t\) is the state we wish to control, \(\lambda\) is the strength of the control, and \(c_t\) is the control process. To minimize the control, the Hamilton--Jacobi--Bellman equation:

\[\frac{\partial u}{\partial t}(t,x) + \Delta u(t,x) - \lambda \Vert \nabla u(t,x) \Vert^2 = 0\]

has a solution \(u(t,x)\) which at \(t=0\) represents the optimal cost of starting from \(x\).

This PDE can be rewritten into the canonical form of the deep BSDE method by setting:

\[\begin{align}
    \mu &= 0, \\
    \sigma &= \overline{\sigma} I, \\
    f &= -\alpha \left \| \sigma^T(s,X_s)\nabla u(s,X_s)) \right \|^{2},
\end{align}\]

where \(\overline{\sigma} = \sqrt{2}\), T = 1 and \(X_0 = (0,. . . , 0) \in R^{100}\).

The Julia code for solving this exact problem in 100 dimensions [[https://github.com/JuliaDiffEq/NeuralNetDiffEq.jl/blob/79225699412bee6590af0a365d6ae2393a1c1af8/test/NNPDEHan_tests.jl#L166-L211][can be found here]]

** Connections of Reservoir Computing to Scientific Machine Learning
:PROPERTIES:
:CUSTOM_ID: connections-of-reservoir-computing-to-scientific-machine-learning
:END:
Reservoir computing techniques are an alternative to the "full" neural network techniques we have previously discussed. However, the process of training neural networks has a few caveats which can cause difficulties in real systems:

1. The tangent space diverges exponentially fast when the system is chaotic, meaning that results of both forward and reverse automatic differentiation techniques (and the related adjoints) are divergent on these kinds of systems.
2. It is hard for neural networks to represent stiff systems. There are many reasons for this, one being that neural networks [[https://arxiv.org/abs/1806.08734][tend to drop high frequency behavior]].

There are ways being investigated to alleviate these issues. For example, [[https://www.sciencedirect.com/science/article/pii/S0021999117304783][shadow adjoints]] can give a non-divergent average sense of a derivative on ergodic chaotic systems, but is significantly more expensive than the traditional adjoint.

To get around these caveats, some research teams have investigated alternatives which do not require gradient-based optimization. The clear frontrunner in this field is a type of architecture called [[http://www.scholarpedia.org/article/Echo_state_network][echo state networks]]. A simplified formulation of an echo state network essentially fixes a neural network that defines a reservoir, i.e.

\[x_{n+1} = \sigma(W x_n + W_{fb} y_n)\] \[y_n = g(W_{out} x_n)\]

where \(W\) and \(W_{fb}\) are fixed random matrices that are chosen before the training process, \(x_n\) is called the reservoir state, and \(y_n\) is the output state for the observables. The idea is to find a projection \(W_{out}\) from the high dimensional random reservoir \(x\) to model the timeseries by \(y\). If the reservoir is a big enough and nonlinear enough random system, there should in theory exist a projection from that random system that matches any potential timeseries. Indeed, one can prove that echo state networks are universal adaptive filters under certain conditions.

If \(g\) is invertible (and in many cases \(g\) is taken to be the identity), then one can directly apply the inversion of \(g\) to the data. This turns the training of \(W_{out}\), the only non-fixed portion, into a standard least squares regression between the reservoir and the observation series. This is then solved by classical means like SVD factorizations which can be stable in ill-conditioned cases.

Echo state networks have been shown to [[https://arxiv.org/pdf/1906.08829.pdf][accurately reproduce chaotic attractors]] which are shown to be hard to train RNNs against. A demonstration via [[https://github.com/SciML/ReservoirComputing.jl][ReservoirComputing.jl]] clearly highlights this prediction ability:

[[https://user-images.githubusercontent.com/10376688/81470264-42f5c800-91ea-11ea-98a2-a8a8d7d96155.png]] [[https://user-images.githubusercontent.com/10376688/81470281-5a34b580-91ea-11ea-9eea-d2b266da19f4.png]]

However, this methodology still is not tailored to the continuous nature of dynamical systems found in scientific computing. Recent work has extended this methodolgy to allow for a continuous reservoir, i.e. a [[https://arxiv.org/abs/2010.04004][continuous-time echo state network]]. It is shown that using the adaptive points of a stiff ODE integrator gives a non-uniform sampling in time that makes it easier to learn stiff equations from less training points, and demonstrates the ability to learn equations where standard physics-informed neural network (PINN) training techniques fail.

[[https://user-images.githubusercontent.com/1814174/102009514-dc97d180-3d05-11eb-9542-bcd8d0f8b3a4.PNG]]

This area of research is still far less developed than PINNs and neural differential equations but shows promise to more easily learn highly stiff and chaotic systems which are seemingly out of reach for these other methods.

** Automated Equation Discovery: Outputting LaTeX for Dynamical Systems from Data
:PROPERTIES:
:CUSTOM_ID: automated-equation-discovery-outputting-latex-for-dynamical-systems-from-data
:END:
[[https://www.pnas.org/content/116/45/22445][The SINDy algorithm]] enables data-driven discovery of governing equations from data. It leverages the fact that most physical systems have only a few relevant terms that define the dynamics, making the governing equations sparse in a high-dimensional nonlinear function space. Given a set of observations

\[\begin{array}{c}
\mathbf{X}=\left[\begin{array}{c}
\mathbf{x}^{T}\left(t_{1}\right) \\
\mathbf{x}^{T}\left(t_{2}\right) \\
\vdots \\
\mathbf{x}^{T}\left(t_{m}\right)
\end{array}\right]=\left[\begin{array}{cccc}
x_{1}\left(t_{1}\right) & x_{2}\left(t_{1}\right) & \cdots & x_{n}\left(t_{1}\right) \\
x_{1}\left(t_{2}\right) & x_{2}\left(t_{2}\right) & \cdots & x_{n}\left(t_{2}\right) \\
\vdots & \vdots & \ddots & \vdots \\
x_{1}\left(t_{m}\right) & x_{2}\left(t_{m}\right) & \cdots & x_{n}\left(t_{m}\right)
\end{array}\right] \\
\end{array}\]

and a set of derivative observations

\[\begin{array}{c}
\dot{\mathbf{X}}=\left[\begin{array}{c}
\mathbf{x}^{T}\left(t_{1}\right) \\
\dot{\mathbf{x}}^{T}\left(t_{2}\right) \\
\vdots \\
\mathbf{x}^{T}\left(t_{m}\right)
\end{array}\right]=\left[\begin{array}{cccc}
\dot{x}_{1}\left(t_{1}\right) & \dot{x}_{2}\left(t_{1}\right) & \cdots & \dot{x}_{n}\left(t_{1}\right) \\
\dot{x}_{1}\left(t_{2}\right) & \dot{x}_{2}\left(t_{2}\right) & \cdots & \dot{x}_{n}\left(t_{2}\right) \\
\vdots & \vdots & \ddots & \vdots \\
\dot{x}_{1}\left(t_{m}\right) & \dot{x}_{2}\left(t_{m}\right) & \cdots & \dot{x}_{n}\left(t_{m}\right)
\end{array}\right]
\end{array}\]

we can evaluate the observations in a basis \(\Theta(X)\):

\[\Theta(\mathbf{X})=\left[\begin{array}{llllllll}
1 & \mathbf{X} & \mathbf{X}^{P_{2}} & \mathbf{X}^{P_{3}} & \cdots & \sin (\mathbf{X}) & \cos (\mathbf{X}) & \cdots
\end{array}\right]\]

where \(X^{P_i}\) stands for all \(P_i\)th order polynomial terms. For example,

\[\mathbf{X}^{P_{2}}=\left[\begin{array}{cccccc}
x_{1}^{2}\left(t_{1}\right) & x_{1}\left(t_{1}\right) x_{2}\left(t_{1}\right) & \cdots & x_{2}^{2}\left(t_{1}\right) & \cdots & x_{n}^{2}\left(t_{1}\right) \\
x_{1}^{2}\left(t_{2}\right) & x_{1}\left(t_{2}\right) x_{2}\left(t_{2}\right) & \cdots & x_{2}^{2}\left(t_{2}\right) & \cdots & x_{n}^{2}\left(t_{2}\right) \\
\vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
x_{1}^{2}\left(t_{m}\right) & x_{1}\left(t_{m}\right) x_{2}\left(t_{m}\right) & \cdots & x_{2}^{2}\left(t_{m}\right) & \cdots & x_{n}^{2}\left(t_{m}\right)
\end{array}\right]\]

Using these matrices, SINDy finds this sparse basis \(\mathbf{\Xi}\) over a given candidate library \(\mathbf{\Theta}\) by solving the sparse regression problem \(\dot{X} =\mathbf{\Theta}\mathbf{\Xi}\) with \(L_1\) regularization, i.e. minimizing the objective function \(\left\Vert \mathbf{\dot{X}} - \mathbf{\Theta}\mathbf{\Xi} \right\Vert_2 + \lambda \left\Vert \mathbf{\Xi}\right\Vert_1\). This method and other variants of SInDy, along with specialized optimizers for the LASSO \(L_1\) optimization problem, have been implemented in packages like [[https://github.com/SciML/DataDrivenDiffEq.jl][DataDrivenDiffEq.jl]] and [[https://github.com/dynamicslab/pysindy][pysindy]]. The result of these methods is LaTeX for the missing dynamical system.

Notice that to use this method, derivative data \(\dot{X}\) is required. While in most publications on the subject this information is assumed. To find this, \(\dot{X}\) is calculated directly from the time series \(X\) by fitting a cubic spline and taking the approximated derivatives at the observation points. However, for this estimation to be stable one needs a fairly dense timeseries for the interpolation. To alleviate this issue, the [[https://arxiv.org/abs/2001.04385][universal differential equations work]] estimates terms of partially described models and then uses the neural network as an oracle for the derivative values to learn from subsets of the dynamical system. This allows for the neural network's training to smooth out the derivative estimate between points while incorporating extra scientific information.

Other ways are being investigated for incorporating deep learning into the model discovery process. For example, extensions have been investigated where [[https://www.nature.com/articles/s41467-018-07210-0][elements are defined by neural networks representing a basis of the Koopman operator]]. Additionally, much work is going on in improving the efficiency of the symbolic regression methods themselves, and making the methods [[https://royalsocietypublishing.org/doi/full/10.1098/rspa.2020.0279][implicit and parallel]].

** Surrogate Acceleration Methods
:PROPERTIES:
:CUSTOM_ID: surrogate-acceleration-methods
:END:
Another approach for mixing neural networks with differential equations is as a surrogate method. These methods are more mathematically trivial than the previous ideas, but can still achieve interesting results. A full example is explained [[https://youtu.be/FGfx8CQHdQA?t=925][in this video]].

Say we have some function \(g(p)\) which depends on a solution to a differential equation \(u(t;p)\) and choices of parameters \(p\). Computationally how we evaluate this function is we do the following:

- Solve the differential equation with parameters \(p\)
- Evaluate \(g\) on the numerical solution for \(u\)

However, this process is computationally expensive since it requires the numerical solution of \(u\) for every evaluation. Thus, one can look at this setup and see \(g(p)\) itself is a nonlinear function. The idea is to train a neural network to be the function \(g(p)\), i.e. directly put in \(p\) and return the appropriate value without ever solving the differential equation.

The video highlights an important fact about this method: it can be computationally expensive to train this kind of surrogate since many data points \((p,g(p))\) are required. In fact, many more data points than you might use. However, after training, the surrogate network for \(g(p)\) can be a lot faster than the original simulation-based approach. This means that this is a method for accelerating real-time solutions by doing upfront computations. The total compute time will always be more, but in some sense the cost is amortized or shifted to be done before hand, so that the model does not need to be simulated on the fly. This can allow for things like computationally expensive models of drone flight to be used in a real-time controller.

This technique goes a long way back, but some recent examples of this have been shown. For example, there's [[https://arxiv.org/abs/1910.07291][this paper which "accelerated" the solution of the 3-body problem]] using a neural network surrogate trained over a few days to get a 1 million times acceleration (after generating many points beforehand of course! In the paper, notice that it took 10 days to generate the training dataset). Additionally, there is this [[https://fluxml.ai/2019/03/05/dp-vs-rl.html][deep learning trebuchet example]] which showcased that inverse problems, i.e. control or finding parameters, can be completely encapsulated as a \(g(p)\) and learned with sufficient data.
* From Optimization to Probabilistic Programming
With a high degree of probability, all things are probabilistic. In all of the cases we have previously looked at (differential equations, neural networks, neural differential equations, physics-informed neural networks, etc.) we have incorporated data into our models using point estimates, i.e. getting "exact fits". However, data has noise and uncertainty. We want to extend our previous modeling approaches to include probabilistic estimates. This is known as /probabilistic programming/, or Bayesian estimation on general programming models. To approach this topic, we will first introduce the Bayesian way of thinking about variables as random variables, estimating probabilistic programs, and how efficient probabilistic programming frameworks incorporate differentiable programming.

** Bayesian Modeling in a Nutshell
:PROPERTIES:
:CUSTOM_ID: bayesian-modeling-in-a-nutshell
:END:
The idea of Bayesian modeling is to treat your variables as a random variable with respect to some distribution. As a starting point, think about the linear model

\[f(x) = ax\]

The standard way to think of the linear model is that \(a\) is a variable, and so you put a value \(x\) in and compute \(ax\). However, in the Bayesian sense, the value of \(a\) can be a /random variable/. A random variable \(Z\) is a variable which has probability of taking certain values from a /probability distribution/. If we say that \(Z \sim f(y)\), then we are saying that the probability that \(Z\) takes a value in the set \(\Omega\) is:

\[\int_\Omega f(y)dy\]

For example, if \(Z\) is a scalar, then the probability that \(Z \in [0,1]\) is:

\[\int_0^1 f(y)dy\]

Discrete probability distributions can be handled by either using distribution quantities and measures in the integral, or by simply saying \(f(y)\) is the probability that \(Z = y\).

Given this representation of variables, \(ax\) where \(a\) follows a probability distribution induces a probability distribution on \(f(x)\). To numerically acquire this distribution, one can use /Monte Carlo sampling/. This is simply the repeat process of:

1. Sample variables
2. Compute output

Doing this repeatedly then produces samples of \(f(x)\) from which a numerical representation of the distribution can be had. From there, going to a multivariable linear model like \(f(x) = Ax\) is the same idea. Going to \(f(x)\) where \(f\) is an arbitrary program is still the same idea: sample every variable in the program, compute the output, and repeat for many samples. \(f\) can be a neural network where all of the parameters are probabilistic, or it can be an ODE solver with probabilistic parameters.

** Quick Example
:PROPERTIES:
:CUSTOM_ID: quick-example
:END:
Let's do a quick example with the Lotka-Volterra equations. Recall that this is the ordinary differential equation defined by the following system:

#+begin_src julia
using OrdinaryDiffEq, Plots
function lotka_volterra(du,u,p,t)
  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]
  du[2] = -p[3] * u[2] + p[4] * u[1]*u[2]
end
θ = [1.5,1.0,3.0,1.0]
u0 = [1.0;1.0]
tspan = (0.0,10.0)
prob1 = ODEProblem(lotka_volterra,u0,tspan,θ)
sol = solve(prob1,Tsit5())
plot(sol)
#+end_src

Now let's assume that the =θ='s are random. With Julia we can make variables into random variables by using Distributions.jl:

#+begin_src julia
using Distributions
θ = [Uniform(0.5,1.5),Beta(5,1),Normal(3,0.5),Gamma(5,2)]
#+end_src

from which we can sample points and propagate through the solver:

#+begin_src julia
_θ = rand.(θ)
prob1 = ODEProblem(lotka_volterra,u0,tspan,_θ)
sol = solve(prob1,Tsit5())
plot(sol)
#+end_src

and from which we can get an ensemble of solutions:

#+begin_src julia
prob_func = function (prob,i,repeat)
  remake(prob,p=rand.(θ))
end
ensemble_prob = EnsembleProblem(ODEProblem(lotka_volterra,u0,tspan,θ),
                                prob_func = prob_func)
sol = solve(ensemble_prob,Tsit5(),EnsembleThreads(),trajectories=1000)

using DiffEqBase.EnsembleAnalysis
plot(EnsembleSummary(sol))
#+end_src

From just a few variables having probabilities, every variable has an induced probability: there is a probability distribution on the integrator states, the output at time t_i, etc.

** Bayesian Estimation with Point Estimates: Bayes' Rule, Maximum Likelihood, and MAP
:PROPERTIES:
:CUSTOM_ID: bayesian-estimation-with-point-estimates-bayes-rule-maximum-likelihood-and-map
:END:
Recall from our previous studies that the difficult part of modeling is not necessarily the forward modeling approach, rather it's the incorporation of data or the estimation problem that is difficult. When your variables are now random distributions, how do you "fit" them?

The answer comes from Bayes' rule, which is the following. Assume you had a prior distribution \(p(\theta)\) for the probability that \(X\) is a given value \(\theta\). Then the posterior probability distribution, \(p(\theta|D)\), or the distribution which is updated to include data, is given by:

\[p(\theta|D) = \frac{p(D|\theta)p(\theta)}{\int_\Omega p(D|\theta)p(\theta)d\theta}\]

The scaling factor on the denominator is simply a constant to make the distribution integrate 1 (so that the resulting function is a probability distribution!). The numerator is simply the prior distribution multiplied by the likelihood of seeing the data given the value of the random variable. The prior distribution must be given but notice that the likelihood has another name: the likelihood is the model.

The reason why it's the same thing is because the model is what tells you the expected outcomes given a value of the random variable, and your data is on an expected outcome! However, the likelihood encodes a little bit more information in that it again is a distribution and not a point estimate. We need to make a choice for our /measurement distribution/ on our model's results.

**** Quick Question: Why is this referred to as measurement noise? Why is it not process noise?
:PROPERTIES:
:CUSTOM_ID: quick-question-why-is-this-referred-to-as-measurement-noise-why-is-it-not-process-noise
:END:
A common choice for the measurement distribution is the Normal distribution. This comes from the Central Limit Theorem (CLT) which essentially states that, given enough interacting mechanisms, the average values of things "tend to become normally distributed". The true statement of the CLT is much more complex, but that is a decent working definition for practical use. The normal distribution is defined by two parameters, \(\mu\) and \(\sigma\), and is given by the following function:

\[f(x;\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}}\exp(\frac{-(x-\mu)^2}{2\sigma^2})\]

This is a bell curve centered at \(\mu\) with a variance of \(\sigma\). Our best guess for the output, i.e. the model's prediction, should be the average measurement, meaning that \(\mu\) is the result from the simulator. \(\sigma\) is a parameter for how much measurement error we expect (some intuition on \(\sigma\) will come soon).

Let's return to thinking about the ODE example. In this case we have \(\theta\) as a vector of random variables. This means that \(u(t;\theta)\) is a random variable for the ODE \(u'= ...\)'s solution at a given point in time \(t\). If we have a measurement at a time \(t_i\) and assume our measurement noise is normally distributed with some constant measurement noise \(\sigma\), then the likelihood of our data would be \(f(x_i;u(t_i;\theta),\sigma)\) at each data point \((t_i,x_i)\). From probability we know that seeing the composition of events is given by the multiplication of probabilities, so the probability of seeing the full dataset given observations \(D = (t_i,x_i)\) along the timeseries is:

\[p(D|\theta) = \prod_i f(x_i;u(t_i;\theta),\sigma)\]

This can be read as: solve the model with the given parameters, and the probability of having seen the measurement is thus given by a product of normal distribution calculations. Note that in many cases the product is not numerically stable (and grows exponentially), and so the likelihood is transformed to the log-likelihood. To get this expression, we take the log of both sides and notice that the product becomes a summation, and thus:

\[\begin{align}
\log p(D|\theta) &= \sum_i \log f(x_i;u(t_i;\theta),\sigma)\\
                 &= \frac{N}{\sqrt{2\pi}\sigma} + \frac{1}{2\sigma^2} \sum_i -(x-\mu)^2
\end{align}\]

Notice that *maximizing this log-likelihood is equivalent to minimizing the L2 norm of the solution against the data!*. Thus we can see a few things:

1. Previous parameter estimation by minimizing a norm against data can be seen as maximum likelihood with some measurement distribution. L2 norm corresponds to assuming measurement noise is normally distributed and all of the measurements have the same error variance.
2. By the same derivation, having different error variances with normally distributed errors is equivalent to doing weighted L2 estimation.

This reformulation (generalization?) to likelihoods of probability distributions is known as /maximum likelihood estimation/ (MLE), but is equivalent to our previous forms of parameter estimation using point estimates against data. However, this calculation is ignoring Bayes' rule, and is thus not finding the parameters which have the highest probability. To do that, we need to go back to Bayes' rule which states that:

\[\log p(\theta|D) = \log p(D|\theta) + \log p(\theta) - C\]

Thus, maximizing the log-likelihood is "almost" the same as finding the most probable parameters, except that we need to add weights given \(\log p(\theta)\) from our prior distribution! If we assume our prior distribution is flat, like a uniform distribution, then we have a /non-informative prior/ and the maximum posterior point matches that of the maximum likelihood estimation. However, this formulation allows us to get point estimates in a way that takes into account prior knowledge, and is call /maximum a posteriori estimation/ (MAP).

** Bayesian Estimation of Posterior Distributions with Monte Carlo
:PROPERTIES:
:CUSTOM_ID: bayesian-estimation-of-posterior-distributions-with-monte-carlo
:END:
The previous discussion still solely focused on getting point estimates for the most probable parameters. However, what if we wanted to find the distributions of the parameters, i.e. the full \(p(D|\theta)\)? Outside of very few small models, this cannot be done analytically and is thus the basic problem of probabilistic programming. There are two general approaches:

1. Sampling-based approaches. Sample parameters \(\theta_i\) in such a manner that the array \([\theta_i]\) converges to an array sampled from the true distribution, and thus with enough samples one can capture the distribution numerically.
2. Variational inference. Find some way to represent the probability distribution and push forward the distributions at every step of the program.

**** Recovering Distributions from Sampled Points
:PROPERTIES:
:CUSTOM_ID: recovering-distributions-from-sampled-points
:END:
It's clear from above that if you have a distribution, like =Normal(5,1)=, that you can sample from the distribution to get an array of values which follow the distribution. However, in order for the following sampling approaches to make sense, we need to see how to recover a distribution from discrete samples. So let's say you had a bunch of normally distributed points:

#+begin_src julia
X = Normal(5,1)
x = [rand(X) for i in 1:100]
scatter(x,[1 for i in 1:100])
#+end_src

Notice that there are more points in the areas of higher probability. Thus the density of sampled points gives us an estimate for the probability of having points in a given area. We can then count the number of points in a bin and divide by the total number of points in order to get the probability of being in a specific region. This is depicted by a histogram:

#+begin_src julia
histogram(x)
#+end_src

and we see this converges when we get more points:

#+begin_src julia
histogram([rand(X) for i in 1:10000],normed=true)
using StatsPlots
plot!(X,lw=5)
#+end_src

A continuous form of this is the /kernel density estimate/, which is essentially a smoothed binning approach.

#+begin_src julia
using KernelDensity
plot(kde([rand(X) for i in 1:10000]),lw=5)
plot!(X,lw=5)
#+end_src

Thus, for the sampling-based approaches, we simply need to arrive at an array which is sampled according to the distribution that we want to estimate, and from that array we can recover the distribution.

*** Sampling Distributions with the Metropolis Hastings Algorithm
:PROPERTIES:
:CUSTOM_ID: sampling-distributions-with-the-metropolis-hastings-algorithm
:END:
The Metropolis-Hastings algorithm is the simplest form of /Markov Chain Monte Carlo/ (MCMC) which gives a way of sampling the \(\theta\) distribution. To see how this algorithm works, let's understand the ratio between two points in the posterior probability. If we have \(x_i\) and \(x_j\), the ratio of the two probabilities would be given by:

\[\frac{p(x_i|D)}{p(x_j|D)} = \frac{p(D|x_i)p(x_i)}{p(D|x_j)p(x_j)}\]

(notice that the integration constant cancels). This motivates the idea that all we have to do is ensure we only go to a point \(x_j\) from \(x_i\) with probability difference that matches that ratio, and over time if we do this between "all points" we will have the right number of "each point" in the distribution (quotes because it's continuous). With a bit more rigour we arrive at the following algorithm:

1. Starting at \(x_i\), take \(x_{i+1}\) from a sampling algorithm \(g(x_{i+1}|x_i)\).
2. Calculate \(A = \min\left(1,\frac{p(D|x_{i+1})p(x_{i+1})g(x_i|x_{i+1})}{p(D|x_i)p(x_i)g(x_{i+1}|x_i)}\right)\). Notice that if we require \(g\) to be symmetric, then this simplifies to the probability ratio \(A = \min\left(1,\frac{p(D|x_{i+1})p(x_{i+1})}{p(D|x_i)p(x_i)}\right)\)
3. Use a random number to accept the step with a probability \(A\). Go back to step 1, incrementing \(i\) if accepted, otherwise just repeat.

I.e, we just walk around the space biasing the acceptance of a step by the factor \(\frac{p(x_i|D)}{p(x_j|D)}\) and sooner or later we will have spent the right amount of time in each area, giving the correct distribution.

(This can be rigorously proven, and those details are left out.)

*** The Cost of Bayesian Estimation
:PROPERTIES:
:CUSTOM_ID: the-cost-of-bayesian-estimation
:END:
Let's take a quick moment to understand the high cost of Bayesian posterior estimations. While before we were getting point estimates, now we are trying to recover a full probability distribution, and each accept/reject probability calculation requires evaluating the likelihood at some point. Remember, the likelihood is generated by our simulator, and thus every evaluation here is an ODE solver call or a neural network forward pass! This means that to get good distributions, we are solving the ODE hundreds of thousands of times, i.e. even more than when doing parameter estimation! This is something to keep in mind.

However, notice that this process is trivially parallelizable. We can just have /parallel chains/ on going, i.e. start 16 processes all doing Metropolis-Hastings, and in the end they are all sampling from the same distribution, so the final array can simply be the pooled results of each chain.

*** Hamiltonian Monte Carlo
:PROPERTIES:
:CUSTOM_ID: hamiltonian-monte-carlo
:END:
Metropolis-Hastings is easy to motivate and implement. However, it does not do well in high dimensional spaces because it searches in all directions. For example, it's common for the sampling distribution \(g\) to be a multivariable distribution (i.e. normal in all directions). However, high dimensional objects commonly sit on low dimensional manifolds (known as the /manifold hypothesis/). If that's the case, the most probable set of parameters is something that is low dimensional. For example, parameters may compensate for one another, and so \(\theta_1^2 + \theta_2^2 + \theta_3^2 = 1\) might be the manifold on which all of the most probable choices for \(\theta\) lie, in which case we need sample on the sphere instead of all of \(\mathbb{R}^3\).

However, it's quick to see that this will give Metropolis-Hastings some trouble, since it will use a normal distribution around the current point, and thus even if we start on the sphere, it will have a high chance of trying a point not on the sphere in the next round! This can be depicted as:

[[https://user-images.githubusercontent.com/1814174/69541382-fe85b100-0f56-11ea-8852-ba2044084d43.PNG]]

Recall that every single rejection is still evaluating the likelihood (since it's calculating an acceptance probability, finding it near zero, rejecting and starting again), and every likelihood call is calling our simulator, and so this is sllllllooooooooooooooowwwwwwwww in high dimensions!

What we need to do instead is ensure that we walk along the path of high probability. What we want to do is thus build a vector field that matches our high probability regions

[[https://user-images.githubusercontent.com/1814174/69541530-5ae8d080-0f57-11ea-9673-36affc62d315.PNG]]

and follow said vector field (following a vector field is solving what kind of equation?). The first idea one might have is to use the gradient. However, while this idea has the right intentions, the issue is that the gradient of the probability will average out all of the possible probabilities, and will thus flow towards the mode of the distribution:

[[https://user-images.githubusercontent.com/1814174/69541683-bd41d100-0f57-11ea-9356-dc2f771cca7d.PNG]]

To overcome this issue, we look to physical systems and see that a satellite orbiting a planet always nicely stays on some manifold instead of following the gradient:

[[https://user-images.githubusercontent.com/1814174/69541780-fed27c00-0f57-11ea-913f-6b7135ad5fe4.PNG]]

The reason why it does is because it has momentum. Recall from basic physics that one way to describe a physical system is through /Hamiltonian mechanics/, where \(H(x,p)\) is the energy associated with the state \((x,p)\) (normally \(x\) is location and \(p\) is momentum). Due to conservation of energy, the solution of the dynamical equations leads to \(H(x,p)\) being constant, and thus the dynamics follow the /level sets/ of \(H\). From the Hamiltonian the dynamics of the system are:

\[\begin{align}
\frac{dx}{dt} &=  \frac{dH}{dp}\\
              &= -\frac{dH}{dx}
\end{align}\]

Here we want our Hamiltonian to be our posterior probability, so that way we stay on the manifold of high probability. This means:

\[H(x,p) = - \log \pi(x,p)\]

where \(\pi(x,p) = \pi(p|x)\pi(x)\) (where I am now using \(pi\) for probability since \(p\) is momentum!). So to lift from a probability over parameters to one that includes momentum, we simply need to choose a conditional distribution \(\pi(p|x)\). This would mean that

\[\begin{align}
H(x,p) &= -log \pi(p|x) - \log \pi(x)\\
       &= K(p,x) + V(x)
\end{align}\]

where \(K\) is the kinetic energy and \(V\) is the potential. Thus the potential energy is directly given by the posterior calculation, and the kinetic energy is thus a choice that is used to build the correct Hamiltonian. Hamiltonian Monte Carlo methods then dig into good ways to choose the kinetic energy function. This is done at the start (along with the choice of ODE solver time step) in such a way that it maximizes acceptance probabilities.

*** Connections to Differentiable Programming
:PROPERTIES:
:CUSTOM_ID: connections-to-differentiable-programming
:END:
\(-\frac{dH}{dx}\) requires calculating the gradient of the likelihood function with respect to the parameters, so we are once again using the gradient of our simulator! This means that all of our previous discussion on automatic differentiation and differentiable programming applies to the Hamiltonian Monte Carlo context.

There's another thread to follow that transformations of probability distributions are pushforwards of the Jacobian transformations (given the transformation of an integral formula), and this is used when doing variational inference.

*** Symplectic and Geometric Integration
:PROPERTIES:
:CUSTOM_ID: symplectic-and-geometric-integration
:END:
One way to integrate the system of ODEs which result from the Hamiltonian system is to convert it to a system of first order ODEs and solve it directly. However, this loses information and can result in drift. This is demonstrated by looking at the long time solution of the pendulum:

#+begin_src julia
using ParameterizedFunctions
u0 = [1.,0.]
harmonic! = @ode_def HarmonicOscillator begin
   dv = -x
   dx = v
end
tspan = (0.0,10.0)
tspan = (0.0,10000.0)
prob = ODEProblem(harmonic!,u0,tspan)
sol = solve(prob,Tsit5())
gr(fmt=:png) # Make it a PNG instead of an SVG since there's a lot of points!
plot(sol,vars=(1,2))
#+end_src

#+begin_src julia
plot(sol)
#+end_src

What is an oscillatory system slowly loses energy and falls inward towards the center. To avoid this issue, we can do a few things:

1. Project back to the manifold after steps. That can be costly (but almost might only need to happen every once in awhile!)
2. Use a symplectic integrator.

A /symplectic integrator/ is an integrator who's solution lives on a symplectic manifold, i.e. it preserves area in in the \((x,p)\) ellipses as it numerically approximates the flow. This means that:

- Long-time integrations are truly cyclic with only floating point drift.
- Steps preserve area. In the sense of Hamiltonian Monte Carlo, this means preserve probability and thus increase the acceptance rate.

These properties are demonstrated in [[https://tutorials.juliadiffeq.org/html/models/05-kepler_problem.html][the Kepler problem demo]]. However, note that while the solution lives on a symplectic manifold, it isn't necessarily the correct symplectic manifold. The shift in the manifold is \(\mathcal{O}(\Delta t^k)\) where \(k\) is the order of the method. For more information on symplectic integration, consult [[https://scicomp.stackexchange.com/a/29154/18981][this StackOverflow response which goes into depth]].

*** Application: Bayesian Estimation of Differential Equation Parameters
:PROPERTIES:
:CUSTOM_ID: application-bayesian-estimation-of-differential-equation-parameters
:END:
For a full demo of probabilistic programming on a differential equation system, see [[https://tutorials.juliadiffeq.org/html/models/06-pendulum_bayesian_inference.html][this tutorial on Bayesian inference of pendulum parameteres]] utilizing DifferentialEquations.jl and DiffEqBayes.jl.

** Bayesian Estimation of Posterior Distributions with Variational Inference
:PROPERTIES:
:CUSTOM_ID: bayesian-estimation-of-posterior-distributions-with-variational-inference
:END:
Instead of using sampling, one can use variational inference to push through probability distributions. There are many ways to do variational inference, but a lot of the methods can be very model-specific. However, a recent change to probabilistic programming has been the development of /Automatic Differentiation Variational Inference (ADVI)/: a general variational inference method which is not model-specific and instead uses AD. This has allowed for large expensive models to get effective distributional estimation, something that wasn't previously possible with HMC. In this section we will build up this methodology and understand its performance characteristics.

*** ADVI as Optimization
:PROPERTIES:
:CUSTOM_ID: advi-as-optimization
:END:
In this form of variational inference, we wish to directly estimate the posterior distribution. To do so, we pick a functional form to represent the solution \(q(\theta; \phi)\) where \(\phi\) are latent variables. We want our resulting distribution to fit the posterior, and tus we enforce that:

\[\phi^\ast = \text{argmin}_{\phi} \text{KL} \left(q(\theta; \phi) \Vert p(\theta | D)\right)\]

where KL is the KL-divergence. KL-divergence is a distance function over probability distributions, and so this is simply a cost function over the distance between a chosen distribution and a desired distribution, where when \(\phi\) are good we will have \(q\) as a good approximation to the posterior.

However, the KL divergence lacks an analytical form because it requires knowing the posterior, the quantity we are trying to numerically estimate. However, it turns out that we can instead maximize the /Evidence Lower Bound (ELBO)/:

\[\mathcal{L}(\phi) = \mathbb{E}_{q}[\log p(x,\theta)] - \mathbb{E}_q [\log q(\theta; \phi)]\]

The ELBO is equivalent to the negative KL divergence up to a constant \(\log p(x)\), which means that maximizing this is equivalent to minimizing the KL divergence.

One last detail is necessary in order for this problem to be tractable. To know the set of possible values to optimize over, we assume that the support of \(q\) is a subset of the support of the prior. This means that our prior has to cover the probability distribution, which makes sense and matches Cromwell's rule for MCMC.

At this point we now assume that \(q\) is Gaussian. When we rewrite the ELBO in terms of the standard Gaussian, we receive an expectation that is automatically differentiable. Calculating gradients is thus done with AD. Using only one or a few solves gives a noisy gradient to sample and optimize the latent variables to hone in on latent variables.

** A Note on Implementation of Optimization for Probabilistic Programming
:PROPERTIES:
:CUSTOM_ID: a-note-on-implementation-of-optimization-for-probabilistic-programming
:END:
Variable domains can be constrained. For example, you may require a positive value. This can be handled by a transformation. For example, if \(y\) must be positive, then one can optimize implicitly using \(\exp(y)\) at every point, this allowing \(y\) to be any real value with then \(\exp(y)\) is positive. This turns the problem into an unconstrained optimization over the real numbers, and similar transformations can be done with any of the standard probability distribution's support function.

**** Citation
:PROPERTIES:
:CUSTOM_ID: citation
:END:
For Hamiltonian Monte Carlo, the images were taken from [[https://arxiv.org/pdf/1701.02434.pdf][A Conceptual Introduction to Hamiltonian Monte Carlo]] by Michael Betancourt.
* Global Sensitivity Analysis
Sensitivity analysis is the measure of how sensitive a model is to changes in parameters, i.e. how much the output changes given a change in the input. Clearly, derivatives are a measure of sensitivity, but derivative are /local sensitivity/ measures because they are only the derivative at a single point. However, the idea of probabilistic programming starts to bring up an alternative question: how does the output of a model generally change with a change in the input? This kind of question requires an understanding of /global sensitivity/ of a model. While there isn't a single definition of the concept, there are a few methods that individuals have employed to estimate the global sensitivity.

Reference implementations of these methods can be found in [[https://github.com/SciML/GlobalSensitivity.jl][GlobalSensitivity.jl]]

** Setup for Global Sensitivity
:PROPERTIES:
:CUSTOM_ID: setup-for-global-sensitivity
:END:
In our global sensitivity analysis, we have a model \(f\) and want to understand the relationship

\[y = f(x_i)\]

Recall \(f\) can be a neural network, an ODE solve, etc. where the \(X_i\) are items like initial conditions and parameters. What we want to do is understand how much the total changes in \(y\) can be attributed to changes in specific \(x_i\).

However, this is not an actionable form since we don't know what valid inputs into \(f\) look like. Thus any global sensitivity study at least needs a domain for the \(x_i\), at least in terms of bounds. This is still underdefined because what makes one thing that it's not more likely for \(x_i\) to be near the lower part of the bound instead of the upper part? Thus, for global sensitivity analysis to be well-defined, \(x_i\) must take a distributional form, i.e. be random variables. Thus \(f\) is a deterministic program with probabilistic inputs, and we want to determine the effects of the distributional inputs on the distribution of the output.

*** Reasons for Global Sensitivity Analysis
:PROPERTIES:
:CUSTOM_ID: reasons-for-global-sensitivity-analysis
:END:
What are the things we can learn from doing such a global sensitivity analysis?

1. You can learn what variables would need to be changed to drive the solution in a given direction or control the system. If your model is exact and the parameters are known, the "standard" methods apply, but if your model is only approximate, a global sensitivity metric may be a better prediction as to how variables cause changes.
2. You can learn if there are any variables which do not have a true effect on the output. These variables would be practically unidentifiable from data and models can be reduced by removing the terms. It also is predictive as to robustness properties.
3. You can find ways to automatically sparsify a model by dropping off the components which contribute the least. This matters in automatically generated or automatically detected models, where many pieces may be spurious and global sensitivities would be a method to detect that in a manner that is not sensitive to the chosen parameters.

** Global Sensitivity Analysis Measures
:PROPERTIES:
:CUSTOM_ID: global-sensitivity-analysis-measures
:END:
*** Linear Global Sensitivity Metrics: Correlations and Regressions
:PROPERTIES:
:CUSTOM_ID: linear-global-sensitivity-metrics-correlations-and-regressions
:END:
The first thing that you can do is approximate the full model with a linear surrogate, i.e.

\[y = AX\]

for some linear model. A regression can be done on the outputs of the model in order to find the linear approximation. The best fitting global linear model then gives coefficients for the global sensitivities via the individual effects, i.e. for

\[y = \sum_i \beta_i x_i\],

the \(\beta_i\) are the global effect. Just as with any use of a linear model, the same ideas apply. The coefficient of determination (\(R^2\)) is a measure of how well the model fits. However, one major change needs to be done in order to ensure that the solutions are comparable between different models. The dependence of the solution on the units can cause the coefficients to be large/small. Thus we need to normalize the data, i.e. use the transformation

\[\tilde{x_i} = \frac{x_i-E[x_i]}{V[x_i]}\] \[\tilde{y_i} = \frac{y_i-E[y_i]}{V[y_i]}\]

The normalized coefficients are known as the /Standardized Regression Coefficients/ (SRC) and are a measure of the global effects.

Notice that while the \(\beta_i\) capture the mean effects, it holds that

\[V(y) = \sum_i \beta^2_i x_i\]

and thus the variance due to \(x_i\) can be measured as:

\[SRC_i = \beta_i \sqrt{\frac{V[x_i]}{V[y]}}\]

This interpretation is the same as the solution from the normalized variables.

From the same linear model, two other global sensitivity metrics are defined. The /Correlation Coefficients/ (CC) are simply the correlations:

\[CC_i = \frac{\text{cov}(x_i,y)}{\sqrt{V[x_i]V[y]}}\]

Similarly, the /Partial Correlation Coefficient/ is the correlation coefficient where the linear effect of the other terms are removed, i.e. for \(S_i = {x_1,x_2,\ldots,x_{j-1},x_{j+1},\ldots,x_n}\) we have

\[PCC_{i|S_i} = \frac{\text{cov}(x_i,y|S)j)}{\sqrt{V[x_i|S_i]V[y|S_i]}}\]

*** Derivative-based Global Sensitivity Measures (DGSM)
:PROPERTIES:
:CUSTOM_ID: derivative-based-global-sensitivity-measures-dgsm
:END:
To go beyond just a linear model, one might want to do successive linearization. Since derivatives are a form of linearization, then one may thing to average derivatives. This averaging of derivatives is the DGSM method. If the \(x_i\) are random variables with joint CDF \(F(x)\), then it holds that:

\[v_i = \int_{R^d} \left(\frac{\partial f(x)}{\partial x_i}\right)^2 dF(x) = \mathbb{E}\left[\left(\frac{\partial f(x)}{\partial x_i}\right)^2\right],\]

We can also define the mean measure, which is simply:

\[w_i = \int_{R^d} \frac{\partial f(x)}{\partial x_i} dF(x) = \mathbb{E}\left[\frac{\partial f(x)}{\partial x_i}\right].\]

Thus a global variance estimate would be \(v_i - w_i^2\).

*** ADVI for Global Sensitivity
:PROPERTIES:
:CUSTOM_ID: advi-for-global-sensitivity
:END:
Note that the previously discussed method for probabilistic programming, ADVI, is a method for producing a Gaussian approximation for a probabilistic program. The resulting mean-field or full Gaussian approximations are variance index calculations!

*** The Morris One-At-A-Time (OAT) Method
:PROPERTIES:
:CUSTOM_ID: the-morris-one-at-a-time-oat-method
:END:
Instead of using derivatives, one can use finite difference approximations. Normally you want to use small \(\Delta x\), but if we are averaging derivatives over a large area, then in reality we don't really need a small \(\Delta x\)!

This is where the Morris method comes in. The basic idea is that moving in one direction at a time is a derivative estimate, and if we step large enough then the next derivative estimate may be sufficiently different enough to contribute well to the total approximation. Thus we do the following:

1. Take a random starting point
2. Randomly choose a direction \(i\) and make a change \(\Delta x_i\) only in that direction.
3. Calculate the derivative approximation from that change. Repeat 2 and 3.

Keep doing this for enough steps, and the average of your derivative approximations becomes a global index. Notice that this reuses every simulation as part of two separate estimates, making it much more computationally efficient than the other methods. However, it accounts for average changes and not necessarily measurements gives a value that's a decomposition of a total variance. But its computational cost makes it attractive for making quick estimates of the global sensitivities.

For practical usage, a few changes have to be done. First of all, notice that positive and negative change can cancel out. Thus if one want to measure of associated variance, one should use absolute values or squared differences. Also, one needs to make sure that these trajectories get good coverage of the input space. Define the distance between two trajectories as the sum of the geometric distances between all pairs of points. Generate many more trajectories than necessary and choose the \(r\) trajectories with the largest distance. If the model evaluations are expensive, this is significantly cheap enough in comparison that it's okay to do.

*** Sobol's Method (ANOVA)
:PROPERTIES:
:CUSTOM_ID: sobols-method-anova
:END:
Sobol's method is a true nonlinear decomposition of variance and it is thus considered one of the gold standards. For Sobol's method, we define the decomposition

\[f(x) = f_0 + \sum_i f_i(x_i) + \sum_{i,j} f_{ij}(x_i,x_j) + \ldots\]

where

\[f_0 = \int_\Omega f(x) dx\]

and orthogonality holds:

\[f_{i,j,\ldots}(x_i,x_j,\ldots)dx = 0\]

by the definitions:

\[f_i(x_i) = E(y|x_i) - f_0\]

\[f_{ij}(x_i,y_j) = E(y|x_i,x_j) - f_0 - f_i - f_j\]

Assuming that \(f(x)\) is L2, it holds that

\[\int_\Omega f^2(x)dx - f_0^2 = \sum_s \sum_i \int f^2_{i_1,i_2,\ldots,i_s} dx\]

and thus

\[V[y] = \sum V_i + \sum V_{ij} + \ldots\]

where

\[V_i = V[E_{x_{\sim i}}[y|x_i]]\] \[V_{ij} = V[E_{x_{\sim ij}}[y|x_i,x_j]]-V_i - V_j\]

where \(X_{\sim i}\) means all of the variables except \(X_i\). This means that the total variance can be decomposed into each of these variances.

From there, the fractional contribution to the total variance is thus the index:

\[S_i = \frac{V_i}{Var[y]}\]

and similarly for the second, third, etc. indices.

Additionally, if there are too many variables, one can compute the contribution of \(x_i\) including all of its interactions as:

\[S_{T_i} = \frac{E_{X_{\sim i}}[Var[y|X_{\sim i}]]}{Var[y]} = 1 - \frac{Var_{X_{\sim i}}[E_{X_i}[y|x_{\sim i}]]}{Var[y]}\]

**** Computational Tractability and Quasi-Monte Carlo
:PROPERTIES:
:CUSTOM_ID: computational-tractability-and-quasi-monte-carlo
:END:
Notice that every single expectation has an integral in it, so the variance is defined as integrals of integrals, making this a very challenging calculation. Thus instead of directly calculating the integrals, in many cases Monte Carlo estimators are used. Instead of a pure Monte Carlo method, one generally uses a low-discrepancy sequence (a form of quasi-Monte Carlo) to effectively sample the search space.

The following generates for example a /Sobol sequence/:

#+begin_src julia
using Sobol, Plots
s = SobolSeq(2)
p = hcat([next!(s) for i = 1:1024]...)'
scatter(p[:,1], p[:,2])
#+end_src

Another common quasi-Monte Carlo sequence is the /Latin Hypercube/, which is a generalization of the Latin Square where in every row, column, etc. only one point is given, allowing a linear spread over a high dimensional space.

#+begin_src julia
using LatinHypercubeSampling
p = LHCoptim(120,2,1000)
scatter(p[1][:,1],p[1][:,2])
#+end_src

For a reference library with many different quasi-Monte Carlo samplers, check out [[https://github.com/SciML/QuasiMonteCarlo.jl][QuasiMonteCarlo.jl]].

** Fourier Amplitude Sensitivity Sampling (FAST) and eFAST
:PROPERTIES:
:CUSTOM_ID: fourier-amplitude-sensitivity-sampling-fast-and-efast
:END:
The FAST method is a change to the Sobol method to allow for faster convergence. First transform the variables \(x_i\) onto the space \([0,1]\). Then, instead of the linear decomposition, one decomposes into a Fourier basis:

\[f(x_i,x_2,\ldots,x_n) = \sum_{m_1 = -\infty}^{\infty} \ldots \sum_{m_n = -\infty}^{\infty} C_{m_1m_2\ldots m_n}\exp\left(2\pi i (m_1 x_1 + \ldots + m_n x_n)\right)\]

where

\[C_{m_1m_2\ldots m_n} = \int_0^1 \ldots \int_0^1 f(x_i,x_2,\ldots,x_n) \exp\left(-2\pi i (m_1 x_1 + \ldots + m_n x_n)\right)\]

The ANOVA like decomposition is thus

\[f_0 = C_{0\ldots 0}\]

\[f_j = \sum_{m_j \neq 0} C_{0\ldots 0 m_j 0 \ldots 0} \exp (2\pi i m_j x_j)\]

\[f_{jk} = \sum_{m_j \neq 0} \sum_{m_k \neq 0} C_{0\ldots 0 m_j 0 \ldots m_k 0 \ldots 0} \exp \left(2\pi i (m_j x_j + m_k x_k)\right)\]

The first order conditional variance is thus:

\[V_j = \int_0^1 f_j^2 (x_j) dx_j = \sum_{m_j \neq 0} |C_{0\ldots 0 m_j 0 \ldots 0}|^2\]

or

\[V_j = 2\sum_{m_j = 1}^\infty \left(A_{m_j}^2 + B_{m_j}^2 \right)\]

where \(C_{0\ldots 0 m_j 0 \ldots 0} = A_{m_j} + i B_{m_j}\). By Fourier series we know this to be:

\[A_{m_j} = \int_0^1 \ldots \int_0^1 f(x)\cos(2\pi m_j x_j)dx\]

\[B_{m_j} = \int_0^1 \ldots \int_0^1 f(x)\sin(2\pi m_j x_j)dx\]

**** Implementation via the Ergodic Theorem
:PROPERTIES:
:CUSTOM_ID: implementation-via-the-ergodic-theorem
:END:
Define

\[X_j(s) = \frac{1}{2\pi} (\omega_j s \mod 2\pi)\]

By the ergodic theorem, if \(\omega_j\) are irrational numbers, then the dynamical system will never repeat values and thus it will create a solution that is dense in the plane (Let's prove a bit later). As an animation:

[[https://upload.wikimedia.org/wikipedia/commons/6/64/Search_curve_1.gif]]

(here, \(\omega_1 = \pi\) and \(\omega_2 = 7\))

This means that:

\[A_{m_j} = \lim_{T\rightarrow \infty} \frac{1}{2T} \int_{-T}^T f(x)\cos(m_j \omega_j s)ds\]

\[B_{m_j} = \lim_{T\rightarrow \infty} \frac{1}{2T} \int_{-T}^T f(x)\sin(m_j \omega_j s)ds\]

i.e. the multidimensional integral can be approximated by the integral over a single line.

One can satisfy this approximately to get a simpler form for the integral. Using \(\omega_i\) as integers, the integral is periodic and so only integrating over \(2\pi\) is required. This would mean that:

\[A_{m_j} \approx \frac{1}{2\pi} \int_{-\pi}^\pi f(x)\cos(m_j \omega_j s)ds\]

\[B_{m_j} \approx \frac{1}{2\pi} \int_{-\pi}^\pi f(x)\sin(m_j \omega_j s)ds\]

It's only approximate since the sequence cannot be dense. For example, with \(\omega_1 = 11\) and \(\omega_2 = 7\):

[[https://upload.wikimedia.org/wikipedia/commons/2/29/Search_curve_3.gif]]

A higher period thus gives a better fill of the space and thus a better approximation, but may require a more points. However, this transformation makes the true integrals simple one dimensional quadratures which can be efficiently computed.

To get the total index from this method, one can calculate the total contribution of the complementary set, i.e. \(V_{c_i} = \sum_{j \neq i} V_j\) and then

\[S_{T_i} = 1 - S_{c_i}\]

Note that this then is a fast measure for the total contribution of variable \(i\), including all higher-order nonlinear interactions, all from one-dimensional integrals! (This extension is called extended FAST or eFAST)

**** Proof of the Ergodic Theorem
:PROPERTIES:
:CUSTOM_ID: proof-of-the-ergodic-theorem
:END:
Look at the map \(x_{n+1} = x_n + \alpha (\text{mod} 1)\), where \(\alpha\) is irrational. This is the irrational rotation map that corresponds to our problem. We wish to prove that in any interval \(I\), there is a point of our orbit in this interval.

First let's prove a useful result: our points get arbitrarily close. Assume that for some finite \(\epsilon\) that no two points are \(\epsilon\) apart. This means that we at most have spacings of \(\epsilon\) between the points, and thus we have at most \(\frac{2\pi}{\epsilon}\) points (rounded up). This means our orbit is periodic. This means that there is a \(p\) such that

\[x_{n+p} = x_n\]

which means that \(p \alpha = 1\) or \(p = \frac{1}{\alpha}\) which is a contradiction since \(\alpha\) is irrational.

Thus for every \(\epsilon\) there are two points which are \(\epsilon\) apart. Now take any arbitrary \(I\). Let \(\epsilon < d/2\) where \(d\) is the length of the interval. We have just shown that there are two points \(\epsilon\) apart, so there is a point that is \(x_{n+m}\) and \(x_{n+k}\) which are \(<\epsilon\) apart. Assuming WLOG \(m>k\), this means that \(m-k\) rotations takes one from \(x_{n+k}\) to \(x_{n+m}\), and so \(m-k\) rotations is a rotation by \(\epsilon\). If we do \(\frac{1}{\epsilon}\) rounded up rotations, we will then cover the space with intervals of length epsilon, each with one point of the orbit in it. Since \(\epsilon < d/2\), one of those intervals is completely encapsulated in \(I\), which means there is at least one point in our orbit that is in \(I\).

Thus for every interval we have at least one point in our orbit that lies in it, proving that the rotation map with irrational \(\alpha\) is dense. Note that during the proof we essentially showed as well that if \(\alpha\) is rational, then the map is periodic based on the denominator of the map in its reduced form.

** A Quick Note on Parallelism
:PROPERTIES:
:CUSTOM_ID: a-quick-note-on-parallelism
:END:
Very quick note: all of these are hyper parallel since it does the same calculation per parameter or trajectory, and each calculation is long. For quasi-Monte Carlo, after generating "good enough" trajectories, one can evaluate the model at all points in parallel, and then simply do the GSA index measurement. For FAST, one can do each quadrature in parallel.
* Code Profiling and Optimization
This is just a quick look into code profiling. By now we should be writing high performance parallel code which is combining machine learning and scientific computing techniques and doing large-scale parameter analyses on the models. However, at this point it may be difficult to understand where our performance difficulties lie. This is where we turn to code profiling tooling.
** Type Inference Checking
:PROPERTIES:
:CUSTOM_ID: type-inference-checking
:END:
The most common way for code to slow down is via type-inference issues. One can normally work through them by "thinking like a compiler" and seeing what would be inferable. For example, a common issue is to not concretely type one's types. For example:

#+begin_src julia
struct MyStruct
  a::AbstractArray
end
x = MyStruct([1,2,3])
function f(x)
  x.a[1]
end
using InteractiveUtils
@code_warntype f(x)
#+end_src

In this case, the return type is not inferred and using =MyStruct= will generate slow code. The reason for this is quite simple: =x.a= can only be inferred as =AbstractArray=, and thus the element type =x.a[1]= and the exact dispatch cannot be known until the function finds out at runtime what kind of array it is. As a result, the compiler throws the only thing it can: it puts =Any= as the inferred type and runs slow code.

We can instead utilize a concrete struct or use a parametric type to create a family of related structs:

#+begin_src julia
struct MyStruct2{A <: AbstractArray}
  a::A
end
x2 = MyStruct2([1,2,3])
@code_warntype f(x2)
#+end_src

and now it's inferred because the information that it would need is inferrable.

But what if we needed help? The first tool of course is =@code_warntype=. But for deeper functions you may want more tooling. A nice tool is Traceur.jl which will alert you to the lines at which you have performance issues. In our example we see:

#+begin_src julia;eval=false
using Traceur
@trace f(x)
#+end_src

#+begin_example
┌ Warning: dynamic dispatch to Base.getindex(Base.getfield(x, a), 1)
└ @ none:-1
┌ Warning: f returns Any
└ @ none:2
#+end_example

which points out our first problem is getting the untyped array out of the =MyStruct=. On larger functions it can do even more:

#+begin_src julia;eval=false
function naive_sum(xs)
  s = 0
  for x in xs
    s += x
  end
  return s
end
@trace naive_sum([1.])
#+end_src

#+begin_example
┌ Warning:  is assigned as Tuple{Int64,Int64}
└ @ array.jl:-1
┌ Warning:  is assigned as Nothing
└ @ array.jl:-1
┌ Warning:  is assigned as Union{Nothing, Tuple{Float64,Int64}}
└ @ none:-1
┌ Warning:  is assigned as Union{Nothing, Tuple{Float64,Int64}}
└ @ none:-1
┌ Warning: s is assigned as Int64
└ @ none:-1
┌ Warning: s is assigned as Float64
└ @ none:-1
┌ Warning: naive_sum returns Union{Float64, Int64}
└ @ none:2
#+end_example

and alert you to multiple lines which are causing problems.

However, for even larger functions you can still have many issues that are hard to dig into with Julia a linear tool. For thus, Cthulhu.jl's =@descend= macro lets you interactively dig into the function to find the problematic lines. For the best introduction, watch [[https://www.youtube.com/watch?v=qf9oA09wxXY][Valentin Churavy's JuliaCon 2019 talk]]

** Flame Graphs
:PROPERTIES:
:CUSTOM_ID: flame-graphs
:END:
Flame graphs are a common tool for illustrating performance. To demonstrate this let's look at the solution to an ODE from DifferentialEquations.jl's OrdinaryDiffEq.jl. The code is the following:

#+begin_src julia
using OrdinaryDiffEq
function lorenz(du,u,p,t)
 du[1] = 10.0(u[2]-u[1])
 du[2] = u[1]*(28.0-u[3]) - u[2]
 du[3] = u[1]*u[2] - (8/3)*u[3]
end
u0 = [1.0;0.0;0.0]
tspan = (0.0,100.0)
prob = ODEProblem(lorenz,u0,tspan)
sol = solve(prob,Tsit5())
using Plots; plot(sol,vars=(1,2,3))
#+end_src

To generate the flame graph, first we want to create a profile. To do this we will use the Profile module's =@profile=. Note that a profile should be "sufficiently large", so on quick functions you may want to run the code plenty of times. Make sure the profile does not include compilation if you want good results!

#+begin_src julia
# No compilation in the results
sol = solve(prob,Tsit5())

using Profile
# Profile 1000 runs
@profile for i in 1:1000 sol = solve(prob,Tsit5()) end
#+end_src

This profiler is a statistical or sampling profiler, which means it periodically samples where it is at in a code and thus understands the hotspots in the code by tallying how many samples are in a certain area. We can first visualize this by printing it out:

#+begin_src julia
#Profile.print()
#+end_src

However, that printout can often times be hard to read. Instead, we can visualize it with a /flame graph/. There are many ways to get the flame graph, if you're in Juno, you can simply do:

#+begin_src julia;eval=false
Juno.profiler()
#+end_src

[[https://user-images.githubusercontent.com/1814174/69931716-45633180-1496-11ea-888e-e7bcde939083.PNG]]

(Note that if you're not using Juno, there are equivalent tools in the package ecosystem. ProfileView.jl is a very simple flame graph generator, and PProf.jl exports to Google PProf which has many more features)

Each block corresponds to a function call. The horizontal length is the amount of time spent in that function, while the vertical grouping is for call nesting, i.e. you are below the function that called you. The portion that is circled is where the mouse pointer was at, and while hovering over this it said what function it corresponded to: =recursivecopy= in RecursiveArrayTools.jl. If we click on this, it sends us to the hotspot:

[[https://user-images.githubusercontent.com/1814174/69931830-ad197c80-1496-11ea-80bd-4c0f134d120f.PNG]]

Juno gives you a light indicator that tells you how much time is spent at a given line. Here we see that most of the time is spent inside of the =map= operation which calls =recursivecopy= on the elements, which then does =copy(a)=.

This tells us that the main cost of our code is the part that is copying the arrays to save them! Thus let's generate a new profile where the ODE solver saves less:

#+begin_src julia
Profile.clear()
# No compilation in the results
sol = solve(prob,Tsit5(),save_everystep=false)

# Profile 1000 runs
@profile for i in 1:1000 sol = solve(prob,Tsit5(),save_everystep=false) end
#+end_src

#+begin_src julia;eval=false
Juno.profiler()
#+end_src

[[https://user-images.githubusercontent.com/1814174/69931923-08e40580-1497-11ea-9583-2d76af8316e4.PNG]]

Now we see that the majority of the time is spent in the =perform_step!= method, which is:

[[https://user-images.githubusercontent.com/1814174/69931943-2add8800-1497-11ea-94f8-b244dcac12f4.PNG]]

We can notice that there is still quite a bit of jitter in the profile since each of the =f= calls here should be exactly the same length, but upping the number of solves in the loop would help with that:

#+begin_src julia
@profile for i in 1:10000 sol = solve(prob,Tsit5(),save_everystep=false) end
#+end_src

#+begin_src julia;eval=false
Juno.profiler()
#+end_src

[[https://user-images.githubusercontent.com/1814174/69932011-755f0480-1497-11ea-8839-ecc8f5d7c9fc.PNG]]

Now that this looks like a fairly good profile, we can use this to dig in and find out what lines need to be optimized!
* Uncertainty Programming, Generalized Uncertainty Quantification
In this lecture we will mix two separate topics: uncertainty quantification and adaptivity of algorithms. Using compiler-based tooling, similar to how automatic differentiation and probabilistic programming toolchains, we will show how one can begin to pushforward uncertainties of a model or calculation. This leads to an idea of /uncertainty programming/, a term which is not in use but should be justified by these notes.
** What is Uncertainty Quantification?
:PROPERTIES:
:CUSTOM_ID: what-is-uncertainty-quantification
:END:
Uncertainty quantification is the identification and quantification of sources of uncertainty. In our training of a neural differential equation, we have seen that the question of uncertainty can quickly become muddled. Results are inexact because of:

- Truncation errors in the ODE solve
- Truncation errors in the adjoint ODE solve
- Truncation errors in the interpolation calculation
- Numerical errors in every dot product along the way (!)
- Numerical errors in matrix multiplication and linear solving (latter when implicit)
- Numerical errors in backpropagation
- Measurement errors in our fitting data
- Randomness in the optimizer (when stochastic, like ADAM)
- What is the error in the model specification / model form?

"How correct is my model?" is thus a very involved question, since you'd have to know that every source of uncertainty is contained. In some cases we have rigorous mathematical results proving bounds. In other cases, we need to find empirical ways to quantify what's going on using our known bounds.

** Some High Level UQ Techniques
:PROPERTIES:
:CUSTOM_ID: some-high-level-uq-techniques
:END:
Two high level UQ techniques fall out of methodologies we have recently discussed. If we fit a model \(f\) to data, be it a neural network, a neural ODE, or some physical ODE model, we can fit it probabilistically using the Bayesian estimation or probabilistic programming tools previously described. With this form of fitting, one can ask the question "what are the likely results from the model given these parameter distributions?", which can then be answered through Monte Carlo sampling.

Another form of high level UQ is global sensitivity analysis, which gives a measurement for how much the output is going to vary over a wide range and thus relates uncertainties in the input to uncertainties in the output.

** Pushforward Methods for Uncertainties
:PROPERTIES:
:CUSTOM_ID: pushforward-methods-for-uncertainties
:END:
Instead of relying on expensive Monte Carlo methods for the pushforward of an uncertainty, we can derive a more programmatic approach to uncertainty quantification through the use of uncertain number arithmetic.

To start, let's first revive the old physics way to doing simple uncertainty quantification. If you have two numbers, \(x = a \pm b\), one might remember the rules like,

\[\alpha + a = (\alpha + a) \pm b\] \[\alpha a = \alpha a \pm |\alpha| b\]

Let's investigate this a bit more and see if we can develop an arithmetic, like dual numbers, to then propagate through whole programs. This idea comes from the arithmetic on normally distributed random variables. If we interpret \(x \sim N(a,b)\), i.e. a normally distributed random variable with mean \(a\) and standard deviation \(b\), then the distributions follow that:

\[\alpha + a \sim N(\alpha + a,b)\] \[\alpha a \sim N(\alpha a,|\alpha|b)\]

From here we can begin to expand to multiple variables. If \(f = Ax\) where \(x ~ N(\mu,\Sigma)\) is a multidimensional random variable, then

\[E[f] = A\mu\]

and

\[V[f] = A \Sigma A^T\]

Now take a nonlinear \(f(x)\). By a Taylor expansion we have that

\[f(x) = f_0 + Jx + \ldots\]

i.e. the linear approximation is \(f_0 + Jx\) where \(f_0 = f(\mu)\) and \(J\) is the Jacobian matrix. If we do a pushforward on the linear approximation, we receive

\[f(x) ~ N(f(\mu),J\Sigma J^T)\]

which gives the rules for the pushforward on any possible function through the linearization. But the linearization is the same as the forward differencing ones, meaning that we can augment existing tooling for forward-mode automatic differentiation to perform pushforwards of uncertain quantities. A library which does this is [[https://github.com/JuliaPhysics/Measurements.jl][Measurements.jl]]. Note that this library additionally tracks the correlations between each variable so that the second order terms are accurate.

*** Measurements.jl in Practice: Measurements on DifferentialEquations
:PROPERTIES:
:CUSTOM_ID: measurements.jl-in-practice-measurements-on-differentialequations
:END:
Since DifferentialEquations.jl takes in arbitrary number types, we can have it recompile to do the arithmetics of uncertainty propagation. For example, the following solves the pendulum of arbitrary amplitude with respect to uncertain parameters and initial conditions:

\[\ddot{\theta} + \frac{g}{L} \sin(\theta) = 0\]

#+begin_src julia
using OrdinaryDiffEq, Measurements
gaccel = 9.79 ± 0.02; # Gravitational constants
L = 1.00 ± 0.01; # Length of the pendulum

#Initial Conditions
u₀ = [0 ± 0, π / 3 ± 0.02] # Initial speed and initial angle
tspan = (0.0, 6.3)

#Define the problem
function simplependulum(du,u,p,t)
    θ  = u[1]
    dθ = u[2]
    du[1] = dθ
    du[2] = -(gaccel/L) * sin(θ)
end

#Pass to solvers
prob = ODEProblem(simplependulum, u₀, tspan)
sol = solve(prob, Tsit5(), reltol = 1e-6)

using Plots
plot(sol,plotdensity=100,vars=2)
#+end_src

From here it is clear that as the pendulum goes forward, the uncertainty grows since the exact period is unclear. Notice another nice feature is on display here: the [[http://docs.juliaplots.org/latest/recipes/][Plots.jl Recipe System]]. The plotting just worked because the recipes are a type-recursive system. The three steps were:

1. The DifferentialEquations solution recipe transformed the ODE solution into an array of measurement variables
2. The Measurements recipe transformed the measurement variables into an array of floats along with a series of error bars
3. This array of floats was recognized as a native format, and thus the plot was made.

Note that this idea of discretizing distributions and pushing them through a full calculation can be done with more accuracy by using things like orthogonal polynomial expansions. This is the /polynomial chaos expansion/ approach which we will not cover, but there is a [[https://github.com/timueh/PolyChaos.jl][PolyChaos.jl package]] one can explore.

** Quantifying Numerical Uncertainty with Intervals
:PROPERTIES:
:CUSTOM_ID: quantifying-numerical-uncertainty-with-intervals
:END:
While Measurements gives a sense of uncertainty quantification for unknown inputs, a different form of uncertainty quantification is that for floating point uncertainties. For example, when you calculate \(sin(2.3)\) on your computer, this has an error in the approximation, and what if we wanted to push these errors forward to get an interval which bounds the possible values given the numerical uncertainty? This is done via /interval arithmetic/.

For this we can use [[https://juliaintervals.github.io/IntervalArithmetic.jl/latest/][IntervalArithmetic.jl]]. The idea of interval arithmetic is to work rigorously on sets of real numbers, i.e.

\[[a,b] = \{x\in\mathbb{R} : a\leq x \leq b\}\]

We can construct an interval given the =interval= method:

#+begin_src julia
using IntervalArithmetic
x = interval(0.1,0.2)
#+end_src

or using the shorthand

#+begin_src julia
0.1..0.2
#+end_src

Here the operator catches the constants at compile time and notices that 0.1 and 0.2 cannot be exactly represented in floating point numbers, and thus on the left side it rounds down by one floating point number and on the right it rounds up by one floating point number to make sure the set rigorously contains the correct value.

From here, non-monotone functions can propagate intervals like:

#+begin_src julia;eval=false
[a, b]^2 := [a^2, b^2]  if 0 < a < b
          = [0, max(a^2, b^2)]  if a < 0 < b
          = [b^2, a^2] if a < b < 0
#+end_src

The rules can get fairly complicated and may need to be derived for each individual elementary function, but, just like automatic differentiation, recursion can be performed to get to a bottom of primitives which is known to then propagate forward the intervals.

Because this form of uncertainty quantification is rigorous, we can prove theorem on it. For example, let's say we want to show that

#+begin_src julia
h(x) = x^2 - 2
#+end_src

has no roots in \([3,4]\). Since these are rigorous bounds, it holds that

#+begin_src julia
h(3..4)
#+end_src

is a rigorous bound on the possible values, and thus it must not have a root in this interval.

*** Problems with Interval Arithmetic
:PROPERTIES:
:CUSTOM_ID: problems-with-interval-arithmetic
:END:
Interval arithmetic is nice... but it can have some issues. It's rigorous but it's also conservative, meaning that the intervals can be much larger than one would expect given the actual uncertainties seen in practice. One phenomena which causes this can be seen by looking at that pendulum:

#+begin_src julia
gaccel = 9.77..9.81; # Gravitational constants
L = 0.99..1.01; # Length of the pendulum

#Initial Conditions
u₀ = [0..0, ((π / 3)-0.02)..((π / 3)+0.02)] # Initial speed and initial angle
tspan = (0.0, 6.3)

#Define the problem
function simplependulum(du,u,p,t)
    θ  = u[1]
    dθ = u[2]
    du[1] = dθ
    du[2] = -(gaccel/L) * sin(θ)
end

#Pass to solvers
prob = ODEProblem(simplependulum, u₀, tspan)
sol = solve(prob, Tsit5(), adaptive=false, dt=0.1, reltol = 1e-6)
#+end_src

While we start out with reasonably small intervals, it turns out that every operation is calculating "what is the largest I could be? What is the smallest I could be?". Comparing these extremes at every operation means that, yes, by the end, given the uncertainty in the period, the solution lies in the interval \([-11642.4,11652.2]\), but that's not a particularly helpful estimate! This demonstrates the exponential explosion of interval estimates.

But note that part of why it got so large is because we started with "such large" intervals. If we only used this to measure the uncertainty of the floating point arithmetic, then the intervals are much better contained:

#+begin_src julia
gaccel = 9.8..9.8; # Gravitational constants
L = 1.0..1.0; # Length of the pendulum

#Initial Conditions
u₀ = [0..0, (π / 3)..(π / 3)] # Initial speed and initial angle
tspan = (0.0, 6.3)

#Define the problem
function simplependulum(du,u,p,t)
    θ  = u[1]
    dθ = u[2]
    du[1] = dθ
    du[2] = -(gaccel/L) * sin(θ)
end

#Pass to solvers
prob = ODEProblem(simplependulum, u₀, tspan)
sol = solve(prob, Vern9(), adaptive=false, dt=0.001, reltol = 1e-6)
#+end_src

** Contextual Uncertainty Quantification
:PROPERTIES:
:CUSTOM_ID: contextual-uncertainty-quantification
:END:
Those previous methods were non-contextual and worked directly through program modification. However, by not "clumping" interactions, uncertainty quantification can have overestimates like is seen with the interval growth. Thus, just like with reverse-mode AD, can we instead look for higher order uncertainty primitives on which to build such a system? When digging into reverse-mode AD, we saw that adjoint problems in engineering corresponded to the the reverse-mode rules for things like linear solve, eigenvalue problems, and the solution of ODEs. There does not seem to be a general analogue in the case of uncertainty quantification, but there is hope. Since this is a big enough field, people have found special cases where uncertainty can be quantified in interesting manners. Let's look specifically at ODEs.

** Quantifying Uncertainty in ODE Solves for Adaptivity
:PROPERTIES:
:CUSTOM_ID: quantifying-uncertainty-in-ode-solves-for-adaptivity
:END:
However, in some sense, adaptive numerical methods work by embedding a form of uncertainty quantification. Let's take a look at the Bogaki-Shampine method for solving ODEs:

Notice that there's a \(y_{n+1}\) and a \(z_{n+1}\) for two separate solutions for the next time step. It so happens that \(y\) is \(\mathcal{O}(\Delta t^3)\) while \(z\) is \(\mathcal{O}(\Delta t^2)\), meaning that \(E = z_{n+1} - y_{n+1}\) is a \(\mathcal{O}(\Delta t^2)\) estimate for the error in a given step, since the two must both be "\(\Delta t^2\) close enough" to the true solution. Similarly, when we looked at the Dormand-Prince method in our homework, the tableau:

#+caption: DP tableau
[[https://user-images.githubusercontent.com/1814174/70629597-3b5cd380-1bf8-11ea-8a16-07bb5bdc0c3a.PNG]]

had a second row as well, with the first being \(\mathcal{O}(n^5)\) and the second being \(\mathcal{O}(n^4)\). Thus these Runge-Kutta methods naturally have error estimators. In standard usage, they are compared to the tolerances, like:

\[q = \frac{E}{\text{reltol}\max(z_n,z_{n+1}) + \text{abstol}}\]

and when \(q<1\), the \(\Delta t\) gives an error larger than the tolerances and so the step is rejected, decreased, and tried again. In many cases, one may control the error proportionally to this error estimator, i.e. the next \(\Delta t\) is the product \(q \Delta t\).

That's all for adapting to a tolerance, but can we use this to propagate uncertainties? It turns out we can. This is known as the ProbInts method. Essentially, instead of an ODE, we can think of having solved a stochastic differential equation whose additive noise term of size which matches our error estimate. Specifically, adding a noise which is normally distributed with mean zero and standard deviation \((\Delta t)^{p}\), where \(p\) is the order of the adaptive error estimate (i.e. the order of the lower approximation), is an approximation to the possible values that could have occurred given the noise that was seen. By adding this at every step, we can then recover a distribution of possible solutions/trajectories.

*** ProbInts in Action
:PROPERTIES:
:CUSTOM_ID: probints-in-action
:END:
#+begin_src julia
using DiffEqUncertainty
function fitz(du,u,p,t)
  V,R = u
  a,b,c = p
  du[1] = c*(V - V^3/3 + R)
  du[2] = -(1/c)*(V -  a - b*R)
end
u0 = [-1.0;1.0]
tspan = (0.0,20.0)
p = (0.2,0.2,3.0)
prob = ODEProblem(fitz,u0,tspan,p)

cb = AdaptiveProbIntsUncertainty(5) # 5th order method
sol = solve(prob,Tsit5())
ensemble_prob = EnsembleProblem(prob)
sim = solve(ensemble_prob,Tsit5(),trajectories=100,callback=cb)
plot(sim,vars=(0,1),linealpha=0.4)
#+end_src

#+begin_src julia
cb = AdaptiveProbIntsUncertainty(5)
sol = solve(prob,Tsit5())
ensemble_prob = EnsembleProblem(prob)
sim = solve(ensemble_prob,Tsit5(),trajectories=100,callback=cb,abstol=1e-3,reltol=1e-1)
plot(sim,vars=(0,1),linealpha=0.4)
#+end_src

Notice that while an interval estimate would have grown to allow all extremes together, this form keeps the trajectories alive, allowing them to fall back to the mode, which decreases the true uncertainty. This is thus a good explanation as to why general methods will overestimate uncertainty.

** Adjoints of Uncertainty and the Koopman Operator
:PROPERTIES:
:CUSTOM_ID: adjoints-of-uncertainty-and-the-koopman-operator
:END:
Everything that we've demonstrated here so far can be thought of as "forward mode uncertainty quantification". For every example we have constructed a method such that, for a known probability distribution in =x=, we build the probability distribution of the output of the program, and then compute quantities from that. On a dynamical system this pushforward of a measure is denoted by the Frobenius-Perron operator. With a pushforward operator \(P\) and an initial uncertainty density \(f\), we can represent calculating the expected value of some cost function on the solution via:

\[\mathbb{E}[g(x)|X \sim Pf] = \int_{S(A)} P f(x) g(x) dx\]

where \(S\) is the program, i.e. \(S(A)\) is the total set of points by pushing every value of \(A\) through our program, and \(P f(x)\) is the pushforward operator applied to the probability distribution. What this means is that, to calculate the expectation on the output of our program, like to calculate the mean value of the ODE's solution given uncertainty in the parameters, we can pushforward the probability distribution to construct \(Pf\) and on this probability distribution calculate the expected value of some \(g\) cost function on the solution.

The problem, as seen earlier, is that pushing forward entire probability distributions is a fairly expensive process. We can instead think about doing the adjoint to this cost function, i.e. pulling back the cost function and computing it on the initial density. In terms of inner product notation, this would be doing:

\[\langle Pf,g \rangle = \langle f, Ug \rangle\]

meaning \(U\) is the adjoint operator to the pushforward \(P\). This operator is known as the Koopman operator. There are many properties one can use about the Koopman operator, one special property being it's a linear operator on the space of observables, but it also gives a nice expression for computing uncertainty expectations. Using the Koopman operator, we can rewrite the expectation as:

\[\mathbb{E}[g(x)|X \sim Pf] = \mathbb{E}[Ug(x)|X \sim f]\]

or perform the integral on the pullback of the cost function, i.e.

\[\mathbb{E}[g(x)|X \sim f] = \int_A Ug(x) f(x) dx\]

In images it looks like:

#+caption: Koopman vs FP
[[https://user-images.githubusercontent.com/1814174/102001466-a7b55b80-3cc0-11eb-9208-0f751fdca590.PNG]]

This expression gives us a fast way to compute expectations on the program output without having to compute the full uncertainty distribution on the output. This can thus be used for /optimization under uncertainty/, i.e. the optimization of loss functions with respect to expectations of the program's output under the assumption of given input uncertainty distributions. For more information, see [[https://arxiv.org/abs/2008.08737][The Koopman Expectation: An Operator Theoretic Method for Efficient Analysis and Optimization of Uncertain Hybrid Dynamical Systems]].
