:PROPERTIES:
:ID:       276580cc-0875-4123-afb0-7af1583390d8
:END:
#+TITLE: Approximate inference
#+FILETAGS: :literature:prml:hub:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

Evaluating the posterior distribution \(p(\mathbf{Z} \mid \mathbf{X})\) of latent variables \(\mathbf{Z}\) given observed data \(\mathbf{X}\) and computing expectations with respect to this distribution are key tasks in probabilistic models. This is crucial for algorithms like EM, where the expectation of the complete-data log likelihood is needed. Exact evaluation is often infeasible due to dimension of the latent space being prohibitively large or the complex form of the posterior distribution, making analytical solutions or numerical integration impractical. For discrete variables, summing over all possible hidden states can be exponentially large and computationally prohibitive. Therefore, one often needs to resort to approximations. Approximate inference methods can never generate exact results but in practice they generate results that are useful enough. Approximation schemes fall into two categories: deterministic and stochastic.

* Deterministic approximate inference
These schemes involve analytical approximations to the posterior distribution, making assumptions about it, for example, that it factorizes in a specific way or has a parametric form like a Gaussian. 

Nodes on deterministic approximate inference techniques:

1) [[id:8887bc33-8dbe-4eb7-8b48-2805a8192a58][Variational inference]]
2) [[id:6314241e-c495-40f3-911d-8ee714e0f3f6][Variational mixture of Gaussians]]
3) [[id:579475ee-d060-4134-a4c3-b85f0d154090][Variational linear regression]]
4) [[id:5ddc68ef-cdc5-4ee6-a503-5c0a22e7d4db][Variational message passing]]
5) [[id:218b5d87-e143-4868-945e-5961f9d6f042][Local variational methods]]
6) [[id:3483f000-f38f-4f01-b565-ab742aac6369][Variational logistic regression]]
7) [[id:f9d1cc72-c5f6-4634-bf7a-eab7287bbc5c][Expectation propagation]]

* Stochastic approximate inference
These schemes involve numerical approximations to the posterior distribution, often by conjuring a variety of sampling schemes.

Nodes on stochastic approximate inference techniques:

1) [[id:b2ff8620-2b4a-47cf-acce-34f9f98017a7][Monte Carlo methods]]
2) [[id:445a4a27-e6ae-4489-a94e-317da55da276][Rejection sampling]]
3) [[id:b249a035-6fe6-4386-9d85-27fa118a250f][Adaptive rejection sampling]]
4) [[id:29eafee0-ad1c-401c-b2c3-392d695ce391][Importance sampling]]
5) [[id:619002bf-c1e1-46aa-a152-e36e022956ca][Sampling-importance-resampling]]
6) [[id:b49da9ab-a297-4994-91f4-14ecf6ad34a2][Monte Carlo EM algorithm]]
7) [[id:045e74eb-0528-4ea6-94d9-41c3aa7d8440][Markov chain Monte Carlo]] (also see [[id:1fca5323-ea8f-4726-ad6c-72a5373adfd9][Markov chains]])
8) [[id:a16f32b2-f6c0-4022-be37-2a3c5677159c][Metropolis-Hastings algorithm]]
9) [[id:092fa14b-4820-488f-aa87-0631d3acee2f][Gibbs sampling]]
10) [[id:6df2d0d5-80ce-4bf1-b226-c80cf219216c][Slice sampling]]
11) [[id:a8c96306-f0f4-4192-aca6-d5a1024dde14][Hybrid Monte Carlo algorithm]]
12) [[id:b39465f1-fc0b-4207-8934-956b87d9139d][Confronting the partition function]] (see node links therein)
