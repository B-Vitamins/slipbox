:PROPERTIES:
:ID:       f5772738-f405-482b-9a1b-bb4c6a5f6ae0
:END:
#+TITLE: Information
#+FILETAGS: :problem: :SPOP: :statmech:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org
#+BEGIN: clocktable :maxlevel 2 :scope file :emphasize nil
#+CAPTION: Clock summary at [2024-04-14 Sun 05:50]
| Headline        | Time   |
|-----------------+--------|
| *Total time*    | *1:28* |
|-----------------+--------|
| 2.6 Information | 1:28   |
#+END
* 2.6 Information
:LOGBOOK:
CLOCK: [2024-04-12 Fri 22:30]--[2024-04-12 Fri 23:58] =>  1:28
:END:
*Consider the velocity of a gas particle in one dimension* \( (-\infty < v < \infty) \).
** 2.6.1
*Find the unbiased probability density* \( P_1(v) \), *subject only to the constraint that the average speed is* \( c \), *that is,* \( \langle |v| \rangle = c\).

We will maximize the entropy subject to the constraint of normalization and average speed, the constraints imposed using Lagrange multipliers. By definition

\begin{align*}
S \equiv -\langle\ln p\rangle=-\int_{-\infty}^{\infty} p_1(v) \ln p_1(v) d v +\underbrace{\lambda_1\left(1-\int_{-\infty}^{\infty} p_1(v) d v\right)}_{\text{PDF is normalized}} +\underbrace{\lambda_{2} \left(c-\int_{-\infty}^{\infty} p_1(v)|v| d v\right)}_{\text{average speed}}.
\end{align*}

\begin{align*}
\partial_{p_{1}} S &= -\ln p_1(v)-1-\lambda_1-\lambda_2|(v)|=0 \Rightarrow p_1(v)=\exp \left(-1-\lambda_1-\lambda_2|v|\right).
\end{align*}

To find \(\lambda_{1}\) and \(\lambda_{2}\), we use the constraints

\begin{align*}
\int_{-\infty}^{\infty} p_1(v) d v=1 &\Rightarrow \int_{-\infty}^{\infty} \exp \left(-1-\lambda_1-\lambda_2|v|\right) d v=1 \\
& \Rightarrow 2 \exp \left(-1-\lambda_1\right) \int_0^{\infty} \exp \left(-\lambda_2 v\right) d v=1 \text {. } \\
& \Rightarrow \exp \left(-1-\lambda_1\right) / \lambda_2=1 / 2. \\
\end{align*}

\begin{align*}
\int_{-\infty}^{\infty} p_1(v)|v| d v= c &= 2 \exp \big(-1-\lambda_1\big) \int_0^{\infty} v p_1(v) d v \\
& = 2 \exp \big(-1-\lambda_1\big) \int_0^{\infty} v \exp \big(-\lambda_2 \, v\big) d v \\
& =2 \exp \big(-1-\lambda_1\big)\bigg[\exp \big(-\lambda_2 \, v\big)\bigg(\frac{-\lambda_2 \, v-1}{\lambda_2^2}\bigg)\bigg]_0^{\infty} \\
& = 2 \exp \big(-1-\lambda_1\big)\big(-1/\lambda_2^2\big) \\
& \Rightarrow c = 1 / \lambda_{2}.
\end{align*}

Substituting in the expression for \(p_{1} (v)\), we recover the PDF

\begin{align*}
p_1(v)=\frac{1}{2 c} \exp \bigg(\frac{-|v|}{c}\bigg).
\end{align*}

** 2.6.2
*Now find the probability density* \( P_2(v) \), *given only the constraint of average kinetic energy,* \( \langle mv^2/2 \rangle = mc^2/2\).*

Once again, we will maximize the entropy subject to the constraint of normalization and average kinetic energy

\begin{align*}
S=-\langle\ln p\rangle=-\int_{-\infty}^{\infty} p_2(v) \ln p_2(v) d v +\underbrace{\lambda_1\bigg(1-\int_{-\infty}^{\infty} p_2(v) d v\bigg)}_{\text{PDF is normalized}} +\underbrace{\lambda_{2} \bigg(\frac{m c^2}{2} - \int_{-\infty}^{\infty} p_2(v) \bigg(\frac{m v^{2}}{2} \bigg) d v\bigg)}_{\text{average kinetic energy}}.
\end{align*}
\[
\partial_{p_{2}} S = - \ln p_2(v) - 1 - \lambda_1 - \lambda_2 \frac{m v^2}{2} = 0 \Rightarrow p_2(v) = \exp \left(-1 - \lambda_1 - \lambda_2 \frac{m v^2}{2}\right).
\]

As before, we use the constraints for normalization and average kinetic energy to find \( \lambda_1 \) and \( \lambda_2 \).

\[
\int_{-\infty}^{\infty} \exp \left(-1 - \lambda_1 - \lambda_2 \frac{m v^2}{2}\right) dv = 1 = \sqrt{\frac{2 \pi}{\lambda_2 m}} \exp(-1 - \lambda_1).
\]

\begin{align*}
\exp \left(-1 - \lambda_1 \right) \int_{-\infty}^{\infty} & v^2 \exp \left(- \lambda_2 \frac{m v^2}{2}\right) dv \\
= c^2 &= \exp(-1 - \lambda_1) \sqrt{\frac{2 \pi}{\lambda_2^3 m^3}} = \frac{1}{m \lambda_{2}}\\
\end{align*}

Substituting in the expression for \(p_{2} (v)\), we recover the PDF

\begin{align*}
p_{2}(v)=\frac{1}{\sqrt{2 \pi c^2}} \exp \left(-\frac{v^2}{2 c^2}\right) .
\end{align*}
** 2.6.3
*Which of the above statements provides more information on the velocity? Quantify the difference in information in terms of* \( I_2 - I_1 \equiv (\langle \ln p_2 \rangle - \langle \ln p_1 \rangle)/\ln 2\).

For two PDFs on the same support

\begin{align*}
I_1+S_1 / \ln 2=I_2+S_2 / \ln 2 \Rightarrow & I_1-I_2=\frac{1}{\ln 2}\left(S_2-S_1\right),
\end{align*}

i.e., the difference in information is proportional to the difference in entropy. Let us calculate the entropy of each of the above distributions

\begin{align*}
S_1 &=-\left\langle\ln p_1\right\rangle=-\int_{-\infty}^{\infty} p_1(v) \ln [p(v)] d v \\
& =-\int_{-\infty}^{\infty} \frac{1}{2 c} \exp \left(-\frac{|v|}{c}\right)\left[-\ln (2 c)-\frac{|v|}{c}\right] d v \\
& =\int_{-\infty}^{\infty} \frac{\ln (2 c)}{2 c} \exp \left(\frac{-|v|}{c}\right) d v+\int_{-\infty}^{\infty} \frac{1}{2 c} \frac{|v|}{c} \exp \left(\frac{-|v|}{c}\right) d v \\
& =\frac{\ln (2 c)}{c} \int_0^{\infty} \exp \left(\frac{-|v|}{c}\right) d v+\frac{1}{c} \int_0^{\infty} \frac{|v|}{c} \exp \left(-\frac{|v|}{c}\right) d v \\
& =\frac{\ln (2 c)}{c} \times c + \frac{1}{c^2}\left[\exp \left(\frac{-v}{c}\right)\left(\frac{(-1 / c) v-1}{(-1 / c)^2}\right)\right]_0^{\infty} \\
& =\ln (2 c)+1=\ln 2+\ln c+1,
\end{align*}

\begin{align*}
S_2 &=-\left\langle\ln p_2\right\rangle=-\int_{-\infty}^{\infty} p_2(v) \ln \left[p_2(v)\right] d v \\
& =-\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} \cdot \frac{1}{c} \cdot \exp \left(\frac{-v^2}{2 c^2}\right) \ln \left(\frac{1}{\sqrt{2 \pi c^2}} \cdot \exp \bigg[\frac{-v^2}{2 c^2}\bigg]\right) d v \\
& =-\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi c^2}} \exp \left(\frac{-v^2}{2 c^2}\right)\left[-\ln \left(\sqrt{2 \pi c^2}\right)-\frac{v^2}{2 c^2}\right] d v \\
& =\frac{\ln \big(\sqrt{2 \pi c^2}\big)}{\sqrt{2 \pi c^2}} \int_{-\infty}^{\infty} \exp \left(\frac{-v^2}{2 c^2}\right) d v+\int_{-\infty}^{\infty}\left(\frac{v^2}{2 c^2}\right)\left(\frac{1}{\sqrt{2 c^2}}\right) \exp \left(\frac{-v^2}{2 c^2}\right) d v \\
& =\frac{\ln \big(\sqrt{2 \pi c^2}\big)}{\sqrt{2 \pi c^2}} \sqrt{\frac{2 \pi}{1 / c^2}}+\frac{1}{2 c^3 \sqrt{2 \pi}} \sqrt{\frac{\pi}{\left(1 / 2 c^2\right)^3}} \cdot \frac{1}{2} \\
& =\frac{1}{2}+\frac{1}{2} \ln (2 \pi)+\ln (c).
\end{align*}

Now, the difference in information of \(p_{1} (v)\) and \(p_{2} (v)\) is

\begin{align*}
I_1-I_2 = \frac{1}{\ln 2}\left(S_2-S_1\right)= \frac{1}{2 \ln (2)}[\ln (\pi)-\ln (2)-1] \approx -0.3956.
\end{align*}

Thus, the constraint of constant energy provider \(0.3956\) more bits of information. Why is this? Because revealing the average value of \(v^2\) is the same as revealing the first two moments of the distributions, while revealing the average speed \(|v|\) is the same as only revealing the first moment. This reflects in the distributions we ultimately construct, starting from the uniform distribution. The constraint of average speed yields a Laplace PDF while the constraint of average kinetic energy yields a Gaussian PDF. The entropy \(S\) is a measure of the PDF's /dispersion/. The Gaussian PDF has less entropy than the Laplace PDF because it falls faster as we move away from the origin. The Laplace PDF's slower fall means that it is more /disperse/, i.e., it has more entropy. All probability distributions satisfy a conservation law: sum of its information and entropy is a constant. Because the Gaussian PDF has less entropy than the Laplace PDF, it has more information compared to the Laplace PDF; approximately 0.3956 bits more information.