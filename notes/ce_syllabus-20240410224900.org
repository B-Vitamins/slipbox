:PROPERTIES:
:ID:       ecf10cac-8fe8-417c-84de-c4a85d17c464
:END:
#+TITLE: CE syllabus
#+FILETAGS: :fleeting: :phd:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org
#+BEGIN: clocktable :maxlevel 2 :scope nil :emphasize nil
#+CAPTION: Clock summary at [2024-06-08 Sat 11:07]
| Headline                                  | Time   |      |
|-------------------------------------------+--------+------|
| *Total time*                              | *6:59* |      |
|-------------------------------------------+--------+------|
| PH 354: Computational Physics             | 6:59   |      |
| \_  Machine Representation, Precision,... |        | 6:36 |
| \_  Roots of Equations                    |        | 0:23 |
#+END
* PH 202: Statistical Mechanics
** Probability Theory
** Fundamental Postulate, Phase Space
** Micro-Canonical Ensemble, Connection with Thermodynamics
** Canonical Ensemble
** Classical Ideal Gas
** Harmonic Oscillators
** Ising Model and Paramagnetism
** Thermodynamic Potentials
** Legendre Transformations
** Monte Carlo Methods

* E0 270: Machine Learning
** Learning as Optimization, Linear Regression
** Probabilistic View: ML and MAP Estimates
** Logistic Regression: Gradient Descent, Stochastic Gradient Methods
** Perceptron, and Perceptron Convergence Theorem
** Feedforward Neural Networks, Backpropagation Algorithm
** Undirected Graphical Models, Markov Random Fields
** Introduction to MCMC and Gibbs Sampling
** Restricted Boltzmann Machine
** EM Algorithm
** Mixture Models

* PH 354: Computational Physics
** Machine Representation, Precision, and Errors
:LOGBOOK:
CLOCK: [2024-06-07 Fri 18:02]--[2024-06-07 Fri 22:03] =>  4:01
CLOCK: [2024-06-07 Fri 14:09]--[2024-06-07 Fri 15:43] =>  1:34
CLOCK: [2024-06-07 Fri 12:41]--[2024-06-07 Fri 13:29] =>  0:48
CLOCK: [2024-06-07 Fri 12:25]--[2024-06-07 Fri 12:38] =>  0:13
:END:
*** Machine representation
A computer represents numbers in a binary form. Hence, every computer has a limit how small/large a number can be.

#+NAME: Word
#+begin_definition latex
In computing, a word is the natural unit of data used by a particular processor design. A word is a fixed-sized datum handled as a unit by the instruction set or the hardware of the processor.
#+end_definition

#+NAME: Word length
#+begin_definition latex
The word length is the size of a word (in bits or bytes).
#+end_definition

+ The majority of the registers in a processor are usually word-sized.
+ The largest datum that can be transferred to and from the working memory in a single operation is a word in many (not all) architectures.
+ The largest possible address size, used to designate a location in memory, is typically a word length.
+ Most common architecture have a word length of 4 bytes (32 bits) or 8 bytes (64 bits).
**** Integer representation
- Integers are represented exactly on a computer.
- If using /fixed-precision arithmetic/, the range of a integer depends /word length/ of the processor and the /type/ of the integer as defined by the programming language:
  + 32 bit signed :: The range is from \( - 2^{31} + 1 \) to \( 2^{31} - 1 \).
  + 32 bit unsigned :: The range is from \( - 2^{32} + 1 \) to \( 2^{32} - 1 \).
  + 64 bit signed :: The range is from \( - 2^{63} + 1 \) to \( 2^{63} - 1 \).
  + 64 bit unsigned :: The range is from \( - 2^{64} + 1 \) to \( 2^{64} - 1 \).
- If using /arbitrary-precision arithmetic/, the range of an integer depends only the memory of the host system. Arbitrary-precision arithmetic is available out of the box in languages like Lisp, Python, Perl, Haskell, Ruby and Raku.
**** Floating-point representation

+ To recover a number from its floating-point representation use the formula

  \[
  \frac{s}{b^{(p-1)}} \times b^{e}
  \]

where \( s \) is the /significand/, \( p \) is the /precision/ (the number of digits in the significand), \( b \) is the /base/, and \( e \) is the /exponent/.

***** Single precision

#+begin_src latex :file ~/.local/images/binary32.png :results file graphics
\begin{tikzpicture}[scale=0.5, transform shape]
    % Define font sizes
    \newcommand{\largefont}{\fontsize{20}{22}\selectfont}

    % Draw Sign bit
    \draw[fill=gray!20] (2,0) rectangle (2.5,0.5);
    \node at (2.25, 1.5) {\largefont Sign};

    % Draw Exponent bits
    \foreach \i in {1,...,8}
        \draw[fill=green!20] (5.0 + \i/2,0) rectangle (5.0 + \i/2+0.5,0.5);
    \node at (7.5, 1.5) {\largefont Exponent};

    % Draw Mantissa bits
    \foreach \i in {9,...,31}
        \draw[fill=red!20] (8.0 + \i/2,0) rectangle (8.0 + \i/2+0.5,0.5);
    \node at (18.0, 1.5) {\largefont Significand};

    % Label bit sections
    \node[below] at (2.25, -0.5) {\largefont 1 bit};
    \node[below] at (7.5, -0.5) {\largefont 8 bits};
    \node[below] at (18.0, -0.5) {\largefont 23 bits};

\end{tikzpicture}
#+end_src

#+RESULTS:
[[file:~/.local/images/binary32.png]]

+ Range of exponent :: \([-127,\,127]\) (\(2^{127} \sim 10^{+38}\))
+ Single precision :: \(6-7\) decimal places (\(1 / 2^{23} \sim 10^{-7}\))
+ Range max :: \(\pm 3.4 \times 10^{38}\).
+ Range min :: \(\pm 1.4 \times 10^{-45}\).

#+begin_src latex :file ~/.local/images/binary32-overflow-underflow.png :results file graphics
\begin{tikzpicture}
    % Draw the number line
    \draw[thick] (-8,0) -- (8,0);

    % Overflow and Underflow regions
    \fill[red!20] (-8, -0.5) rectangle (-5.5, 0.5);
    \fill[red!20] (5.5, -0.5) rectangle (8, 0.5);
    \fill[red!20] (-1, 0.5) rectangle (1, 1.5);

    % Overflow and Underflow labels
    \node[text=black] at (-6.75, 0) {\large\textbf{Overflow}};
    \node[text=black] at (6.75, 0) {\large\textbf{Overflow}};
    \node[text=black] at (0, 1.0) {\large\textbf{Underflow}};

    % Zero mark
    \draw[thick] (0,-0.2) -- (0,0.2);
    \node[below] at (0, -0.5) {\large 0};

    % Exponent labels in green boxes
    \node[fill=green!20, inner sep=2pt, text=black] at (-5.5, -1) {\large $-10^{38}$};
    \node[fill=green!20, inner sep=2pt, text=black] at (-1, -1) {\large $-10^{-45}$};
    \node[fill=green!20, inner sep=2pt, text=black] at (1, -1) {\large $10^{-45}$};
    \node[fill=green!20, inner sep=2pt, text=black] at (5.5, -1) {\large $10^{38}$};

    % Draw lines to separate regions
    \draw[thick] (-5.5, -0.5) -- (-5.5, 0.5);
    \draw[thick] (5.5, -0.5) -- (5.5, 0.5);
    \draw[thick] (-1, -0.5) -- (-1, 0.5);
    \draw[thick] (1, -0.5) -- (1, 0.5);
\end{tikzpicture}
#+end_src

#+RESULTS:
[[file:~/.local/images/binary32-overflow-underflow.png]]


+ Example: \(152,853.5047\)
  + Significand :: 1,528,535,047
  + Precision :: 10
  + Base :: 10
  + Exponent :: 5
  + In scientific notation: \(1.528535047 \times 10^{5}\).

+ The numbers that can be represented are *not* /uniformly spaced/; the difference between two consecutive numbers that can be represented varies with their exponent.

#+ATTR_HTML: :width 800px
[[file:~/.local/images/non-compactness-fp-representation.png]]
#+CAPTION: Single-precision floating-point numbers on a number line: the green lines mark values that can be represented.

#+ATTR_HTML: :width 800px
[[file:~/.local/images/non-compactness-fp-representation-2.png]]
#+CAPTION: Augmented version above showing both signs of representable values

****** Example: Bohr's radius
+ In it common to encounter problems in which naive application of single precision floating-point arithmetic is inadequate. Consider the formula for Bohr's radius

\begin{align*}
a_0=\frac{4 \pi \epsilon_0 \hbar^2}{m_e e^2}
\end{align*}

where

\begin{align*}
\begin{gathered}
\epsilon_0=8.85 \times 10^{-12} \mathrm{C}^2 / \mathrm{N} / \mathrm{m}^2 \\
\hbar=6.63 \times 10^{-34} / 2 \pi \mathrm{J} \mathrm{s} \\
m_e=9.11 \times 10^{-31} \mathrm{Kg} \\
e=1.60 \times 10^{-19} \mathrm{C}
\end{gathered}
\end{align*}

+ Numerator is: \(1.24 \times 10^{-78}\) and Denominator is: \(2.33 \times 10^{-68}\).
+ To solve the problem of representation, we can:
  + Restructure the equation,
  + Change units - work in atomic units where all these quantites are \(\mathcal{O}(1)\),
  + Increase precision.

***** Double precision

#+begin_src latex :file ~/.local/images/binary64.png :results file graphics
\begin{tikzpicture}[scale=0.5, transform shape]
    % Define font sizes
    \newcommand{\largefont}{\fontsize{20}{22}\selectfont}

    % Draw Sign bit
    \draw[fill=gray!20] (2,0) rectangle (2.5,0.5);
    \node at (2.25, 1.5) {\largefont Sign};

    % Draw Exponent bits
    \foreach \i in {1,...,11}
        \draw[fill=green!20] (5.0 + \i/2,0) rectangle (5.0 + \i/2+0.5,0.5);
    \node at (8.0, 1.5) {\largefont Exponent};

    % Draw Mantissa bits
    \foreach \i in {9,...,52}
        \draw[fill=red!20] (8.0 + \i/2,0) rectangle (8.0 + \i/2+0.5,0.5);
    \node at (25.0, 1.5) {\largefont Significand};

    % Label bit sections
    \node[below] at (2.25, -0.5) {\largefont 1 bit};
    \node[below] at (8.0, -0.5) {\largefont 11 bits};
    \node[below] at (25.0, -0.5) {\largefont 52 bits};
\end{tikzpicture}
#+end_src

#+RESULTS:
[[file:~/.local/images/binary64.png]]

+ Range of exponent :: \([-1023,\,1023]\) (\(2^{1023} \sim 10^{+308}\))
+ Double precision :: \(15-16\) decimal places (\(1 / 2^{52} \sim 1.2 \times 10^{-15}\))
+ Range max :: \(\pm 1.78 \times 10^{308}\)
+ Range min :: \(\pm 4.94 \times 10^{-324}\)

#+begin_src latex :file ~/.local/images/binary64-overflow-underflow.png :results file graphics
\begin{tikzpicture}
    % Draw the number line
    \draw[thick] (-8,0) -- (8,0);

    % Overflow and Underflow regions
    \fill[red!20] (-8, -0.5) rectangle (-5.5, 0.5);
    \fill[red!20] (5.5, -0.5) rectangle (8, 0.5);
    \fill[red!20] (-1, 0.5) rectangle (1, 1.5);

    % Overflow and Underflow labels
    \node[text=black] at (-6.75, 0) {\large\textbf{Overflow}};
    \node[text=black] at (6.75, 0) {\large\textbf{Overflow}};
    \node[text=black] at (0, 1.0) {\large\textbf{Underflow}};

    % Zero mark
    \draw[thick] (0,-0.2) -- (0,0.2);
    \node[below] at (0, -0.5) {\large 0};

    % Exponent labels in green boxes
    \node[fill=green!20, inner sep=2pt, text=black] at (-5.5, -1) {\large $-10^{308}$};
    \node[fill=green!20, inner sep=2pt, text=black] at (-1, -1) {\large $-10^{-324}$};
    \node[fill=green!20, inner sep=2pt, text=black] at (1, -1) {\large $10^{-324}$};
    \node[fill=green!20, inner sep=2pt, text=black] at (5.5, -1) {\large $10^{308}$};

    % Draw lines to separate regions
    \draw[thick] (-5.5, -0.5) -- (-5.5, 0.5);
    \draw[thick] (5.5, -0.5) -- (5.5, 0.5);
    \draw[thick] (-1, -0.5) -- (-1, 0.5);
    \draw[thick] (1, -0.5) -- (1, 0.5);
\end{tikzpicture}
#+end_src

#+RESULTS:
[[file:~/.local/images/binary64-overflow-underflow.png]]


+ In python, negative overflow set to -inf, positive to +inf, and underflow is set to 0.

*** Machine precision
#+NAME: Machine precision
#+begin_definition latex
The machine epsilon, denoted \(\epsilon_{\text {mach }}\), is the smallest number \(\epsilon\) such that \(f l(1+\epsilon)>1\). Thus, \(f l(1+\delta)=f l(1)=1\) whenever \(|\delta|<\epsilon_{\text {mach }}\).

It is the smallest difference between two numbers that the computer recognizes.
#+end_definition

#+NAME: Machine epsilon
#+begin_definition latex
The machine epsilon, denoted \(\epsilon_{\text {mach}}\), is the maximum possible absolute relative error in representing a nonzero real number \(x\) in a floating point number system.

\begin{align*}
\epsilon_{\text {mach }}=\max _x \frac{|x-f l(x)|}{|x|}
\end{align*}
#+end_definition

+ Machine epsilon can be used to measure the level of round-off error in the floating-point number system.

#+begin_src python :results output
import numpy as np

def machineEpsilon(func=float):
    machine_epsilon = func(1)
    while func(1) + func(machine_epsilon) != func(1):
        machine_epsilon_last = machine_epsilon
        machine_epsilon = func(machine_epsilon) / func(2)
    return machine_epsilon_last

# Calculate machine epsilon for different float types
float_eps = machineEpsilon(float)
np_float16_eps = machineEpsilon(np.float16)
np_float32_eps = machineEpsilon(np.float32)
np_float64_eps = machineEpsilon(np.float64)
np_longdouble_eps = machineEpsilon(np.longdouble)

# Print the results with appropriate precision
print(f"float: {float_eps:.16g}")
print(f"np.float16: {np_float16_eps:.4g}")
print(f"np.float32: {np_float32_eps:.7g}")
print(f"np.float64: {np_float64_eps:.16g}")
print(f"np.longdouble: {np_longdouble_eps:.19g}")
#+end_src

#+RESULTS:
: float: 2.220446049250313e-16
: np.float16: 0.0009766
: np.float32: 1.192093e-07
: np.float64: 2.220446049250313e-16
: np.longdouble: 1.084202172485504434e-19

*** Errors
**** Round-off errors

#+NAME: Round-off error
#+begin_definition latex
The difference between the true value of the number and its value on the computer is called round-off error.
#+end_definition

+ Any floating-point number with significant figures greater than the precision of its representation is subject to round-off errors.
***** Round-off error on a number behaves similarly to measurement error in a laboratory experiment

#+NAME: Error constant
#+begin_definition latex
Assume that the error is a uniformly distributed random number with standard deviation \(\sigma=C x\), where \( C \) is a constant that depends on the floating-point representation under use.
#+end_definition

+ Typically, \( C \simeq b^{-p} \) where \( b \) is the base and \( p \) is the precision. Using base-10, \(C \simeq 10^{-16}\) for double precision floating-point representation.

+ When quoting the error on a calculation we typically give the value of the standard deviation \(\sigma\). (We can't give the value of the error \(\epsilon\) itself, since we don't know it-if we did, then we could calculate \(x+\epsilon\) and recover the exact value for the quantity of interest, so there would in effect be no error in the calculation at all.)
+ In many ways the rounding error on a number behaves similarly to measurement error in a laboratory experiment, and the rules for combining errors are the same.

****** Combining rounding error (addition and subtraction)
Let \(x_1\) and \(x_2\) be random variables with standard deviations \(\sigma_1\) and \(\sigma_2\) respectively. The variance \(\sigma^2\) of the random variable \( x_1 + x_2 \) or \( x_1 - x_2 \) is given by

\begin{align*}
\sigma^2=\sigma_1^2+\sigma_2^2 \Longrightarrow \sigma=\sqrt{\sigma_1^2+\sigma_2^2}
\end{align*}

Since \(\sigma_1=C x_1\) and \(\sigma_2=C x_2\)

\begin{align*}
\sigma=\sqrt{C^2 x_1^2+C^2 x_2^2}=C \sqrt{x_1^2+x_2^2}.
\end{align*}

The result trivially generalizes to the sum of \(N\) numbers \(x_1 \ldots x_N\) with errors having standard deviation \(\sigma_i=C x_i\):

\begin{align*}
\sigma^2=\sum_{i=1}^N \sigma_i^2=\sum_{i=1}^N C^2 x_i^2=C^2 N \overline{x^2}
\end{align*}

where \(\overline{x^2}\) is the mean-square value of \(x\). Thus the standard deviation on the final result is

\begin{align*}
\sigma=C \sqrt{N \overline{x^2}}.
\end{align*}

The standard deviation behaves as \( \sigma \sim \sqrt{N} \).

The relative error on \(\sum_i x_i\) is given by

\begin{align*}
\frac{\sigma}{\sum_i x_i}=\frac{C \sqrt{N \overline{x^2}}}{N \bar{x}}=\frac{C}{\sqrt{N}} \frac{\sqrt{\overline{x^2}}}{\bar{x}},
\end{align*}

where \(\bar{x}=N^{-1} \sum_i x_i\) is the mean value of \(x\).

The relative error behaves as \( \sigma \sim \frac{1}{\sqrt{N}} \).

****** Combining rounding error (multiplication and division)
Let \(x_1\) and \(x_2\) be random variables with standard deviations \(\sigma_1\) and \(\sigma_2\) respectively. Suppose that \( x_2 \neq 0 \). The variance \(\sigma^2\) of the random variable \( x_1 \cdot x_2 \) or \( x_1 / x_2 \) is given by

\begin{align*}
\frac{\sigma^2}{x^2}=\frac{\sigma_1^2}{x_1^2}+\frac{\sigma_2^2}{x_2^2} .
\end{align*}

Since \(\sigma_1=C x_1\) and \(\sigma_2=C x_2\)

\begin{align*}
\sigma= \sqrt{x^2 \, (C^2 + C^2)} = \sqrt{2} \, C \, x.
\end{align*}

***** Round-off error leads to loss of significant digits
+ If repeated arithmetic operations are performed on two numbers represented by double precision floating-points and if a number of these operations introduce rounding errors, then there is an erosion of significant figures, the least significant digits being eroded first.

+ The loss of significant digits can occur in so many ways that it defies useful classification and lack systematic cures!

+ The average accumulated multiplication error after \(\mathrm{N}\) multiplications is \(\sqrt{N} \epsilon_{\text{mach}}\), where \(  \)\( \epsilon_{\text{mach}} \) is the machine epsilon.
***** A tiny round-off error can grow rapidly if the algorithm being used is not numerically stable
+ Sometimes the problem is not round-off errors but numerical stability of the algorithm. Even tiny round-off errors grow rapidly if algorithm is not numerically stable.
****** Example 1
Consider

\begin{align*}
& x=1000000000000000.0 \\
& y=1000000000000001.2345678901234
\end{align*}

The true difference \( y-x \) is \(1.2345678901234\). Since the number of significant digits in \( y \) is \(29 \) but the precision of double precision floating-point representation is \( 16 \), the computer rounds \( y \) so that

\begin{align*}
& x=1000000000000000.0 \\
& y=1000000000000001.2
\end{align*}

The value of \( y - x \) on the computer is thus \(y-x=1.2\). 

So instead of 16 significant figures we only have 2 significant figures.
****** Example 2
+ If the difference between two numbers is very small, comparable with the error on the numbers, i.e., with the accuracy of the computer, then the fractional error can become large, leading to an erosion of significant figures. Here is an example showing a dramatic erosion. Consider the numbers

\begin{align*}
x=1, \quad y=1+10^{-14} \sqrt{2}.
\end{align*}

Trivially we see that

\begin{align*}
10^{14} \, (y-x)=\sqrt{2}.
\end{align*}

#+begin_src python :results output
from math import sqrt
x = 1.0
y = 1.0 + (1e-14)*sqrt(2)
print((1e14)*(y-x))
print(sqrt(2))
#+end_src

#+RESULTS:
: 1.4210854715202004
: 1.4142135623730951

+ Only the first digit to the right of the decimal point is significant. The rest is garbage.
****** Example 3
Here's an example that illustrates how numerical instability can introduce round-off error.

Suppose we want to calculate the series \(a_n=\phi^n n=0,1,2 \ldots\) where \(\phi\) is the golden ratio:

\begin{align*}
\phi=\frac{\sqrt{5}-1}{2}
\end{align*}

We will compare two distinct methods:

+ Method 1

\begin{align*}
a_n = a_{n-1} \cdot \phi, \quad a_0 = 1.
\end{align*}

  + Requires only multiplication.
  + If the initial \(\phi=\phi_c+\epsilon_m\), the error in \(\phi^n\) is \(\sqrt{n} \epsilon_m\).

+ Method 2

\begin{align*}
a_n = a_{n-2} - a_{n-1}, \quad a_0 = 1, \, a_1 = \phi.
\end{align*}

  + Relies on \(\phi\) being the root of the qudratic equation: \(\phi^2+\phi-1=0\).
  + If you iterate the above equation, you can get the algorithm. If you simplify each term, you see Fibonacci numbers: \(f_n\).
- Then, \(\phi^n=(-1)^{n-1} f_n \phi+(-1)^n f_{n-1}\).
- Now: \(\phi^n=(-1)^{n-1} f_n\left(\phi_c+\epsilon_m\right)+(-1)^n f_{n-1}\)
- Error in \(\phi^n\) is \(f_n \epsilon_m\).
- When \(\phi_c^n \sim f_n \epsilon_m\), the algorithm fails!

Thus Method 1 is stable, while Method 2 is not.

**** Truncation errors

#+NAME: Truncation error
#+begin_definition latex
An error caused by approximating a mathematical process.
#+end_definition

+ Truncation error is better controlled, e.g., by choosing a more accurate method, than round off error which is set by the precision of the floating-point representation being used.

***** Infinite series
A summation series for \(e^x\) is given by an infinite series such as

\begin{align*}
e^x=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+\cdots
\end{align*}

In reality, we can only use a finite number of these terms as it would take an infinite amount of computational time to make use of all of them. So let's suppose we use only three terms of the series, then

\begin{align*}
e^x \approx 1+x+\frac{x^2}{2!}
\end{align*}

In this case, the truncation error is \(\frac{x^3}{3!}+\frac{x^4}{4!}+\cdots\)

****** Example
*Given the following infinite series, find the truncation error for* \(x=0.75\) *if only the first three terms of the series are used.*

\begin{align*}
S=1+x+x^2+x^3+\cdots, \quad|x|<1
\end{align*}

Using only first three terms of the series gives

\begin{align*}
S_3 & =(1+x+x^2)_{x=0.75} \\
& =1+0.75+(0.75)^2 \\
& =2.3125
\end{align*}

The sum of an infinite geometrical series

\begin{align*}
S=a+a r+a r^2+a r^3+\cdots, r<1
\end{align*}

is given by

\begin{align*}
S=\frac{a}{1-r}
\end{align*}

For our series, \(a=1\) and \(r=0.75\), to give

\begin{align*}
S=\frac{1}{1-0.75}=4
\end{align*}

The truncation error hence is

\begin{align*}
\mathrm{TE}=4-2.3125=1.6875
\end{align*}

***** Differentiation
+ Consider the approximation

\begin{align*}
f^{\prime}(x)=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h} \approx \frac{f(x+h)-f(x)}{h} \quad h > 0
\end{align*}

+ The error caused by choosing \(h\) to be finite is a truncation error in the mathematical process of differentiation.
****** Example
*Find the truncation in calculating the first derivative of* \(f(x)=5 x^3\) at \(x=7\) *using a step size of* \(h=0.25\).

The first derivative of \(f(x)=5 x^3\) is

\begin{align*}
f^{\prime}(x) & =15 x^2,
\end{align*}

and at \( x = 7 \),

\begin{align*}
f^{\prime}(7) & =735.
\end{align*}

The approximate value is given by

\begin{align*}
f^{\prime}(7)=\frac{f(7+0.25)-f(7)}{0.25}=761.5625
\end{align*}

The truncation error is hence

\begin{align*}
\mathrm{TE}=735-761.5625=-26.5625
\end{align*}

***** Integration
+ Consider the approximation

\begin{align*}
\int_a^b f(x) d x= \lim_{N \to \infty} \sum_{i=1}^N f\left(x_i^*\right) \Delta x_i \approx \sum_{i=1}^N f\left(x_i^*\right) \Delta x_i, \quad N < \infty
\end{align*}

where \(\Delta x_i=x_i-x_{i-1} = (b-a)/ N\) and \(x_i^* \in\left[x_{i-1}, \, x_i\right]\).

+ The error caused by choosing \(N\) to be finite is a truncation error in the mathematical process of integration.

+ Consider the approximation

\begin{align*}
\int_0^{\infty} f(x) d x = \lim_{L \to \infty} \int_0^{L} f(x) d x  \approx \int_0^L f(x) d x, \quad L < \infty
\end{align*}

+ The error caused by choosing \(L\) to be finite is a truncation error in the mathematical process of integration.

+ In some cases, a change of variables can help avoid this kind of truncation error provided it does not introduce any singularities.

+ In some cases the "tails" can be evaluated analytically which also helps avoid truncation errors of this kind (see Example 2).

+ Note that this approximation (whether a change of variables or analytic evaluation of the tails eliminates the truncation error or not) still requires the evaluation of \( \int_0^L f(x) d x \) which will introduce truncation errors due the approximation

\begin{align*}
\int_a^b f(x) d x= \lim_{N \to \infty} \sum_{i=1}^N f\left(x_i^*\right) \Delta x_i \approx \sum_{i=1}^N f\left(x_i^*\right) \Delta x_i, \quad N < \infty.
\end{align*}

****** Example 1
*For the integral*

\begin{align*}
\int_3^9 x^2 d x
\end{align*}

*find the truncation error if a two-segment left-hand Riemann sum is used with equal width of segments.*

We have the exact value as

\begin{align*}
\int_3^9 x^2 d x & =\left[\frac{x^3}{3}\right]_3^9 =\left[\frac{9^3-3^3}{3}\right] =234.
\end{align*}

Using two rectangles of equal width to approximate the area under the curve, the approximate value of the integral
\begin{align*}
\int_3^9 x^2 d x &= (x^2)\vert_{x=3} (6-3) + (x^2)\vert_{x=6} (9-6) \\
& =(3^2) 3 + (6^2) 3 \\
& =27+108 =135.
\end{align*}

The truncation error is hence

\begin{align*}
\mathrm{TE}=234 - 135 = 99.
\end{align*}

****** Example 2
 *For the integral*

\begin{align*}
\int_0^{\infty} \frac{\sqrt{x}}{x^2+1}=\int_0^L \frac{\sqrt{x}}{x^2+1}+\int_L^{\infty} \frac{\sqrt{x}}{x^2+1} \approx \int_0^L \frac{\sqrt{x}}{x^2+1}
\end{align*}

*show that truncation error introduced by dropping* \( \int_L^{\infty} \frac{\sqrt{x}}{x^2+1} \) *can be made to vanish for* \( L > 1 \).

For \(L>1\) we have

\begin{align*}
\int_L^{\infty} \frac{\sqrt{x}}{x^2+1} \approx \int_L^{\infty} \frac{1}{x^{\frac{3}{2}}}=\frac{2}{\sqrt{L}},
\end{align*}

so we can write

\begin{align*}
\int_0^{\infty} \frac{\sqrt{x}}{x^2+1}=\int_0^L \frac{\sqrt{x}}{x^2+1}+\int_L^{\infty} \frac{\sqrt{x}}{x^2+1} = \int_0^L \frac{\sqrt{x}}{x^2+1} + \frac{2}{\sqrt{L}}.
\end{align*}
** Roots of Equations
:LOGBOOK:
CLOCK: [2024-06-07 Fri 22:09]--[2024-06-07 Fri 22:32] =>  0:23
:END:
*** Introduction

#+begin_problem
Given a continuous non-linear function \(f(x)\), find the value \(x=c\) such that \(f(c)=0\).
#+end_problem

+ The non-linear equation \(f(x)=0\) may be an algebraic equation (roots of polynomials) or a transcendental equation.

+ Equations of a single variable:

  \[x^2-6 x+9 = 0\]

  \[x^2-\cos (x) = 0\]

  \[\exp (x) \ln (x^2)-x \cos (x) = 0\]

+ Equations of two variables:

  \[y(x^3-1) = x^4\]

  \[x^2+y^2 =1\]

+ Non-linear equations of a single variable can have no roots at all, have complex root(s) but no real roots, a single real root, two or more simple real roots, two or more multiple roots, some combination of simple root(s) and multiple root(s), etc.

+ All non-linear equations have to be solved iteratively. Therefore, we /bound the solution/ - find a rough estimate of the solution that can be used as an initial guess in an iterative process that refines the solution to a specified tolerance. We always start out with a guess - an approximate root.

+ Starting from the guess we iterate - the better the guess, the better our chances of converging to the solution and fewer the number of iterations required to converge to the solution. So, it is crucial to do a good job of bounding the solution and obtain a good approximate root.

**** Bounding the solution
+ If possible, the root should be bracketed between two points at which the value of the non-linear function changes sign.
+ Graph the functions to get an idea of the solution and get an approximate solution. In case of multiple solutions it also ensures that you have the solution that you want.
+ To omit the sketch is to throw away the most important problem solving tool you possess - geometric intuition.
+ Incremental search - i.e., start with different guesses and search.
+ Past experience with the problem or a similar one.
+ Solution of a simplified approximate model.
+ Previous solution in a sequence of solutions.
*** Closed-domain method
**** Bisection method
***** Introduction
***** Algorithm
***** Analysis
***** Examples
***** Limitations
**** False position method
***** Introduction
***** Algorithm
***** Analysis
***** Examples
***** Limitations
*** Open-domain method
**** Newton's method for one variable
***** Introduction
***** Algorithm
***** Analysis
***** Examples
***** Limitations
**** Newton's method for two variables
***** Introduction
***** Algorithm
***** Analysis
***** Examples
***** Limitations
**** Method of secants
***** Introduction
***** Algorithm
***** Analysis
***** Examples
***** Limitations
**** Muller's method
***** Introduction
***** Algorithm
***** Analysis
***** Examples
***** Limitations
**** Fixed point iteration
***** Introduction
***** Algorithm
***** Analysis
***** Examples
***** Limitations
**** Brute force method
***** Introduction
***** Algorithm
***** Analysis
****** Step size in brute force methods
***** Examples
***** Limitations
** Quadrature
*** Exact integration
*** Numerical integration
*** Direct fit polynomials
*** Riemann integral
*** Simple integration methods: left endpoint Riemann sum
*** Simple integration methods: right endpoint Riemann sum
*** Simple integration methods: midpoint Riemann sum
*** Comparison of left, right, and midpoint Riemann sum
*** Better method: trapezoidal approximation
*** Comparison of mid point and trapezoidal
*** First order interpolation
*** Integration with second order interpolation
*** Lagrange interpolation
*** Newton's divided differences interpolation
*** Error in interpolation
*** Recap of Newton's divided differences
*** The trapezoidal rule (revisited)
*** The Simpson's rule (revisited)
*** The Simpson's 3/8 rule
*** Other view on numerical quadrature
*** Integration error
*** Richardson extrapolation
*** Romberg integration
*** Gaussian quadrature
*** Gaussian quadrature on [-1, 1]
*** Finding quadrature nodes and weights
*** Gaussian quadrature on [-1, 1]
*** Gaussian quadrature on [a, b]
*** Gaussian quadrature
*** Adaptive integration
*** Adaptive Simpson's quadrature
*** Adaptive integration
*** Special cases
*** Special cases: improper integrals
*** Change of variable
*** Elimination of the singularity
*** Ignoring the singularity
*** Proceeding to the limit
*** Truncation of the interval
*** Numerical evaluation of the Cauchy Principal Value
*** Special cases: indefinite integrals
*** Multiple integrals
*** Conclusions
** Random Numbers and Monte Carlo
** Fourier Methods
** Ordinary Differential Equations
** Numerical Linear Algebra
*** Matrix vector product

- Let \(x\) be a \(n\)-dimensional column vector and \(A\) be an \(m \times n\) matrix ( \(m\) rows and \(n\) columns).

- The matrix vector product \(b=A x\) is the \(m\)-dimensional column vector:

\begin{align*}
b_i=\sum_{j=1}^n a_{i j} x_j \quad i=1, \ldots, m .
\end{align*}

- Let \(a_j\) denote column \(j\) of \(A\), an \(m\)-vector. Then rewriting the above equation:

\begin{align*}
b=A x=\sum_{j=1}^n x_j a_j
\end{align*}

\[
b = \left[ \begin{array}{c|c|c|c}
    & & & \\
    & & & \\
    a_1 & a_2 & \cdots & a_n \\
    & & & \\
    & & & 
\end{array} \right]
\left[ \begin{array}{c}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
\end{array} \right]
= x_1 \left[ \begin{array}{c}
    \\
    \\
    a_1 \\
    \\
    \\
\end{array} \right] +
x_2 \left[ \begin{array}{c}
    \\
    \\
    a_2 \\
    \\
    \\
\end{array} \right] + \cdots +
x_n \left[ \begin{array}{c}
    \\
    \\
    a_n \\
    \\
    \\
\end{array} \right]
\]

- \(b\) can also be thought of as a linear combination of the columns \(a_j\)
- \(A x=b\) is usually thought of as \(A\) acting on \(x\) to produce \(b\)
- \(A x=b\) can also be thought of as \(x\) acting on \(A\) to produce \(b\) !
- From \(x=A^{-1} b, x\) can be thought of just as the result of application of \(A^{-1}\) to \(b\).
- Alternatively, \(A^{-1} b\) is the vector of coefficients of the expansion of \(b\) in the basis of columns of \(A\).
*** QR decomposition
**** Motivation
- The concept of QR decomposition stands as a pivotal algorithmic idea in numerical linear algebra, fundamental to numerous applications.
- Central to these applications is the interest in the column spaces of a matrix \(A\), which are defined by successive spans of its columns \(a_1, a_2, \ldots\):

\begin{align*}
\left\langle a_1 \right\rangle \subseteq \left\langle a_1, a_2 \right\rangle \subseteq \left\langle a_1, a_2, a_3 \right\rangle \subseteq \ldots
\end{align*}

where \(\langle \cdots \rangle\) denotes the subspace spanned by the included vectors.

- QR factorization ingeniously constructs a sequence of orthonormal vectors \(q_1, q_2, \ldots\), each spanning these successive column spaces.
- Let us consider a matrix \(A\) in \(\mathbb{C}^{m \times n}\) where \(m \geq n\). The goal is for the orthonormal vectors \(q_1, q_2, \ldots\) to satisfy:

\begin{align*}
\left\langle q_1, q_2, \ldots, q_j \right\rangle = \left\langle a_1, a_2, \ldots, a_j \right\rangle \quad \text{for } j = 1, \ldots, n
\end{align*}

- This is achieved by expressing matrix \(A\) as a product of two matrices:
\[
\left[ \begin{array}{c|c|c|c}
    & & & \\
    & & & \\
    a_1 & a_2 & \cdots & a_n \\
    & & & \\
\end{array} \right] = 
\left[ \begin{array}{c|c|c|c}
    & & & \\
    & & & \\
    q_1 & q_2 & \cdots & q_n \\
    & & & \\
\end{array} \right]
\left[ \begin{array}{cccc}
    r_{11} & r_{12} & \cdots & r_{1n} \\
    & r_{22} & \cdots & \vdots \\
    & & \ddots & \vdots \\
    & & & r_{nn}
\end{array} \right]
\]

- Consequently, each \(a_i\) can be expressed as a linear combination of the vectors \(q_1, \ldots, q_k\), and reciprocally:
\begin{align*}
a_1 & = r_{11} q_1, \\
a_2 & = r_{12} q_1 + r_{22} q_2, \\
& \vdots \\
a_n & = r_{n1} q_1 + r_{n2} q_2 + \cdots + r_{nn} q_n.
\end{align*}

- In the real matrix case (\(A \in \mathbb{R}^{m \times n}\)), vectors \(q_1, q_2, \ldots, q_n\) form an orthonormal set in \(\mathbb{R}^m\):
\begin{align*}
q_i^T q_j = \delta_{ij}
\end{align*}
where \(\delta_{ij}\) is the Kronecker delta.

- The diagonal elements \(r_{ii}\) are non-zero. If any \(r_{ii} < 0\), we can switch the signs of \(r_{ii}, \ldots, r_{in}\) and the corresponding \(q_i\) to ensure \(r_{ii} > 0\), which guarantees the uniqueness of \(Q\) and \(R\).

**** Motivation
- One algorithmic idea in numerical linear algebra that is more important than all others.
- In many applications, we are interested in column spaces of a matrix \(A\). These are successive spaces spanned by the columns \(a_1, a_2, \ldots\) of \(A\) :

\begin{align*}
\left\langle a_1\right\rangle \subseteq\left\langle a_1, a_2\right\rangle \subseteq\left\langle a_1, a_2, a_3\right\rangle \subseteq \ldots
\end{align*}

where \(\langle\cdots\rangle\) denotes the subspace spanned by whatever vectors are included in the brackets.

- The idea of QR factorization is to construct a sequence of orthonormal vectors, \(q_1, q_2, \ldots\) that span these successive spaces.
- Assume for the moment that \(A \in \mathbb{C}^{m \times n}(m \geq n)\), we want the sequence \(q_1, q_2, \ldots\) to have the property:

\begin{align*}
\left\langle q_1, q_2, \ldots, q_j\right\rangle=\left\langle a_1, a_2, \ldots, a_j\right\rangle \quad j=1, \ldots, n
\end{align*}

- This amounts to

\[
\left[ \begin{array}{c|c|c|c}
   & & & \\
    & & & \\
    a_1 & a_2 & \cdots & a_n
   & & & \\
    & & &
\end{array} \right] = 
\left[ \begin{array}{c|c|c|c}
   & & & \\
    & & & \\
    q_1 & q_2 & \cdots & q_n
   & & & \\
    & & &
\end{array} \right]
\left[ \begin{array}{cccc}
    r_{11} & r_{12} & \cdots & r_{1n} \\
    & r_{22} & \cdots & \vdots \\
    & & \ddots & \vdots \\
    & & & r_{nn}
\end{array} \right]
\]

- Then \(a_1, \ldots, a_k\) can be expressed as a linear combination of \(q_1, \ldots q_k\), and vice versa!

- Written out the equations are:

\begin{align*}
a_1 & =r_{11} q_1 \\
a_2 & =r_{12} q_1+r_{22} q_2 \\
& \vdots \\
a_n & =r_{n 1} q_1+r_{2 n} q_2+\cdots+r_{n n} q_n
\end{align*}

- If \(A \in \mathbb{R}^{m \times n}\), vectors \(q_1, q_2, \ldots, q_n\) are orthonormal \(m\)-vectors:

\begin{align*}
q_i^T q_j=\delta_{i j}
\end{align*}

- Diagonal elements \(r_{i i}\) are non-zero.
- If \(r_{i i}<0\), one can switch the signs of \(r_{i i}, \ldots, r_{i n}\) and the vector \(q_i\).
- Require \(r_{i i}>0\); this makes \(Q\) and \(R\) unique.
**** Definition
***** Square matrix

#+NAME: QR decomposition (real square matrix)
#+begin_definition latex
Let \( A \) be any real square matrix. It can be decomposed as
\[
A = QR,
\]
where \( Q \) is an orthogonal matrix (its columns are orthogonal unit vectors, implying that \( Q^{\top} = Q^{-1} \)), and \( R \) is an upper triangular matrix.
#+end_definition

#+NAME: QR decomposition (complex square matrix)
#+begin_definition latex
Let \( A \) be any complex square matrix. It can be decomposed as
\[
A = QR,
\]
where \( Q \) is a unitary matrix (its conjugate transpose \( Q^{\dagger} = Q^{-1} \)), and \( R \) is an upper triangular matrix.
#+end_definition

#+NAME: Uniqueness of QR factorization
#+begin_corollary latex
For an invertible matrix \( A \), the QR factorization is unique provided that the diagonal elements of \( R \) are positive.
#+end_corollary

#+NAME: Orthonormal Basis of Column Span
#+begin_corollary latex
For any \( 1 \leq k \leq n \), the first \( k \) columns of \( Q \) form an orthonormal basis for the span of the first \( k \) columns of \( A \).
#+end_corollary

#+NAME: Orthonormal Basis of Column Space
#+begin_corollary latex
If \( A \) has \( n \) linearly independent columns, then the first \( n \) columns of \( Q \) provide an orthonormal basis for the column space of \( A \).
#+end_corollary

+ The fact that any column \(k\) of \(A\) only depends on the first \(k\) columns of \(Q\) corresponds to the triangular form of \(R\).

***** Rectangular matrix
#+NAME: QR decomposition (rectangular matrix)
#+begin_definition latex
Let \( A \) be a complex \( m \times n \) matrix, where \( m \geq n \). \( A \) can be factorized into the product of an \( m \times m \) unitary matrix \( Q \) and an \( m \times n \) upper triangular matrix \( R \). The matrix \( R \) can be further partitioned as:
\[
A = Q R = Q \left[ \begin{array}{c}
R_1 \\
0 
\end{array} \right] = \left[ \begin{array}{ll}
Q_1 & Q_2 
\end{array} \right] \left[ \begin{array}{c}
R_1 \\
0
\end{array} \right] = Q_1 R_1,
\]
where \( R_1 \) is an \( n \times n \) upper triangular matrix, \( 0 \) represents an \((m-n) \times n\) zero matrix, \( Q_1 \) is \( m \times n \), and \( Q_2 \) is \( m \times (m-n) \). Both \( Q_1 \) and \( Q_2 \) contain orthogonal columns.
#+end_definition

#+NAME: Uniqueness in QR decomposition (rectangular matrix)
#+begin_corollary latex
If \( R_1 \) in the QR decomposition of a rectangular matrix \( A \) has positive diagonal elements, then \( R_1 \) and \( Q_1 \) are unique. However, \( Q_2 \) is generally not unique unless additional constraints are specified.
#+end_corollary

#+NAME: Relation to Cholesky decomposition
#+begin_corollary latex
For a complex matrix \( A \), the matrix \( R_1 \) in the QR decomposition is equal to the upper triangular factor in the Cholesky decomposition of \( A^* A \). If \( A \) is real, this simplifies to \( A^\top A \).
#+end_corollary

**** 

- \(\begin{aligned} {\left[\begin{array}{ccc}-1 & -1 & 1 \\ 1 & 3 & 3 \\ -1 & -1 & 5 \\ 1 & 3 & 7\end{array}\right] } & =\left[\begin{array}{ccc}-1 / 2 & 1 / 2 & -1 / 2 \\ 1 / 2 & 1 / 2 & -1 / 2 \\ -1 / 2 & 1 / 2 & 1 / 2 \\ 1 / 2 & 1 / 2 & 1 / 2\end{array}\right]\left[\begin{array}{lll}2 & 4 & 2 \\ 0 & 2 & 8 \\ 0 & 0 & 4\end{array}\right] \\ & =\left[q_1\left|q_2\right| \ldots \mid q_n\right]\left[\begin{array}{llll}r_{11} & r_{12} & \cdots & r_{1 n} \\ & r_{22} & & \\ & & \ddots & \vdots \\ & & & r_{n n}\end{array}\right] \\ & =Q R\end{aligned}\)

- Compute the \(\mathrm{QR}\) factorization of \(A: A=Q R\).
- Compute \(y=Q^T b\)
- Solve \(R x=y\) for \(x\) : This is just backward substitution as \(R\) is upper triangular.

*** 