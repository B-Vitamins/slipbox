:PROPERTIES:
:ID:       ccd3ee2a-9a9a-45b3-a71e-ccdf55b7d155
:END:
#+TITLE: Complexity (early)
#+FILETAGS: :fleeting: :slides: :presentation:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org
#+STARTUP: beamer indent hidestars
#+LANGUAGE:  en
#+OPTIONS:   H:2 num:t toc:f \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LaTeX_CLASS_OPTIONS: [8pt]
#+LaTeX_CLASS: beamer
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{multimedia}
#+LATEX_HEADER: \newcommand\mitdbar{\text{\ulcshape\slshape Ä‘}}
#+OPTIONS: H:2
* Restricted Boltzmann machines (RBM)
** RBM: Definition and properties
+ The energy function or /Hamiltonian/ for a *restricted Boltzmann machine* (RBM) $N$ *visible spins* $\boldsymbol{v} \in \{1, -1\}^{N}$ and $M$ *hidden spins* \(\boldsymbol{h} \in \{1, -1\}^{M}\), and /coupling constants/ $\mathbold{J} \equiv \left \lbrace J_{ij}  \right \rbrace_{i=1 \cdots M, j=1 \cdots N}$ is

\[
H \left(\boldsymbol{v}, \boldsymbol{h}, \boldsymbol{\theta} \right) = -\sum^M_{i=1} \sum^N_{j=1} h_i J_{i j} v_j
\]

#+begin_src latex
\begin{figure}[h]
\centering
\begin{minipage}[b]{0.25\linewidth}
   \includegraphics[width=\linewidth]{rbm-dot.png}
\end{minipage}
\hspace{0.05\linewidth}
\begin{minipage}[b]{0.25\linewidth}
   \includegraphics[width=\linewidth]{rbm-circo.png}
\end{minipage}
\hspace{0.05\linewidth}
\begin{minipage}[b]{0.25\linewidth}
   \includegraphics[width=\linewidth]{rbm-neato.png}
\end{minipage}
\caption{\textbf{Restricted Boltzmann machines}: All of these represent the same RBM. Interpretation of nodes of any one type (black or white) as \textit{visible} or \textit{hidden} is arbitrary. The key thing to notice is 1) a given visible (hidden) spin \textit{is not} coupled to \textit{any} other visible (hidden) spin; the lattice is \textbf{bipartite}, 2) a given visible (hidden) spin \textit{is} coupled to \textit{every} other hidden (visible) spin; the lattice is \textbf{fully-connected}.}
\end{figure}
#+end_src
* Deep Boltzmann machines (DBM)
** DBM: Definition and properties
+ The deep Boltzmann machine is a generalization of the RBM with multiple hidden layers. It is also an *energy based model* - an /energy function/ \(H\) (to be introduced) parametrizes the distribution over the variables of the model.

  \begin{align*}
  p(\{ \boldsymbol{S}_{l} \}_{l=0 \ldots L})= \dfrac{1}{\mathcal{Z}} \exp \left \lbrace - \beta H(\boldsymbol{S})  \right \rbrace.
  \end{align*}

+ \(\boldsymbol{S}_{l=0} \equiv \boldsymbol{v}\) is called the *visible layer* - these are the /variables the machine is trying to model/. Then rest \(\boldsymbol{S}_{l \neq 0}\) are called *hidden layers*. The *marginal distribution* over the visible spins of a DBM is a trace over the spins of the hidden layers

  \begin{align*}
  p(\boldsymbol{v})= \dfrac{1}{Z} \sum_{\{\boldsymbol{h}\}} \exp \left \lbrace - \beta H(\boldsymbol{v}, \{\boldsymbol{h} \})  \right \rbrace.
  \end{align*}

+ A DBM has a *layered* *bipartite* structure, where there are no /intra-layer connections/ but each layer is /fully connected/ to each of its neighboring layers. This structure implies /closed form factorial conditional distributions/ \(p(\boldsymbol{S}_{l} | \boldsymbol{S}_{l-1}, \boldsymbol{S}_{l+1})\) which can be sampled from using /mean-field approximations/ or /Monte-Carlo methods/.
** DBM: Definition and properties
#+begin_src latex
\begin{figure}
  \centering
  \includegraphics[width=5cm,height=6cm]{vanilla-dbm.png}
  \caption{\textbf{A deep Boltzmann machine (DBM)}: Architecture comprising one visible layer with three units (silver) and three hidden layers with four, five, and four units. Edges between layers signify full interconnections, reflecting the bipartite structure that allows the efficient Gibbs sampling utilized in DBMs.}
\end{figure}

#+end_src
** DBM: Definition and properties
+ Training a DBM proceeds via *maximum likelihood estimation*. The negative log-likelihood given a single training example $\boldsymbol{v}$ is

\begin{align*}
- \ln \mathcal{L}(\boldsymbol{\theta} \mid \boldsymbol{v}) &= -\ln p(\boldsymbol{v} \mid \boldsymbol{\theta}) \\
&= - \ln \sum_{\{\boldsymbol{h}\}} \exp \left \lbrace - \beta H(\boldsymbol{v}, \{\boldsymbol{h} \})  \right \rbrace + \ln \mathcal{Z} \\
&= - \ln \sum_{\{\boldsymbol{h}\}} \exp \left \lbrace - \beta H(\boldsymbol{v}, \{\boldsymbol{h} \})  \right \rbrace + \ln \sum_{\boldsymbol{v}, \{\boldsymbol{h} \}} \exp \left \lbrace - \beta H(\boldsymbol{v}, \{ \boldsymbol{h} \})  \right \rbrace \\
&= - \mathcal{F}^c(\boldsymbol{v}) + \mathcal{F},
\end{align*}

+ \(\mathcal{F}^c(\boldsymbol{v})\) is interpreted as a /clamped/ *free energy*. It is /tractible/. \(\mathcal{F}\) is the *free energy* of the entire DBM. It is /intractible/.

+ Minimizing the negative log-likelihood is equivalent to minimizing the *free energy*. The task is essentially that of learning the parametrization of a energy function - called in the literature an *inverse Ising problem*. The problem is challenging because \(\mathcal{F}\) is /intractible/, one has to find ways of approximating it. 
* Representational capacity
** Feedforward nets: Universal approximators of functions
 #+begin_theorem
+ Let $I_n$ denote the $n$ dimensional unit cube, $[0,1]^n$. The space of continuous functions on $I_n$ is denoted by $C\left(I_n\right)$. Let \(\sigma\) be a *continuous*, *sigmoidal*, and *discriminatory* function. Given any $f \in C\left(I_n\right)$ and $\varepsilon>0$, there exists a sum, $G(x)$

  \begin{align*}
  G(x)=\sum_{j=1}^N \alpha_j \sigma\left(y_j^{\mathrm{T}} x+\theta_j\right)
  \end{align*}

  such that

  \begin{align*}
  \left \lvert G(x)-f(x) \right \rvert < \varepsilon \qquad \text {for all} \qquad x \in I_n
  \end{align*}
#+end_theorem

+ This theorem is one among a class of *universal approximator theorems* that demonstrate the ability of feedfoward architectures to approximate a wide variety of functions. These theorems come in many flavours: *arbitrary width* ones which demand /bounded number of layers/ but an /arbitrarily large number of neurons per layer/, ones with *arbitrary depth* where an /arbitrarily large number of layers/ is allowed, each with /bounded number of neurons/. There are also cases where the analysis demands *bounded depth and bounded width*.
** Energy based models: Universal approximators of distributions
  #+begin_theorem
  /Any distribution over $\{0,1\}^n$ can be approximated arbitrarily well (in the sense of the $K L$ divergence) with an RBM with $k+1$ hidden units where $k$ is the number of input vectors whose probability is not 0./
 #+end_theorem

+ Let $I_n$ denote the $n$ dimensional unit cube, $[0,1]^n$. The space of probability density functions on $I_n$ is denoted by $D\left(I_n\right)$. Let \( M(L, \boldsymbol{N})\) denote a neural network with \( L \) hidden layers, where \(\boldsymbol{N} \equiv {\{N_l\}_{l=0\ldots L}}\) i.e., each layer \( l \) consists of \( N_l \) units. The set of all distributions \( D_{\boldsymbol{M}} \left(I_n\right) \) that can be approximated by  neural network \( M(L, \boldsymbol{N})\) is a measure of its *representational capacity*.

+ These theorems (both for functions and distributions) are by nature *declarative* - they indicate the theoretical capacity to represent complex functions, but don't necessarily provide *algorithms* to construct the appropriate architecture nor make statements about an *optimal representation*.
+ The analysis of representational capacity of energy based models is not nearly as diverse as that which exists for feed forward nets. *There does not exist a clear and commonly agreed upon understanding of what advantage layering brings.*
** Representational capacity, inherent structures, spin glass complexity
+ /Representational capacity/ as defined in the previous slide alludes to a quantity called the *inherent structures* - /single-spin-flip stable states/ of the Hamiltonian. It also relates to the quantity called *complexity* often calculated in the context of spin glasses.

+ To be able to /represent/ a given probability distribution, the energy function at the very least must generate a ensemble with /at least as many modes as are present in the distribution/. This intuition is formalized by the following definition:

#+begin_definition
   For an $L$ layered DBM with $M(1), \ldots, M(L)$ hidden units and $N$ visible units the *Inherent Structure Capacity (ISC)*, denoted by $C\left(N, \boldsymbol{M}\right)$ is defined as the (normalized) logarithm of the expected number of modes of all possible distributions generated over the visible units by the DBM.
   \begin{align*}
   \mathcal{C}\left(N, \boldsymbol{M} \right) = N^{-1} \ln \left[ \mathcal{N}_s (\boldsymbol{v}) \right]
   \end{align*}
 
  where \(\mathcal{N}_s (\boldsymbol{v})\) denotes the local minima of DBM with visible units clamped to a pattern $\boldsymbol{v}$.
#+end_definition
* Complexity for a DBM with L hidden layers
** A Hamiltonian for a deep Boltzmann machine
+ An *energy function* or *Hamiltonian* for a DBM with \(L\) hidden layers:

  \begin{align*}
  H_{(\boldsymbol{J}, N(l))} (\boldsymbol{S}) = - \frac{1}{2}
  \sum_{l=0}^{L}
  \sum_{l^{\prime} = l \pm 1}
  \sum_{i=1}^{N(l)} \sum_{j=1}^{N(l^{\prime})} 
  S_{il} J_{(il)(jl^{\prime})} S_{j l^{\prime}}
  \quad
  l^{\prime} = L+1 \equiv 0,
  \quad
  l^{\prime} = -1 \equiv L
  \end{align*}

+ Here \(S_{il} \in \left \lbrace 1, -1 \right \rbrace\) is a /Ising-like variable/ attached to site \(i\) of layer \(l\). \(N(l)\) is the /layer function/ giving the number of spins in layer \(l\). \(J_{(il)(jl^{\prime})}\) is the coefficient for the /interaction energy/ between spins \(S_{il}\) and \(S_{jl^{\prime}}\)

+ The couplings for the exchange interaction between layer \(l\) and \(l^{\prime}\) are distributed as

  \begin{align*}
  P(J_{(il)(jl^{\prime})}) \equiv \sqrt{\dfrac{\widehat{N}_{l l^{\prime}}}{2 \pi J_l^2}} \exp \left \lbrace - \dfrac{\widehat{N}_{l l^{\prime}}}{2 J_l^2} \left( J_{(il) (jl^{\prime})} - \dfrac{\hat{J}_l}{\widehat{N}_{l l^{\prime}}} \right)^{2} \right \rbrace
  \end{align*}

  where we have introduced \(\widehat{N}_{ll^{\prime}} \equiv \sqrt{N(l) N(l^{\prime})}\) to ensure extensivity of the free energy.
** A Hamiltonian for a deep Boltzmann machine
+ The previously described Hamiltonian describes a DBM that turns in on itself so that the hidden layer \(L\) connects back with the visible layer. Such an architecture requires one additional weight matrix (\(L+1)\) in total). For the traditional DBM the following modification to the Hamiltonian is necessary

  \begin{align*}
  H_{(\boldsymbol{J}, N(l))} (\boldsymbol{S}) = - \frac{1}{2}
  \sum_{l=0}^{L}
  \sum_{l^{\prime} = l \pm 1}
  \sum_{i=1}^{N(l)} \sum_{j=1}^{N(l^{\prime})} 
  S_{il} J_{(il) (jl^{\prime})} S_{j l^{\prime}} (1 - \delta_{l0}) (1 - \delta_{lL})
  \end{align*}

#+begin_src latex
  \begin{figure}[ht]
    \begin{minipage}[b]{0.45\linewidth}
      \centering
      \includegraphics[width=0.6\textwidth]{circular-dbm.png}
      \caption{The last hidden layer connects back with the visible layer (black).}
      \label{fig:a}
    \end{minipage}
    \hspace{0.5cm}
    \begin{minipage}[b]{0.45\linewidth}
      \centering
      \includegraphics[width=0.6\textwidth]{vanilla-dbm.png}
      \caption{A regular DBM with analogous nodes and connectivity profiles.}
      \label{fig:b}
    \end{minipage}
  \end{figure}
#+end_src
** Plefka's expansion for the DBM and the TAP equations
+ Installing auxiliary fields \(\lambda^{il} (\beta)\) we construct the functional
  \begin{align*}
  - \beta G_{\left(\beta, \boldsymbol{J}, \{\lambda^{il} \}\right)} [m_{il}] = 
  \ln \operatorname{Tr}
  \exp \left \lbrace -
  \beta H_{\boldsymbol{J}} (\boldsymbol{S}) + 
  \lambda^{il} (\beta) \left( S_{il} - m_{il} \right)
  \right \rbrace
  \end{align*}

+ A high-temperature expansion around \(\beta = 0\) will yield the TAP equations

  \begin{align*}
  - \beta G_{\left(0, \boldsymbol{J}, \{\lambda^{il} \}\right)} [m_{il}] &= 
  -\beta G_{\left(0, \boldsymbol{J}, \{\lambda^{il} \}\right)} [m_{il}]
  - \beta \partial_{\beta} G_{\left(0, \boldsymbol{J}, \{\lambda^{il} \}\right)} [m_{il}]\\
  &\qquad \qquad \qquad - 
  2^{-1} \beta^2 \partial_{\beta}^2 G_{\left(0, \boldsymbol{J}, \{\lambda^{il} \}\right)} [m_{il}] + \mathcal{O} (\beta^3)
  \end{align*}

+ This is called *Plefka's expansion*. The form we present here has been taken from *Georges and Yedidia* who interpreted it as a /high temperature expansion/.
+ The terms of this expansion give systematic corrections to the mean-field approximation to desired order in \(\beta\). Retaining terms till order \(\mathcal{O} (\beta)\) yields the /Curie-Weiss mean field approximation/. Retaining terms till \(\mathcal{O} (\beta^2)\) yield the TAP equations. The additional term is called the /Onsager reaction term/.
** Plefka's expansion for the DBM and the TAP equations
+ We will skip explicit calculations of each of the terms by arguing as follows: if we look at \(\frac{1}{2} \sum_{ij} S_{il} J_{(il)(j l^{\prime})} S_{j l^{\prime}}\) for either of \(l^{\prime} = \pm 1\) in isolation and ignore the layer indices, we realize that it is simply the Hamiltonian for the Sherrington Kirkpatrick model whose TAP free energy to order \(\beta^2\) reads

  \begin{align*}
  -\beta G [m_{i}] &= -\sum_i\left[\frac{1+m_i}{2} \ln \left(\frac{1+m_i}{2}\right)+\frac{1-m_i}{2} \ln \left(\frac{1-m_i}{2}\right)\right] \\
  & + \frac{\beta}{2} \sum_{i j} J_{i j} m_i m_j +\frac{\beta^2}{4} \sum_{i j} J_{i j}^2\left(1-m_i^2\right)\left(1-m_j^2\right).
  \end{align*}

+ Then we re-introduce the layer indices (pairing \(l\) with \(i\) and \(l^{\prime}\) with \(j\)). Further, \(J_{ijll^{\prime}} \approx J^2_l/\widehat{N}_{ll^{\prime}}\) since \([J_{ijl l^{\prime}}^2 - J_{l}^2/\widehat{N}_{ll^{\prime}}] \sim \mathcal{O} (\widehat{N}_{l l^{\prime}}^{-1})\) vanishes when \(\widehat{N}_{ll^{\prime}}\). With \(Q_{l} (\boldsymbol{m}_{l}) \equiv N(l)^{-1} \sum_{i=1}^{N(l)} m_{il}^2\) and \(\gamma_{l l^{\prime}} \equiv \sqrt{N(l)/N(l^{\prime})}\), we retrieve an expression for the free energy.
** The TAP free energy for the DBM
+ The (intensive, Gibbs) free energy as a (normalized) sum of /(intensive) free energies per layer/

  \begin{align*}
  \mathcal{F} [\boldsymbol{m}] &= L^{-1} \sum_{l=0}^{L} \mathcal{F}_l [\boldsymbol{m}_{l}] \\
  &= - L^{-1} \sum_{l=0}^{L} N(l)^{-1} \left[ \sum_{i=1}^{N(l)} \left( \frac{1+m_{il}}{2} \ln \left(\frac{1+m_{il}}{2}\right) + \frac{1-m_{il}}{2} \ln \left(\frac{1-m_{il}}{2}\right) \right) \right] \\
  &\quad + L^{-1} \sum_{l=0}^{L} N(l)^{-1} \sum_{l^{\prime} = l \pm 1} \left( \frac{\beta}{2} \sum_{i,j = 1}^{N(l),N(l^{\prime})} J_{(il)(jl^{\prime})} m_{il} m_{jl^{\prime}} \right) \\
  &\qquad + L^{-1} \sum_{l=0}^{L} \sum_{l^{\prime} = l \pm 1} \left( \frac{\gamma_{ll^{\prime}} \beta^2 J_{l}^2}{4} \left[1- Q_l (\boldsymbol{m}_{l})\right] \left[ 1- Q_{l^{\prime}} (\boldsymbol{m}_{l^{\prime}}) \right] \right).
  \end{align*}

+ The DBM's /layered bipartite structure/ gives rise to a /bilinear form/ \(\left[1- Q_l (m_{il})\right] \left[ 1- Q_{l^{\prime}} (m_{jl^{\prime}}) \right]\) instead of the /quadratic form/  \(\left[1- Q (m_{i})\right]^2\) that appears in the SK model.
** TAP equations for a DBM
+ Demanding \(\delta \mathcal{F} [m_{il}]/ \delta m_{il} = 0\) yields a set of \(\sum_{l=0}^{L} N(l)\) TAP equations whose solutions yield \(m_{il} \equiv \left \langle S_{il}  \right \rangle \) - the expectation for the spin \(i\) in  layer \(l\) of the DBM.

\begin{align*}
\boxed{
m_{il} = \tanh \frac{1}{2} \sum_{l^{\prime} = l \pm 1} L^{-1} \left(\beta \sum_j J_{(il) (jl^{\prime})} m_{jl^{\prime}} - \gamma_{l l^{\prime}} \beta^2 J_l^2 \left[ 1 - Q_{l^{\prime}} (\boldsymbol{m}_{l^{\prime}}) \right] m_{il} \right)
}
\end{align*}

+ A useful rewrite of the TAP equation is

\begin{align*}
\boldsymbol{G} (m_{il}) &\equiv 
\overbrace{\operatorname{arctanh} m_{il} +
\sum_{l^{\prime} = l \pm 1} \frac{\gamma_{l l^{\prime}}}{2 L} \beta^2 J_l^2 \left[ 1 - Q_{l^{\prime}} (\boldsymbol{m}_{l^{\prime}}) \right] m_{il}}^{g(m_{il})} \\
&\qquad \qquad -
\beta \sum_{l^{\prime} = l \pm 1} \frac{1}{2 L} \sum_j J_{(il) (jl^{\prime})} m_{jl^{\prime}} = 0
\end{align*}
** Digression
+ There are articles in the literature that demonstrate, in the case of RBMs, that deterministic training where one performs a *fixed-point iteration of the TAP equations* (and weight retrieval thereafter)

  \begin{align*}
  & m_j^h[t+1] \leftarrow \sigma \left(\sum_i W_{i j} m_i^v[t]-W_{i j}^2\left(m_j^h[t]-\frac{1}{2}\right)\left(m_i^v[t]-\left(m_i^v[t]\right)^2\right)\right), \\
  & m_i^v[t+1] \leftarrow \sigma \left(\sum_j W_{i j} m_j^h[t+1]-W_{i j}^2\left(m_i^v[t]-\frac{1}{2}\right)\left(m_j^h[t+1]-\left(m_j^h[t+1]\right)^2\right)\right),
  \end{align*}

  perform at par with more conventional Monte Carlo methods like *contrastive divergence* and its various sophistications with the added benefit of being faster, deterministic, and in principle allowing corrections to arbitrary order. There are rigorous proofs for the convergence of this iteration for the SK model. The convergence properties seem to extend to the RBM as well, an /a posteriori/ argument being that /an RBM is an SK model over a bipartite lattice/.

+ /It maybe worthwhile to construct a similar iteration for the DBM from the TAP equations we have found and study its convergence properties. In case the iteration is well behaved, this could be a novel way of deterministic joint training of DBMs via mean-field approximations./
** A rewrite for the DBM's TAP free energy
+ On multiplying \(\boldsymbol{G} (m_{il})\) with \(N(l)^{-1} \sum_i m_{il}\) we get

\begin{align*}
&- L^{-1} \sum_{l=0}^{L} N(l)^{-1} \sum_{l^{\prime} = l \pm 1} \left( \frac{\beta}{2} \sum_{i,j = 1}^{N(l),N(l^{\prime})} J_{(il)(jl^{\prime})} m_{il} m_{jl^{\prime}} \right) = \\
&N(l)^{-1} \sum_{i} m_{il} \operatorname{arctanh} m_{il}  -
\gamma_{l l^{\prime}}^2 \beta^2 J_l^2 \left[1-Q_{l^{\prime}} (\boldsymbol{m}_{l^{\prime}})\right] \left( N(l)^{-1} \sum_i m_{il}^2 \right) \Bigg ].
\end{align*}

+ If we substitute this in the original expression for \(\mathcal{F} [m_{il}]\) after introducing

  \begin{align*}
  N \equiv \sum_l N(l), \qquad  \alpha_l \equiv \frac{N(l)}{\sum_l N(l)} = \frac{N(l)}{N}
  \end{align*}

  we will retrieve a free energy for the DBM as a /sum over single sites/.
** A rewrite for the DBM's TAP free energy
+ The rewrite is
\begin{align*}
&N \mathcal{F} [\boldsymbol{m}] = L^{-1} \sum_{l=0}^{L} \sum_{i=1}^{N(l)} \mathcal{F}_{il} [m_{il}] \\
&= \sum_{l=0}^{L} (\alpha(l) \thinspace L)^{-1} \sum_{i=1}^{N(l)} m_{il} \operatorname{arctanh} m_{il} \\
&- \sum_{l=0}^{L} (\alpha(l) \thinspace L)^{-1} \sum_{i=1}^{N(l)} \frac{1+m_{il}}{2} \ln \left(\frac{1+m_{il}}{2}\right) + \frac{1-m_{il}}{2} \ln \left(\frac{1-m_{il}}{2}\right) \\
&\quad - \sum_{l=0}^{L} (\alpha(l) \thinspace L)^{-1} \sum_{l^{\prime} = l \pm 1} \sum_{i=1}^{N(l)} \frac{\gamma_{l l^{\prime}} \beta^2 J_l^2}{4}\left[1 + \left\{ Q_l\left(\boldsymbol{m}_{l}\right)-Q_{l^{\prime}}\left(\boldsymbol{m}_{l^{\prime}}\right) \right\} -Q_{l^{\prime}}\left(\boldsymbol{m}_{l^{\prime}}\right) Q_l\left(\boldsymbol{m}_{l}\right)\right] \\
\end{align*}

+ The equivalent of the third term above in the SK model is \(\frac{\beta^2 J_l^2}{4} \left(1 - Q(m)^2\right)\). /It *maybe* possible to re-order terms in the sum and argue that \(\left \{ Q_l\left(m_{i l}\right)-Q_{l^{\prime}}\left(m_{j l^{\prime}}\right) \right \}\) infact vanishes/.
** Density of pure states
+ Given a specific value \(\mathcal{F}\) for the (intensive) free energy \(\mathcal{F} [\boldsymbol{m}]\), we are interested in enumerating the number of /temperature-dependent/ *pure states* (minima of the free energy defined in terms of \(\boldsymbol{m}\)). A pure state must satisfy the conditions:

  1) The free energy \(N \mathcal{F} (\boldsymbol{m}) = \sum_{l=0}^{L} \sum_{i=1}^{N(l)} \mathcal{F}_{il} [m_{il}]\) must be equal to the given value \(\mathcal{F}\),
  2) \(\boldsymbol{m} \equiv \{m_{il} \}\) must be a solution of the TAP equation and take a value \(\boldsymbol{m}\) that is consistent with the given value of \(\mathcal{F}\),
  3) The quantity \(Q_l (\boldsymbol{m}_{l}) \equiv N(l)^{-1} \sum_{i}^{N(l)} m_{il}^2\) must take a set of values \(\{Q_l\}\) that is consistent with the given value of \(\mathcal{F}\) as well as the solutions \(\{\boldsymbol{m}_l \}\) of the TAP equations.

  \begin{align*}
  &\mathcal{N} (\mathcal{F}) = 
  \overbrace{\int_{-1}^{1} \prod_{l=0}^{L} \prod_{i=0}^{N(l)} \mathrm{d} m_{il} 
  \int_0^{1} \prod_{l=0}^{L} \mathrm{d} Q_{l}}^{\text{integrating over all solutions}}
  \thinspace \times
  \overbrace{\delta \left( N \mathcal{F} - \sum_{l=0}^{L} \sum_{i=1}^{N(l)} \mathcal{F}_{il} [m_{il}] \right)}^{\text{free energy must be \(\mathcal{F}\)}} \\
  &\times
  \left \lvert \operatorname{det} \boldsymbol{G}^{\prime} \right \rvert 
  \underbrace{\prod_{l=0}^{L} \prod_{i=1}^{N(l)}
  \delta \left( g(m_{il}) - \beta \sum_{l^{\prime} = l \pm 1} \sum_j J_{(il) (jl^{\prime})} m_{jl^{\prime}} \right)}_{\text{the TAP system must admit \( \{m_{il}\}\)}} \times 
  \underbrace{\prod_{l=0}^{L} \delta \left( N(l) Q_l - \sum_{i=1}^{N(l)} m_{il}^2 \right)}_{\text{\(Q_{l}\) must be consistent with \(\{ m_{il} \}\)}}
  \end{align*}

** Digression
+ We have integrated the measure \(\mathrm{d} \boldsymbol{m} \thinspace \mathrm{d} \boldsymbol{Q}\) over the space of solutions, hooking delta functions that pick out regions compliant with our demands. \(\left \lvert \operatorname{det} \boldsymbol{G}^{\prime} \right \rvert \) is the absolute value of the determinant of \(\boldsymbol{G}\)'s Jacobian matrix - its elements are \(\partial_{m_{jl^{\prime}}}\boldsymbol{G} (m_{il})\).

+ This is a specialization of the *Kac-Rice formula*, specifically the *area formula* which is the rigorous version of a non-rigorous intuition: for a smooth function $f: \mathbb{R} \rightarrow \mathbb{R}$, and $T \subseteq \mathbb{R}$, denoting $N_f(u, T)$ the number of solutions to the equation $f(t)=u$ with $t \in T$

\begin{align*}
N_f(u, T) & =\int_{f(T)} \delta(v-u) \mathrm{d} v =\int_T \delta(f(t)-u)\left|f^{\prime}(t)\right| \mathrm{d} t
\end{align*}

+ The pure states in general do not coincide with the /temperature-independent/ *inherent structures* (/single-spin-flip stable states/ of the Hamiltonian defined in terms of \(\boldsymbol{S}\)) for model that /not fully-connected/ and have /finite couplings/.
** Density of pure states
+ Using the Fourier representation of the delta function

\begin{align*}
&\mathcal{N} (\mathcal{F}) = 
\left\lvert \operatorname{det} \boldsymbol{G}^{\prime} \right\rvert
\int_{-\mathrm{i}\infty}^{\mathrm{i}\infty} \prod_{l=0}^{L} \left(\frac{\mathrm{d} u_l}{2 \pi \mathrm{i}}\right)
\times \int_{0}^{1} \prod_{l=0}^{L} \mathrm{d} Q_{l} \\
&\qquad
\int_{-1}^{1} \prod_{l=0}^{L} \prod_{i=0}^{N(l)} \mathrm{d} m_{il} 
\int_{-\mathrm{i}\infty}^{\mathrm{i}\infty} \prod_{l=0}^{L} \prod_{i=1}^{N(l)} \left(\frac{\mathrm{d} x_{il}}{2 \pi \mathrm{i}}\right)
\int_{-\mathrm{i}\infty}^{\mathrm{i}\infty} \frac{\mathrm{d} \lambda}{2 \pi \mathrm{i}} \\
&\qquad \qquad \qquad \times \exp \Bigg \{ - N \left( \lambda \mathcal{F} + \sum_{l=0}^{L} u_l \alpha_l Q_l \right) + \sum_{l=0}^{L} u_l \sum_{i=1}^{N(l)} m_{il}^2 + \lambda \sum_{l=0}^{L} \sum_{i=1}^{N(l)} \mathcal{F}_{il} [m_{il}] \\
&\qquad \qquad \qquad \qquad \qquad + \sum_{l=0}^{L}
x_{il} g(m_{il}) - \beta \sum_{l^{\prime} = l \pm 1} \sum_{j=0}^{N(l^{\prime})} J_{(il)(jl^{\prime})} \left( m_{jl^{\prime}} x_{il} + m_{il} x_{jl^{\prime}} \right) \Bigg \} \\
\end{align*}

+ Much of the difficulty in any evaluation of the area formula lies in finding the determinant of the Jacobian of the function whose solutions are sought, i.e., in finding \(\lvert \operatorname{det} \boldsymbol{G}^{\prime} \rvert\). 
** The Jacobian of the TAP system
+ The matrix elements of \(\partial_{m_{jl^{\prime}}}\boldsymbol{G} (m_{il})\) are

\begin{align*}
G_{(il) (jl^{\prime})}^{\prime} &= 
\delta_{i j} \delta_{l l^{\prime}} 
\left[
\frac{1}{1-m_{i l}^2}-\beta^2 J_l^2
\left[1 - Q_{l} (\boldsymbol{m}_{l})
\right]
\right] \\
&\qquad \qquad -
\sum_{l^{\prime} = l \pm 1} 
\left[
2 \gamma_{l l^{\prime}} \beta^2 J_l^2 N
\left(l^{\prime}\right)^{-1} 
m_{j l^{\prime}} m_{i l} 
-\beta J_{i j l l^{\prime}}\right]
\delta_{l (l^{\prime} \mp 1)} \\
&\qquad \approx
\delta_{i j} \delta_{l l^{\prime}}
\underbrace{\left[
\frac{1}{1-m_{i l}^2}-\beta^2 J_l^2
\left[1 - Q_{l} (\boldsymbol{m}_{l})
\right]
\right]}_{a_{il}} +
\sum_{l^{\prime} = l \pm 1} \beta J_{(il) (jl^{\prime})} \delta_{l (l^{\prime} \mp 1)}
\end{align*}

+ The modulus on \(\operatorname{det} \boldsymbol{G}^{\prime}\) is usually dropped in the first step. The only justification for this is the trivial one: we are /not interested/ in solutions \(m_{il}\) of the TAP system for which \(\lvert \operatorname{det} \boldsymbol{G}^{\prime} \rvert > 0\) and \(0 > \operatorname{det} \boldsymbol{G}^{\prime}\).

+ Thereafter, we need to construct \(\left[ \operatorname{det} \boldsymbol{G} \right]_{\boldsymbol{J}}\) for which there are several distinct approaches in the literature. There are subtleties associated with this maneuver which needs consideration - since -\(\ln \mathcal{N} (\mathcal{F})\) is a self-averaging quantity not \(\mathcal{N} (\mathcal{F})\) so we should be evaluating  \(\left[ \ln \mathcal{N} (\mathcal{F})  \right]_{\boldsymbol{J}}\) (*remains to be done*).
* Numerical Explorations
** The energy landscape of DBMs
+ Consider the exchange interaction \(-J_{ij} S_{i} S_{j}\). It is possible to define a *single-spin energy*, the sum of all terms to which an individual spin \( S_{i} \) contributes:

  \[
  e_k = -\sum_{(ij)} J_{ij} S_{i} S_{j} \left(\delta_{i k}+\delta_{j k}\right)
  \]

+ The Hamiltonian can then be expressed as:

  \[
  H = \frac{1}{2} \sum_k e_k = E
  \]

+ The energy change associated with a single spin flip can be represented:

  \begin{align*}
  \mathbf{S} \rightarrow \mathbf{S}' = \left(s_1, \ldots, s_{k-1}, -s_k, s_{k+1}, \ldots\right), \qquad e_k \rightarrow e_k' = -e_k
  \end{align*}
  
  so that

  \begin{align*}
  H \left(\mathbf{S}'\right) & - H (\mathbf{S}) = -2 e_k
  \end{align*}

+ A *metastable state*, where every single spin flip increases energy, exists if:

  \begin{align*}
  e_k < 0 \text{ for all } k
  \end{align*}
** The energy landscape of DBMs
+ The generalization to the case of DBMs where localizing a spin takes two indices is straightforward. Thereafter, we define as a *control parameter* the number of spins with positive energies

  \begin{align*}
  n^{\mathrm{P}}(\mathbf{S})=\sum_{i=1}^N \Theta\left(e_i\right),
  \end{align*}

+ Using a method called the *Wang-Landau algorithm* it is possible to construct weights

  \begin{align*}
  \omega_k(m)=\left(\sum_{\mathbf{S}} \delta_{n^{P} (\mathbf{S}), m} \exp \left \lbrace  -\beta_k H (\mathbf{S}) \right \rbrace\right)^{-1}
  \end{align*}

+ If we *re-weight* the Boltzmann factors of the \(k^{\text{th}}\) replica in a *replica-exchange Monte Carlo* simulation with these weights

  \begin{align*}
  P_k(\mathbf{S}) \propto \omega_k\left(n^{\mathrm{p}}(\mathbf{S})\right) e^{-\beta_k H (\mathbf{S})}
  \end{align*}

  we end up with samples have have a *flat histogram* over \(n^{\mathrm{P}}(\mathbf{S})\).
** The energy landscape of DBMs

+ This allows us to get configurations from regions of the phase phase which would otherwise be /notoriously inaccessible/. Conversely, it also supresses over-representation of states of intermediate values of \(n^{\mathrm{P}}(\mathbf{S})\). Some preliminary calculations using the Wang-Landau scheme emit weights \(\{\omega_k(m)\}\) that /span more than 30 orders of magnitude/.

+ Sanity checks (by sampling from the weighted ensemble and retrieving back a flat distribution of \(n^{\mathrm{P}}(\mathbf{S})\)) *remain to be done*. Thereafter a technique called *weighted-histogram analysis method* (WHAM) (*remains to be done*) can be used to construct the distribution of the metastable states.
