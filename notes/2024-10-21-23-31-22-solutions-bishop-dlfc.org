:PROPERTIES:
:ID:       5da04b9e-8e39-49cb-97a8-0acbdcb5af38
:END:
#+TITLE: Solutions to Deep Learning by Christopher M. Bishop
#+FILETAGS: :problem:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org
#+BEGIN: clocktable :maxlevel 2 :scope nil :emphasize nil
#+CAPTION: Clock summary at [2024-10-21 Mon 23:35]
| Headline     | Time   |
|--------------+--------|
| *Total time* | *0:00* |
#+END
* Probabilities
** Notes
*** The rules of probability
**** The sum and product rules
**** Bayes's theorem
**** Prior and posterior probabilities
**** Indendent variables
**** Example: medical screening
*** Probability densities
**** Example distributions
**** Expectations and covariances
*** The Guassian distribution
**** Mean and variance
**** Likelihood function
**** Bias of maximum likelihood
**** Linear regression
*** Transformation of densities
**** Multivariate distributions
*** Information theory
**** Entropy
**** Physical perspective
**** Differential entropy
**** Maximum entropy
**** Kullback-Leibler divergence
**** Conditional entropy
**** Mutual information
*** Bayesian probabilities
**** Model parameters
**** Regularization
**** Bayesian machine learning
** 2.1 Cancer Screening
In the cancer screening example, we used a prior probability of cancer of \(p(C= 1)=0.01\). In reality, the prevalence of cancer is generally very much lower. Consider a situation in which \(p(C=1)=0.001\), and recompute the probability of having cancer given a positive test \(p(C=1 \mid T=1)\). Intuitively, the result can appear surprising to many people since the test seems to have high accuracy and yet a positive test still leads to a low probability of having cancer.

** 2.2 Non-transitive Dice
Deterministic numbers satisfy the property of transitivity, so that if \(x>y\) and \(y>z\) then it follows that \(x>z\). When we go to random numbers, however, this property need no longer apply. Figure 2.16 shows a set of four cubical dice that have been arranged in a cyclic order. Show that each of the four dice has a \(2/3\) probability of rolling a higher number than the previous die in the cycle. Such dice are known as non-transitive dice, and the specific examples shown here are called Efron dice.

** 2.3 Convolution of Distributions
Consider a variable \(\mathbf{y}\) given by the sum of two independent random variables \(\mathbf{y}=\mathbf{u}+\mathbf{v}\) where \(\mathbf{u} \sim p_{\mathbf{u}}(\mathbf{u})\) and \(\mathbf{v} \sim p_{\mathbf{v}}(\mathbf{v})\). Show that the distribution \(p_{\mathbf{y}}(\mathbf{y})\) is given by
\[
p(\mathbf{y})=\int p_{\mathbf{u}}(\mathbf{u}) p_{\mathbf{v}}(\mathbf{y}-\mathbf{u}) \mathrm{d} \mathbf{u} .
\]
This is known as the convolution of \(p_{\mathbf{u}}(\mathbf{u})\) and \(p_{\mathbf{v}}(\mathbf{v})\).
** 2.4 Uniform Distribution Normalization
Verify that the uniform distribution (2.33) is correctly normalized, and find expressions for its mean and variance.
** 2.5 Exponential and Laplace Normalization
Verify that the exponential distribution (2.34) and the Laplace distribution (2.35) are correctly normalized.
** 2.6 Empirical Density Normalization
Using the properties of the Dirac delta function, show that the empirical density (2.37) is correctly normalized.
** 2.7 Expectation Approximation
By making use of the empirical density (2.37), show that the expectation given by (2.39) can be approximated by a sum over a finite set of samples drawn from the density in the form (2.40).
** 2.8 Variance of Function
Using the definition (2.44), show that var \([f(x)]\) satisfies (2.45).
** 2.9 Independence and Covariance
Show that if two variables \(x\) and \(y\) are independent, then their covariance is zero.
** 2.10 Mean and Variance of Sum
Suppose that the two variables \(x\) and \(z\) are statistically independent. Show that the mean and variance of their sum satisfies
\begin{align*}
\mathbb{E}[x+z] & =\mathbb{E}[x]+\mathbb{E}[z] \\
\operatorname{var}[x+z] & =\operatorname{var}[x]+\operatorname{var}[z] .
\end{align*}
** 2.11 Conditional Expectation and Variance
Consider two variables \(x\) and \(y\) with joint distribution \(p(x, y)\). Prove the following two results:
\begin{align*}
\mathbb{E}[x] & =\mathbb{E}_y\left[\mathbb{E}_x[x \mid y]\right] \\
\operatorname{var}[x] & =\mathbb{E}_y\left[\operatorname{var}_x[x \mid y]\right]+\operatorname{var}_y\left[\mathbb{E}_x[x \mid y]\right] .
\end{align*}
Here \(\mathbb{E}_x[x \mid y]\) denotes the expectation of \(x\) under the conditional distribution \(p(x \mid y)\), with a similar notation for the conditional variance.
** 2.12 Gaussian Normalization
In this exercise, we prove the normalization condition (2.51) for the univariate Gaussian. To do this consider, the integral
\[
I=\int_{-\infty}^{\infty} \exp (-\frac{1}{2 \sigma^2} x^2) \mathrm{d} x
\]

which we can evaluate by first writing its square in the form
\[
I^2=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \exp (-\frac{1}{2 \sigma^2} x^2-\frac{1}{2 \sigma^2} y^2) \mathrm{d} x \mathrm{~d} y .
\]
Now make the transformation from Cartesian coordinates \((x, y)\) to polar coordinates \((r, \theta)\) and then substitute \(u=r^2\). Show that, by performing the integrals over \(\theta\) and \(u\) and then taking the square root of both sides, we obtain
\[
I=\left(2 \pi \sigma^2\right)^{1/2} .
\]
Finally, use this result to show that the Gaussian distribution \(\mathcal{N}(x \mid \mu, \sigma^2)\) is normalized.
** 2.13 Gaussian Properties
By using a change of variables, verify that the univariate Gaussian distribution given by (2.49) satisfies (2.52). Next, by differentiating both sides of the normalization condition
\[
\int_{-\infty}^{\infty} \mathcal{N}(x \mid \mu, \sigma^2) \mathrm{d} x=1
\]

with respect to \(\sigma^2\), verify that the Gaussian satisfies (2.53). Finally, show that (2.54) holds.
** 2.14 Mode of Gaussian
Show that the mode (i.e., the maximum) of the Gaussian distribution (2.49) is given by \(\mu\).
** 2.15 Maximum Likelihood Estimates
By setting the derivatives of the \(\log\) likelihood function (2.56) with respect to \(\mu\) and \(\sigma^2\) equal to zero, verify the results (2.57) and (2.58).
** 2.16 Expectation of Gaussian Data
Using the results (2.52) and (2.53), show that
\[
\mathbb{E}\left[x_n x_m\right]=\mu^2+I_{n m} \sigma^2
\]
where \(x_n\) and \(x_m\) denote data points sampled from a Gaussian distribution with mean \(\mu\) and variance \(\sigma^2\) and \(I_{n m}\) satisfies \(I_{n m}=1\) if \(n=m\) and \(I_{n m}=0\) otherwise. Hence prove the results (2.59) and (2.60).
** 2.17 Variance Estimator Expectation
Using the definition (2.61), prove the result (2.62) which shows that the expectation of the variance estimator for a Gaussian distribution based on the true mean is given by the true variance \(\sigma^2\).
** 2.18 Maximizing Likelihood
Show that maximizing (2.66) with respect to \(\sigma^2\) gives the result (2.68).
** 2.19 Transformation of Density
Use the transformation property (2.71) of a probability density under a change of variable to show that any density \(p(y)\) can be obtained from a fixed density \(q(x)\) that is everywhere non-zero by making a nonlinear change of variable \(y=f(x)\) in which \(f(x)\) is a monotonic function so that \(0 \leqslant f^{\prime}(x)<\infty\). Write down the differential equation satisfied by \(f(x)\) and draw a diagram illustrating the transformation of the density.

** 2.20 Jacobian Matrix Elements
Evaluate the elements of the Jacobian matrix for the transformation defined by (2.78) and (2.79).

** 2.21 Entropy and Additivity
In Section 2.5, we introduced the idea of entropy \(h(x)\) as the information gained on observing the value of a random variable \(x\) having distribution \(p(x)\). We saw that, for independent variables \(x\) and \(y\) for which \(p(x, y)=p(x) p(y)\), the entropy functions are additive, so that \(h(x, y)=h(x)+h(y)\). In this exercise, we derive the relation between \(h\) and \(p\) in the form of a function \(h(p)\). First show that \(h(p^2)=2 h(p)\) and, hence, by induction that \(h(p^n)=n h(p)\) where \(n\) is a positive integer. Hence, show that \(h(p^{n/m})=(n/m) h(p)\) where \(m\) is also a positive integer. This implies that \(h(p^x)=x h(p)\) where \(x\) is a positive rational number and, hence, by continuity when it is a positive real number. Finally, show that this implies \(h(p)\) must take the form \(h(p) \propto \ln p\).
** 2.22 Maximum Entropy Distribution
Use a Lagrange multiplier to show that maximization of the entropy (2.86) for a discrete variable gives a distribution in which all of the probabilities \(p(x_i)\) are equal and that the corresponding value of the entropy is then \(\ln M\).
** 2.23 Entropy Bound
Consider an \(M\)-state discrete random variable \(x\), and use Jensen's inequality in the form (2.102) to show that the entropy of its distribution \(p(x)\) satisfies \(\mathrm{H}[x] \leqslant \ln M\).
** 2.24 Maximum Entropy Solution
Use the calculus of variations to show that the stationary point of the functional (2.96) is given by (2.97). Then use the constraints (2.93), (2.94), and (2.95) to eliminate the Lagrange multipliers and, hence, show that the maximum entropy solution is given by the Gaussian (2.98).
** 2.25 Entropy of Gaussian
Use the results (2.94) and (2.95) to show that the entropy of the univariate Gaussian (2.98) is given by (2.99).
** 2.26 KL Divergence of Gaussians
Suppose that \(p(\mathbf{x})\) is some fixed distribution and that we wish to approximate it using a Gaussian distribution \(q(\mathbf{x})=\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma})\). By writing down the form of the Kullback-Leibler divergence \(\mathrm{KL}(p \| q)\) for a Gaussian \(q(\mathbf{x})\) and then differentiating, show that minimization of \(\operatorname{KL}(p \| q)\) with respect to \(\boldsymbol{\mu}\) and \(\boldsymbol{\Sigma}\) leads to the result that \(\boldsymbol{\mu}\) is given by the expectation of \(\mathbf{x}\) under \(p(\mathbf{x})\) and that \(\boldsymbol{\Sigma}\) is given by the covariance.
** 2.27 KL Divergence Calculation
Evaluate the Kullback-Leibler divergence (2.100) between the two Gaussians \(p(x)=\mathcal{N}(x \mid \mu, \sigma^2)\) and \(q(x)=\mathcal{N}(x \mid m, s^2)\).
** 2.28 Alpha Divergence
The alpha family of divergences is defined by
\[
\mathrm{D}_\alpha(p \| q)=\frac{4}{1-\alpha^2}\left(1-\int p(x)^{(1+\alpha)/2} q(x)^{(1-\alpha)/2} \mathrm{~d} x\right)
\]
where \(-\infty<\alpha<\infty\) is a continuous parameter. Show that the Kullback-Leibler divergence \(\mathrm{KL}(p \| q)\) corresponds to \(\alpha \rightarrow 1\). This can be done by writing \(p^{\varepsilon}=\exp (\epsilon \ln p)=1+\epsilon \ln p+O(\epsilon^2)\) and then taking \(\epsilon \rightarrow 0\). Similarly, show that \(\mathrm{KL}(q \| p)\) corresponds to \(\alpha \rightarrow-1\).
** 2.29 Differential Entropy Bound
Consider two variables \(\mathbf{x}\) and \(\mathbf{y}\) having joint distribution \(p(\mathbf{x}, \mathbf{y})\). Show that the differential entropy of this pair of variables satisfies
\[
H[\mathbf{x}, \mathbf{y}] \leqslant \mathrm{H}[\mathbf{x}]+\mathrm{H}[\mathbf{y}]
\]
with equality if, and only if, \(\mathbf{x}\) and \(\mathbf{y}\) are statistically independent.
** 2.30 Entropy of Transformed Variables
Consider a vector \(\mathbf{x}\) of continuous variables with distribution \(p(\mathbf{x})\) and corresponding entropy \(\mathrm{H}[\mathbf{x}]\). Suppose that we make a non-singular linear transformation of \(\mathbf{x}\) to obtain a new variable \(\mathbf{y}=\mathbf{A x}\). Show that the corresponding entropy is given by \(\mathrm{H}[\mathbf{y}]=\mathrm{H}[\mathbf{x}]+\ln \operatorname{det} \mathbf{A}\) where \(\operatorname{det} \mathbf{A}\) denotes the determinant of \(\mathbf{A}\).
** 2.31 Conditional Entropy and Functions
Suppose that the conditional entropy \(\mathrm{H}[y \mid x]\) between two discrete random variables \(x\) and \(y\) is zero. Show that, for all values of \(x\) such that \(p(x)>0\), the variable \(y\) must be a function of \(x\). In other words, for each \(x\) there is only one value of \(y\) such that \(p(y \mid x) \neq 0\).
** 2.32 Strictly Convex Functions
A strictly convex function is defined as one for which every chord lies above the function. Show that this is equivalent to the condition that the second derivative of the function is positive.
** 2.33 Proof by Induction
Using proof by induction, show that the inequality (2.101) for convex functions implies the result (2.102).
** 2.34 KL Divergence and Log Likelihood
Show that, up to an additive constant, the Kullback-Leibler divergence (2.100) between the empirical distribution (2.37) and a model distribution \(q(\mathbf{x} \mid \boldsymbol{\theta})\) is equal to the negative \(\log\) likelihood function.
** 2.35 Conditional Probability Proof
Using the definition (2.107) together with the product rule of probability, prove the result (2.108).
** 2.36 Entropy and Mutual Information
Consider two binary variables \(x\) and \(y\) having the joint distribution given by
\begin{tabular}{c|c|cc}
\multicolumn{2}{c}{0} & \multicolumn{2}{c}{\(y\)} \\
& & 0 & 1 \\
\hline \multirow{2}{*}{\(x\)} & 0 & \(1/3\) & \(1/3\) \\
& 1 & 0 & \(1/3\)
\end{tabular}
Evaluate the following quantities:
(a) \(\mathrm{H}[x]\)
(c) \(\mathrm{H}[y \mid x]\)
(e) \(\mathrm{H}[x, y]\)
(b) \(\mathrm{H}[y]\)
(d) \(\mathrm{H}[x \mid y]\)
(f) \(\mathrm{I}[x, y]\).
Draw a Venn diagram to show the relationship between these various quantities.
** 2.37 Arithmetic Mean and Geometric Mean
By applying Jensen's inequality (2.102) with \(f(x)=\ln x\), show that the arithmetic mean of a set of real numbers is never less than their geometrical mean.
** 2.38 Mutual Information Relation
Using the sum and product rules of probability, show that the mutual information \(I(\mathbf{x}, \mathbf{y})\) satisfies the relation (2.110).
** 2.39 Independence and Covariance Matrix
Suppose that two variables \(z_1\) and \(z_2\) are independent so that \(p(z_1, z_2)=p(z_1) p(z_2)\). Show that the covariance matrix between these variables is diagonal. This shows that independence is a sufficient condition for two variables to be uncorrelated. Now consider two variables \(y_1\) and \(y_2\) where \(y_1\) is symmetrically distributed around 0 and \(y_2=y_1^2\). Write down the conditional distribution \(p(y_2 \mid y_1)\) and observe that this is dependent on \(y_1\), thus showing that the two variables are not independent. Now show that the covariance matrix between these two variables is again diagonal. To do this, use the relation \(p(y_1, y_2)=p(y_1) p(y_2 \mid y_1)\) to show that the off-diagonal terms are zero. This counterexample shows that zero correlation is not a sufficient condition for independence.
** 2.40 Posterior Probability of Bent Coin
Consider the bent coin in Figure 2.2. Assume that the prior probability that the convex side is heads is 0.1. Now suppose the coin is flipped 10 times and we are told that eight of the flips landed heads up and two of the flips landed tails up. Use Bayes' theorem to evaluate the posterior probability that the concave side is heads. Calculate the probability that the next flip will land heads up.
** 2.41 Regularized Error Function
By substituting (2.115) into (2.114) and making use of the result (2.66) for the log likelihood of the linear regression model, derive the result (2.117) for the regularized error function.

* Standard Distributions
** Notes
*** Discrete variables
**** Bernoulli distribution
**** Binomial distribution
**** Multinomial distribution
*** The multivariate Gaussian
**** Geometry of the Gaussian
**** Moments
**** Limitations
**** Conditional distribution
**** Marginal distribution
**** Bayes's theorem
**** Maximum likelihood
**** Sequential estimation
**** Mixtures of Gaussians
*** Periodic variables
**** Von Mises distribution
*** The exponential family
**** Sufficient statistics
*** Nonparametric methods
**** Histograms
**** Kernel densities
**** Nearest-neighbors
** 3.1 Bernoulli Distribution Properties
Verify that the Bernoulli distribution (3.2) satisfies the following properties:

\[
\sum_{x=0}^{1} p(x \mid \mu) =1  \tag{3.191}
\]
\[
\mathbb{E}[x] =\mu  \tag{3.192}
\]
\[
\operatorname{var}[x] =\mu(1-\mu) \tag{3.193}
\]

Show that the entropy \(\mathrm{H}[x]\) of a Bernoulli-distributed random binary variable \(x\) is given by

\[
\mathrm{H}[x]=-\mu \ln \mu-(1-\mu) \ln (1-\mu) \tag{3.194}
\]

** 3.2 Symmetric Bernoulli Distribution
The form of the Bernoulli distribution given by (3.2) is not symmetric between the two values of \(x\). In some situations, it will be more convenient to use an equivalent formulation for which \(x \in\{-1,1\}\), in which case the distribution can be written

\[
p(x \mid \mu)=\left(\frac{1-\mu}{2}\right)^{(1-x) / 2}\left(\frac{1+\mu}{2}\right)^{(1+x) / 2} \tag{3.195}
\]

where \(\mu \in[-1,1]\). Show that the distribution (3.195) is normalized, and evaluate its mean, variance, and entropy.

** 3.3 Binomial Distribution Normalization
In this exercise, we prove that the binomial distribution (3.9) is normalized. First, use the definition (3.10) of the number of combinations of \(m\) identical objects chosen from a total of \(N\) to show that

\[
\binom{N}{m}+\binom{N}{m-1}=\binom{N+1}{m} \tag{3.196}
\]

Use this result to prove by induction the following result:

\[
(1+x)^{N}=\sum_{m=0}^{N}\binom{N}{m} x^{m} \tag{3.197}
\]

which is known as the binomial theorem and which is valid for all real values of \(x\). Finally, show that the binomial distribution is normalized, so that

\[
\sum_{m=0}^{N}\binom{N}{m} \mu^{m}(1-\mu)^{N-m}=1 \tag{3.198}
\]

which can be done by first pulling a factor \((1-\mu)^{N}\) out of the summation and then making use of the binomial theorem.

** 3.4 Mean and Variance of Binomial Distribution
Show that the mean of the binomial distribution is given by (3.11). To do this, differentiate both sides of the normalization condition (3.198) with respect to \(\mu\) and then rearrange to obtain an expression for the mean of \(n\). Similarly, by differentiating (3.198) twice with respect to \(\mu\) and making use of the result (3.11) for the mean of the binomial distribution, prove the result (3.12) for the variance of the binomial.

** 3.5 Mode of Multivariate Gaussian
Show that the mode of the multivariate Gaussian (3.26) is given by \(\boldsymbol{\mu}\).

** 3.6 Linear Transformation of Gaussian
Suppose that \(\mathrm{x}\) has a Gaussian distribution with mean \(\boldsymbol{\mu}\) and covariance \(\boldsymbol{\Sigma}\). Show that the linearly transformed variable \(\mathbf{A x}+\mathbf{b}\) is also Gaussian, and find its mean and covariance.

** 3.7 KL Divergence of Gaussians
Show that the Kullback-Leibler divergence between two Gaussian distributions
\(q(\mathbf{x})=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{q}, \boldsymbol{\Sigma}_{q}\right)\) and
\(p(\mathbf{x})=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{p}, \boldsymbol{\Sigma}_{p}\right)\) is given by

\[
\mathrm{KL}(q(\mathbf{x}) \| p(\mathbf{x})) =\frac{1}{2}\left\{\ln \frac{\left|\boldsymbol{\Sigma}_{p}\right|}{\left|\boldsymbol{\Sigma}_{q}\right|}-D+\operatorname{Tr}\left(\boldsymbol{\Sigma}_{p}^{-1} \boldsymbol{\Sigma}_{q}\right)+\left(\boldsymbol{\mu}_{p}-\boldsymbol{\mu}_{q}\right)^{\mathrm{T}} \boldsymbol{\Sigma}_{p}^{-1}\left(\boldsymbol{\mu}_{p}-\boldsymbol{\mu}_{q}\right)\right\} \tag{3.199}
\]

where \(\operatorname{Tr}(\cdot)\) denotes the trace of a matrix, and \(D\) is the dimensionality of \(\mathbf{x}\).

** 3.8 Maximum Entropy Multivariate Gaussian
This exercise demonstrates that the multivariate distribution with maximum entropy, for a given covariance, is a Gaussian. The entropy of a distribution \(p(\mathbf{x})\) is given by

\[
\mathrm{H}[\mathbf{x}]=-\int p(\mathbf{x}) \ln p(\mathbf{x}) \mathrm{d} \mathbf{x} \tag{3.200}
\]

We wish to maximize \(\mathrm{H}[\mathbf{x}]\) over all distributions \(p(\mathbf{x})\) subject to the constraints that \(p(\mathbf{x})\) is normalized and that it has a specific mean and covariance, so that

\[
\int p(\mathbf{x}) \mathrm{d} \mathbf{x}=1  \tag{3.201}
\]
\[
\int p(\mathbf{x}) \mathbf{x} \mathrm{d} \mathbf{x}=\boldsymbol{\mu}  \tag{3.202}
\]
\[
\int p(\mathbf{x})(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathrm{d} \mathbf{x}=\mathbf{\Sigma} \tag{3.203}
\]

By performing a variational maximization of (3.200) and using Lagrange multipliers to enforce the constraints (3.201), (3.202), and (3.203), show that the maximum likelihood distribution is given by the Gaussian (3.26).

** 3.9 Entropy of Multivariate Gaussian
Show that the entropy of the multivariate Gaussian \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\) is given by

\[
\mathrm{H}[\mathbf{x}]=\frac{1}{2} \ln |\boldsymbol{\Sigma}|+\frac{D}{2}(1+\ln (2 \pi)) \tag{3.204}
\]

where \(D\) is the dimensionality of \(\mathbf{x}\).

** 3.10 Entropy of Sum of Gaussians
Consider two random variables \(x_{1}\) and \(x_{2}\) having Gaussian distributions with means \(\mu_{1}\) and \(\mu_{2}\) and precisions \(\tau_{1}\) and \(\tau_{2}\), respectively. Derive an expression for the differential entropy of the variable \(x=x_{1}+x_{2}\). To do this, first find the distribution of \(x\) by using the relation

\[
p(x)=\int_{-\infty}^{\infty} p\left(x \mid x_{2}\right) p\left(x_{2}\right) \mathrm{d} x_{2} \tag{3.205}
\]

and completing the square in the exponent. Then observe that this represents the convolution of two Gaussian distributions, which itself will be Gaussian, and finally make use of the result (2.99) for the entropy of the univariate Gaussian.

** 3.11 Symmetric Precision Matrix
Consider the multivariate Gaussian distribution given by (3.26). By writing the precision matrix (inverse covariance matrix) as the sum of a symmetric and an antisymmetric matrix, show that the antisymmetric term does not appear in the exponent of the Gaussian, and hence, that the precision matrix may be taken to be symmetric without loss of generality. Because the inverse of a symmetric matrix is also symmetric (see Exercise 3.16), it follows that the covariance matrix may also be chosen to be symmetric without loss of generality.

** 3.12 Real Eigenvalues and Orthonormal Eigenvectors
Consider a real, symmetric matrix \(\Sigma\) whose eigenvalue equation is given by (3.28). By taking the complex conjugate of this equation, subtracting the original equation, and then forming the inner product with eigenvector \(\mathbf{u}_{i}\), show that the eigenvalues \(\lambda_{i}\) are real. Similarly, use the symmetry property of \(\Sigma\) to show that two eigenvectors \(\mathbf{u}_{i}\) and \(\mathbf{u}_{j}\) will be orthogonal provided \(\lambda_{j} \neq \lambda_{i}\). Finally, show that, without loss of generality, the set of eigenvectors can be chosen to be orthonormal, so that they satisfy (3.29), even if some of the eigenvalues are zero.

** 3.13 Eigenvector Expansion
Show that a real, symmetric matrix \(\boldsymbol{\Sigma}\) having the eigenvector equation (3.28) can be expressed as an expansion in the eigenvectors, with coefficients given by the eigenvalues, of the form (3.31). Similarly, show that the inverse matrix \(\Sigma^{-1}\) has a representation of the form (3.32).

** 3.14 Positive Definite Matrix
A positive definite matrix \(\Sigma\) can be defined as one for which the quadratic form

\[
\mathbf{a}^{\mathrm{T}} \boldsymbol{\Sigma} \mathbf{a} \tag{3.206}
\]

is positive for any real value of the vector \(\mathbf{a}\). Show that a necessary and sufficient condition for \(\boldsymbol{\Sigma}\) to be positive definite is that all the eigenvalues \(\lambda_{i}\) of \(\boldsymbol{\Sigma}\), defined by (3.28), are positive.

** 3.15 Independent Parameters in Symmetric Matrix
Show that a real, symmetric matrix of size \(D \times D\) has \(D(D+1) / 2\) independent parameters.

** 3.16 Inverse of Symmetric Matrix
Show that the inverse of a symmetric matrix is itself symmetric.

** 3.17 Volume of Hyperellipsoid
By diagonalizing the coordinate system using the eigenvector expansion (3.31), show that the volume contained within the hyperellipsoid corresponding to a constant Mahalanobis distance \(\Delta\) is given by

\[
V_{D}|\boldsymbol{\Sigma}|^{1 / 2} \Delta^{D} \tag{3.207}
\]

where \(V_{D}\) is the volume of the unit sphere in \(D\) dimensions, and the Mahalanobis distance is defined by (3.27).

** 3.18 Matrix Identity Proof
Prove the identity (3.60) by multiplying both sides by the matrix

\[
\left(\begin{array}{ll}
A & B  \tag{3.208}\\
C & D
\end{array}\right)
\]

and making use of the definition (3.61).

** 3.19 Conditional Distribution Marginalizing a Variable
In Sections 3.2.4 and 3.2.5, we considered the conditional and marginal distributions for a multivariate Gaussian. More generally, we can consider a partitioning of the components of \(\mathbf{x}\) into three groups \(\mathbf{x}_{a}, \mathbf{x}_{b}\), and \(\mathbf{x}_{c}\), with a corresponding partitioning of the mean vector \(\boldsymbol{\mu}\) and of the covariance matrix \(\boldsymbol{\Sigma}\) in the form

\[
\boldsymbol{\mu}=\left(\begin{array}{l}
\boldsymbol{\mu}_{a}  \tag{3.209}\\
\boldsymbol{\mu}_{b} \\
\boldsymbol{\mu}_{c}
\end{array}\right), \quad \boldsymbol{\Sigma}=\left(\begin{array}{lll}
\boldsymbol{\Sigma}_{a a} & \boldsymbol{\Sigma}_{a b} & \boldsymbol{\Sigma}_{a c} \\
\boldsymbol{\Sigma}_{b a} & \boldsymbol{\Sigma}_{b b} & \boldsymbol{\Sigma}_{b c} \\
\boldsymbol{\Sigma}_{c a} & \boldsymbol{\Sigma}_{c b} & \boldsymbol{\Sigma}_{c c}
\end{array}\right)
\]

By making use of the results of Section 3.2, find an expression for the conditional distribution \(p\left(\mathbf{x}_{a} \mid \mathbf{x}_{b}\right)\) in which \(\mathbf{x}_{c}\) has been marginalized out.

** 3.20 Woodbury Matrix Inversion Formula
A very useful result from linear algebra is the Woodbury matrix inversion formula given by

\[
(\mathbf{A}+\mathbf{B C D})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{B}\left(\mathbf{C}^{-1}+\mathbf{D} \mathbf{A}^{-1} \mathbf{B}\right)^{-1} \mathbf{D A}^{-1} \tag{3.210}
\]

By multiplying both sides by \((\mathbf{A}+\mathbf{B C D})\), prove the correctness of this result.

** 3.21 Mean and Covariance of Sum of Random Vectors
Let \(\mathbf{x}\) and \(\mathbf{z}\) be two independent random vectors, so that \(p(\mathbf{x}, \mathbf{z})=p(\mathbf{x}) p(\mathbf{z})\). Show that the mean of their sum \(\mathbf{y}=\mathbf{x}+\mathbf{z}\) is given by the sum of the means of each of the variables separately. Similarly, show that the covariance matrix of \(\mathbf{y}\) is given by the sum of the covariance matrices of \(\mathbf{x}\) and \(\mathbf{z}\).

** 3.22 Marginal and Conditional Distributions
Consider a joint distribution over the variable

\[
\mathrm{z}=\binom{\mathrm{x}}{\mathrm{y}} \tag{3.211}
\]

whose mean and covariance are given by (3.92) and (3.89), respectively. By making use of the results (3.76) and (3.77), show that the marginal distribution \(p(\mathbf{x})\) is given by (3.83). Similarly, by making use of the results (3.65) and (3.66), show that the conditional distribution \(p(\mathbf{y} \mid \mathbf{x})\) is given by (3.84).

** 3.23 Inverse of Precision Matrix
Using the partitioned matrix inversion formula (3.60), show that the inverse of the precision matrix (3.88) is given by the covariance matrix (3.89).

** 3.24 Verification of Result
By starting from (3.91) and making use of the result (3.89), verify the result (3.92).

** 3.25 Marginal Distribution of Sum of Gaussians
Consider two multi-dimensional random vectors \(\mathbf{x}\) and \(\mathbf{z}\) having Gaussian distributions \(p(\mathbf{x})=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}_{\mathbf{x}}, \boldsymbol{\Sigma}_{\mathbf{x}}\right)\) and \(p(\mathbf{z})=\mathcal{N}\left(\mathbf{z} \mid \boldsymbol{\mu}_{\mathbf{z}}, \boldsymbol{\Sigma}_{\mathbf{z}}\right)\), respectively, together with their sum \(\mathbf{y}=\mathbf{x}+\mathbf{z}\). By considering the linear-Gaussian model comprising the product of the marginal distribution \(p(\mathbf{x})\) and the conditional distribution \(p(\mathbf{y} \mid \mathbf{x})\) and making use of the results (3.93) and (3.94), show that the marginal distribution of \(p(\mathbf{y})\) is given by

\[
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \boldsymbol{\mu}_{\mathbf{x}}+\boldsymbol{\mu}_{\mathbf{z}}, \boldsymbol{\Sigma}_{\mathbf{x}}+\boldsymbol{\Sigma}_{\mathbf{z}}\right) \tag{3.212}
\]

** 3.26 Mean and Covariance of Marginal Distribution
This exercise and the next provide practice at manipulating the quadratic forms that arise in linear-Gaussian models, and they also serve as an independent check of results derived in the main text. Consider a joint distribution \(p(\mathbf{x}, \mathbf{y})\) defined by the marginal and conditional distributions given by (3.83) and (3.84). By examining the quadratic form in the exponent of the joint distribution and using the technique of 'completing the square' discussed in Section 3.2, find expressions for the mean and covariance of the marginal distribution \(p(\mathbf{y})\) in which the variable \(\mathbf{x}\) has been integrated out. To do this, make use of the Woodbury matrix inversion formula (3.210). Verify that these results agree with (3.93) and (3.94).

** 3.27 Mean and Covariance of Conditional Distribution
Consider the same joint distribution as in Exercise 3.26, but now use the technique of completing the square to find expressions for the mean and covariance of the conditional distribution \(p(\mathbf{x} \mid \mathbf{y})\). Again, verify that these agree with the corresponding expressions (3.95) and (3.96).

** 3.28 Maximum Likelihood Covariance Matrix
To find the maximum likelihood solution for the covariance matrix of a multivariate Gaussian, we need to maximize the \(\log\) likelihood function (3.102) with respect to \(\Sigma\), noting that the covariance matrix must be symmetric and positive definite. Here we proceed by ignoring these constraints and doing a straightforward maximization. Using the results (A.21), (A.26), and (A.28) from Appendix A, show that the covariance matrix \(\Sigma\) that maximizes the log likelihood function (3.102) is given by the sample covariance (3.106). We note that the final result is necessarily symmetric and positive definite (provided the sample covariance is non-singular).

** 3.29 Expectation of Gaussian Data Points
Use the result (3.42) to prove (3.46). Now, using the results (3.42) and (3.46), show that

\[
\mathbb{E}\left[\mathbf{x}_{n} \mathbf{x}_{m}^{\mathrm{T}}\right]=\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+I_{n m} \boldsymbol{\Sigma} \tag{3.213}
\]

where \(\mathbf{x}_{n}\) denotes a data point sampled from a Gaussian distribution with mean \(\boldsymbol{\mu}\) and covariance \(\boldsymbol{\Sigma}\), and \(I_{n m}\) denotes the \((n, m)\) element of the identity matrix. Hence, prove the result (3.108).

** 3.30 Trigonometric Identities from Euler's Formula
The various trigonometric identities used in the discussion of periodic variables in this chapter can be proven easily from the relation

\[
\exp (i A)=\cos A+i \sin A \tag{3.214}
\]

in which \(i\) is the square root of minus one. By considering the identity

\[
\exp (i A) \exp (-i A)=1 \tag{3.215}
\]

prove the result (3.127). Similarly, using the identity

\[
\cos (A-B)=\Re \exp \{i(A-B)\} \tag{3.216}
\]

where \(\Re\) denotes the real part, prove (3.128). Finally, by using \(\sin (A-B)=\Im \exp \{i(A-B)\}\), where \(\Im\) denotes the imaginary part, prove the result (3.133).

** 3.31 Gaussian Limit of von Mises Distribution
For large \(m\), the von Mises distribution (3.129) becomes sharply peaked around the mode \(\theta_{0}\). By defining \(\xi=m^{1 / 2}\left(\theta-\theta_{0}\right)\) and taking the Taylor expansion of the cosine function given by

\[
\cos \alpha=1-\frac{\alpha^{2}}{2}+O\left(\alpha^{4}\right) \tag{3.217}
\]

show that as \(m \rightarrow \infty\), the von Mises distribution tends to a Gaussian.

** 3.32 Solving for \(\theta_0\)
Using the trigonometric identity (3.133), show that the solution of (3.132) for \(\theta_{0}\) is given by (3.134).

** 3.33 Maxima and Minima of von Mises Distribution
By computing the first and second derivatives of the von Mises distribution (3.129), and using \(I_{0}(m)>0\) for \(m>0\), show that the maximum of the distribution occurs when \(\theta=\theta_{0}\) and that the minimum occurs when \(\theta=\theta_{0}+\pi(\bmod 2 \pi)\).

** 3.34 Maximum Likelihood Concentration Parameter
By making use of the result (3.118) together with (3.134) and the trigonometric identity (3.128), show that the maximum likelihood solution \(m_{\mathrm{ML}}\) for the concentration of the von Mises distribution satisfies \(A\left(m_{\mathrm{ML}}\right)=\bar{r}\) where \(\bar{r}\) is the radius of the mean of the observations viewed as unit vectors in the two-dimensional Euclidean plane, as illustrated in Figure 3.9.

** 3.35 Multivariate Gaussian in Exponential Family Form
Verify that the multivariate Gaussian distribution can be cast in exponential family form (3.138), and derive expressions for \(\boldsymbol{\eta}, \mathbf{u}(\mathbf{x}), h(\mathbf{x})\), and \(g(\boldsymbol{\eta})\) analogous to (3.164) to (3.167).

** 3.36 Covariance of Sufficient Statistics
The result (3.172) showed that the negative gradient of \(\ln g(\boldsymbol{\eta})\) for the exponential family is given by the expectation of \(\mathbf{u}(\mathbf{x})\). By taking the second derivatives of (3.139), show that

\[
-\nabla \nabla \ln g(\boldsymbol{\eta})=\mathbb{E}\left[\mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathrm{T}}\right]-\mathbb{E}[\mathbf{u}(\mathbf{x})] \mathbb{E}\left[\mathbf{u}(\mathbf{x})^{\mathrm{T}}\right]=\operatorname{cov}[\mathbf{u}(\mathbf{x})] \tag{3.218}
\]

** 3.37 Maximum Likelihood Estimator for Density
Consider a histogram-like density model in which the space \(\mathrm{x}\) is divided into fixed regions for which the density \(p(\mathbf{x})\) takes the constant value \(h_{i}\) over the \(i\) th region. The volume of region \(i\) is denoted \(\Delta_{i}\). Suppose we have a set of \(N\) observations of \(\mathbf{x}\) such that \(n_{i}\) of these observations fall in region \(i\). Using a Lagrange multiplier to enforce the normalization constraint on the density, derive an expression for the maximum likelihood estimator for the \(\left\{h_{i}\right\}\).

** 3.38 Improper Distribution of \(K\)-nearest-neighbour Density Model
Show that the \(K\)-nearest-neighbour density model defines an improper distribution whose integral over all space is divergent.

* Single-layer Networks: Regression
** Notes
*** Linear regression
**** Basis functions
**** Likelihood function
**** Maximum likelihood
**** Geometry of least squares
**** Sequential learning
**** Regularized least squares
**** Multiple outputs
*** Decision theory
*** The bias-variance-tradeoff
** 4.1 Minimizing Sum-of-Squares Error
Consider the sum-of-squares error function given by (1.2) in which the function \(y(x, \mathbf{w})\) is given by the polynomial (1.1). Show that the coefficients \(\mathbf{w}=\left\{w_{i}\right\}\) that minimize this error function are given by the solution to the following set of linear equations:

\[
\sum_{j=0}^{M} A_{i j} w_{j}=T_{i} \tag{4.53}
\]

where

\[
A_{i j}=\sum_{n=1}^{N}\left(x_{n}\right)^{i+j}, \quad T_{i}=\sum_{n=1}^{N}\left(x_{n}\right)^{i} t_{n} \tag{4.54}
\]

Here a suffix \(i\) or \(j\) denotes the index of a component, whereas \((x)^{i}\) denotes \(x\) raised to the power of \(i\).

** 4.2 Regularized Sum-of-Squares Error
Write down the set of coupled linear equations, analogous to (4.53), satisfied by the coefficients \(w_{i}\) that minimize the regularized sum-of-squares error function given by (1.4).

** 4.3 Tanh and Logistic Sigmoid Functions
Show that the tanh function defined by

\[
\tanh (a)=\frac{e^{a}-e^{-a}}{e^{a}+e^{-a}} \tag{4.55}
\]

and the logistic sigmoid function defined by (4.6) are related by

\[
\tanh (a)=2 \sigma(2 a)-1 \tag{4.56}
\]

Hence, show that a general linear combination of logistic sigmoid functions of the form

\[
y(x, \mathbf{w})=w_{0}+\sum_{j=1}^{M} w_{j} \sigma\left(\frac{x-\mu_{j}}{s}\right) \tag{4.57}
\]

is equivalent to a linear combination of tanh functions of the form

\[
y(x, \mathbf{u})=u_{0}+\sum_{j=1}^{M} u_{j} \tanh \left(\frac{x-\mu_{j}}{2 s}\right) \tag{4.58}
\]

and find expressions to relate the new parameters \(\left\{u_{1}, \ldots, u_{M}\right\}\) to the original parameters \(\left\{w_{1}, \ldots, w_{M}\right\}\).

** 4.4 Orthogonal Projection
Show that the matrix

\[
\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{\mathrm{T}} \tag{4.59}
\]

takes any vector \(\mathbf{v}\) and projects it onto the space spanned by the columns of \(\boldsymbol{\Phi}\). Use this result to show that the least-squares solution (4.14) corresponds to an orthogonal projection of the vector \(\mathbf{t}\) onto the manifold \(\mathcal{S}\), as shown in Figure 4.3.

** 4.5 Weighted Sum-of-Squares Error
Consider a data set in which each data point \(t_{n}\) is associated with a weighting factor \(r_{n}>0\), so that the sum-of-squares error function becomes

\[
E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N} r_{n}\left\{t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right\}^{2} \tag{4.60}
\]

Find an expression for the solution \(\mathbf{w}^{\star}\) that minimizes this error function. Give two alternative interpretations of the weighted sum-of-squares error function in terms of (i) data-dependent noise variance and (ii) replicated data points.

** 4.6 Regularized Sum-of-Squares Solution
By setting the gradient of (4.26) with respect to \(\mathbf{w}\) to zero, show that the exact minimum of the regularized sum-of-squares error function for linear regression is given by (4.27).

** 4.7 Maximum Likelihood for Multivariate Targets
Consider a linear basis function regression model for a multivariate target variable \(\mathbf{t}\) having a Gaussian distribution of the form

\[
p(\mathbf{t} \mid \mathbf{W}, \boldsymbol{\Sigma})=\mathcal{N}(\mathbf{t} \mid \mathbf{y}(\mathbf{x}, \mathbf{W}), \boldsymbol{\Sigma}) \tag{4.61}
\]

where

\[
\mathbf{y}(\mathbf{x}, \mathbf{W})=\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}) \tag{4.62}
\]

together with a training data set comprising input basis vectors \(\phi\left(\mathrm{x}_{n}\right)\) and corresponding target vectors \(\mathbf{t}_{n}\), with \(n=1, \ldots, N\). Show that the maximum likelihood solution \(\mathbf{W}_{\text{ML}}\) for the parameter matrix \(\mathbf{W}\) has the property that each column is given by an expression of the form (4.14), which was the solution for an isotropic noise distribution. Note that this is independent of the covariance matrix \(\boldsymbol{\Sigma}\). Show that the maximum likelihood solution for \(\Sigma\) is given by

\[
\boldsymbol{\Sigma}=\frac{1}{N} \sum_{n=1}^{N}\left(\mathbf{t}_{n}-\mathbf{W}_{\mathrm{ML}}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)\left(\mathbf{t}_{n}-\mathbf{W}_{\mathrm{ML}}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)^{\mathrm{T}} \tag{4.63}
\]

** 4.8 Generalization to Multiple Targets
Consider the generalization of the squared-loss function (4.35) for a single target variable \(t\) to multiple target variables described by the vector \(\mathbf{t}\) given by

\[
\mathbb{E}[L(\mathbf{t}, \mathbf{f}(\mathbf{x}))]=\iint\|\mathbf{f}(\mathbf{x})-\mathbf{t}\|^{2} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{x} \mathrm{d} \mathbf{t} \tag{4.64}
\]

Using the calculus of variations, show that the function \(\mathbf{f}(\mathbf{x})\) for which this expected loss is minimized is given by

\[
\mathbf{f}(\mathbf{x})=\mathbb{E}_{t}[\mathbf{t} \mid \mathbf{x}] \tag{4.65}
\]

** 4.9 Expected Squared Loss for Multiple Targets
By expansion of the square in (4.64), derive a result analogous to (4.39) and, hence, show that the function \(\mathbf{f}(\mathbf{x})\) that minimizes the expected squared loss for a vector \(t\) of target variables is again given by the conditional expectation of \(t\) in the form (4.65).

** 4.10 Expected \(L_{q}\) Loss Minimization
Rederive the result (4.65) by first expanding (4.64) analogous to (4.39).

** 4.11 Generalized Univariate Gaussian Distribution
The following distribution

\[
p\left(x \mid \sigma^{2}, q\right)=\frac{q}{2\left(2 \sigma^{2}\right)^{1 / q} \Gamma(1 / q)} \exp \left(-\frac{|x|^{q}}{2 \sigma^{2}}\right) \tag{4.66}
\]

is a generalization of the univariate Gaussian distribution. Here \(\Gamma(x)\) is the gamma function defined by

\[
\Gamma(x)=\int_{-\infty}^{\infty} u^{x-1} e^{-u} \mathrm{~d} u \tag{4.67}
\]

Show that this distribution is normalized so that

\[
\int_{-\infty}^{\infty} p\left(x \mid \sigma^{2}, q\right) \mathrm{d} x=1 \tag{4.68}
\]

and that it reduces to the Gaussian when \(q=2\). Consider a regression model in which the target variable is given by \(t=y(\mathbf{x}, \mathbf{w})+\epsilon\) and \(\epsilon\) is a random noise variable drawn from the distribution (4.66). Show that the \(\log\) likelihood function over \(\mathbf{w}\) and \(\sigma^{2}\), for an observed data set of input vectors \(\mathbf{X}=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\right\}\) and corresponding target variables \(\mathbf{t}=\left(t_{1}, \ldots, t_{N}\right)^{\mathrm{T}}\), is given by

\[
\ln p\left(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \sigma^{2}\right)=-\frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}\left|y\left(\mathbf{x}_{n}, \mathbf{w}\right)-t_{n}\right|^{q}-\frac{N}{q} \ln \left(2 \sigma^{2}\right)+\text{const} \tag{4.69}
\]

where 'const' denotes terms independent of both \(\mathbf{w}\) and \(\sigma^{2}\). Note that, as a function of \(\mathbf{w}\), this is the \(L_{q}\) error function considered in Section 4.2.

** 4.12 Minimizing Expected \(L_{q}\) Loss
Consider the expected loss for regression problems under the \(L_{q}\) loss function given by (4.40). Write down the condition that \(y(\mathbf{x})\) must satisfy to minimize \(\mathbb{E}\left[L_{q}\right]\). Show that, for \(q=1\), this solution represents the conditional median, i.e., the function \(y(\mathbf{x})\) such that the probability mass for \(t<y(\mathbf{x})\) is the same as for \(t \geqslant y(\mathbf{x})\). Also show that the minimum expected \(L_{q}\) loss for \(q \rightarrow 0\) is given by the conditional mode, i.e., by the function \(y(\mathbf{x})\) being equal to the value of \(t\) that maximizes \(p(t \mid \mathbf{x})\) for each \(\mathbf{x}\).

* Single-layer Networks: Classification
** Notes
*** Discriminant functions
**** Two classes
**** Multiple classes
**** 1-of-K coding
**** Least squares and classification
*** Decision theory
**** Misclasification rate
**** Expected loss
**** The reject option
**** Inference and decision
**** Classifier accuracy
**** ROC curve
*** Generative classifiers
**** Continuous inputs
**** Maximum likelihood solution
**** Discrete features
**** Exponential family
*** Discriminative classifiers
**** Activation functions
**** Fixed basis functions
**** Logistic regression
**** Multi-class logistic function
**** Probit regression
**** Canonical link functions
** 5.1 Conditional Expectation and Posterior Probability
Consider a classification problem with \(K\) classes and a target vector \(\mathrm{t}\) that uses a 1 -of- \(K\) binary coding scheme. Show that the conditional expectation \(\mathbb{E}[\mathbf{t} \mid \mathbf{x}]\) is given by the posterior probability \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)\).

** 5.2 Convex Hulls and Linear Separability
Given a set of data points \(\left\{\mathbf{x}_{n}\right\}\), we can define the convex hull to be the set of all points \(\mathbf{x}\) given by

\[
\mathbf{x}=\sum_{n} \alpha_{n} \mathbf{x}_{n} \tag{5.96}
\]

where \(\alpha_{n} \geqslant 0\) and \(\sum_{n} \alpha_{n}=1\). Consider a second set of points \(\left\{\mathbf{y}_{n}\right\}\) together with their corresponding convex hull. By definition, the two sets of points will be linearly separable if there exists a vector \(\widehat{\mathbf{w}}\) and a scalar \(w_{0}\) such that \(\widehat{\mathbf{w}}^{\mathrm{T}} \mathbf{x}_{n}+w_{0}>0\) for all \(\mathbf{x}_{n}\) and \(\widehat{\mathbf{w}}^{\mathrm{T}} \mathbf{y}_{n}+w_{0}<0\) for all \(\mathbf{y}_{n}\). Show that if their convex hulls intersect, the two sets of points cannot be linearly separable, and conversely that if they are linearly separable, their convex hulls do not intersect.

** 5.3 Linear Constraints in Sum-of-Squares Error Minimization
Consider the minimization of a sum-of-squares error function (5.14), and suppose that all the target vectors in the training set satisfy a linear constraint

\[
\mathbf{a}^{\mathrm{T}} \mathbf{t}_{n}+b=0 \tag{5.97}
\]

where \(\mathbf{t}_{n}\) corresponds to the \(n\)th row of the matrix \(\mathbf{T}\) in (5.14). Show that as a consequence of this constraint, the elements of the model prediction \(\mathbf{y}(\mathbf{x})\) given by the least-squares solution (5.16) also satisfy this constraint, so that

\[
\mathbf{a}^{\mathrm{T}} \mathbf{y}(\mathbf{x})+b=0 \tag{5.98}
\]

To do so, assume that one of the basis functions \(\phi_{0}(\mathbf{x})=1\) so that the corresponding parameter \(w_{0}\) plays the role of a bias.

** 5.4 Multiple Linear Constraints
Extend the result of Exercise 5.3 to show that if multiple linear constraints are satisfied simultaneously by the target vectors, then the same constraints will also be satisfied by the least-squares prediction of a linear model.

** 5.5 Derivation of the F-score
Use the definition (5.38), along with (5.30) and (5.31) to derive the result (5.39) for the F-score.

** 5.6 Probability of Misclassification
Consider two non-negative numbers \(a\) and \(b\), and show that, if \(a \leqslant b\), then \(a \leqslant(a b)^{1 / 2}\). Use this result to show that, if the decision regions of a two-class classification problem are chosen to minimize the probability of misclassification, this probability will satisfy

\[
p(\text{mistake}) \leqslant \int\left\{p\left(\mathbf{x}, \mathcal{C}_{1}\right) p\left(\mathbf{x}, \mathcal{C}_{2}\right)\right\}^{1 / 2} \mathrm{~d} \mathbf{x} \tag{5.99}
\]

** 5.7 Expected Risk Minimization with Loss Matrix
Given a loss matrix with elements \(L_{k j}\), the expected risk is minimized if, for each \(\mathbf{x}\), we choose the class that minimizes (5.23). Verify that, when the loss matrix is given by \(L_{k j}=1-I_{k j}\), where \(I_{k j}\) are the elements of the identity matrix, this reduces to the criterion of choosing the class having the largest posterior probability. What is the interpretation of this form of loss matrix?

** 5.8 General Loss Matrix
Derive the criterion for minimizing the expected loss when there is a general loss matrix and general prior probabilities for the classes.

** 5.9 Prior Class Probability
Consider the average of the posterior probabilities over a set of \(N\) data points in the form

\[
\frac{1}{N} \sum_{N=1}^{N} p\left(\mathcal{C}_{k} \mid \mathbf{x}_{n}\right) \tag{5.100}
\]

By taking the limit \(N \rightarrow \infty\), show that this quantity approaches the prior class probability \(p\left(\mathcal{C}_{k}\right)\).

** 5.10 Decision Criterion with Reject Option
Consider a classification problem in which the loss incurred when an input vector from class \(\mathcal{C}_{k}\) is classified as belonging to class \(\mathcal{C}_{j}\) is given by the loss matrix \(L_{k j}\) and for which the loss incurred in selecting the reject option is \(\lambda\). Find the decision criterion that will give the minimum expected loss. Verify that this reduces to the reject criterion discussed in Section 5.2.3 when the loss matrix is given by \(L_{k j}=1-I_{k j}\). What is the relationship between \(\lambda\) and the rejection threshold \(\theta\)?

** 5.11 Properties of Logistic Sigmoid Function
Show that the logistic sigmoid function (5.42) satisfies the property \(\sigma(-a)=1-\sigma(a)\) and that its inverse is given by \(\sigma^{-1}(y)=\ln \{y /(1-y)\}\).

** 5.12 Posterior Class Probability in Gaussian Generative Model
Using (5.40) and (5.41), derive the result (5.48) for the posterior class probability in the two-class generative model with Gaussian densities, and verify the results (5.49) and (5.50) for the parameters \(\mathbf{w}\) and \(w_{0}\).

** 5.13 Maximum Likelihood for Prior Probabilities
Consider a generative classification model for \(K\) classes defined by prior class probabilities \(p\left(\mathcal{C}_{k}\right)=\pi_{k}\) and general class-conditional densities \(p\left(\phi \mid \mathcal{C}_{k}\right)\) where \(\phi\) is the input feature vector. Suppose we are given a training data set \(\left\{\phi_{n}, \mathbf{t}_{n}\right\}\) where \(n=1, \ldots, N\), and \(\mathbf{t}_{n}\) is a binary target vector of length \(K\) that uses the 1 -of- \(K\) coding scheme, so that it has components \(t_{n j}=I_{j k}\) if data point \(n\) is from class \(\mathcal{C}_{k}\). Assuming that the data points are drawn independently from this model, show that the maximum-likelihood solution for the prior probabilities is given by

\[
\pi_{k}=\frac{N_{k}}{N} \tag{5.101}
\]

where \(N_{k}\) is the number of data points assigned to class \(\mathcal{C}_{k}\).

** 5.14 Maximum Likelihood for Gaussian Class-Conditional Densities
Consider the classification model of Exercise 5.13 and now suppose that the class-conditional densities are given by Gaussian distributions with a shared covariance matrix, so that

\[
p\left(\boldsymbol{\phi} \mid \mathcal{C}_{k}\right)=\mathcal{N}\left(\boldsymbol{\phi} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}\right) \tag{5.102}
\]

Show that the maximum likelihood solution for the mean of the Gaussian distribution for class \(\mathcal{C}_{k}\) is given by

\[
\boldsymbol{\mu}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} t_{n k} \boldsymbol{\phi}_{n} \tag{5.103}
\]

which represents the mean of those feature vectors assigned to class \(\mathcal{C}_{k}\). Similarly, show that the maximum likelihood solution for the shared covariance matrix is given by

\[
\boldsymbol{\Sigma}=\sum_{k=1}^{K} \frac{N_{k}}{N} \mathbf{S}_{k} \tag{5.104}
\]

where

\[
\mathbf{S}_{k}=\frac

* Deep Neural Networks
** Notes
*** Limitations of fixed basis functions
**** The curse of dimensionality
**** High-dimensional spaces
**** Data manifolds
**** Data-dependent basis functions
*** Multilayer networks
**** Parameter matrices
**** Universal approximation
**** Hidden unit activation functions
**** Weight-space symmetries
*** Deep networks
**** Hierarchical representations
**** Distributed representations
**** Representation learning
**** Transfer learning
**** Contrastive learning
**** General network architectures
**** Tensors
*** Error functions
**** Regression
**** Binary classification
**** Multiclass classification
*** Mixture density networks
**** Robot kinematics example
**** Conditional mixture distributions
**** Gradient optimization
**** Predictive distribution
** 6.1 Surface Area and Volume of Hypersphere
Use the result (2.126) to derive an expression for the surface area \(S_{D}\) and the volume \(V_{D}\) of a hypersphere of unit radius in \(D\) dimensions. To do this, consider the following result, which is obtained by transforming from Cartesian to polar coordinates:

\[
\prod_{i=1}^{D} \int_{-\infty}^{\infty} e^{-x_{i}^{2}} \mathrm{~d} x_{i}=S_{D} \int_{0}^{\infty} e^{-r^{2}} r^{D-1} \mathrm{~d} r \tag{6.51}
\]

Using the gamma function, defined by

\[
\Gamma(x)=\int_{0}^{\infty} t^{x-1} e^{-t} \mathrm{~d} t \tag{6.52}
\]

together with (2.126), evaluate both sides of this equation, and hence show that

\[
S_{D}=\frac{2 \pi^{D / 2}}{\Gamma(D / 2)} \tag{6.53}
\]

Next, by integrating with respect to the radius from 0 to 1, show that the volume of the unit hypersphere in \(D\) dimensions is given by

\[
V_{D}=\frac{S_{D}}{D} \tag{6.54}
\]

Finally, use the results \(\Gamma(1)=1\) and \(\Gamma(3 / 2)=\sqrt{\pi} / 2\) to show that (6.53) and (6.54) reduce to the usual expressions for \(D=2\) and \(D=3\).

** 6.2 Volume Ratio of Hypersphere to Hypercube
Consider a hypersphere of radius \(a\) in \(D\) dimensions together with the concentric hypercube of side \(2 a\), so that the hypersphere touches the hypercube at the centres of each of its sides. By using the results of Exercise 6.1, show that the ratio of the volume of the hypersphere to the volume of the cube is given by

\[
\frac{\text{volume of hypersphere}}{\text{volume of cube}}=\frac{\pi^{D / 2}}{D 2^{D-1} \Gamma(D / 2)} \tag{6.55}
\]

Now make use of Stirling's formula in the form

\[
\Gamma(x+1) \simeq(2 \pi)^{1 / 2} e^{-x} x^{x+1 / 2} \tag{6.56}
\]

which is valid for \(x \gg 1\), to show that, as \(D \rightarrow \infty\), the ratio (6.55) goes to zero. Show also that the distance from the centre of the hypercube to one of the corners divided by the perpendicular distance to one of the sides is \(\sqrt{D}\), which therefore goes to \(\infty\) as \(D \rightarrow \infty\). From these results, we see that, in a space of high dimensionality, most of the volume of a cube is concentrated in the large number of corners, which themselves become very long 'spikes'!

** 6.3 Gaussian Distribution in High Dimensions
In this exercise, we explore the behaviour of the Gaussian distribution in high-dimensional spaces. Consider a Gaussian distribution in \(D\) dimensions given by

\[
p(\mathbf{x})=\frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{\|\mathbf{x}\|^{2}}{2 \sigma^{2}}\right) \tag{6.57}
\]

We wish to find the density as a function of the radius in polar coordinates in which the direction variables have been integrated out. To do this, show that the integral of the probability density over a thin shell of radius \(r\) and thickness \(\epsilon\), where \(\epsilon \ll 1\), is given by \(p(r) \epsilon\) where

\[
p(r)=\frac{S_{D} r^{D-1}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) \tag{6.58}
\]

where \(S_{D}\) is the surface area of a unit hypersphere in \(D\) dimensions. Show that the function \(p(r)\) has a single stationary point located, for large \(D\), at \(\widehat{r} \simeq \sqrt{D} \sigma\). By considering \(p(\widehat{r}+\epsilon)\) where \(\epsilon \ll \widehat{r}\), show that for large \(D\),

\[
p(\widehat{r}+\epsilon)=p(\widehat{r}) \exp \left(-\frac{3 \epsilon^{2}}{2 \sigma^{2}}\right) \tag{6.59}
\]

which shows that \(\widehat{r}\) is a maximum of the radial probability density and also that \(p(r)\) decays exponentially away from its maximum at \(\widehat{r}\) with length scale \(\sigma\). We have already seen that \(\sigma \ll \widehat{r}\) for large \(D\), and so we see that most of the probability mass is concentrated in a thin shell at large radius. Finally, show that the probability density \(p(\mathbf{x})\) is larger at the origin than at the radius \(\widehat{r}\) by a factor of \(\exp (D / 2)\). We therefore see that most of the probability mass in a high-dimensional Gaussian distribution is located at a different radius from the region of high probability density.

** 6.4 Logistic Sigmoid and Tanh Activation Functions
Consider a two-layer network function of the form (6.11) in which the hidden-unit nonlinear activation functions \(h(\cdot)\) are given by logistic sigmoid functions of the form

\[
\sigma(a)=\{1+\exp (-a)\}^{-1} \tag{6.60}
\]

Show that there exists an equivalent network, which computes exactly the same function, but with hidden-unit activation functions given by \(\tanh (a)\) where the tanh function is defined by (6.14). Hint: first find the relation between \(\sigma(a)\) and \(\tanh (a)\), and then show that the parameters of the two networks differ by linear transformations.

** 6.5 Swish Activation Function
The swish activation function (Ramachandran, Zoph, and Le, 2017) is defined by

\[
h(x)=x \sigma(\beta x) \tag{6.61}
\]

where \(\sigma(x)\) is the logistic-sigmoid activation function defined by (6.13). When used in a neural network, \(\beta\) can be treated as a learnable parameter. Either sketch or plot using software graphs of the swish activation function as well as its first derivative for \(\beta=0.1, \beta=1.0\), and \(\beta=10\). Show that when \(\beta \rightarrow \infty\), the swish function becomes the ReLU function.

** 6.6 Derivative of Tanh Activation Function
We saw in (5.72) that the derivative of the logistic-sigmoid activation function can be expressed in terms of the function value itself. Derive the corresponding result for the tanh activation function defined by (6.14).

** 6.7 Properties of Softplus Activation Function
Show that the softplus activation function \(\zeta(a)\) given by (6.16) satisfies the properties:

\[
\begin{array}{r}
\zeta(a)-\zeta(-a)=a \tag{6.62}\\
\ln \sigma(a)=-\zeta(-a) \tag{6.63}\\
\frac{\mathrm{d} \zeta(a)}{\mathrm{d} a}=\sigma(a) \tag{6.64}\\
\zeta^{-1}(a)=\ln (\exp (a)-1)
\end{array} \tag{6.65}
\]

where \(\sigma(a)\) is the logistic-sigmoid activation function given by (6.13).

** 6.8 Minimizing Error Function with Respect to Variance
Show that minimization of the error function (6.25) with respect to the variance \(\sigma^{2}\) gives the result (6.27).

** 6.9 Likelihood Maximization for Multioutput Neural Network
Show that maximizing the likelihood function under the conditional distribution (6.28) for a multioutput neural network is equivalent to minimizing the sum-of-squares error function (6.29). Also, show that the noise variance that minimizes this error function is given by (6.30).

** 6.10 Regression with Multiple Target Variables
Consider a regression problem involving multiple target variables in which it is assumed that the distribution of the targets, conditioned on the input vector \(\mathbf{x}\), is a Gaussian of the form

\[
p(\mathbf{t} \mid \mathbf{x}, \mathbf{w})=\mathcal{N}(\mathbf{t} \mid \mathbf{y}(\mathbf{x}, \mathbf{w}), \boldsymbol{\Sigma}) \tag{6.66}
\]

where \(\mathbf{y}(\mathbf{x}, \mathbf{w})\) is the output of a neural network with input vector \(\mathbf{x}\) and weight vector \(\mathbf{w}\), and \(\boldsymbol{\Sigma}\) is the covariance of the assumed Gaussian noise on the targets. Given a set of independent observations of \(\mathbf{x}\) and \(\mathbf{t}\), write down the error function that must be minimized to find the maximum likelihood solution for \(\mathbf{w}\), if we assume that \(\Sigma\) is fixed and known. Now assume that \(\Sigma\) is also to be determined from the data, and write down an expression for the maximum likelihood solution for \(\Sigma\). Note that the optimizations of \(\mathbf{w}\) and \(\boldsymbol{\Sigma}\) are now coupled, in contrast to the case of independent target variables discussed in Section 6.4.1.

** 6.11 Error Function for Binary Classification with Label Noise
Consider a binary classification problem in which the target values are \(t \in\{0,1\}\), with a network output \(y(\mathbf{x}, \mathbf{w})\) that represents \(p(t=1 \mid \mathbf{x})\), and suppose that there is a probability \(\epsilon\) that the class label on a training data point has been incorrectly set. Assuming i.i.d. data, write down the error function corresponding to the negative \(\log\) likelihood. Verify that the error function (6.33) is obtained when \(\epsilon=0\). Note that this error function makes the model robust to incorrectly labelled data, in contrast to the usual cross-entropy error function.

** 6.12 Error Function for Classification with Tanh Output
The error function (6.33) for binary classification problems was derived for a network having a logistic-sigmoid output activation function, so that \(0 \leqslant y(\mathbf{x}, \mathbf{w}) \leqslant 1\), and data having target values \(t \in\{0,1\}\). Derive the corresponding error function if we consider a network having an output \(-1 \leqslant y(\mathbf{x}, \mathbf{w}) \leqslant 1\) and target values \(t=1\) for class \(\mathcal{C}_{1}\) and \(t=-1\) for class \(\mathcal{C}_{2}\). What would be the appropriate choice of output-unit activation function?

** 6.13 Maximum Likelihood for Multiclass Neural Network
Show that maximizing the likelihood for a multi-class neural network model in which the network outputs have the interpretation \(y_{k}(\mathbf{x}, \mathbf{w})=p\left(t_{k}=1 \mid \mathbf{x}\right)\) is equivalent to minimizing the cross-entropy error function (6.36).

** 6.14 Derivative of Error Function with Logistic-Sigmoid Output
Show that the derivative of the error function (6.33) with respect to the preactivation \(a_{k}\) for an output unit having a logistic-sigmoid activation function \(y_{k}=\sigma\left(a_{k}\right)\), where \(\sigma(a)\) is given by (6.13), satisfies (6.31).

** 6.15 Derivative of Error Function with Softmax Output
Show that the derivative of the error function (6.36) with respect to the preactivation \(a_{k}\) for output units having a softmax activation function (6.37) satisfies (6.31).

** 6.16 Forward Kinematics of Robot Arm
Write down a pair of equations that express the Cartesian coordinates \(\left(x_{1}, x_{2}\right)\) for the robot arm shown in Figure 6.16 in terms of the joint angles \(\theta_{1}\) and \(\theta_{2}\) and the lengths \(L_{1}\) and \(L_{2}\) of the links. Assume the origin of the coordinate system is given by the attachment point of the lower arm. These equations define the forward kinematics of the robot arm.

** 6.17 Posterior Probabilities in Mixture Distribution
Show that the variable \(\gamma_{n k}\) defined by (6.44) can be viewed as the posterior probabilities \(p(k \mid \mathbf{t})\) for the components of the mixture distribution (6.38) in which the mixing coefficients \(\pi_{k}(\mathbf{x})\) are viewed as \(\mathbf{x}\)-dependent prior probabilities \(p(k)\).

** 6.18 Derivative of Error Function for Mixture Coefficients
Derive the result (6.45) for the derivative of the error function with respect to the network output pre-activations controlling the mixing coefficients in the mixture density network.

** 6.19 Derivative of Error Function for Component Means
Derive the result (6.46) for the derivative of the error function with respect to the network output pre-activations controlling the component means in the mixture density network.

** 6.20 Derivative of Error Function for Component Variances
Derive the result (6.47) for the derivative of the error function with respect to the network output pre-activations controlling the component variances in the mixture density network.

** 6.21 Conditional Mean and Variance of Mixture Density Network
Verify the results (6.48) and (6.50) for the conditional mean and variance of the mixture density network model.

* Gradient Descent
** Notes
*** Error Surfaces
*** Gradient Descent Optimization
*** Convergence
*** Normalization
** 7.1 Derivation of Error Function Form
By substituting (7.10) into (7.7) and using (7.8) and (7.9), show that the error function (7.7) can be written in the form (7.11).

** 7.2 Positive Definite Hessian Matrix
Consider a Hessian matrix \(\mathbf{H}\) with eigenvector equation (7.8). By setting the vector \(\mathbf{v}\) in (7.14) equal to each of the eigenvectors \(\mathbf{u}_{i}\) in turn, show that \(\mathbf{H}\) is positive definite if, and only if, all its eigenvalues are positive.

** 7.3 Local Minimum Condition for Error Function
By considering the local Taylor expansion (7.7) of an error function about a stationary point \(\mathbf{w}^{\star}\), show that the necessary and sufficient condition for the stationary point to be a local minimum of the error function is that the Hessian matrix \(\mathbf{H}\), defined by (7.5) with \(\widehat{\mathbf{w}}=\mathbf{w}^{\star}\), is positive definite.

** 7.4 Hessian Matrix for Linear Regression Model
Consider a linear regression model with a single input variable \(x\) and a single output variable \(y\) of the form

\[
y(x, w, b)=w x+b \tag{7.61}
\]

together with a sum-of-squares error function given by

\[
E(w, b)=\frac{1}{2} \sum_{n=1}^{N}\left\{y\left(x_{n}, w, b\right)-t_{n}\right\}^{2} \tag{7.62}
\]

Derive expressions for the elements of the \(2 \times 2\) Hessian matrix given by the second derivatives of the error function with respect to the weight parameter \(w\) and bias parameter \(b\). Show that the trace and the determinant of this Hessian are both positive. Since the trace represents the sum of the eigenvalue and the determinant corresponds to the product of the eigenvalues, then both eigenvalues are positive and hence the stationary point of the error function is a minimum.

** 7.5 Hessian Matrix for Single-Layer Classification Model
Consider a single-layer classification model with a single input variable \(x\) and a single output variable \(y\) of the form

\[
y(x, w, b)=\sigma(w x+b) \tag{7.63}
\]

where \(\sigma(\cdot)\) is the logistic sigmoid function defined by (5.42) together with a crossentropy error function given by

\[
E(w, b)=\sum_{n=1}^{N}\left\{t_{n} \ln y\left(x_{n}, w, b\right)+\left(1-t_{n}\right) \ln \left(1-y\left(x_{n}, w, b\right)\right)\right\} \tag{7.64}
\]

Derive expressions for the elements of the \(2 \times 2\) Hessian matrix given by the second derivatives of the error function with respect to the weight parameter \(w\) and bias parameter \(b\). Show that the trace and the determinant of this Hessian are both positive. Since the trace represents the sum of the eigenvalue and the determinant corresponds to the product of the eigenvalues, then both eigenvalues are positive and hence the stationary point of the error function is a minimum.

** 7.6 Contours of Constant Error for Quadratic Error Function
Consider a quadratic error function defined by (7.7) in which the Hessian matrix \(\mathbf{H}\) has an eigenvalue equation given by (7.8). Show that the contours of constant error are ellipses whose axes are aligned with the eigenvectors \(\mathbf{u}_{i}\) with lengths that are inversely proportional to the square roots of the corresponding eigenvalues \(\lambda_{i}\).

** 7.7 Independent Elements in Quadratic Error Function
Show that, as a consequence of the symmetry of the Hessian matrix \(\mathbf{H}\), the number of independent elements in the quadratic error function (7.3) is given by \(W(W+3) / 2\).

** 7.8 Expectation of Squared Error of Sample Mean
Consider a set of values \(x_{1}, \ldots, x_{N}\) drawn from a distribution with mean \(\mu\) and variance \(\sigma^{2}\), and define the sample mean to be

\[
\bar{x}=\frac{1}{N} \sum_{n=1}^{N} x_{n} \tag{7.65}
\]

Show that the expectation of the squared error \((\bar{x}-\mu)^{2}\) with respect to the distribution from which the data is drawn is given by \(\sigma^{2} / N\). This shows that the RMS error in the sample mean is given by \(\sigma / \sqrt{N}\), which decreases relatively slowly as the sample size \(N\) increases.

** 7.9 Initialization of Weights in Layered Network
Consider a layered network that computes the functions (7.19) and (7.20) in layer \(l\). Suppose we initialize the weights using a Gaussian \(\mathcal{N}\left(0, \epsilon^{2}\right)\), and suppose that the outputs \(z_{j}^{(l-1)}\) of the units in layer \(l-1\) have variance \(\lambda^{2}\). By using the form of the ReLU activation function, show that the mean and variance of the outputs in layer \(l\) are given by (7.21) and (7.22), respectively. Hence, show that if we want the units in layer \(l\) also to have pre-activations with variance \(\lambda^{2}\) then the value of \(\epsilon\) should be given by (7.23).

** 7.10 Batch Gradient Descent Update with Hessian Matrix
By making use of (7.7), (7.8), and (7.10), derive the results (7.24) and (7.25), which express the gradient vector and a general weight update, as expansions in the eigenvectors of the Hessian matrix. Use these results, together with the eigenvector orthonormality relation (7.9) and the batch gradient descent formula (7.16), to derive the result (7.26) for the batch gradient descent update expressed in terms of the coefficients \(\left\{\alpha_{i}\right\}\).

** 7.11 Nesterov Momentum Gradient Update
Consider a smoothly varying error surface with low curvature such that the gradient varies only slowly with position. Show that, for small values of the learning rate and momentum parameters, the Nesterov momentum gradient update defined by (7.34) is equivalent to the standard gradient descent with momentum defined by (7.31).

** 7.12 Bias Correction for Exponentially Weighted Moving Average
Consider a sequence of values \(\left\{x_{1}, \ldots, x_{N}\right\}\) of some variable \(x\), and suppose we compute an exponentially weighted moving average using the formula

\[
\mu_{n}=\beta \mu_{n-1}+(1-\beta) x_{n} \tag{7.66}
\]

where \(0 \leqslant \beta \leqslant 1\). By making use of the following result for the sum of a finite geometric series

\[
\sum_{k=1}^{n} \beta^{k-1}=\frac{1-\beta^{n}}{1-\beta} \tag{7.67}
\]

show that if the sequence of averages is initialized using \(\mu_{0}=0\), then the estimators are biased and that the bias can be corrected using

\[
\widehat{\mu}_{n}=\frac{\mu_{n}}{1-\beta^{n}} \tag{7.68}
\]

** 7.13 Orthogonality in Line Search Method
In gradient descent, the weight vector \(\mathbf{w}\) is updated by taking a step in weight space in the direction of the negative gradient governed by a learning rate parameter \(\eta\). Suppose instead that we choose a direction \(\mathbf{d}\) in weight space along which we minimize the error function, given the current weight vector \(\mathbf{w}^{(\tau)}\). This involves minimizing the quantity

\[
E\left(\mathbf{w}^{(\tau)}+\lambda \mathbf{d}\right) \tag{7.69}
\]

as a function of \(\lambda\) to give a value \(\lambda^{\star}\) corresponding to a new weight vector \(\mathbf{w}^{(\tau+1)}\). Show that the gradient of \(E(\mathbf{w})\) at \(\mathbf{w}^{(\tau+1)}\) is orthogonal to the vector \(\mathbf{d}\). This is known as a 'line search' method and it forms the basis for a variety of numerical optimization algorithms (Bishop, 1995b).

** 7.14 Renormalized Input Variables
Show that the renormalized input variables defined by (7.50), where \(\mu_{i}\) is defined by (7.48) and \(\sigma_{i}^{2}\) is defined by (7.49), have zero mean and unit variance.

* Backpropagation
** Notes
*** Evaluation of Gradients
**** Single-layer networks
**** General feed-forward networks
**** A simple example
**** Numerical differentiation
**** The Jacobian matrix
**** The Hessian matrix
*** Automatic Differentiation
**** Forward-mode automatic differentiation
**** Reverse-mode automatic differentiation
** SOLVED 8.1 Backpropagation Formula
CLOSED: [2024-11-05 Tue 23:30]
:LOGBOOK:
CLOCK: [2024-11-05 Tue 23:12]--[2024-11-05 Tue 23:29] =>  0:17
:END:
*By making use of (8.5), (8.6), (8.8), and (8.12), verify the backpropagation formula (8.13) for evaluating the derivatives of an error function.*
Using (8.5), and (8.6), \( a_{k} = \sum_{j} w_{kj} z_{j} = \sum_{j} w_{kj} h(a_j)\) so that \( \partial_{a_{j}} a_{k} = w_{kj}  \, \mathrm{D} h \) Using (8.8), \( \partial_{a_{k}} E_{n} \equiv \delta_{k}\) by definition. Substituting \( \partial_{a_{j}} a_{k} = w_{kj}  \, \mathrm{D} h \) and \( \partial_{a_{k}} E_{n} \equiv \delta_{k}\) in (8.12) yields the backpropagation formula (8.13):

\begin{align*}\label{backpropagation-formula}
\boxed{\delta_{j} (a_{j}) = \mathrm{D} h (a_{j}) \cdot \sum_{k} w_{kj} \cdot \delta_{k} (a_{k}).}
\end{align*}

** 8.2 Backpropagation Formula in Matrix Notation
Consider a network that consists of layers and rewrite the backpropagation formula (8.13) in matrix notation by starting with the forward propagation equation (6.19). Note that the result involves multiplication by the transposes of the matrices.

** 8.3 Verification of Terms in Taylor Expansion
By using a Taylor expansion, verify that the terms that are \(\mathcal{O}(\epsilon)\) cancel on the right-hand side of (8.25).

** 8.4 Skip-Layer Connections in Two-Layer Network
Consider a two-layer network of the form shown in Figure 6.9 with the addition of extra parameters corresponding to skip-layer connections that go directly from the inputs to the outputs. By extending the discussion of Section 8.1.3, write down the equations for the derivatives of the error function with respect to these additional parameters.

** 8.5 Alternative Formalism for Evaluating Jacobian
In Section 8.1.5, we derived a procedure for evaluating the Jacobian matrix of a neural network using a backpropagation procedure. Derive an alternative formalism for finding the Jacobian based on forward propagation equations.

** 8.6 Elements of Hessian Matrix for Two-Layer Neural Network
Consider a two-layer neural network, and define the quantities

\[
\delta_{k}=\frac{\partial E_{n}}{\partial a_{k}}, \quad M_{k k^{\prime}} \equiv \frac{\partial^{2} E_{n}}{\partial a_{k} \partial a_{k^{\prime}}} \tag{8.77}
\]

Derive expressions for the elements of the Hessian matrix expressed in terms of \(\delta_{k}\) and \(M_{k k^{\prime}}\) for elements in which (i) both weights are in the second layer, (ii) both weights are in the first layer, and (iii) one weight is in each layer.

** 8.7 Hessian of Two-Layer Network with Skip-Layer Connections
Extend the results of Exercise 8.6 for the exact Hessian of a two-layer network to include skip-layer connections that go directly from inputs to outputs.

** 8.8 Outer Product Approximation to Hessian Matrix
The outer product approximation to the Hessian matrix for a neural network using a sum-of-squares error function is given by (8.40). Extend this result for multiple outputs.

** 8.9 Second Derivative of Squared-Loss Function
Consider a squared-loss function of the form

\[
E(\mathbf{w})=\frac{1}{2} \iint\{y(\mathbf{x}, \mathbf{w})-t\}^{2} p(\mathbf{x}, t) \mathrm{d} \mathbf{x} \mathrm{d} t \tag{8.78}
\]

where \(y(\mathbf{x}, \mathbf{w})\) is a parametric function such as a neural network. The result (4.37) shows that the function \(y(\mathbf{x}, \mathbf{w})\) that minimizes this error is given by the conditional expectation of \(t\) given \(\mathbf{x}\). Use this result to show that the second derivative of \(E\) with respect to two elements \(w_{r}\) and \(w_{s}\) of the vector \(\mathbf{w}\), is given by

\[
\frac{\partial^{2} E}{\partial w_{r} \partial w_{s}}=\int \frac{\partial y}{\partial w_{r}} \frac{\partial y}{\partial w_{s}} p(\mathbf{x}) \mathrm{d} \mathbf{x} \tag{8.79}
\]

Note that, for a finite sample from \(p(\mathbf{x})\), we obtain (8.40).

** 8.10 Outer Product Approximation for Hessian: Logistic-Sigmoid
Derive the expression (8.41) for the outer product approximation of a Hessian matrix for a network having a single output with a logistic-sigmoid output-unit activation function and a cross-entropy error function, corresponding to the result (8.40) for the sum-of-squares error function.

** 8.11 Outer Product Approximation for Hessian: Softmax
Derive an expression for the outer product approximation of a Hessian matrix for a network having \(K\) outputs with a softmax output-unit activation function and a cross-entropy error function, corresponding to the result (8.40) for the sum-of-squares error function.
** 8.12 Inverse of Hessian Matrix with Outer Product Approximation
Consider the matrix identity
\[
\left(\mathbf{M}+\mathbf{v v}^{\mathrm{T}}\right)^{-1}=\mathbf{M}^{-1}-\frac{\left(\mathbf{M}^{-1} \mathbf{v}\right)\left(\mathbf{v}^{\mathrm{T}} \mathbf{M}^{-1}\right)}{1+\mathbf{v}^{\mathrm{T}} \mathbf{M}^{-1} \mathbf{v}} \tag{8.80}
\]
which is simply a special case of the Woodbury identity (A.7). By applying (8.80) to the outer product approximation (8.40) for a Hessian matrix, derive a formula that allows the inverse of the Hessian matrix to be computed by making one pass through the training data and updating the inverse Hessian with each data point. Note that the algorithm can initialized using \(\mathbf{H}=\alpha \mathbf{I}\) where \(\alpha\) is a small constant, and that the results are not particularly sensitive to the precise value of \(\alpha\).
** 8.13 Derivative of Equation (8.47)
Verify that the derivative of (8.47) is given by (8.48).

** 8.14 Logistic Map Function and Derivatives
The logistic map is a function defined by the iterative relation \(L_{n+1}(x)=4 L_{n}(x)\left(1-L_{n}(x)\right)\) with \(L_{1}(x)=x\). Write down the evaluation trace equations for \(L_{2}(x), L_{3}(x)\), and \(L_{4}(x)\), and then write down expressions for the corresponding derivatives \(L_{1}^{\prime}(x), L_{2}^{\prime}(x), L_{3}^{\prime}(x)\), and \(L_{4}^{\prime}(x)\). Do not simplify the expressions but instead simply note how the complexity of the formulae for the derivatives grows much more rapidly than the expressions for the functions themselves.

** 8.15 Forward-Mode Tangent Variable Evaluation Equations
Starting from the evaluation trace equations (8.50) to (8.56) for the example function (8.49), use (8.57) to derive the forward-mode tangent variable evaluation equations (8.58) to (8.64).

** 8.16 Reverse-Mode Adjoint Variable Evaluation Equations
Starting from the evaluation trace equations (8.50) to (8.56) for the example function (8.49), and referring to Figure 8.4, use (8.69) to derive the reverse-mode adjoint variable evaluation equations (8.70) to (8.76).

** 8.17 Derivatives with Automatic Differentiation
Consider the example function (8.49). Write down an expression for \(\partial f / \partial x_{1}\) and evaluate this function for \(x_{1}=1\) and \(x_{2}=2\). Now use the evaluation trace equations (8.50) to (8.56) to evaluate the variables \(v_{1}\) to \(v_{7}\) and then use the evaluation trace equations of forward-mode automatic differentiation to evaluate the tangent variables \(\dot{v}_{1}\) to \(\dot{v}_{7}\) and to confirm that the resulting value of \(\partial f / \partial x_{1}\) agrees with that found directly. Similarly, use the evaluation trace equations of reverse-mode automatic differentiation (8.70) to (8.76) to evaluate the adjoint variables \(\bar{v}_{7}\) to \(\bar{v}_{1}\) and again confirm that the resulting value of \(\partial f / \partial x_{1}\) agrees with that found directly.

** 8.18 Jacobian Product with Forward-Mode Automatic Differentiation
By expressing an arbitrary vector \(\mathbf{r}=\left(r_{1}, \ldots, r_{D}\right)^{\mathrm{T}}\) as a linear combination of the unit vectors \(\mathbf{e}_{i}\), where \(i=1, \ldots, D\), show that the product of the Jacobian of a function with \(\mathbf{r}\) in the form (8.67) can be evaluated using a single pass of forward-mode automatic differentiation by setting \(\dot{\mathbf{x}}=\mathbf{r}\).

* Regularization
** Notes
*** Inductive Bias
**** Inverse problems
**** No free lunch theorem
**** Symmetry and invariance
**** Equivariance
*** Weight Decay
**** Consistent regularizers
**** Generalized weight decay
*** Learning Curves
**** Early stopping
**** Double descent
*** Parameter Sharing
**** Soft weight sharing
*** Residual Connections
*** Model Averaging
**** Dropout
** 9.1 Verifying Group Axioms for Rotations and Translations
By considering each of the four group axioms in turn, show that the set of all possible rotations of a square through (positive or negative) multiples of \(90^{\circ}\), together with the binary operation of composing rotations, forms a group. Similarly, show that the set of all continuous translations of an object in a two-dimensional plane also forms a group.

** 9.2 Linear Model with Gaussian Noise
Consider a linear model of the form

\[
\begin{align*}
y(\mathbf{x}, \mathbf{w})=w_{0}+\sum_{i=1}^{D} w_{i} x_{i} \tag{9.52}
\end{align*}
\]

together with a sum-of-squares error function of the form

\[
\begin{align*}
E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{y\left(\mathbf{x}_{n}, \mathbf{w}\right)-t_{n}\right\}^{2} \tag{9.53}
\end{align*}
\]

Now suppose that Gaussian noise \(\epsilon_{i}\) with zero mean and variance \(\sigma^{2}\) is added independently to each of the input variables \(x_{i}\). By making use of \(\mathbb{E}\left[\epsilon_{i}\right]=0\) and \(\mathbb{E}\left[\epsilon_{i} \epsilon_{j}\right]=\delta_{i j} \sigma^{2}\), show that minimizing \(E_{D}\) averaged over the noise distribution is equivalent to minimizing the sum-of-squares error for noise-free input variables with the addition of a weight-decay regularization term, in which the bias parameter \(w_{0}\) is omitted from the regularizer.

** 9.3 Gradient Descent with Quadratic Regularizer
Consider an error function that consists simply of the quadratic regularizer

\[
\begin{align*}
\Omega(\mathbf{w})=-\frac{1}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \tag{9.54}
\end{align*}
\]

together with the gradient descent update formula

\[
\begin{align*}
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau+1)}-\eta \nabla \Omega(\mathbf{w}) \tag{9.55}
\end{align*}
\]

By considering the limit of infinitesimal updates, write down a corresponding differential equation for the evolution of \(\mathbf{w}\). Write down the solution of this equation starting from an initial value \(\mathbf{w}_{0}\), and show that the elements of \(\mathbf{w}\) decay exponentially to zero.

** 9.4 Invariance under Transformation
Verify that the network function defined by (9.6) and (9.7) is invariant under the transformation (9.8) applied to the inputs, provided the weights and biases are simultaneously transformed using (9.9) and (9.10). Similarly, show that the network outputs can be transformed according to (9.11) by applying the transformation (9.12) and (9.13) to the second-layer weights and biases.

** 9.5 Regularized Error Function with Lagrange Multipliers
By using Lagrange multipliers, show that minimizing the regularized error function given by (9.19) is equivalent to minimizing the unregularized error function \(E(\mathbf{w})\) subject to the constraint (9.20). Discuss the relationship between the parameters \(\eta\) and \(\lambda\).

** 9.6 Convergence of Gradient Descent with Quadratic Error Function
Consider a quadratic error function of the form

\[
\begin{align*}
E=E_{0}+\frac{1}{2}\left(\mathbf{w}-\mathbf{w}^{\star}\right)^{\mathrm{T}} \mathbf{H}\left(\mathbf{w}-\mathbf{w}^{\star}\right) \tag{9.56}
\end{align*}
\]

where \(\mathbf{w}^{\star}\) represents the minimum, and the Hessian matrix \(\mathbf{H}\) is positive definite and constant. Suppose the initial weight vector \(\mathbf{w}^{(0)}\) is chosen to be at the origin and is updated using simple gradient descent:

\[
\begin{align*}
\mathbf{w}^{(\tau)}=\mathbf{w}^{(\tau-1)}-\rho \nabla E \tag{9.57}
\end{align*}
\]

where \(\tau\) denotes the step number, and \(\rho\) is the learning rate (which is assumed to be small). Show that, after \(\tau\) steps, the components of the weight vector parallel to the eigenvectors of \(\mathbf{H}\) can be written

\[
\begin{align*}
w_{j}^{(\tau)}=\left\{1-\left(1-\rho \eta_{j}\right)^{\tau}\right\} w_{j}^{\star} \tag{9.58}
\end{align*}
\]

where \(w_{j}=\mathbf{w}^{\mathrm{T}} \mathbf{u}_{j}\), and \(\mathbf{u}_{j}\) and \(\eta_{j}\) are the eigenvectors and eigenvalues of \(\mathbf{H}\), respectively, defined by

\[
\begin{align*}
\mathbf{H} \mathbf{u}_{j}=\eta_{j} \mathbf{u}_{j} \tag{9.59}
\end{align*}
\]

Show that as \(\tau \rightarrow \infty\), this gives \(\mathbf{w}^{(\tau)} \rightarrow \mathbf{w}^{\star}\) as expected, provided \(\left|1-\rho \eta_{j}\right|<1\). Now suppose that training is halted after a finite number \(\tau\) of steps. Show that the components of the weight vector parallel to the eigenvectors of the Hessian satisfy

\[
\begin{align*}
\begin{align*}
& w_{j}^{(\tau)} \simeq w_{j}^{\star} \quad \text { when } \quad \eta_{j} \gg(\rho \tau)^{-1}  \tag{9.60}\\
& \left|w_{j}^{(\tau)}\right| \ll\left|w_{j}^{\star}\right| \text { when } \eta_{j} \ll(\rho \tau)^{-1} \text {. }
\end{align*} \tag{9.61}
\end{align*}
\]

This result shows that \((\rho \tau)^{-1}\) plays an analogous role to the regularization parameter \(\lambda\) in weight decay.

** 9.7 Constrained Weights in Neural Networks
Consider a neural network in which multiple weights are constrained to have the same value. Discuss how the standard backpropagation algorithm must be modified to ensure that such constraints are satisfied when evaluating the derivatives of an error function with respect to the adjustable parameters in the network.

** 9.8 Mixture Distribution
Consider a mixture distribution defined by

\[
\begin{align*}
p(w)=\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w \mid \mu_{j}, \sigma_{j}^{2}\right) \tag{9.62}
\end{align*}
\]

in which \(\left\{\pi_{j}\right\}\) can be viewed as prior probabilities \(p(j)\) for the corresponding Gaussian components. Using Bayes' theorem, show that the corresponding posterior probabilities \(p(j \mid w)\) are given by (9.24).

** 9.9 Verification of Result (9.25)
Using (9.21), (9.22), (9.23), and (9.24) verify the result (9.25).

** 9.10 Verification of Result (9.26)
Using (9.21), (9.22), (9.23), and (9.24) verify the result (9.26).

** 9.11 Verification of Result (9.28)
Using (9.21), (9.22), (9.23), and (9.24) verify the result (9.28).

** 9.12 Derivatives of Mixing Coefficients
Show that the derivatives of the mixing coefficients \(\left\{\pi_{k}\right\}\) defined by (9.30) with respect to the auxiliary parameters \(\left\{\eta_{j}\right\}\) are given by

\[
\begin{align*}
\frac{\partial \pi_{k}}{\partial \eta_{j}}=\delta_{j k} \pi_{j}-\pi_{j} \pi_{k} \tag{9.63}
\end{align*}
\]

Hence, by making use of the constraint \(\sum_{k} \gamma_{k}\left(w_{i}\right)=1\) for all \(i\), derive the result \((9.31)\).

** 9.13 Combining Network Functions
Verify that combining (9.35), (9.36), and (9.37) gives a single overall equation for the whole network in the form (9.40).

** 9.14 Expected Error for Committee Model
The expected sum-of-squares error \(E_{\mathrm{Av}}\) for a simple committee model can be defined by (9.46), and the expected error of the committee itself is given by (9.47). Assuming that the individual errors satisfy (9.48) and (9.49), derive the result (9.50).

** 9.15 Jensen's Inequality for Committee Models
By making use of Jensen's inequality (2.102) for the special case of the convex function \(f(x)=x^{2}\), show that the average expected sum-of-squares error \(E_{\mathrm{AV}}\) of the members of a simple committee model, given by (9.46), and the expected error \(E_{\mathrm{COM}}\) of the committee itself, given by (9.47), satisfy

\[
\begin{align*}
E_{\mathrm{COM}} \leqslant E_{\mathrm{AV}} \tag{9.64}
\end{align*}
\]

** 9.16 Jensen's Inequality for General Error Functions
By making use of Jensen's inequality (2.102), show that the result (9.64) derived in the previous exercise holds for any error function \(E(y)\), not just sum-of-squares, provided it is a convex function of \(y\).

** 9.17 Unequal Weighting of Committee Models
Consider a committee in which we allow unequal weighting of the constituent models, so that

\[
\begin{align*}
y_{\mathrm{COM}}(\mathbf{x})=\sum_{m=1}^{M} \alpha_{m} y_{m}(\mathbf{x}) \tag{9.65}
\end{align*}
\]

To ensure that the predictions \(y_{\mathrm{COM}}(\mathbf{x})\) remain within sensible limits, suppose that we require that they be bounded at each value of \(\mathbf{x}\) by the minimum and maximum values given by any of the members of the committee, so that

\[
\begin{align*}
y_{\min }(\mathbf{x}) \leqslant y_{\mathrm{COM}}(\mathbf{x}) \leqslant y_{\max }(\mathbf{x}) \tag{9.66}
\end{align*}
\]

Show that a necessary and sufficient condition for this constraint is that the coefficients \(\alpha_{m}\) satisfy

\[
\begin{align*}
\alpha_{m} \geqslant 0, \quad \sum_{m=1}^{M} \alpha_{m}=1 \tag{9.67}
\end{align*}
\]

** 9.18 Dropout Regularization in Linear Regression
Here we explore the effect of dropout regularization on a simple linear regression model trained using least squares. Consider a model of the form

\[
\begin{align*}
y_{k}=\sum_{i=1}^{D} w_{k i} x_{i} \tag{9.68}
\end{align*}
\]

along with a sum-of-squares error function given by

\[
\begin{align*}
E(\mathbf{W})=\sum_{n=1}^{N} \sum_{k=1}^{K}\left\{t_{n k}-\sum_{i=1}^{D} w_{k i} R_{n i} x_{n i}\right\}^{2} \tag{9.69}
\end{align*}
\]

where the elements \(R_{n i} \in\{0,1\}\) of the dropout matrix are chosen randomly from a Bernoulli distribution with parameter \(\rho\). We now take an expectation over the distribution of random dropout parameters. Show that

\[
\begin{align*}
\begin{align*}
\mathbb{E}\left[R_{n i}\right] & =\rho  \tag{9.70}\\
\mathbb{E}\left[R_{n i} R_{n j}\right] & =\delta_{i j} \rho+\left(1-\delta_{i j}\right) \rho^{2}
\end{align*} \tag{9.71}
\end{align*}
\]

Hence, show that the expected error function for this dropout model is given by

\[
\begin{align*}
\begin{align*}
\mathbb{E}[E(\mathbf{W})]= & \sum_{n=1}^{N} \sum_{k=1}^{K}\left\{y_{n k}-\rho \sum_{i=1}^{D} w_{k i} x_{n i}\right\}^{2}  \tag{9.72}\\
& +\rho(1-\rho) \sum_{n=1}^{N} \sum_{k=1}^{K} \sum_{i=1}^{D} w_{k i}^{2} x_{n i}^{2}
\end{align*} \tag{9.73}
\end{align*}
\]

Thus, we see that the expected error function corresponds to a sum-of-squares error with a quadratic regularizer in which the regularization coefficient is scaled separately for each input variable according to the data values seen by that input. Finally, write down a closed-form solution for the weight matrix that minimizes this regularized error function.

* Convolutional Networks
** 10.1 Maximizing Scalar Product
Consider a fixed weight vector \(\mathbf{w}\) and show that the input vector \(\mathbf{x}\) that maximizes the scalar product \(\mathbf{w}^{\mathrm{T}} \mathbf{x}\), subject to the constraint that \(\|\mathbf{x}\|^{2}\) is constant, is given by \(\mathbf{x}=\alpha \mathbf{w}\) for some scalar \(\alpha\). This can most easily be done using a Lagrange multiplier.

** 10.2 Convolution as Fully Connected
Consider a convolutional network layer with a one-dimensional input array and a one-dimensional feature map as shown in Figure 10.2, in which the input array has dimensionality 5 and the filters have width 3 with a stride of 1. Show that this can be expressed as a special case of a fully connected layer by writing down the weight matrix in which missing connections are replaced by zeros and where shared parameters are indicated by using replicated entries. Ignore any bias parameters.

** 10.3 Convolution Output Calculation
Explicitly calculate the output of the following convolution of a \(4 \times 4\) input matrix with a \(2 \times 2\) filter:

\[
\begin{matrix}
2 & 5 & -3 & 0 \\
0 & 6 & 0 & -4 \\
-1 & -3 & 0 & 2 \\
5 & 0 & 0 & 3 \\
\end{matrix}
\]

** 10.4 Summation Limits for Convolution
If an image \(\mathbf{I}\) has \(J \times K\) pixels and a filter \(\mathbf{K}\) has \(L \times M\) elements, write down the limits for the two summations in (10.2). In the mathematics literature, the operation (10.2) would be called a cross-correlation, whereas a convolution would be defined by

\[
C(j, k)=\sum_{l} \sum_{m} I(j-l, k-m) K(l, m) .
\]

Write down the limits for the summations in (10.19). Show that (10.19) can be written in the equivalent 'flipped' form

\[
C(j, k)=\sum_{l} \sum_{m} I(j+l, k+m) K(l, m)
\]

and again write down the limits for the summations.

** 10.5 Continuous Variable Convolution
In mathematics, a convolution for a continuous variable \(x\) is defined by

\[
F(x)=\int_{-\infty}^{\infty} G(y) k(x-y) \mathrm{d} y
\]

where \(k(x-y)\) is the kernel function. By considering a discrete approximation to the integral, explain the relationship to a convolutional layer, defined by (10.19), in a CNN.

** 10.6 Padding and Convolution
Consider an image of size \(J \times K\) that is padded with an additional \(P\) pixels on all sides and which is then convolved using a kernel of size \(M \times M\) where \(M\) is an odd number. Show that if we choose \(P=(M-1)/2\), then the resulting feature map will have size \(J \times K\) and hence will be the same size as the original image.

** 10.7 Feature Map Dimensionality
Show that if a kernel of size \(M \times M\) is convolved with an image of size \(J \times K\) with padding of depth \(P\) and strides of length \(S\), then the dimensionality of the resulting feature map is given by (10.5).

** 10.8 VGG-16 Parameters
For each of the 16 layers in the VGG-16 CNN shown in Figure 10.10, evaluate (i) the number of weights (i.e., connections) including biases and (ii) the number of independently learnable parameters. Confirm that the total number of learnable parameters in the network is approximately 138 million.

** 10.9 Separable Kernel Convolution
Consider a convolution of the form (10.2) and suppose that the kernel is separable so that

\[
K(l, m)=F(l) G(m)
\]

for some functions \(F(\cdot)\) and \(G(\cdot)\). Show that instead of performing a single two-dimensional convolution, it is now possible to compute the same answer using two one-dimensional convolutions, thereby resulting in a significant improvement in efficiency.

** 10.10 DeepDream Gradient Optimization
The DeepDream update procedure involves setting the \(\delta\) variables for backpropagation equal to the pre-activations of the nodes in the chosen layer and then running backpropagation to the input layer to get a gradient vector over the pixels of the image. Show that this can be derived as a gradient optimization with respect to the pixels of an image \(\mathbf{I}\) applied to the function (10.12).

** 10.11 Object Detection Network
When designing a neural network to detect objects from \(C\) different classes in an image, we can use a 1-of-\((C+1)\) class label with one variable for each object class and one additional variable representing a 'background' class, i.e., an input image region that does not contain an object belonging to any of the defined classes. The network will then output a vector of probabilities of length \((C+1)\). Alternatively, we can use a single binary variable to denote the presence or absence of an object and then use a separate 1-of-\(C\) vector to denote the specific object class. In this case, the network outputs a single probability representing the presence of an object and a separate set of probabilities over the class label. Write down the relationship between these two sets of probabilities.

** 10.12 Computational Steps in CNN
Calculate the number of computational steps required to make one forward pass through the convolutional network shown in Figure 10.22, ignoring biases and ignoring the evaluation of activation functions. Similarly, calculate the total number of computational steps for a single forward pass through the expanded network shown in Figure 10.23. Finally, evaluate the ratio of nine repeated naive applications of the network in Figure 10.22 to an \(8 \times 8\) image compared to a single application of the network in Figure 10.23. This ratio indicates the improvement in efficiency from using a convolutional implementation of the sliding window technique.

** 10.13 Transpose Convolution
In this exercise, we use one-dimensional vectors to demonstrate why a convolutional up-sampling is sometimes called a transpose convolution. Consider a one-dimensional strided convolutional layer with an input having four units with activations \((x_1, x_2, x_3, x_4)\), which is padded with zeros to give \((0, x_1, x_2, x_3, x_4, 0)\), and a filter with parameters \((w_1, w_2, w_3)\). Write down the one-dimensional activation vector of the output layer assuming a stride of 2. Express this output in the form of a matrix \(\mathbf{A}\) multiplied by the vector \((0, x_1, x_2, x_3, x_4, 0)\). Now consider an up-sampling convolution in which the input layer has activations \((z_1, z_2)\) with a filter having values \((w_1, w_2, w_3)\) and an output stride of 2. Write down the six-dimensional output vector assuming that overlapping filter values are summed and that the activation function is just the identity. Show that this can be expressed as a matrix multiplication using the transpose matrix \(\mathbf{A}^{\mathrm{T}}\).

* Structured Distributions
** 11.1 Joint Distribution Normalization
By marginalizing out the variables in order, show that the representation (11.6) for the joint distribution of a directed graph is correctly normalized, provided each of the conditional distributions is normalized.

** 11.2 No Directed Cycles
Show that the property of there being no directed cycles in a directed graph follows from the statement that there exists an ordered numbering of the nodes such that for each node there are no links going to a lower-numbered node.

** 11.3 Conditional Independence in Binary Variables
Consider three binary variables \(a, b, c \in \{0,1\}\) having the joint distribution given in Table 11.1. Show by direct evaluation that this distribution has the property that \(a\) and \(b\) are marginally dependent, so that \(p(a, b) \neq p(a) p(b)\), but that they become independent when conditioned on \(c\), so that \(p(a, b \mid c)=p(a \mid c) p(b \mid c)\) for both \(c=0\) and \(c=1\).

| \(a\) | \(b\) | \(c\) | \(p(a, b, c)\) |
|-------+-------+-------+----------------|
|     0 |     0 |     0 |          0.192 |
|     0 |     0 |     1 |          0.144 |
|     0 |     1 |     0 |          0.048 |
|     0 |     1 |     1 |          0.216 |
|     1 |     0 |     0 |          0.192 |
|     1 |     0 |     1 |          0.064 |
|     1 |     1 |     0 |          0.048 |
|     1 |     1 |     1 |          0.096 |

** 11.4 Conditional Probabilities and Joint Distribution
Evaluate the distributions \(p(a), p(b \mid c)\), and \(p(c \mid a)\) corresponding to the joint distribution given in Table 11.1. Hence, show by direct evaluation that \(p(a, b, c)= p(a) p(c \mid a) p(b \mid c)\). Draw the corresponding directed graph.

** 11.5 Noisy-OR Interpretation
For the model shown in Figure 11.6, we have seen that the number of parameters required to specify the conditional distribution \(p(y \mid x_1, \ldots, x_M)\), where \(x_i \in \{0,1\}\), could be reduced from \(2^M\) to \(M+1\) by making use of the logistic sigmoid representation (11.8). An alternative representation (Pearl, 1988) is given by

\[
p(y=1 \mid x_1, \ldots, x_M)=1-\left(1-\mu_0\right) \prod_{i=1}^{M}\left(1-\mu_i\right)^{x_i}
\]

where the parameters \(\mu_i\) represent the probabilities \(p(x_i=1)\) and \(\mu_0\) is an additional parameter satisfying \(0 \leqslant \mu_0 \leqslant 1\). The conditional distribution (11.49) is known as the noisy-OR. Show that this can be interpreted as a 'soft' (probabilistic) form of the logical OR function (i.e., the function that gives \(y=1\) whenever at least one of the \(x_i=1\)). Discuss the interpretation of \(\mu_0\).

** 11.6 Linear-Gaussian Model Mean Recursion
Starting from the definition (11.9) for the conditional distributions, derive the recursion relation (11.12) for the mean of the joint distribution for a linear-Gaussian model.

** 11.7 Linear-Gaussian Model Covariance Recursion
Starting from the definition (11.9) for the conditional distributions, derive the recursion relation (11.13) for the covariance matrix of the joint distribution for a linear-Gaussian model.

** 11.8 Parameters in Covariance Matrix
Show that the number of parameters in the covariance matrix of a fully connected linear-Gaussian graphical model over \(D\) variables defined by (11.9) is \(D(D+1) / 2\).

** 11.9 Mean and Covariance for Specific Graph
Using the recursion relations (11.12) and (11.13), show that the mean and covariance of the joint distribution for the graph shown in Figure 11.7 are given by (11.14) and (11.15), respectively.

** 11.10 Joint Gaussian Distribution
Verify that the joint distribution over a set of vector-valued variables defined by a linear-Gaussian model in which each node corresponds to a distribution of the form (11.16) is itself a Gaussian.

** 11.11 Conditional Independence
Show that \(a \Perp b, c \mid d\) implies \(a \Perp b \mid d\).

** 11.12 d-Separation and Conditional Independence
Using the d-separation criterion, show that the conditional distribution for a node \(x\) in a directed graph, conditioned on all the nodes in the Markov blanket, is independent of the remaining variables in the graph.

** 11.13 Directed Graph Independence
Consider the directed graph shown in Figure 11.32 in which none of the variables is observed. Show that \(a \Perp b \mid \emptyset\). Suppose we now observe the variable \(d\). Show that in general \(a \not \Perp b \mid d\).

** 11.14 Driver Reliability
Consider the example of the car fuel system shown in Figure 11.20, and suppose that instead of observing the state of the fuel gauge \(G\) directly, the gauge is seen by the driver \(D\), who reports to us the reading on the gauge. This report says that the gauge shows either that the tank is full \(D=1\) or that it is empty \(D=0\). Our driver is a bit unreliable, as expressed through the following probabilities:

\[
p(D=1 \mid G=1) =0.9, \quad p(D=0 \mid G=0) =0.9
\]

Suppose that the driver tells us that the fuel gauge shows empty, in other words that we observe \(D=0\). Evaluate the probability that the tank is empty given only this observation. Similarly, evaluate the corresponding probability given also the observation that the battery is flat, and note that this second probability is lower. Discuss the intuition behind this result, and relate the result to Figure 11.32.

** 11.15 Naive Bayes Maximum Likelihood
Suppose we train a naive Bayes model, with the assumption (11.37), using maximum likelihood. Assume that each of the class-conditional densities \(p(x^{(l)} \mid \mathcal{C}_k)\) is governed by its own independent parameters \(w^{(l)}\). Show that the maximum likelihood solution involves fitting each of the class-conditional densities using the corresponding observed data vectors \(x_1^{(l)}, \ldots, x_N^{(l)}\) by maximizing the likelihood with respect to the corresponding class label data, and then setting the class priors \(p(\mathcal{C}_k)\) to the fraction of training data points in each class.

** 11.16 Conditional Independence Verification
Consider the joint probability distribution (11.44) corresponding to the directed graph of Figure 11.29. Using the sum and product rules of probability, verify that this joint distribution satisfies the conditional independence property (11.45) for \(n=2, \ldots, N\). Similarly, show that the second-order Markov model described by the joint distribution (11.46) satisfies the conditional independence property

\[
p(x_n \mid x_1, \ldots, x_{n-1})=p(x_n \mid x_{n-1}, x_{n-2})
\]

for \(n=3, \ldots, N\).

** 11.17 d-Separation Verification
Use d-separation, as discussed in Section 11.2, to verify that the Markov model shown in Figure 11.29 having \(N\) nodes in total satisfies the conditional independence properties (11.45) for \(n=2, \ldots, N\). Similarly, show that a model described by the graph in Figure 11.30 in which there are \(N\) nodes in total satisfies the conditional independence properties (11.52) for \(n=3, \ldots, N\).

** 11.18 Second-Order to First-Order Markov
Consider a second-order Markov process described by the graph in Figure 11.30. By combining adjacent pairs of variables, show that this can be expressed as a first-order Markov process over the new variables.

** 11.19 State-Space Model Independence
By using d-separation, show that the distribution \(p(x_1, \ldots, x_N)\) of the observed data for the state-space model represented by the directed graph in Figure 11.31 does not satisfy any conditional independence properties and hence does not exhibit the Markov property at any finite order.

* Transformers
** 12.1 Coefficient Constraints
Consider a set of coefficients \(a_{nm}\), for \(m=1, \ldots, N\), with the properties that

\[
a_{nm} \geqslant 0, \quad \sum_{m} a_{nm}=1
\]

By using a Lagrange multiplier show that the coefficients must also satisfy

\[
a_{nm} \leqslant 1 \text{ for } n=1, \ldots, N
\]

Without loss of generality, consider the maximi

** 12.2 Softmax Constraints
Verify that the softmax function (12.5) satisfies the constraints (12.3) and (12.4) for any values of the vectors \(\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\).

** 12.3 Orthogonal Input Vectors
Consider the input vectors \(\mathbf{x}_{n}\) in the simple transformation defined by (12.2), in which the weighting coefficients \(a_{nm}\) are defined by (12.5). Show that if all the input vectors are orthogonal, so that \(\mathbf{x}_{n}^{\mathrm{T}} \mathbf{x}_{m}=0\) for \(n \neq m\), then the output vectors will simply be equal to the input vectors so that \(\mathbf{y}_{n}=\mathbf{x}_{n}\) for \(n=1, \ldots, N\).

** 12.4 Expected Value of Inner Product
Consider two independent random vectors \(\mathbf{a}\) and \(\mathbf{b}\) each of dimension \(D\) and each being drawn from a Gaussian distribution with zero mean and unit variance \(\mathcal{N}(\cdot \mid \mathbf{0}, \mathbf{I})\). Show that the expected value of \((\mathbf{a}^{\mathrm{T}} \mathbf{b})^{2}\) is given by \(D\).

** 12.5 Multi-Head Attention Reformulation
Show that multi-head attention defined by (12.19) can be rewritten in the form

\[
\mathbf{Y}=\sum_{h=1}^{H} \mathbf{H}_{h} \mathbf{X} \mathbf{W}^{(h)}
\]

where \(\mathbf{H}_{h}\) is given by (12.15) and we have defined

\[
\mathbf{W}^{(h)}=\mathbf{W}_{h}^{(v)} \mathbf{W}_{h}^{(o)}
\]

Here we have partitioned the matrix \(\mathbf{W}^{(0)}\) horizontally into sub-matrices denoted \(\mathbf{W}_{h}^{(0)}\) each of dimension \(D_{\mathrm{v}} \times D\), corresponding to the vertical segments of the Figure 12.7 concatenated attention matrix. Since \(D_{\mathrm{v}}\) is typically smaller than \(D\), for example \(D_{\mathrm{v}}=D / H\) is a common choice, this combined matrix is rank deficient. Therefore, using a fully flexible matrix to replace \(\mathbf{W}_{h}^{(\mathrm{v})} \mathbf{W}_{h}^{(\circ)}\) would not be equivalent to the original formulation given in the text.

** 12.6 Self-Attention as Fully Connected Network
Express the self-attention function (12.14) as a fully connected network in the form of a matrix that maps the full input sequence of concatenated word vectors into an output vector of the same dimension. Note that such a matrix would have \(\mathcal{O}(N^{2} D^{2})\) parameters. Show that the self-attention network corresponds to a sparse version of this matrix with parameter sharing. Draw a sketch showing the structure of this matrix, indicating which blocks of parameters are shared and which blocks have all elements equal to zero.

** 12.7 Equivariance of Multi-Head Attention
Show that if we omit the positional encoding of input vectors then the outputs of a multi-head attention layer defined by (12.19) are equivariant with respect to a reordering of the input sequence.

** 12.8 Orthogonality in High Dimensions
Consider two \(D\)-dimensional unit vectors \(\mathbf{a}\) and \(\mathbf{b}\), satisfying \(\|\mathbf{a}\|=1\) and \(\|\mathbf{b}\|=1\), drawn from a random distribution. Assume that the distribution is symmetrical around the origin, i.e., it depends only on the distance from the origin and not the direction. Show that for large values of \(D\) the magnitude of the cosine of the angle between these vectors is close to zero and hence that these random vectors are nearly orthogonal in a high-dimensional space. To do this, consider an orthonormal basis set \(\{\mathbf{u}_{i}\}\) where \(\mathbf{u}_{i}^{\mathrm{T}} \mathbf{u}_{j}=\delta_{i j}\) and express \(\mathbf{a}\) and \(\mathbf{b}\) as expansions in this basis.

** 12.9 Position Encoding and Linear Transformation
Consider a position encoding in which the input token vector \(\mathbf{x}\) is concatenated with a position-encoding vector \(\mathbf{e}\). Show that when this concatenated vector undergoes a general linear transformation by multiplication using a matrix, the result can be expressed as the sum of a linearly transformed input and a linearly transformed position vector.

** 12.10 Positional Encoding Linearity
Show that the positional encoding defined by (12.25) has the property that, for a fixed offset \(k\), the encoding at position \(n+k\) can be represented as a linear combination of the encoding at position \(n\) with coefficients that depend only on \(k\) and not on \(n\). To do this make use of the following trigonometric identities:

\[
\cos (A+B) = \cos A \cos B - \sin A \sin B, \quad \sin (A+B) = \cos A \sin B + \sin A \cos B
\]

Show that if the encoding is based purely on sine functions, without cosine functions, then this property no longer holds.

** 12.11 Bag-of-Words Maximum Likelihood
Consider the bag-of-words model (12.28) in which each of the component distributions \(p(\mathbf{x}_{n})\) is given by a general probability table that is shared across all words. Show that the maximum likelihood solution, given a training set of vectors, is given by a table whose entries are the fractions of times each word occurs in the training set.

** 12.12 Autoregressive Language Model
Consider the autoregressive language model given by (12.31) and suppose that the terms \(p(\mathbf{x}_{n} \mid \mathbf{x}_{1}, \ldots, \mathbf{x}_{n-1})\) on the right-hand side are represented by general probability tables. Show that the number of entries in these tables grows exponentially with the value of \(n\).

** 12.13 Convenience of n-Grams
When using \(n\)-grams it is usual to train the \(n\)-gram and \((n-1)\)-gram models at the same time and then compute the conditional probability using the product rule of probability in the form

\[
p(\mathbf{x}_{n} \mid \mathbf{x}_{n-L+1}, \ldots, \mathbf{x}_{n-1})=\frac{p_{L}(\mathbf{x}_{n-L+1}, \ldots, \mathbf{x}_{n})}{p_{L-1}(\mathbf{x}_{n-L+1}, \ldots, \mathbf{x}_{n-1})}
\]

Explain why this is more convenient than storing the left-hand-side directly, and show that to obtain the correct probabilities the final token from each sequence must be omitted when evaluating \(p_{L-1}(\cdots)\).

** 12.14 Pseudo-Code for RNN Inference
Write down pseudo-code for the inference process in a trained RNN with an architecture of the form depicted in Figure 12.13.

** 12.15 Marginal and Conditional Distributions
Consider a sequence of two tokens \(y_{1}\) and \(y_{2}\) each of which can take the states \(A\) or \(B\). The table below shows the joint probability distribution \(p(y_{1}, y_{2})\):

|             | \(y_{1}=A\) | \(y_{1}=B\) |
|-------------+-------------+-------------|
| \(y_{2}=A\) |         0.0 |         0.4 |
| \(y_{2}=B\) |         0.1 |        0.25 |

We see that the most probable sequence is \(y_{1}=B, y_{2}=B\) and that this has probability 0.4. Using the sum and product rules of probability, write down the values of the marginal distribution \(p(y_{1})\) and the conditional distribution \(p(y_{2} \mid y_{1})\). Show that if we first maximize \(p(y_{1})\) to give a value \(y_{1}^{\star}\) and then subsequently maximize \(p(y_{2} \mid y_{1}^{\star})\) then we obtain a sequence that is different from the overall most probable sequence. Find the probability of the sequence.

** 12.16 BERT-Large Model Parameters
The BERT-Large model (Devlin et al., 2018) has a maximum input length of 512 tokens, each of dimensionality \(D=1,024\) and taken from a vocabulary of 30,000. It has 24 transformer layers each with 16 self-attention heads with \(D_{\mathrm{q}}=D_{\mathrm{k}}=D_{\mathrm{v}}= 64\), and the MLP position-wise networks have two layers with 4,096 hidden nodes. Show that the total number of parameters in the BERT encoder transformer language model is approximately 340 million.

* Graph Neural Networks
** 13.1 Permutation Matrix
Show that the permutation \((A, B, C, D, E) \rightarrow (C, E, A, D, B)\) corresponding to the two choices of node ordering in Figure 13.2 can be expressed in the form (13.5) with a permutation matrix given by (13.1).

** 13.2 Degree of Nodes
Show that the number of edges connected to each node of a graph is given by the corresponding diagonal element of the matrix \(\mathbf{A}^{2}\) where \(\mathbf{A}\) is the adjacency matrix.

** 13.3 Adjacency Matrix to Graph
Draw the graph whose adjacency matrix is given by

\[
\mathbf{A}=\left(\begin{array}{lllll}
0 & 1 & 1 & 0 & 1 \\
1 & 0 & 1 & 1 & 1 \\
1 & 1 & 0 & 1 & 0 \\
0 & 1 & 1 & 0 & 0 \\
1 & 1 & 0 & 0 & 0
\end{array}\right)
\]

** 13.4 Permutation of Data Matrix
Show that the effect of pre-multiplying a data matrix \(\mathbf{X}\) using a permutation matrix \(\mathbf{P}\) defined by (13.3) is to create a new data matrix \(\widetilde{\mathbf{X}}\) given by (13.4) whose rows are permuted according to the permutation function \(\pi(\cdot)\).

** 13.5 Permutation of Adjacency Matrix
Show that the transformed adjacency matrix \(\widetilde{\mathbf{A}}\) defined by (13.5), where \(\mathbf{P}\) is defined by (13.3), is such that both the rows and the columns are permuted according to the permutation function \(\pi(\cdot)\) relative to the original adjacency matrix \(\mathbf{A}\).

** 13.6 Matrix Form of Update Equations
In this exercise, we write the update equations (13.16) as graph-level equations using matrices. To keep the notation uncluttered, we omit the layer index \(l\). First, gather the node-embedding vectors \(\{\mathbf{h}_{n}\}\) into an \(N \times D\) matrix \(\mathbf{H}\) in which row \(n\) is given by \(\mathbf{h}_{n}^{\mathrm{T}}\). Then show that the neighbourhood-aggregated vectors \(\mathbf{z}_{n}\) given by

\[
\mathbf{z}_{n}=\sum_{m \in \mathcal{N}(n)} \mathbf{h}_{m}
\]

can be written in matrix form as \(\mathbf{Z}=\mathbf{AH}\) where \(\mathbf{Z}\) is the \(N \times D\) matrix in which row \(n\) is given by \(\mathbf{z}_{n}^{\mathrm{T}}\), and \(\mathbf{A}\) is the adjacency matrix. Finally, show that the argument to the nonlinear activation function in (13.16) can be written in matrix form as

\[
\mathbf{AHW}_{\text{neigh}}+\mathbf{H} \mathbf{W}_{\text{self}}+\mathbf{1}_{D} \mathbf{b}^{\mathrm{T}}
\]

where \(\mathbf{1}_{D}\) is the \(D\)-dimensional column vector in which all elements are 1.

** 13.7 Equivariance of Graph Convolutional Network
By making use of the equivariance property (13.19) for layer \(l\) of a deep graph convolutional network along with the permutation property (13.4) for the node variables, show that a complete deep graph convolutional network defined by (13.18) is also equivariant.

** 13.8 Equivariance of Attention Weights
Explain why the aggregation function defined by (13.24), in which the attention weights are given by (13.28), is equivariant under a reordering of the nodes in the graph.

** 13.9 Graph Attention Network and Transformers
Show that a graph attention network in which the graph is fully connected, so that there is an edge between every pair of nodes, is equivalent to a standard transformer architecture.

** 13.10 Invariance of Messages under Transformations
When a coordinate system is translated, the location of an object defined by that coordinate system is transformed using

\[
\widetilde{\mathbf{r}}=\mathbf{r}+\mathbf{c}
\]

where \(\mathbf{c}\) is a fixed vector describing the translation. Similarly, if the coordinate system is rotated and/or mirror reflected, the location vector of an object is transformed using

\[
\widetilde{\mathbf{r}}=\mathbf{R r}
\]

where \(\mathbf{R}\) is an orthogonal matrix whose inverse is given by its transpose so that

\[
\mathbf{R} \mathbf{R}^{\mathrm{T}}=\mathbf{R}^{\mathrm{T}} \mathbf{R}=\mathbf{I}
\]

Using these properties, show that under translations, rotations, and reflections, the messages in (13.38), (13.40), and (13.41) are invariant, and that the coordinate embeddings given by (13.39) are equivariant.

* Sampling
** 14.1 Unbiased Estimator
Show that \(\bar{f}\) defined by (14.2) is an unbiased estimator, in other words that the expectation of the right-hand side is equal to \(\mathbb{E}[f(\mathbf{z})]\).

** 14.2 Variance of Estimator
Show that \(\bar{f}\) defined by (14.2) has variance given by (14.4).

** 14.3 Uniform to Given Distribution
Suppose that \(z\) is a random variable with uniform distribution over \((0,1)\) and that we transform \(z\) using \(y=h^{-1}(z)\) where \(h(y)\) is given by (14.6). Show that \(y\) has the distribution \(p(y)\).

** 14.4 Uniform to Cauchy Distribution
Given a random variable \(z\) that is uniformly distributed over \((0,1)\), find a transformation \(y=f(z)\) such that \(y\) has a Cauchy distribution given by (14.8).

** 14.5 Distribution over Unit Circle
Suppose that \(z_{1}\) and \(z_{2}\) are uniformly distributed over the unit circle, as shown in Figure 14.3, and that we make the change of variables given by (14.10) and (14.11). Show that \((y_{1}, y_{2})\) will be distributed according to (14.12).

** 14.6 Multivariate Gaussian Sampling
Let \(\mathbf{z}\) be a \(D\)-dimensional random variable having a Gaussian distribution with zero mean and unit covariance matrix, and suppose that the positive definite symmetric matrix \(\boldsymbol{\Sigma}\) has the Cholesky decomposition \(\boldsymbol{\Sigma}=\mathbf{L} \mathbf{L}^{\mathrm{T}}\), where \(\mathbf{L}\) is a lower triangular matrix (i.e., one with zeros above the leading diagonal). Show that the variable \(\mathbf{y}=\boldsymbol{\mu}+\mathbf{Lz}\) has a Gaussian distribution with mean \(\boldsymbol{\mu}\) and covariance \(\boldsymbol{\Sigma}\). This provides a technique for generating samples from a general multivariate Gaussian using samples from a univariate Gaussian having zero mean and unit variance.

** 14.7 Rejection Sampling
In this exercise, we show more carefully that rejection sampling does indeed draw samples from the desired distribution \(p(\mathbf{z})\). Suppose the proposal distribution is \(q(\mathbf{z})\). Show that the probability of a sample value \(\mathbf{z}\) being accepted is given by \(\widetilde{p}(\mathbf{z}) / k q(\mathbf{z})\) where \(\widetilde{p}\) is any unnormalized distribution that is proportional to \(p(\mathbf{z})\), and the constant \(k\) is set to the smallest value that ensures \(k q(\mathbf{z}) \geqslant \widetilde{p}(\mathbf{z})\) for all values of \(\mathbf{z}\). Note that the probability of drawing a value \(\mathbf{z}\) is given by the probability of drawing that value from \(q(\mathbf{z})\) times the probability of accepting that value given that it has been drawn. Make use of this, along with the sum and product rules of probability, to write down the normalized form for the distribution over \(\mathbf{z}\), and show that it equals \(p(\mathbf{z})\).

** 14.8 Uniform to Cauchy Transformation
Suppose that \(z\) has a uniform distribution over the interval \([0,1]\). Show that the variable \(y=b \tan z+c\) has a Cauchy distribution given by (14.16).

** 14.9 Coefficients for Adaptive Rejection Sampling
Determine expressions for the coefficients \(k_{i}\) in the envelope distribution (14.17) for adaptive rejection sampling using the requirements of continuity and normalization.

** 14.10 Algorithm for Piecewise Exponential Distribution
By making use of the technique discussed in Section 14.1.2 for sampling from a single exponential distribution, devise an algorithm for sampling from the piecewise exponential distribution defined by (14.17).

** 14.11 Random Walk Variance
Show that the simple random walk over the integers defined by (14.28), (14.29), and (14.30) has the property that \(\mathbb{E}\left[\left(z^{(\tau)}\right)^{2}\right]=\mathbb{E}\left[\left(z^{(\tau-1)}\right)^{2}\right]+1 / 2\) and hence by induction that \(\mathbb{E}\left[\left(z^{(\tau)}\right)^{2}\right]=\tau / 2\).

** 14.12 Gibbs Sampling Detailed Balance
Show that the Gibbs sampling algorithm, discussed in Section 14.2.4, satisfies detailed balance as defined by (14.34).

** 14.13 Gibbs Sampling Ergodicity
Consider the distribution shown in Figure 14.14. Discuss whether the standard Gibbs sampling procedure for this distribution is ergodic and therefore whether it would sample correctly from this distribution.

** 14.14 Over-Relaxation Update
Verify that the over-relaxation update (14.46), in which \(z_{i}\) has mean \(\mu_{i}\) and variance \(\sigma_{i}\) and where \(\nu\) has zero mean and unit variance gives a value \(z_{i}^{\prime}\) with mean \(\mu_{i}\) and variance \(\sigma_{i}^{2}\).

** 14.15 Likelihood Weighted Sampling
Show that in likelihood weighted sampling from a directed graph the importance sampling weights are given by (14.48).

** 14.16 Normalization of Distribution
Show that the distribution (14.50) is normalized with respect to \(x\) provided \(Z(\mathbf{w})\) satisfies (14.51).

** 14.17 Gradient of Log Likelihood
By making use of (14.50) show that the gradient of the log likelihood function for an energy-based model can be written in the form (14.52).

** 14.18 Gradient of Log Likelihood with Energy-Based Model
By making use of (14.54), (14.55), and (14.56), show that the gradient of the log likelihood function for an energy-based model can be written in the form (14.57).

* Discrete Latent Variables
** 15.1 Convergence of \(K\)-means Algorithm
Consider the \(K\)-means algorithm discussed in Section 15.1. Show that as a consequence of there being a finite number of possible assignments for the set of discrete indicator variables \(r_{nk}\) and that for each such assignment there is a unique optimum for the \(\{\boldsymbol{\mu}_{k}\}\), the \(K\)-means algorithm must converge after a finite number of iterations.

** 15.2 Sequential \(K\)-means Algorithm
In this exercise we derive the sequential form for the \(K\)-means algorithm. At each step we consider a new data point \(\mathbf{x}_{n}\), and only the prototype vector that is closest to \(\mathbf{x}_{n}\) is updated. Starting from the expression (15.4) for the prototype vectors in the batch setting, separate out the contribution from the final data point \(\mathbf{x}_{n}\). By rearranging the formula, show that this update takes the form (15.5). Note that, since no approximation is made in this derivation, the resulting prototype vectors will have the property that they each equal the mean of all the data vectors that were assigned to them.

** 15.3 Marginal Distribution of Gaussian Mixture Model
Consider a Gaussian mixture model in which the marginal distribution \(p(\mathbf{z})\) for the latent variable is given by (15.9) and the conditional distribution \(p(\mathbf{x} \mid \mathbf{z})\) for the observed variable is given by (15.10). Show that the marginal distribution \(p(\mathbf{x})\), obtained by summing \(p(\mathbf{z}) p(\mathbf{x} \mid \mathbf{z})\) over all possible values of \(\mathbf{z}\), is a Gaussian mixture of the form (15.6).

** 15.4 Parameter Settings in Mixture Model
Show that the number of equivalent parameter settings due to interchange symmetries in a mixture model with \(K\) components is \(K!\).

** 15.5 EM Algorithm for Posterior Maximization
Suppose we wish to use the EM algorithm to maximize the posterior distribution over parameters \(p(\boldsymbol{\theta} \mid \mathbf{X})\) for a model containing latent variables, where \(\mathbf{X}\) is the observed data set. Show that the E step remains the same as in the maximum likelihood case, whereas in the M step the quantity to be maximized is given by \(\mathcal{Q}(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}) + \ln p(\boldsymbol{\theta})\) where \(\mathcal{Q}(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}})\) is defined by (15.23).

** 15.6 Posterior Factorization in GMM
Consider the directed graph for a Gaussian mixture model shown in Figure 15.9. By making use of the d-separation criterion, show that the posterior distribution of the latent variables factorizes with respect to the different data points so that

\[
p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}) = \prod_{n=1}^{N} p(\mathbf{z}_{n} \mid \mathbf{x}_{n}, \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})
\]

** 15.7 EM Equations for GMM with Common Covariance
Consider a special case of a Gaussian mixture model in which the covariance matrices \(\Sigma_{k}\) of the components are all constrained to have a common value \(\Sigma\). Derive the EM equations for maximizing the likelihood function under such a model.

** 15.8 Complete-Data Log Likelihood Maximization
Verify that maximization of the complete-data log likelihood (15.26) for a Gaussian mixture model leads to the result that the means and covariances of each component are fitted independently to the corresponding group of data points and that the mixing coefficients are given by the fractions of points in each group.

** 15.9 Maximizing \(\boldsymbol{\mu}_{k}\) in GMM
Show that if we maximize (15.30) with respect to \(\boldsymbol{\mu}_{k}\) while keeping the responsibilities \(\gamma(z_{nk})\) fixed, we obtain the closed-form solution given by (15.16).

** 15.10 Maximizing \(\boldsymbol{\Sigma}_{k}\) and \(\pi_{k}\) in GMM
Show that if we maximize (15.30) with respect to \(\boldsymbol{\Sigma}_{k}\) and \(\pi_{k}\) while keeping the responsibilities \(\gamma(z_{nk})\) fixed, we obtain the closed-form solutions given by (15.18) and (15.21).

** 15.11 Conditional Density in Mixture Model
Consider a density model given by a mixture distribution:

\[
p(\mathbf{x}) = \sum_{k=1}^{K} \pi_{k} p(\mathbf{x} \mid k)
\]

and suppose that we partition the vector \(\mathbf{x}\) into two parts so that \(\mathbf{x} = (\mathbf{x}_{a}, \mathbf{x}_{b})\). Show that the conditional density \(p(\mathbf{x}_{b} \mid \mathbf{x}_{a})\) is itself a mixture distribution, and find expressions for the mixing coefficients and for the component densities.

** 15.12 Relationship Between \(K\)-means and EM
In Section 15.3.2, we obtained a relationship between \(K\) means and EM for Gaussian mixtures by considering a mixture model in which all components have covariance \(\epsilon \mathbf{I}\). Show that in the limit \(\epsilon \rightarrow 0\), maximizing the expected complete-data log likelihood for this model, given by (15.30), is equivalent to minimizing the error measure \(J\) for the \(K\)-means algorithm given by (15.1).

** 15.13 Mean and Covariance of Bernoulli Distribution
Verify the results (15.35) and (15.36) for the mean and covariance of the Bernoulli distribution.

** 15.14 Mean and Covariance of Mixture Distribution
Consider a mixture distribution of the form

\[
p(\mathbf{x}) = \sum_{k=1}^{K} \pi_{k} p(\mathbf{x} \mid k)
\]

where the elements of \(\mathbf{x}\) could be discrete or continuous or a combination of these. Denote the mean and covariance of \(p(\mathbf{x} \mid k)\) by \(\boldsymbol{\mu}_{k}\) and \(\boldsymbol{\Sigma}_{k}\), respectively. By making use of the results of Exercise 15.13, show that the mean and covariance of the mixture distribution are given by (15.39) and (15.40).

** 15.15 Convergence of Bernoulli Mixture Model
Using the re-estimation equations for the EM algorithm, show that a mixture of Bernoulli distributions, with its parameters set to values corresponding to a maximum of the likelihood function, has the property that

\[
\mathbb{E}[\mathbf{x}] = \frac{1}{N} \sum_{n=1}^{N} \mathbf{x}_{n} \equiv \overline{\mathbf{x}}
\]

Hence, show that if the parameters of this model are initialized such that all components have the same mean \(\boldsymbol{\mu}_{k} = \widehat{\boldsymbol{\mu}}\) for \(k=1, \ldots, K\), then the EM algorithm will converge after one iteration, for any choice of the initial mixing coefficients, and that this solution has the property \(\boldsymbol{\mu}_{k} = \overline{\mathbf{x}}\). Note that this represents a degenerate case of the mixture model in which all the components are identical, and in practice we try to avoid such solutions by using an appropriate initialization.

** 15.16 Joint Distribution of Bernoulli Mixture Model
Consider the joint distribution of latent and observed variables for the Bernoulli distribution obtained by forming the product of \(p(\mathbf{x} \mid \mathbf{z}, \boldsymbol{\mu})\) given by (15.42) and \(p(\mathbf{z} \mid \boldsymbol{\pi})\) given by (15.43). Show that if we marginalize this joint distribution with respect to \(\mathbf{z}\), then we obtain (15.37).

** 15.17 M-step for Bernoulli Mixture Model
Show that if we maximize the expected complete-data log likelihood function (15.45) for a mixture of Bernoulli distributions with respect to \(\boldsymbol{\mu}_{k}\), we obtain the M-step equation (15.49).

** 15.18 Mixing Coefficients in Bernoulli Mixture Model
Show that if we maximize the expected complete-data log likelihood function (15.45) for a mixture of Bernoulli distributions with respect to the mixing coefficients \(\pi_{k}\), and use a Lagrange multiplier to enforce the summation constraint, we obtain the M-step equation (15.50).

** 15.19 Bounded Incomplete-Data Log Likelihood
Show that as a consequence of the constraint \(0 \leqslant p(\mathbf{x}_{n} \mid \boldsymbol{\mu}_{k}) \leqslant 1\) for the discrete variable \(\mathbf{x}_{n}\), the incomplete-data log likelihood function for a mixture of Bernoulli distributions is bounded above and hence that there are no singularities for which the likelihood goes to infinity.

** 15.20 EM Algorithm for Multinomial Mixture Model
Consider a \(D\)-dimensional variable \(\mathbf{x}\) each of whose components \(i\) is itself a multinomial variable of degree \(M\) so that \(\mathbf{x}\) is a binary vector with components \(x_{ij}\) where \(i=1, \ldots, D\) and \(j=1, \ldots, M\), subject to the constraint that \(\sum_{j} x_{ij} = 1\) for all \(i\). Suppose that the distribution of these variables is described by a mixture of the discrete multinomial distributions so that

\[
p(\mathbf{x}) = \sum_{k=1}^{K} \pi_{k} p(\mathbf{x} \mid \boldsymbol{\mu}_{k})
\]

where

\[
p(\mathbf{x} \mid \boldsymbol{\mu}_{k}) = \prod_{i=1}^{D} \prod_{j=1}^{M} \mu_{kij}^{x_{ij}}
\]

The parameters \(\mu_{kij}\) represent the probabilities \(p(x_{ij} = 1 \mid \boldsymbol{\mu}_{k})\) and must satisfy \(0 \leqslant \mu_{kij} \leqslant 1\) together with the constraint \(\sum_{j} \mu_{kij} = 1\) for all values of \(k\) and \(i\). Given an observed data set \(\{\mathbf{x}_{n}\}\), where \(n = 1, \ldots, N\), derive the E-step and M-step equations of the EM algorithm for optimizing the mixing coefficients \(\pi_{k}\) and the component parameters \(\mu_{kij}\) of this distribution by maximum likelihood.

** 15.21 Relation Between \(\mathcal{L}(q, \boldsymbol{\theta})\) and *\(\mathrm{KL}(q \| p)\)
Verify the relation (15.52) in which \(\mathcal{L}(q, \boldsymbol{\theta})\) and \(\mathrm{KL}(q \| p)\) are defined by (15.53) and (15.54), respectively.

** 15.22 Gradient of \(\mathcal{L}(q, \boldsymbol{\theta})\)
Show that the lower bound \(\mathcal{L}(q, \boldsymbol{\theta})\) given by (15.53), with \(q(\mathbf{Z}) = p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{(\text{old})})\), has the same gradient with respect to \(\boldsymbol{\theta}\) as the log likelihood function \(\ln p(\mathbf{X} \mid \boldsymbol{\theta})\) at the point \(\boldsymbol{\theta} = \boldsymbol{\theta}^{\text{(old)}}\).

** 15.23 Incremental EM Algorithm for Gaussian Mixtures
Consider the incremental form of the EM algorithm for a mixture of Gaussians, in which the responsibilities are recomputed only for a specific data point \(\mathbf{x}_{m}\). Starting from the M-step formulae (15.16) and (15.17), derive the results (15.60) and (15.61) for updating the component means.

** 15.24 Incremental Updates for Covariance and Mixing Coefficients
Derive M-step formulae for updating the covariance matrices and mixing coefficients in a Gaussian mixture model when the responsibilities are updated incrementally, analogous to the result (15.60) for updating the means.

* Continuous Latent Variables
** 16.1 Maximizing Variance Projection
In this exercise, we use proof by induction to show that the linear projection onto an \(M\)-dimensional subspace that maximizes the variance of the projected data is defined by the \(M\) eigenvectors of the data covariance matrix \(\mathbf{S}\), given by (16.3), corresponding to the \(M\) largest eigenvalues. In Section 16.1, this result was proven for \(M=1\). Now suppose the result holds for some general value of \(M\) and show that it consequently holds for dimensionality \(M+1\). To do this, first set the derivative of the variance of the projected data with respect to a vector \(\mathbf{u}_{M+1}\) defining the new direction in data space equal to zero. This should be done subject to the constraints that \(\mathbf{u}_{M+1}\) are orthogonal to the existing vectors \(\mathbf{u}_{1}, \ldots, \mathbf{u}_{M}\), and also that it is normalized to unit length. Use Lagrange multipliers to enforce these constraints. Then make use of the orthonormality properties of the vectors \(\mathbf{u}_{1}, \ldots, \mathbf{u}_{M}\) to show that the new vector \(\mathbf{u}_{M+1}\) is an eigenvector of \(\mathbf{S}\). Finally, show that the variance is maximized if the eigenvector is chosen to be the one corresponding to eigenvalue \(\lambda_{M+1}\) where the eigenvalues have been ordered in decreasing value.

** 16.2 Minimum PCA Error
Show that the minimum value of the PCA error measure \(J\) given by (16.15) with respect to the \(\mathbf{u}_{i}\), subject to the orthonormality constraints (16.7), is obtained when the \(\mathbf{u}_{i}\) are eigenvectors of the data covariance matrix \(\mathbf{S}\). To do this, introduce a matrix \(\mathbf{H}\) of Lagrange multipliers, one for each constraint, so that the modified error measure, in matrix notation reads

\[
\widetilde{J}=\operatorname{Tr}\left\{\widehat{\mathbf{U}}^{\mathrm{T}} \mathbf{S} \widehat{\mathbf{U}}\right\}+\operatorname{Tr}\left\{\mathbf{H}\left(\mathbf{I}-\widehat{\mathbf{U}}^{\mathrm{T}} \widehat{\mathbf{U}}\right)\right\}
\]

where \(\widehat{\mathbf{U}}\) is a matrix of dimension \(D \times(D-M)\) whose columns are given by \(\mathbf{u}_{i}\). Now minimize \(\widetilde{J}\) with respect to \(\widehat{\mathbf{U}}\) and show that the solution satisfies \(\mathbf{S} \widehat{\mathbf{U}}=\widehat{\mathbf{U}} \mathbf{H}\). Clearly, one possible solution is that the columns of \(\widehat{\mathbf{U}}\) are eigenvectors of \(\mathbf{S}\), in which case \(\mathbf{H}\) is a diagonal matrix containing the corresponding eigenvalues. To obtain the general solution, show that \(\mathbf{H}\) can be assumed to be a symmetric matrix, and by using its eigenvector expansion, show that the general solution to \(\widehat{\mathbf{U}}=\widehat{\mathbf{U}} \mathbf{H}\) gives the same value for \(\widetilde{J}\) as the specific solution in which the columns of \(\widehat{\mathbf{U}}\) are the eigenvectors of \(\mathbf{S}\). Because these solutions are all equivalent, it is convenient to choose the eigenvector solution.

** 16.3 Normalization of Eigenvectors
Verify that the eigenvectors defined by (16.30) are normalized to unit length, assuming that the eigenvectors \(\mathbf{v}_{i}\) have unit length.

** 16.4 General Gaussian Distribution
Suppose we replace the zero-mean, unit-covariance latent space distribution (16.31) in the probabilistic PCA model by a general Gaussian distribution of the form \(\mathcal{N}(\mathbf{z} \mid \mathbf{m}, \boldsymbol{\Sigma})\). By redefining the parameters of the model, show that this leads to an identical model for the marginal distribution \(p(\mathbf{x})\) over the observed variables for any valid choice of \(\mathbf{m}\) and \(\Sigma\).

** 16.5 Gaussian Distribution of Linear Transformation
Let \(\mathbf{x}\) be a \(D\)-dimensional random variable having a Gaussian distribution given by \(\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\), and consider the \(M\)-dimensional random variable given by \(\mathbf{y}=\mathbf{A} \mathbf{x}+\mathbf{b}\) where \(\mathbf{A}\) is an \(M \times D\) matrix. Show that \(\mathbf{y}\) also has a Gaussian distribution, and find expressions for its mean and covariance. Discuss the form of this Gaussian distribution for \(M<D\), for \(M=D\), and for \(M>D\).

** 16.6 Marginal Distribution in Probabilistic PCA
By making use of the results (2.122) and (2.123) for the mean and covariance of a general distribution, derive the result (16.35) for the marginal distribution \(p(\mathbf{x})\) in the probabilistic PCA model.

** 16.7 Probabilistic Graph for Probabilistic PCA
Draw a directed probabilistic graph for the probabilistic PCA model described in Section 16.2 in which the components of the observed variable \(\mathbf{x}\) are shown explicitly as separate nodes. Hence, verify that the probabilistic PCA model has the same independence structure as the naive Bayes model discussed in Section 11.2.3.

** 16.8 Posterior Distribution in Probabilistic PCA
By making use of the result (3.100), show that the posterior distribution \(p(\mathbf{z} \mid \mathbf{x})\) for the probabilistic PCA model is given by (16.43).

** 16.9 Maximum Likelihood Estimation of \(\boldsymbol{\mu}\)
Verify that maximizing the \(\log\) likelihood (16.44) for the probabilistic PCA model with respect to the parameter \(\boldsymbol{\mu}\) gives the result \(\boldsymbol{\mu}_{\mathrm{ML}}=\overline{\mathrm{x}}\) where \(\overline{\mathrm{x}}\) is the mean of the data vectors.

** 16.10 Stationary Point of Log Likelihood
By evaluating the second derivatives of the log likelihood function (16.44) for the probabilistic PCA model with respect to the parameter \(\boldsymbol{\mu}\), show that the stationary point \(\boldsymbol{\mu}_{\mathrm{ML}}=\overline{\mathrm{x}}\) represents the unique maximum.

** 16.11 Orthogonal Projection in Probabilistic PCA
Show that in the limit \(\sigma^{2} \rightarrow 0\), the posterior mean for the probabilistic PCA model becomes an orthogonal projection onto the principal subspace, as in conventional PCA.

** 16.12 Posterior Mean Shift in Probabilistic PCA
For \(\sigma^{2}>0\) show that the posterior mean in the probabilistic PCA model is shifted towards the origin relative to the orthogonal projection.

** 16.13 Optimal Reconstruction in Probabilistic PCA
Show that the optimal reconstruction of a data point under probabilistic PCA, according to the least-squares projection cost of conventional PCA, is given by

\[
\widetilde{\mathbf{x}}=\mathbf{W}_{\mathrm{ML}}\left(\mathbf{W}_{\mathrm{ML}}^{\mathrm{T}} \mathbf{W}_{\mathrm{ML}}\right)^{-1} \mathbf{M} \mathbb{E}[\mathbf{z} \mid \mathbf{x}]
\]

** 16.14 Independent Parameters in Covariance Matrix
The number of independent parameters in the covariance matrix for a probabilistic PCA model with an \(M\)-dimensional latent space and a \(D\)-dimensional data space is given by (16.52). Verify that for \(M=D-1\), the number of independent parameters is the same as in a general covariance Gaussian, whereas for \(M=0\) it is the same as for a Gaussian with an isotropic covariance.

** 16.15 Independent Parameters in Factor Analysis Model
Derive an expression for the number of independent parameters in the factor analysis model described in Section 16.2.4.

** 16.16 Invariance Under Rotations in Factor Analysis Model
Show that the factor analysis model described in Section 16.2.4 is invariant under rotations of the latent space coordinates.

** 16.17 Invariance Under Linear Transformation in Latent-Variable Model
Consider a linear-Gaussian latent-variable model having a latent space distribution \(p(\mathbf{z})=\mathcal{N}(\mathbf{x} \mid \mathbf{0}, \mathbf{I})\) and a conditional distribution for the observed variable \(p(\mathbf{x} \mid \mathbf{z})=\mathcal{N}(\mathbf{x} \mid \mathbf{W z}+\boldsymbol{\mu}, \boldsymbol{\Phi})\) where \(\boldsymbol{\Phi}\) is an arbitrary symmetric positive-definite noise covariance matrix. Now suppose that we make a non-singular linear transformation of the data variables \(\mathbf{x} \rightarrow \mathbf{A} \mathbf{x}\), where \(\mathbf{A}\) is a \(D \times D\) matrix. If \(\boldsymbol{\mu}_{\mathrm{ML}}, \mathbf{W}_{\mathrm{ML}},\) and \(\boldsymbol{\Phi}_{\mathrm{ML}}\) represent the maximum likelihood solution corresponding to the original un-transformed data, show that \(\mathbf{A} \boldsymbol{\mu}_{\mathrm{ML}}, \mathbf{A W}_{\mathrm{ML}},\) and \(\mathbf{A} \boldsymbol{\Phi}_{\mathrm{ML}} \mathbf{A}^{\mathrm{T}}\) represent the corresponding maximum likelihood solution for the transformed data set. Finally, show that the form of the model is preserved in two cases: (i) \(\mathbf{A}\) is a diagonal matrix and \(\boldsymbol{\Phi}\) is a diagonal matrix. This corresponds to factor analysis. The transformed \(\boldsymbol{\Phi}\) remains diagonal, and hence factor analysis is covariant under component-wise re-scaling of the data variables; (ii) \(\mathbf{A}\) is orthogonal and \(\boldsymbol{\Phi}\) is proportional to the unit matrix so that \(\boldsymbol{\Phi}=\sigma^{2} \mathbf{I}\). This corresponds to probabilistic PCA. The transformed \(\boldsymbol{\Phi}\) matrix remains proportional to the unit matrix, and hence probabilistic PCA is covariant under a rotation of the axes of the data space, as is the case for conventional PCA.

** 16.18 Log Likelihood Function for Continuous Latent Variables
Verify that the \(\log\) likelihood function for a model with continuous latent variables can be written as the sum of two terms in the form (16.57) in which the terms are defined by (16.58) and (16.59). This can be done by using the product rule of probability in the form

\[
p(\mathbf{x}, \mathbf{z} \mid \mathbf{w})=p(\mathbf{z} \mid \mathbf{x}, \mathbf{w}) p(\mathbf{x} \mid \mathbf{w})
\]

and then substituting for \(p(\mathbf{x}, \mathbf{z} \mid \mathbf{w})\) in (16.58).

** 16.19 ELBO for i.i.d. Data
Show that, for a set of i.i.d. data, the evidence lower bound (ELBO) takes the form (16.63).

** 16.20 Graphical Model for Mixture of Probabilistic PCA Models
Draw a directed probabilistic graphical model representing a discrete mixture of probabilistic PCA models in which each PCA model has its own values of \(\mathbf{W}\), \(\boldsymbol{\mu}\), and \(\sigma^{2}\). Now draw a modified graph in which these parameter values are shared between the components of the mixture.

** 16.21 M-step Equations for Probabilistic PCA
Derive the M-step equations (16.68) and (16.69) for the probabilistic PCA model by maximizing the expected complete-data log likelihood function given by (16.65).

** 16.22 EM Algorithm for Missing Data in Probabilistic PCA
One benefit of a probabilistic formulation of principal component analysis is that it can be applied to a data set in which some of the values are missing, provided they are missing at random. Derive the EM algorithm for maximizing the likelihood function for the probabilistic PCA model in this situation. Note that the \(\left\{\mathbf{z}_{n}\right\}\), as well as the missing data values that are components of the vectors \(\left\{\mathbf{x}_{n}\right\}\), are now latent variables. Show that in the special case in which all the data values are observed, this reduces to the EM algorithm for probabilistic PCA derived in Section 16.3.2.

** 16.23 Sum-of-Squares Reconstruction Cost
Let \(\mathbf{W}\) be a \(D \times M\) matrix whose columns define a linear subspace of dimensionality \(M\) embedded within a data space of dimensionality \(D\), and let \(\boldsymbol{\mu}\) be a \(D\)-dimensional vector. Given a data set \(\left\{\mathbf{x}_{n}\right\}\) where \(n=1, \ldots, N\), we can approximate the data points using a linear mapping from a set of \(M\)-dimensional vectors \(\left\{\mathbf{z}_{n}\right\}\), so that \(\mathbf{x}_{n}\) is approximated by \(\mathbf{W z}_{n}+\boldsymbol{\mu}\). The associated sum-of-squares reconstruction cost is given by

\[
J=\sum_{n=1}^{N}\left\|\mathbf{x}_{n}-\boldsymbol{\mu}-\mathbf{W z}_{n}\right\|^{2}
\]

First show that minimizing \(J\) with respect to \(\boldsymbol{\mu}\) leads to an analogous expression with \(\mathbf{x}_{n}\) and \(\mathbf{z}_{n}\) replaced by zero-mean variables \(\mathbf{x}_{n}-\overline{\mathbf{x}}\) and \(\mathbf{z}_{n}-\overline{\mathbf{z}}\), respectively, where \(\overline{\mathbf{x}}\) and \(\overline{\mathbf{z}}\) denote sample means. Then show that minimizing \(J\) with respect to \(\mathbf{z}_{n}\), where \(W\) is kept fixed, gives rise to the PCA E step (16.70), and that minimizing \(J\) with respect to \(\mathbf{W}\), where \(\left\{\mathbf{z}_{n}\right\}\) is kept fixed, gives rise to the PCA M step (16.71).

** 16.24 E Step of EM Algorithm for Factor Analysis
Derive the formulae (16.72) and (16.73) for the E step of the EM algorithm for factor analysis. Note that from the result of Exercise 16.26, the parameter \(\boldsymbol{\mu}\) can be replaced by the sample mean \(\overline{\mathbf{x}}\).

** 16.25 M Step of EM Algorithm for Factor Analysis
Write down an expression for the expected complete-data log likelihood function for the factor analysis model, and hence derive the corresponding M-step equations (16.75) and (16.76).

** 16.26 Stationary Point of Log Likelihood for Factor Analysis
By considering second derivatives, show that the only stationary point of the log likelihood function for the factor analysis model discussed in Section 16.2.4 with respect to the parameter \(\boldsymbol{\mu}\) is given by the sample mean defined by (16.1). Furthermore, show that this stationary point is a maximum.

* Generative Adversarial Networks
** 17.1 Stationary Point in GAN
We would like the GAN error function (17.6) to have the property that, given sufficiently flexible neural networks, the stationary point is obtained when the generator distribution matches the true data distribution. In this exercise we prove this result for network models with infinite flexibility by optimizing over the full space of probability distributions \(p_{\mathrm{G}}(\mathbf{x})\) and over the full space of functions \(d(\mathbf{x})\) corresponding to the generative and discriminative networks, respectively. Specifically, we assume that the discriminative model is optimized in an inner loop, giving rise to an effective outer loop error function for the generative model. First, show that, in the limit of an infinite number of data samples, the GAN error function (17.6) can be rewritten in the form

\[
E\left(p_{\mathrm{G}}, d\right)=-\int p_{\text {data }}(\mathbf{x}) \ln d(\mathbf{x}) \mathrm{d} \mathbf{x}-\int p_{\mathrm{G}}(\mathbf{x}) \ln (1-d(\mathbf{x})) \mathrm{d} \mathbf{x}
\]

where \(p_{\text {data }}(\mathbf{x})\) is the fixed distribution of real data points. Now consider a variational optimization over all functions \(d(\mathbf{x})\). Show that, for a fixed generative network, the solution for the discriminator \(d(\mathbf{x})\) that minimizes \(E\) is given by

\[
d^{\star}(\mathbf{x})=\frac{p_{\text {data }}(\mathbf{x})}{p_{\text {data }}(\mathbf{x})+p_{\mathrm{G}}(\mathbf{x})}
\]

Hence, show that the error function \(E\) can be written as a function of the generator network \(p_{\mathrm{G}}(\mathbf{x})\) in the form

\[
\begin{align*}
C\left(p_{\mathrm{G}}\right)= & -\int p_{\text {data }}(\mathbf{x}) \ln \left\{\frac{p_{\mathrm{data}}(\mathbf{x})}{p_{\text {data }}(\mathbf{x})+p_{\mathrm{G}}(\mathbf{x})}\right\} \mathrm{d} \mathbf{x} \\
& -\int p_{\mathrm{G}}(\mathbf{x}) \ln \left\{\frac{p_{\mathrm{G}}(\mathbf{x})}{p_{\text {data }}(\mathbf{x})+p_{\mathrm{G}}(\mathbf{x})}\right\} \mathrm{d} \mathbf{x}
\end{align*}
\]

Now show that this can be rewritten in the form

\[
C\left(p_{\mathrm{G}}\right)=-\ln (4)+\mathrm{KL}\left(p_{\text {data }} \| \frac{p_{\text {data }}+p_{\mathrm{G}}}{2}\right)+\mathrm{KL}\left(p_{\mathrm{G}} \| \frac{p_{\text {data }}+p_{\mathrm{G}}}{2}\right)
\]

where the Kullback-Leibler divergence \(\mathrm{KL}(p \| q)\) is defined by (2.100). Finally, using the property that \(\mathrm{KL}(p \| q) \geqslant 0\) with equality if, and only if, \(p(\mathbf{x})=q(\mathbf{x})\) for all \(\mathbf{x}\), show that the minimum of \(C\left(p_{\mathrm{G}}\right)\) occurs when \(p_{\mathrm{G}}(\mathbf{x})=p_{\text {data }}(\mathbf{x})\). Note that the sum of the two Kullback-Leibler divergence terms in (17.17) is known as the Jensen-Shannon divergence between \(p_{\text {data }}\) and \(p_{\mathrm{G}}\). Like the Kullback-Leibler divergence, this is a non-negative quantity that vanishes if, and only if, the two distributions are equal, but unlike the \(\mathrm{KL}\) divergence, it is symmetric with respect to the two distributions.

** 17.2 Saddle Point in GAN Training
In this exercise we explore the problems that can arise from the adversarial nature of GAN training. Consider a cost function \(E(a, b)=ab\) defined over two parameters \(a\) and \(b\), analogous to the parameters of a generative and discriminative network, respectively. Show that the point \(a=0, b=0\) is a stationary point of the cost function. By considering the second derivatives along the lines \(b=a\) and \(b=-a\), show that the point \(a=0, b=0\) is a saddle point. Now suppose that we optimize this error function by taking infinitesimal steps, so that the variables become functions of continuous time \(a(t), b(t)\) defined by a continuous-time gradient descent, in which the parameter \(a(t)\) of the generative network is updated so as to increase \(E(a, b)\), whereas the parameter \(b(t)\) is updated so as to decrease \(E(a, b)\). Show that the evolution of the parameters is governed by the equations

\[
\frac{\mathrm{d} a}{\mathrm{~d} t}=\eta \frac{\partial E}{\partial a}, \quad \frac{\mathrm{d} b}{\mathrm{~d} t}=-\eta \frac{\partial E}{\partial b}
\]

Hence, show that \(a(t)\) satisfies the second-order differential equation

\[
\frac{\mathrm{d}^{2} a}{\mathrm{~d} t^{2}}=-\eta^{2} a(t)
\]

Verify that the following expression is a solution of (17.19):

\[
a(t)=C \cos (\eta t)+D \sin (\eta t)
\]

where \(C\) and \(D\) are arbitrary constants. If the system is initialized at \(t=0\) with the values \(a=1, b=0\), find the values of \(C\) and \(D\) and hence show that the resulting values of \(a(t)\) and \(b(t)\) trace out a circle of unit radius in \(a, b\) space centred on the origin, and that they therefore never converge to the saddle point.

** 17.3 Optimal Discriminator Output for Dog Images
Consider a GAN in which the training set consists of equal numbers of cat and dog images and in which the generator network has learned to produce high quality images of dogs. Show that, when presented with a dog image, the optimal output for the discriminator network (trained to generate the probability that the image is real) is \(1 / 3\).

* Normalizing Flows
** 18.1 Density Transformation under Change of Variables
Consider a transformation \(\mathbf{x}=\mathbf{f}(\mathbf{z})\) along with its inverse \(\mathbf{z}=\mathbf{g}(\mathbf{x})\). By differentiating \(\mathbf{x}=\mathbf{f}(\mathbf{g}(\mathbf{x}))\), show that

\[
\mathbf{J K}=\mathbf{I}
\]

where \(\mathbf{I}\) is the identity matrix, and \(\mathbf{J}\) and \(\mathbf{K}\) are matrices with elements

\[
J_{ij}=\frac{\partial g_{i}}{\partial x_{j}}, \quad K_{ij}=\frac{\partial f_{i}}{\partial z_{j}}
\]

Using the result that the determinant of a product of matrices is the product of their determinants, show that

\[
\operatorname{det}(\mathbf{J})=\frac{1}{\operatorname{det}(\mathbf{K})}
\]

Hence, show that the formula (18.1) for the transformation of a density under a change of variables can be rewritten as

\[
p_{\mathbf{x}}(\mathbf{x})=p_{\mathbf{z}}(\mathbf{g}(\mathbf{x}))|\operatorname{det} \mathbf{K}|^{-1}
\]

where \(\mathbf{K}\) is evaluated at \(\mathbf{z}=\mathbf{g}(\mathbf{x})\).

** 18.2 Inverse Function of Invertible Transformations
Consider a sequence of invertible transformations of the form

\[
\mathbf{x}=\mathbf{f}_{1}\left(\mathbf{f}_{2}\left(\cdots \mathbf{f}_{M-1}\left(\mathbf{f}_{M}(\mathbf{z})\right) \cdots\right)\right)
\]

Show that the inverse function is given by

\[
\mathbf{z}=\mathbf{f}_{M}^{-1}\left(\mathbf{f}_{M-1}^{-1}\left(\cdots \mathbf{f}_{2}^{-1}\left(\mathbf{f}_{1}^{-1}(\mathbf{x})\right) \cdots\right)\right)
\]

** 18.3 Jacobian of Linear Change of Variables
Consider a linear change of variables of the form

\[
\mathbf{x}=\mathbf{z}+\mathbf{b}
\]

Show that the Jacobian of this transformation is the identity matrix. Interpret this result by comparing the volume of a small region of \(\mathbf{z}\)-space with the volume of the corresponding region of \(\mathbf{x}\)-space.

** 18.4 Jacobian of Autoregressive Normalizing Flow
Show that the Jacobian of the autoregressive normalizing flow transformation given by (18.18) is a lower triangular matrix. The determinant of such a matrix is given by the product of the terms on the leading diagonal and is therefore easily evaluated.

** 18.5 Forward Propagation in Residual Network
Consider the forward propagation equation for a residual network given by (18.21) in which we consider a small increment \(\epsilon\) in the 'time' variable \(t\) :

\[
\mathbf{z}^{(t+\epsilon)}=\mathbf{z}^{(t)}+\epsilon \mathbf{f}\left(\mathbf{z}^{(t)}, \mathbf{w}\right)
\]

Here the additive contribution from the neural network is scaled by \(\epsilon\). Note that (18.21) corresponds to the case \(\epsilon=1\). By taking the limit \(\epsilon \rightarrow 0\), derive the forward propagation differential equation given by (18.22).

** 18.6 Backpropagation in Neural ODE
In this exercise and the next we provide an informal derivation of the backpropagation and gradient evaluation equations for a neural ODE. A more formal derivation of these results can be found in Chen et al. (2018). Write down the backpropagation equation corresponding to the forward equation (18.38). By taking the limit \(\epsilon \rightarrow 0\), derive the backward propagation equation (18.25), where \(a(t)\) is defined by (18.24).

** 18.7 Gradient of Loss Function for Residual Network
By making use of the result (8.10), write down an expression for the gradient of a loss function \(L(\mathbf{z}(T))\) for a multilayered residual network defined by (18.38) in which all layers share the same parameter vector \(\mathbf{w}\). By taking the limit \(\epsilon \rightarrow 0\), derive the equation (18.26) for the derivative of the loss function.

** 18.8 Density Transformation for One-Dimensional Distributions
In this exercise we give an informal derivation of (18.28) for one-dimensional distributions. Consider a distribution \(q(z)\) at time \(t\) that is transformed to a new distribution \(p(x)\) at time \(t+\delta t\) as a result of a transformation from \(z\) to \(x\). Also consider nearby values \(z\) and \(z+\Delta z\) along with corresponding values \(x\) and \(x+\Delta x\) as shown in Figure 18.7. First, write down an equation that expresses that the probability mass in the interval \(\Delta z\) is the same as that in the interval \(\Delta x\). Second, write down an equation that shows how the probability density changes in going from \(t\) to \(t+\delta t\), expressed in terms of the derivative \(\mathrm{d} q(t) / \mathrm{d} t\). Third, write down an equation for \(\Delta x\) in terms of \(\Delta z\) by introducing the function \(f(z)=\mathrm{d} z / \mathrm{d} t\). Finally, by combining these three equations and taking the limit \(\delta t \rightarrow 0\), show that

\[
\frac{\mathrm{d}}{\mathrm{d} t} \ln q(z)=-f^{\prime}(z)
\]

which is the one-dimensional version of (18.28).

** 18.9 Flow Lines and Differential Equation
The flow lines in Figure 18.6 were plotted by taking a set of equally spaced values and using the inverse of the cumulative distribution function at each value of \(t\) to plot the corresponding points in \(\mathbf{z}\)-space. Show that this is equivalent to using the differential equation (18.27) to compute the flow lines where \(f\) is defined by (18.28).

** 18.10 Base Density of Continuous Normalizing Flow
Using the differential equation (18.27) write down an expression for the base density of a continuous normalizing flow in terms of the output density, expressed as an integral over \(t\). Hence, by making use of the fact that changing the sign of a definite integral is equivalent to swapping the limits on that integral, show that the computational cost of inverting a continuous normalizing flow is the same as that needed to evaluate the forward flow.

** 18.11 Expectation in Hutchinson Trace Estimator
Show that the expectation of the right-hand side in the Hutchinson trace estimator (18.30) is equal to \(\operatorname{Tr}(\mathbf{A})\) for any value of \(M\). This shows that the estimator is unbiased.

* Autoencoders
** 19.1 Gradient of Integral with Monte Carlo Estimator
Show that, for any distribution \(q(\mathbf{z} \mid \phi)\) and any function \(G(\mathbf{z})\), the following relation holds:

\[
\nabla_{\phi} \int q(\mathbf{z} \mid \phi) G(\mathbf{z}) \mathrm{d} \mathbf{z}=\int q(\mathbf{z} \mid \phi) G(\mathbf{z}) \nabla_{\phi} \ln q(\mathbf{z} \mid \phi) \mathrm{d} \mathbf{z}
\]

Hence, show that the left-hand side of (19.20) can be approximated by the following Monte Carlo estimator:

\[
\nabla_{\phi} \int q(\mathbf{z} \mid \phi) G(\mathbf{z}) \mathrm{d} \mathbf{z} \simeq \sum_{i} G\left(\mathbf{z}^{(i)}\right) \nabla_{\phi} \ln q\left(\mathbf{z}^{(i)} \mid \phi\right)
\]

where the samples \(\left\{\mathbf{z}^{(i)}\right\}\) are drawn independently from the distribution \(q(\mathbf{z} \mid \phi)\). Verify that this estimator is unbiased, i.e., that the average value of the right-hand side of (19.21), averaged over the distribution of the samples, is equal to the left-hand side. In principle, by setting \(G(\mathbf{z})=p(\mathbf{x} \mid \mathbf{z}, \mathbf{w})\), this result would allow the gradient of the second term on the right-hand side of (19.14) with respect to \(\phi\) to be evaluated without making use of the reparameterization trick. Also, because this method is unbiased, it will give the exact answer in the limit of an infinite number of samples. However, the reparameterization trick is more efficient, meaning that fewer samples are needed to get good accuracy, because it directly computes the change of \(p(\mathbf{x} \mid \mathbf{z}, \mathbf{w})\) due to the change in \(\mathbf{z}\) that results from a change in \(\phi\).

** 19.2 Gaussian Distribution from Zero-Mean Unit-Variance
Verify that if \(\epsilon\) has a zero-mean unit-variance Gaussian distribution, then the variable \(z\) in (19.17) will have a Gaussian distribution with mean \(\mu\) and variance \(\sigma^{2}\).

** 19.3 General Covariance VAE Encoder Network
In this exercise we extend the diagonal covariance VAE encoder network (19.13) to one with a general covariance matrix. Consider a \(K\)-dimensional random vector drawn from a simple Gaussian:

\[
\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{z} \mid \mathbf{0}, \mathbf{I})
\]

which is then linearly transformed using the relation

\[
\mathbf{z}=\boldsymbol{\mu}+\mathbf{L} \boldsymbol{\epsilon}
\]

where \(\mathbf{L}\) is a lower-triangular matrix (i.e., a \(K \times K\) matrix with all elements above the leading diagonal being zero). Show that \(\mathbf{z}\) has a distribution \(\mathcal{N}(\mathbf{z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})\), and write down an expression for \(\Sigma\) in terms of \(\mathbf{L}\). Explain why the diagonal elements of \(\mathbf{L}\) must be non-negative. Describe how \(\boldsymbol{\mu}\) and \(\mathbf{L}\) can be expressed as the outputs of a neural network, and discuss suitable choices for output-unit activation functions.

** 19.4 KL Divergence Term in VAE
Evaluate the Kullback-Leibler divergence term in (19.14). Hence, show how the gradients of this term with respect to \(\mathbf{w}\) and \(\phi\) can be evaluated for training the encoder and decoder networks.

** 19.5 ELBO in Alternate Form (1)
We have seen that the ELBO given by (19.11) can be written in the form (19.14). Show that it can also be written as

\[
\begin{align*}
\mathcal{L}_{n}(\mathbf{w}, \boldsymbol{\phi})= & \int q\left(\mathbf{z}_{n} \mid \mathbf{x}_{n}, \boldsymbol{\phi}\right) \ln \left\{p\left(\mathbf{x}_{n} \mid \mathbf{z}_{n}, \mathbf{w}\right) p\left(\mathbf{z}_{n}\right)\right\} \mathrm{d} \mathbf{z}_{n} \\
& -\int q\left(\mathbf{z}_{n} \mid \mathbf{x}_{n}, \boldsymbol{\phi}\right) \ln q\left(\mathbf{z}_{n} \mid \mathbf{x}_{n}, \boldsymbol{\phi}\right) \mathrm{d} \mathbf{z}_{n}
\end{align*}
\]

** 19.6 ELBO in Alternate Form (2)
Show that the ELBO given by (19.11) can be written in the form

\[
\begin{align*}
\mathcal{L}_{n}(\mathbf{w}, \boldsymbol{\phi})= & \int q\left(\mathbf{z}_{n} \mid \mathbf{x}_{n}, \boldsymbol{\phi}\right) \ln p\left(\mathbf{z}_{n}\right) \mathrm{d} \mathbf{z}_{n} \\
& +\int q\left(\mathbf{z}_{n} \mid \mathbf{x}_{n}, \boldsymbol{\phi}\right) \ln \left\{\frac{p\left(\mathbf{x}_{n} \mid \mathbf{z}_{n}, \mathbf{w}\right)}{q\left(\mathbf{z}_{n} \mid \mathbf{x}_{n}, \boldsymbol{\phi}\right)}\right\} \mathrm{d} \mathbf{z}_{n}
\end{align*}
\]

* Diffusion Models
** 20.1 Mean and Covariance of \(\mathbf{z}_{t}\)
Using (20.3) write down expressions for the mean and covariance of \(\mathbf{z}_{t}\) in terms of the mean and covariance of \(\mathbf{z}_{t-1}\). Hence, show that for \(0<\beta_{t}<1\) the mean of the distribution of \(\mathbf{z}_{t}\) is closer to zero than the mean of \(\mathbf{z}_{t-1}\) and that the covariance of \(\mathbf{z}_{t}\) is closer to the unit matrix \(\mathbf{I}\) than the covariance of \(\mathbf{z}_{t-1}\).

** 20.2 Transformation (20.1) and (20.2)
Show that the transformation (20.1) can be written in the equivalent form (20.2).

** 20.3 Marginal Distribution of \(\mathbf{x}_{t}\) using Induction
In this exercise we use proof by induction to show that the marginal distribution of \(\mathbf{x}_{t}\) for the forward process of the diffusion model, as defined by (20.4), is given by (20.6) where \(\alpha_{t}\) is defined by (20.7). First verify that (20.6) holds when \(t=1\). Now assume that (20.6) is true for some particular value of \(t\) and derive the corresponding result for the value \(t+1\). To do this, it is easiest to write the forward process using the representation (20.3) and to make use of the result (3.212), which shows that the sum of two independent Gaussian random variables is itself a Gaussian in which the means and covariances are additive.

** 20.4 Limit of \(T \rightarrow \infty\)
By using the result (20.6), where \(\alpha_{t}\) is defined by (20.7), show that in the limit \(T \rightarrow \infty\) we obtain (20.9).

** 20.5 Covariance of Sum of Independent Random Variables
Consider two independent random variables \(\mathbf{a}\) and \(\mathbf{b}\) along with a fixed scalar \(\lambda\). Show that

\[
\begin{align*}
\operatorname{cov}[\mathbf{a}+\mathbf{b}] & =\operatorname{cov}[\mathbf{a}]+\operatorname{cov}[\mathbf{b}] \\
\operatorname{cov}[\lambda \mathbf{a}] & =\lambda^{2} \operatorname{cov}[\mathbf{a}]
\end{align*}
\]

Use these results to show that if the distribution of \(\mathbf{z}_{t-1}\) has zero mean and unit covariance, then the distribution of \(\mathbf{z}_{t}\), defined by (20.3), will also have zero mean and unit covariance, irrespective of the value of \(\beta_{t}\).

** 20.6 Completing the Square for Bayes' Theorem
In this exercise we will use the technique of completing the square to derive the result (20.15) starting from Bayes' theorem (20.13). First note that the two terms in the numerator on the right-hand side of (20.13), given by (20.4) and (20.6), both take the form of exponentials of quadratic functions of \(\mathbf{z}_{t-1}\). The required distribution is therefore a Gaussian, and so we need only to find its mean and covariance. To do this, consider only the terms in the exponentials that depend on \(\mathbf{z}_{t-1}\) and note that the product of two exponentials is the exponential of the sum of the two exponents. Gather together all the terms that are quadratic in \(\mathbf{z}_{t-1}\) as well as those that are linear in \(\mathbf{z}_{t-1}\) and then rearrange them in the form \(\left(\mathbf{z}_{t-1}-\mathbf{m}_{t}\right)^{\mathrm{T}} \mathbf{S}_{t}^{-1}\left(\mathbf{z}_{t-1}-\mathbf{m}_{t}\right)\). Then, by inspection, find expressions for \(\mathbf{m}_{t}\left(\mathbf{x}, \mathbf{z}_{t}\right)\) and \(\mathbf{S}_{t}\). Note that additive terms that are independent of \(\mathbf{z}_{t-1}\) can be ignored.

** 20.7 Gaussian Approximation for Reverse Process
In this exercise we show that the reverse of the conditional distribution \(q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}\right)\) for the forward noise process in a diffusion model can be approximated by a Gaussian when the noise variance is small. Consider the inverse conditional distribution \(q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}\right)\) given by Bayes' theorem in the form (20.11) where the forward distribution \(q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}\right)\) is given by (20.4). By taking the logarithm of both sides of (20.11) and then making a Taylor expansion of \(q\left(\mathbf{z}_{t-1}\right)\) centred on the value \(\mathbf{z}_{t}\), show that, for small values of the noise variance \(\beta_{t}\), the distribution \(q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}\right)\) is approximately a Gaussian with mean \(\mathbf{z}_{t}\) and covariance \(\beta_{t} \mathbf{I}\). Find expressions for the lowest-order corrections to the mean and to the covariance as expansions in powers of \(\beta_{t}\).

** 20.8 Log Likelihood Function for Diffusion Model
By substituting the product rule of probability in the form (20.24) into the definition (20.22) of the ELBO for the diffusion model and making use of the definition (20.23) of the Kullback-Leibler divergence, verify that the log likelihood function can be written as the sum of a lower bound and a Kullback-Leibler divergence in the form (20.21).

** 20.9 ELBO for Diffusion Model in Alternative Form
Verify that the ELBO for the diffusion model given by (20.31) can be written in the form (20.32) where the Kullback-Leibler divergence is defined by (20.23).

** 20.10 Additional Term in the ELBO
When we derived the ELBO for the diffusion model given by (20.32), we omitted the first and third terms in (20.26) because they are independent of \(\mathbf{w}\). Similarly we omitted the second term in the right-hand side of (20.30) because this is also independent of \(\mathbf{w}\). Show that if all of these omitted terms are retained they lead to an additional term in the ELBO \(\mathcal{L}(\mathbf{x})\) given by

\[
\mathrm{KL}\left(q\left(\mathbf{z}_{T} \mid \mathbf{x}\right) \| p\left(\mathbf{z}_{T}\right)\right)
\]

Note that the noise process is constructed in such a way that the distribution \(q\left(\mathbf{z}_{T} \mid \mathbf{x}\right)\) is equal to the Gaussian \(\mathcal{N}(\mathbf{x} \mid \mathbf{0}, \mathbf{I})\). Similarly, the distribution \(p\left(\mathbf{z}_{T}\right)\) is defined to be equal to \(\mathcal{N}(\mathbf{x} \mid \mathbf{0}, \mathbf{I})\), and hence the two distributions in (20.63) are equal and so the Kullback-Leibler divergence vanishes.

** 20.11 Consistency Terms in the ELBO
By making use of (20.15) for the distribution \(q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{x}\right)\) and (20.18) for the distribution \(p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right)\), show that the Kullback-Leibler divergence appearing in the consistency terms in (20.32) is given by (20.33).

** 20.12 Mean in Terms of Original Data Vector and Noise
By substituting (20.34) into (20.16) rewrite the mean \(\mathbf{m}_{t}\left(\mathbf{x}, \mathbf{z}_{t}\right)\) in terms of the original data vector \(\mathbf{x}\) and the noise \(\boldsymbol{\epsilon}\) in the form (20.35), where \(\alpha_{t}\) is defined by (20.7).

** 20.13 Reconstruction Term in ELBO
Show that the reconstruction term (20.38) in the ELBO for diffusion models can be written in the form (20.39). To do this, substitute for \(\boldsymbol{\mu}\left(\mathbf{z}_{1}, \mathbf{w}, 1\right)\) using (20.36) and substitute for \(\mathrm{x}\) using (20.1), and then make use of \(\alpha_{1}=\left(1-\beta_{1}\right)\), which follows from (20.7).

** 20.14 Matrix Elements of the Score Function
The score function is defined by \(\mathbf{s}(\mathbf{x})=\nabla_{\mathbf{x}} p(\mathbf{x} \mid \mathbf{w})\) and is therefore a vector of the same dimensionality as the input vector \(\mathrm{x}\). Consider a matrix whose elements are given by

\[
M_{i j}=\frac{\partial s_{i}}{\partial x_{j}}-\frac{\partial s_{j}}{\partial x_{i}}
\]

Show that if the score function is defined by taking the gradient \(\mathbf{s}=\nabla_{\mathbf{x}} \phi(\mathbf{x})\) of the output of a neural network with a single output variable \(\phi(\mathbf{x})\), then all the matrix elements \(M_{i j}=0\) for all pairs \(i, j\). Note that if the score function \(\mathbf{s}(\mathbf{x})=\nabla_{\mathbf{x}} p(\mathbf{x} \mid \mathbf{w})\) is instead represented directly by a deep neural network with the same number of outputs as inputs, then only the diagonal matrix elements \(M_{i i}=0\), and so the output of the network does not in general correspond to the gradient of any scalar function.

** 20.15 Computational Complexity of Evaluating the Score
Consider a deep neural network representation \(\mathrm{s}(\mathrm{x}, \mathbf{w})\) for the score function defined by (20.42), where \(\mathrm{x}\) and \(\mathrm{s}\) have dimensionality \(D\). Compare the computational complexity of evaluating the score for a network with \(D\) outputs that represents the score function directly with one that computes a single scalar function \(\phi(\mathbf{x}, \mathbf{w})\) in which the score function is computed indirectly through automatic differentiation. Show that the latter approach is typically more computationally expensive.

** 20.16 Integration by Parts for Score Matching
We cannot minimize the score function (20.43) directly because we do not know the functional form of the true data density \(p(\mathbf{x})\), and therefore we cannot write down an expression for the score function \(\nabla_{\mathbf{x}} \ln p(\mathbf{x})\). However, by using integration by parts (Hyvrinen, 2005), we can rewrite (20.43) in the form

\[
J(\mathbf{w})=\int\left\{\nabla \cdot \mathbf{s}(\mathbf{x}, \mathbf{w})+\frac{1}{2}\|\mathbf{s}(\mathbf{x}, \mathbf{w})\|^{2}\right\} p(\mathbf{x}) \mathrm{d} \mathbf{x}+\text { const }
\]

where the constant term is independent of the network parameters \(\mathbf{w}\), and the divergence \(\nabla \cdot \mathbf{s}(\mathbf{x}, \mathbf{w})\) is defined by

\[
\nabla \cdot \mathbf{s}=\sum_{i=1}^{D} \frac{\partial s_{i}}{\partial x_{i}}=\sum_{i=1}^{D} \frac{\partial^{2} \ln p(\mathbf{x})}{\partial x_{i}^{2}}
\]

in which \(D\) is the dimensionality of \(\mathbf{x}\). Derive the result (20.65) by first expanding the square in (20.43) and noting that the term involving \(\|\mathbf{s}(\mathbf{x}, \mathbf{w})\|^{2}\) already appears in (20.43) whereas the term involving \(\left\|\mathbf{s}_{\mathcal{D}}\right\|^{2}\) can be absorbed into the additive constant, where we have defined \(\mathbf{s}_{\mathcal{D}}=\nabla \ln p_{\mathcal{D}}(\mathbf{x})\). Now consider the formula

\[
\frac{\mathrm{d}}{\mathrm{d} x}\{p(x) g(x)\}=\frac{\mathrm{d} p(x)}{\mathrm{d} x} g(x)+p(x) \frac{\mathrm{d} g(x)}{\mathrm{d} x}
\]

for the derivative of the product of two functions. Integrate both sides of this formula with respect to \(x\) and rearrange to obtain the integration-by-parts formula:

\[
\int_{-\infty}^{\infty} \frac{\mathrm{d} p(x)}{\mathrm{d} x} g(x) \mathrm{d} x=-\int_{-\infty}^{\infty} \frac{\mathrm{d} g(x)}{\mathrm{d} x} p(x) \mathrm{d} x
\]

where we have assumed that \(p(\infty)=p(-\infty)=0\). Apply this result together with the definition \(\mathbf{s}_{\mathcal{D}}=\nabla \ln p(\mathbf{x})\) to the term involving \(\mathbf{s}(\mathbf{x}, \mathbf{w})^{\mathrm{T}} \mathbf{S}_{\mathcal{D}}\) to complete the proof. Note that the evaluation of the second derivatives in (20.66) requires a separate backward propagation pass for each derivative and hence has an overall computational cost that grows quadratically with the dimensionality \(D\) of the data space (Martens, Sutskever, and Swersky, 2012). This precludes the direct application of this loss function to spaces of high dimensionality, and so techniques such as sliced score matching (Song et al., 2019) have been developed to help address this inefficiency.

** 20.17 Equivalence of Score Function Loss
In this exercise we show that the score function loss (20.50) is equivalent, up to an additive constant, to the form (20.49). To do this, first expand the square in (20.49) and by using (20.47) show that the term in \(\mathbf{s}^{\mathrm{T}} \mathbf{s}\) from (20.49) is the same as the corresponding term obtained by expanding the square in (20.50). Next note that the term in \(\left\|\nabla_{\mathbf{z}} \ln q\right\|^{2}\) in (20.49) is independent of \(\mathbf{w}\) and likewise that the corresponding term in (20.50) is also independent of \(\mathbf{w}\), and so these can be viewed as additive constants in the loss function and play no role in training. Finally, consider the crossterm in (20.49). By substituting for \(q(\mathbf{z})\) using (20.47), show that this is equal to the corresponding cross-term from (20.50). Hence, show that the two loss functions are equal up to an additive constant.

** 20.18 Mixture of Two Disjoint Distributions
Consider a probability distribution that consists of a mixture of two disjoint distributions (i.e., distributions with the property that when one of them is non-zero the other must be zero) of the form

\[
p(\mathbf{x})=\lambda p_{A}(\mathbf{x})+(1-\lambda) p_{B}(\mathbf{x})
\]

Show that when the score function, defined by (20.42), is evaluated for any given point \(\mathbf{x}\), the mixing coefficient \(\lambda\) does not appear. From this it follows that Langevin dynamics defined by (14.61) will not sample from the two component distributions with the correct proportions. This problem is resolved by adding noise from a broad distribution, as discussed in the text.

** 20.19 Continuous-Time Limit of Forward Noise Process
For discrete steps, the forward noise process in a diffusion model is defined by (20.3). Here we take the continuous-time limit and convert this to an SDE. We first introduce a continuously-changing variance function \(\beta(t)\) such that \(\beta_{t}=\beta(t) \Delta t\). By making a Taylor expansion of the square root in the first term on the right-handside of (20.3), show that the infinitesimal update can be written in the form

\[
\mathrm{d} \mathbf{z}=-\frac{1}{2} \beta(t) \mathbf{z} \mathrm{d} t+\sqrt{\beta(t)} \mathrm{d} \mathbf{v}
\]

We see that this is a special case of the general SDE (20.55).

** 20.20 Score Function Transformation
By using (20.58) to replace \(\nabla_{\mathbf{x}} \ln p(\mathbf{c} \mid \mathbf{x})\), show that the score function in (20.59) can be written in the form (20.60).


