:PROPERTIES:
:ID:       fc08c0b0-19ee-4ee2-9e1e-b1066a5e6f1e
:END:
#+TITLE: Model selection
#+FILETAGS: :literature:prml:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

In [[id:be34bbdc-c85f-49cf-a457-48fe82abce89][Polynomial curve fitting]] using least squares, an optimal polynomial order yields the best generalization. The polynomial order controls model complexity by setting the number of free parameters. With regularized least squares, the regularization coefficient \(\lambda\) also regulates model complexity. More complex models, such as mixture distributions or neural networks, have multiple parameters influencing complexity. Determining these parameters aims to maximize predictive performance on new data. Besides tuning complexity parameters for a given model, exploring various model types is often necessary to find the best fit for a specific application.

[[id:adca5f2b-1056-4cb6-b5d4-02be3ccc6e54][Maximum likelihood]] performance on the training set is a poor predictor for unseen data due to over-fitting. If data is abundant, a portion can be used to train various models and their complexity parameters, evaluated on independent validation sets, with the best model selected based on predictive performance. Repeated model design on limited data can overfit validation data, necessitating a third test set for final performance evaluation.

In applications with limited data, maximizing training data use while avoiding noisy performance estimates from small validation sets is crucial. *Cross-validation* addresses this by partitioning data into \(S\) groups, using \(S-1\) for training and the remaining for validation, repeated \(S\) times, with performance scores averaged. This approach utilizes \((S-1)/S\) of data for training and all data for performance assessment. For scarce data, \(S=N\) (leave-one-out cross-validation) may be used.

#+ATTR_HTML: :width 400px
[[file:~/.local/images/prml-1-18.png]]

The technique of \(S\)-fold cross-validation, illustrated above for the case of \(S=4\), involves taking the available data and partitioning it into \(S\) groups (in the simplest case these are of equal size). Then \(S-1\) of the groups are used to train a set of models that are then evaluated on the remaining group. This procedure is then repeated for all \(S\) possible choices for the held-out group, indicated here by the red blocks, and the performance scores from the \(S\) runs are then averaged.

Cross-validation's drawback is the increased number of training runs by a factor of \(S\), which is computationally burdensome for expensive training models. Multiple complexity parameters further exacerbate this, potentially requiring exponential training runs. A better approach would rely solely on training data, allowing multiple hyperparameters and model types comparison in a single training run, avoiding over-fitting bias.

Various 'information criteria' like the Akaike information criterion (AIC) have been proposed to correct maximum likelihood bias by adding a penalty term for complex models. AIC selects the model maximizing

\[
\ln p\left(\mathcal{D} \mid \mathbf{w}_{\mathrm{ML}}\right) - M
\]

where \(p\left(\mathcal{D} \mid \mathbf{w}_{\text {ML }}\right)\) is the *best-fit log likelihood*, and \(M\) is the *number of adjustable parameters*. However, these criteria overlook model parameter uncertainty and often favor overly simple models.

