:PROPERTIES:
:ID:       9edc5f00-5227-4762-ac98-43f1893df310
:END:
#+TITLE: Multiple outputs (linear basis function models)
#+FILETAGS: :literature:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

The aim of *regression* is to predict the value of continuous target variables \( t \) given a \( D \)-dimensional input vector \( \mathbf{x} \) (see [[id:a3b84f9d-c03a-4579-9c82-47bba805c09b][Linear basis function models]] and [[id:35e57234-01f9-418c-a9fa-f94040015281][Linear models for regression]]). So far, we have considered the case of a single target variable \(t\) (see [[id:2967ed4f-6501-4d36-8446-50e536e66a2c][Maximum likelihood (linear basis function models)]]) In some applications, we may wish to predict \(K>1\) target variables, which we denote collectively by the target vector \(\mathbf{t}\). We use the same set of basis functions to model all of the components of the target vector so that

\begin{align*}
\mathbf{y}(\mathbf{x}, \mathbf{w})=\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}) \tag{1}
\end{align*}

where \(\mathbf{y}\) is a \(K\)-dimensional column vector, \(\mathbf{W}\) is an \(M \times K\) matrix of parameters, and \(\phi(\mathbf{x})\) is an \(M\)-dimensional column vector with elements \(\phi_{j}(\mathbf{x})\), with \(\phi_{0}(\mathbf{x})=1\) as before.

Suppose we take the conditional distribution of the target vector to be an isotropic Gaussian of the form

\begin{align*}
p(\mathbf{t} \mid \mathbf{x}, \mathbf{W}, \beta)=\mathcal{N}\left(\mathbf{t} \mid \mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}), \beta^{-1} \mathbf{I}\right) \tag{2}
\end{align*}

If we have a set of observations \(\mathbf{t}_{1}, \ldots, \mathbf{t}_{N}\), we can combine these into a matrix \(\mathbf{T}\) of size \(N \times K\) such that the \(n^{\text {th }}\) row is given by \(\mathbf{t}_{n}^{\mathrm{T}}\). Similarly, we can combine the input vectors \(\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\) into a matrix \(\mathbf{X}\). The [[id:82831310-4909-4f8b-8db0-ff0d8bef8b0b][Likelihood function]] is then given by

\begin{align*}
\ln p(\mathbf{T} \mid \mathbf{X}, \mathbf{W}, \beta) & =\sum_{n=1}^{N} \mathcal{N}\left(\mathbf{t}_{n} \mid \mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right), \beta^{-1} \mathbf{I}\right) \\
\end{align*}

where

\[
\mathcal{N}(x \mid \mu, \sigma^2)=\frac{1}{(2 \pi \sigma^2)^{1 / 2}} \exp \bigg(-\frac{(x-\mu)^2}{2 \sigma^2}\bigg).
\]

The log-likelihood function is then given by

\begin{align*}
\boxed{
\ln p(\mathbf{T} \mid \mathbf{X}, \mathbf{W}, \beta) =\frac{N K}{2} \ln \left(\frac{\beta}{2 \pi}\right)-\frac{\beta}{2} \sum_{n=1}^{N}\left\|\mathbf{t}_{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right\|^{2}  \tag{3}
}
\end{align*}

Demanding vanishing gradient \( \nabla_{\mathbf{W}} (\cdot) \) yield the maximum likelihood solution

\begin{align*}
\boxed{
\mathbf{W}_{\mathrm{ML}}=\left(\boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{\mathrm{T}} \mathbf{T} \tag{4}
}
\end{align*}

If we examine this result for each target variable \(t_{k}\), we have

\begin{align*}
\mathbf{w}_{k}=\left(\boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{\mathrm{T}} \mathbf{t}_{k}=\boldsymbol{\Phi}^{\dagger} \mathbf{t}_{k} \tag{5}
\end{align*}

where \(\mathbf{t}_{k}\) is an \(N\)-dimensional column vector with components \(t_{n k}\) for \(n=1, \ldots N\). Thus the solution to the regression problem decouples between the different target variables, and we need only compute a single pseudo-inverse matrix \(\boldsymbol{\Phi}^{\dagger}\), which is shared by all of the vectors \(\mathbf{w}_{k}\).

The extension to general Gaussian noise distributions having arbitrary covariance matrices is straightforward. Again, this leads to a decoupling into \(K\) independent regression problems.
