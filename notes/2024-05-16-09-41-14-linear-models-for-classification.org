:PROPERTIES:
:ID:       748061c5-f975-4773-90e4-3a5b35ceb8cd
:END:
#+TITLE: Linear models for classification
#+FILETAGS: :literature:prml:hub:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org

The goal in classification is to take an input vector \(\mathbf{x}\) and to assign it to one of \(K\) discrete classes \(\mathcal{C}_{k}\) where \(k=1, \ldots, K\). In the most common scenario, the classes are taken to be disjoint, so that each input is assigned to one and only one class. The input space is thereby divided into *decision regions* whose boundaries are called *decision boundaries* or *decision surfaces*. Models in which the decision surfaces are linear functions of the input vector \(\mathbf{x}\) are called /linear models for classification/. In linear models for classification, the decision boundaries are defined by \((D-1)\)-dimensional hyperplanes within the \(D\)-dimensional input space.

[[id:fe110473-ca15-4c42-ad93-f45a4ea7c4a2][The Laplace approximation]]
[[id:d4e1b57b-5547-45aa-b134-f32593415c23][Bayesian logistic regression]]

* Target variable representation
There are various ways of using target values to represent class labels:
1) For \( K = 2 \) classes, the most convenient representation is the binary representation in which there is a single target variable \(t \in\{0,1\}\) such that \(t=1\) represents class \(\mathcal{C}_{1}\) and \(t=0\) represents class \(\mathcal{C}_{2}\).
2) For \( K = 2 \) classes, the most convenient representation is the binary representation in which there is a single target variable \(t \in\{-1,1\}\) such that \(t=1\) represents class \(\mathcal{C}_{1}\) and \(t=-1\) represents class \(\mathcal{C}_{2}\). This is used in [[id:124bf23e-b47c-4acb-98f0-5b5fb9f8c85a][Perceptron algorithm]].
2) For \(K>2\) classes, it is convenient to use a 1-of-\(K\) coding scheme in which \(\mathbf{t}\) is a vector of length \(K\) such that if the class is \(\mathcal{C}_{j}\), then all elements \(t_{k}\) of \(\mathbf{t}\) are zero except element \(t_{j}\), which takes the value 1. For instance, if we have \(K=5\) classes, then a pattern from class 2 would be given the target vector \(\mathbf{t}=(0,1,0,0,0)^{\mathrm{T}}\).
* Approaches to classification problems
1) construct a [[id:43aa39fd-b16a-429b-b59a-408240ae3523][discriminant function]] that directly assigns each vector \(\mathrm{x}\) to a specific class,
2) construct a [[id:623b54a5-2318-4774-922b-b0e5e42959da][probabilistic generative models]] which models the class-conditional densities given by \(p\left(\mathbf{x} \mid \mathcal{C}_{k}\right)\), together with the prior probabilities \(p\left(\mathcal{C}_{k}\right)\) for the classes. The [[id:e166b400-40c8-410b-bb5b-0e0decca5f4c][posterior probabilities]] can then be calculated using [[id:731eff69-04dd-45f2-b2a7-968e9c44dc3e][Bayes's theorem]]

       \begin{align*}
       p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)=\frac{p\left(\mathbf{x} \mid \mathcal{C}_{k}\right)              p\left(\mathcal{C}_{k}\right)}{p(\mathbf{x})}.
      \end{align*}   
3) construct a [[id:7ba4082f-5e4b-416b-915c-d42ea674f52c][probabilistic discriminative models]] which model the [[id:391465bc-1399-40f0-b049-738c1a64d6fb][conditional probability distribution]] \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)\) in an [[id:01b2eedd-71b0-428f-bc4b-146b7068af36][inference]] stage by modeling the [[id:e166b400-40c8-410b-bb5b-0e0decca5f4c][posterior probabilities]] \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)\) directly, by representing them as parametric models and then optimizing the parameters using a training set; use this distribution to make optimal decisions.
* Fixed basis function models for classification
The various [[id:28d575a5-08f9-4f78-bb0b-b7fedcbb24d3][classification]] models need not necessarily work directly with the original input vector \(\mathbf{x}\). All of the algorithms are equally applicable if we first make a fixed nonlinear transformation of the inputs using a vector of basis functions \(\phi(\mathbf{x})\). *The resulting decision boundaries will be linear in the feature space* \(\phi\), *and these correspond to nonlinear decision boundaries in the original* \( \mathbf{x} \) *space*, as illustrated in the figure below

#+ATTR_HTML: :width 250px
[[file:~/.local/images/prml-4-12a.png]]
The plot above shows the original input space \(\left(x_1, x_2\right)\) together with data points from two classes labelled red and blue. Two 'Gaussian' basis functions \(\phi_1(\mathbf{x})\) and \(\phi_2(\mathbf{x})\) are defined in this space with centres shown by the green crosses and with contours shown by the green circles. 

#+ATTR_HTML: :width 250px
[[file:~/.local/images/prml-4-12b.png]]
The plot above shows the corresponding feature space \(\left(\phi_1, \phi_2\right)\) together with the linear decision boundary obtained given by a [[id:8d6d1914-142b-43aa-b9fd-4ea7404a40ed][Logistic regression model]]. This corresponds to a nonlinear [[id:61799020-5bb6-41c0-943b-7476700d1d15][decision boundary]] in the original input space, shown by the black curve in the previous plot.

*Classes that are* [[id:46d7eeb7-d01e-46a8-a4ea-6b27285b265f][linearly separable]] *in the feature space* \(\phi(\mathbf{x})\) *need not be linearly separable in the input space* \(\mathbf{x}\).

For many problems of practical interest, there is significant overlap between the class-conditional densities \(p\left(\mathbf{x} \mid \mathcal{C}_{k}\right)\). This corresponds to posterior probabilities \(p\left(\mathcal{C}_{k} \mid \mathbf{x}\right)\), which, for at least some values of \(\mathbf{x}\), are not 0 or 1. *Note that nonlinear transformations* \(\phi(\mathbf{x})\) *cannot remove such class overlap*. In such cases, the optimal solution is obtained by modeling the [[id:e166b400-40c8-410b-bb5b-0e0decca5f4c][posterior probability]] accurately and then applying standard [[id:869cf352-d95e-471d-a21a-c618a35c51dd][decision theory]]. In fact, nonlinear transformations can increase the level of overlap, or create overlap where none existed in the input space. However, suitable choices of non-linearity can make the process of modeling the posterior probabilities easier.