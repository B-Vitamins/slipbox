:PROPERTIES:
:ID:       6e7146da-2e33-40f4-9824-e2e014d54829
:END:
#+TITLE: Evolution of entropy
#+FILETAGS: :problem: :SPOP: :statmech:
#+SETUPFILE: ~/.config/emacs/setup/setupfile.org
#+BEGIN: clocktable :maxlevel 2 :scope nil :emphasize nil
#+CAPTION:
#+END
* 3.2 Evolution of entropy
:LOGBOOK:
CLOCK: [2024-04-20 Sat 18:39]--[2024-04-20 Sat 20:23] =>  1:44
:END:
*The normalized ensemble density is a probability in the phase space* \( \Gamma \). *This probability has an associated entropy* \( S(t) = - \int d\Gamma\,\rho(\Gamma,\,t) \ln \rho(\Gamma,\,t) \).

Next: [[id:7e249727-e935-4729-87a9-d575fc34052e][The Vlasov equation]]
 
** 3.2.1
*Show that if* \( \rho(\Gamma,\,t) \) *satisfies Liouville's equation for a Hamiltonian* \(H\), \(\mathrm{D}_t S = 0 \).

By definition

\[S(t) \equiv -\langle\ln \rho(\Gamma,\,t)\rangle=-\int d\Gamma\,\rho(\Gamma,\,t) \ln \rho(\Gamma,\,t).\]

Therefore \( \mathrm{D}_t S=-\int d\Gamma\,\big[(\partial_{t} \, \rho) \ln \rho + \partial_t \rho \big]=-\int d\Gamma\,(\partial_{t} \, \rho) (\ln \rho + 1)\). Substituting [[id:a2da6b4b-5ecc-4ad5-9268-33aeab1643f6][Liouville's equation]] \(\partial_t \rho = - \{\rho, H \}\), we arrive at

\[
\mathrm{D}_t S = \int d\Gamma\,\sum_{i=1}^{3N} \big[ (\partial_q \rho) \, (\partial_p H) - (\partial_p \rho) \,
(\partial_q H) \big] (\ln \rho + 1).
\]

We will integrating by parts, i.e., \( \int_a^b F\,dG = FG \vert_a^b - \int_a^b G\,dF \).

\begin{align*}
\mathrm{D}_t S &= \int d \Gamma\, \sum_{i=1}^{3N} \bigg[\rho \partial_{p_i} \big[\partial_{q_i} H (\ln \rho + 1)\big] -  \rho \partial_{q_i} \big[ \partial_{p_i} H (\ln \rho + 1)\big] \bigg] \\
&= \int d \Gamma\, \sum_{i=1}^{3N} \bigg[\rho (\partial_{p_i q_i} H)\,(\ln \rho + 1) + (\partial_{q_i} H)\,(\partial_{p_i} \rho) - \rho (\partial_{q_i p_i} H)\,(\ln \rho + 1) - (\partial_{p_i} H)\,( \partial_{q_i} \rho) \bigg] \\
&= \int d \Gamma\, \sum_{i=1}^{3N} \bigg[(\partial_{q_i} H)\,(\partial_{p_i} \rho) - (\partial_{p_i} H)\,( \partial_{q_i} \rho) \bigg].
\end{align*}

Integrating by parts again

\begin{align*}
\mathrm{D}_t S &= \int d\Gamma\, \sum_{i=1}^{3N} \bigg[(\partial_{q_i} H)\,(\partial_{p_i} \rho) - (\partial_{p_i} H)\,( \partial_{q_i} \rho) \bigg] \\
&= \int d\Gamma\, \sum_{i=1}^{3N} \bigg[\rho\,(\partial_{p_iq_i} H) - \rho\,(\partial_{q_i p_i} H) \bigg] = 0.
\end{align*}

Thus \( \mathrm{D}_t S = 0 \).

** 3.2.2
*Using the method of Lagrange multipliers, find the function* \( \rho_{max}(\Gamma) \) *that maximizes the functional* \( S[\rho] \), *subject to the constraint of fixed average energy,* \( \langle H \rangle = \int d\Gamma\,\rho\,H  = E\).

With [[id:425bdc45-56d7-483e-8767-95232a6542f9][Lagrange multipliers]] \(\alpha\), and \(\beta\)

\begin{align*}
S[\rho] & =-\int d\Gamma\,\rho(\Gamma) \ln \rho(\Gamma) - \alpha \bigg(\int d\Gamma\, \rho(\Gamma)-1 \bigg) - \beta \bigg(\int d\Gamma\,\rho(\Gamma)H - E \bigg).
\end{align*}

so that

\[\partial_{\rho} S (\rho_{\text{max}}) = -\ln \rho_{\text{max}}(\Gamma)- \alpha -\beta H-1=0.\]

Solving for \(\rho_{\text{max}} (\Gamma)\) we get

\[\rho_{\text{max}} (\Gamma) = \overbrace{\exp (-\alpha-1)}^{C} \exp (-\beta H) =C \exp (-\beta H).\]

** 3.2.3
*Show that the solution to part (b) is stationary, that is,* \(\mathrm{D}_t\,\rho_{\text{max}} = 0 \).

\begin{align*}
\partial_t\,\rho_{\text{max}} &= - \big\{\rho_{\text{max}},\,H\big\} = - \{C \exp (-\beta H), H\} \\
&= C \beta (\partial_q H) (\partial_p H) \exp (-\beta H) - C \beta (\partial_p H) (\partial_q H) \exp (-\beta H) \\
&= 0.
\end{align*}

** 3.2.4
*How can one reconcile the result in (a) with the observed increase in entropy as the system approaches the equilibrium density in (b)? (Hint: Think of the situation encountered in the previous problem.)*

In a strict sense, one cannot reconcile the results in (3.2.1) and (3.2.2). However, if ones view of the system is a coarse-grained, then while the \( \ln_2 M \) term in

\begin{equation*}
I = \ln_2 (M) + \sum_i p_i \, \ln p_i.
\end{equation*}

is unaffected, the \( \sum_i p_i \, \ln p_i \) term decreases as a result of such a procedure. Coarse-graning reduces information at the scale \(\epsilon_{\text{cg}}\) by turning all probability distributions over outcomes contained in \( \epsilon_{\text{cg}} \) into a uniform distribution. In such a scenario the results can be reconciled because the information loss via coarse-graining results in an increase in the distribution's entropy, since \( \mathrm{D}_t I = - \mathrm{D}_t S\). Note that there is no many-to-one reduction in outcomes, which would decrease entropy. The support is unaffected, it is the probability density function that is systematically redefined so that it is more and more disperse, having greater entropy.


