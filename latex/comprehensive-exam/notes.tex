\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{centernot}
\usepackage{amsmath}
\usepackage{multimedia}
\usepackage{wrapfig}
\usepackage{biblatex}
\addbibresource{refs.bib}

\title{Inherent structures of the deep Boltzmann machine}
\date{\today}
\author{Ayan Das}

\begin{document}
\maketitle
\section*{One}
\begin{enumerate}
    \item Welcome everyone. Today we begin by our discussion with Deep Boltzmann Machines, also known as DBMs, which are crucial statistical models in the field of deep learning.
    \item DBMs have been widely applied across various pattern recognition tasks, proving their fundamental importance in modern AI technologies.
    \item Despite their widespread use, there are still unanswered questions about how DBMs' architectures relate to the probability distributions they are capable of representing.
    \item How does a DBM's architecture relate to the probability distributions it can represent?
    \item How to determine the optimal architecture for a given application?
    \item Why does one architecture work better than another for a specific application?
    \item One key question involves determining the most optimal architecture for a specific application, which remains a challenge due to the complexity of these models.
    \item This leads us to the process of designing and tuning DBMs, which is largely trial and error, highlighting the need for more theoretical grounding in their development.
\end{enumerate}

\section*{Two}
\begin{enumerate}
    \item Moving forward, let's explore why both the depth and the width of a network are critical through several influential theorems.
    \item Starting with a theorem by Le Roux and Bengio, it's established that a DBM with just \(k+1\) hidden units can approximate any distribution over binary vectors of length \(n\).
    \item Next, Montufar's theorem tells us that a DBM with a single visible layer and multiple hidden layers can universally approximate any probability distribution, provided the number of layers is sufficient.
    \item Another theorem by Montufar tells us that a single hidden layer DBM can effectively model any distribution of visible binary variables with a precise number of hidden units.
\end{enumerate}

\section*{Three}
\begin{enumerate}
    \item As we've discussed the theoretical capabilities of DBMs, let's question the necessity of depth and width in these networks.
    \item Firstly, if a single-layer DBM is already a universal approximator, why should we consider adding more layers?
    \item Conversely, if a deep, lean DBM can model any distribution, what is the benefit of increasing the width of layers?
    \item These theoretical results, while insightful, often overlook practical concerns such as optimality and real-world applicability.
    \item The parameters required by these models are often exponential in terms of input variables, making the theoretical bounds sometimes overly generous.
    \item In practice, DBMs can achieve significant performance even with modest architectures, challenging these theoretical limits.
    \item Therefore, we need imperative results that relate realistic architectures to model capacity and provide guidelines for constructing optimal DBMs.
    \item Our goal here today will be to generate results of the second type, using techniques from statistical physics and in the specific context of DBMs.
\end{enumerate}

\section*{Four}
\begin{enumerate}
    \item To understand the physical underpinnings of DBMs, let's start with some fundamental definitions related to Markov random fields.
    \item Firstly, an undirected graph is defined as a tuple consisting of a set of nodes and a set of edges that connect these nodes.
    \item Next, we introduce the Local Markov property. It states that the state of any node in the graph is conditionally independent of all other nodes given its immediate neighbors.
    \item This implies that for any node, its condition given all other nodes is the same as its condition given just its neighbors, highlighting a lack of conditional dependencies with non-neighboring nodes.
    \item Finally, a Markov random field, or MRF, is defined as a collection of random variables that adhere to the Local Markov property across a joint probability distribution.
\end{enumerate}

\section*{Five}
\begin{enumerate}
    \item Now, let's talk about the Hammersley-Clifford theorem. The theorem establishes that the joint distribution of any Markov random field can be represented by the Gibbs-Boltzmann distribution, associated with an appropriately chosen energy function.
    \item First, we need the definition of a clique. A clique is a subset of nodes where every pair of nodes is directly connected.
    \item Now, according to the Hammersley-Clifford theorem, if a probability distribution on a graph satisfies the local Markov properties and is strictly positive, it can be uniquely factorized over the cliques of the graph.
    \item This factorization allows us to express the joint distribution as a product of potential functions over these cliques, which can also be represented in terms of an energy function.
    \item This theorem, also called the fundamental theorem of field, is the basis of all energy-based models.
\end{enumerate}

\section*{Six}
\begin{enumerate}
    \item Deep Boltzmann Machines (DBMs) are energy-based models, closely related to the Ising model with random interactions, an essential concept in statistical physics.
    \item DBMs can be seen as a specialized subset of general Boltzmann machines, which are akin to the Sherrington-Kirkpatrick model, a seminal model in the study of spin glasses.
    \item Sigma, or \(\sigma\), represents the state of the entire DBM, with the state space being a Cartesian product of individual unit states across all layers.
    \item The total configuration space of the DBM is multiplicative in the configuration space of each layer.
    \item The Hamiltonian is represented by a triple sum where the outermost sum spans all layers from the visible to the top hidden layer.
    \item Each unit in a layer interacts pairwise with units in the subsequent layer.
    \item Under the Gibbs-Boltzmann distribution, probabilities are assigned to each state \(\sigma\), defined by the exponential of the negative Hamiltonian, normalized by the partition function \(Z\).
\end{enumerate}

\section*{Seven}
\begin{enumerate}
    \item In Deep Boltzmann Machines, randomness in couplings often introduces a phenomenon known as frustration in microscopic interactions.
    \item Frustration occurs when a spin and its neighbors cannot simultaneously achieve their minimum energy configurations, leading to conflicts within the system.
    \item This frustration can result in metastability, where the energy function harbors a large number of local minima, known as inherent structures.
    \item Inherent structures, as defined by Stillinger and Weber, are configurations where the energy function is at a local minimum.
    \item Each inherent structure is associated with a basin of attraction, which includes all configurations that converge to this structure through gradient descent.
    \item The concept of inherent structures and their basins of attraction helps us understand the landscape of possible states in a DBM and the dynamics of reaching these states.
\end{enumerate}

\section*{Eight}
\begin{enumerate}
    \item Building on our understanding of inherent structures and basins of attraction, let's discuss how these concepts help decompose the configuration space of DBMs into distinct basins.
    \item Each basin correlates to an inherent structure, which acts as a local attractor for the dynamics within that basin.
    \item Bansal, Anand, and Bhattacharya have applied this decomposition approach to analyze one and two hidden layer DBMs.
    \item They introduced a new metric known as the Inherent Structure Capacity (ISC) to measure and design optimal DBM architectures, especially when constrained by a parameter budget.
    \item Using the ISC metric, they demonstrated that an optimal DBM architecture can achieve significant savings in parameters, up to an order of magnitude compared to traditional designs.
    \item Furthermore, their research proves that in specific scenarios, DBMs with two hidden layers exhibit superior model capacity compared to those with just one hidden layer.
\end{enumerate}

\section*{Nine}
\begin{enumerate}
    \item Let's dive into some critical definitions that help us measure the stability and capacity of DBM architectures.
    \item Starting with the concept of a one-flip stable state, this refers to a configuration, \( s^* \), where all configurations within one Hamming distance have a higher energy, making \( s^* \) a local minimum.
    \item This means that if you were to flip any single unit in \( s^* \), the resulting configuration would always have greater energy, confirming the stability of \( s^* \).
    \item Now, moving on to the Inherent Structure Capacity (ISC), which is defined for an \( L \)-layered DBM with \( m_1, \ldots, m_L \) hidden units and \( n \) visible units.
    \item ISC is calculated as the logarithm normalized by \( n \), the number of visible units, of the expected number of modes across all possible distributions generated over the visible units by the DBM.
    \item In simpler terms, ISC measures the diversity of configurations that a DBM can robustly represent, providing a quantitative metric to evaluate its complexity and capability.
\end{enumerate}

\section*{Ten}
\begin{enumerate}
    \item Let's discuss key findings on the Inherent Structure Capacity (ISC) for RBMs and DBMs under different configurations and parameter budgets.
    \item Starting with RBMs, it has been found that as the number of units in a single hidden layer increases indefinitely, the ISC converges to a limit of 0.585. This shows the finite limit of complexity that can be modeled by an RBM with increasing hidden unit count.
    \item For DBMs with two layers, where the first layer is wide and the second is narrow, the ISC has an upper bound. This bound is determined by the ratio of hidden units in the first layer to the visible units, and the ISC cannot exceed \(1 + \alpha_2 \log_2(1.5)\).
    \item This result underscores the impact of layer width and the relationship between layers on the model's capacity to represent different structures.
    \item Lastly, when considering a parameter budget constraint, the maximum ISC achievable depends on the ratio \(c\), which is the budget relative to the number of visible units squared.
    \item If \(c \geq 1\), the ISC can reach up to \( \min(1, \sqrt{c \log(1.29)}) \). For \(c < 1\), the maximum ISC is determined by \(c \log_2[1 - (1/2) \operatorname{erf}(-\sqrt{\pi c})]\).
    \item These findings highlight how the distribution of units across layers and the total number of parameters available can significantly impact the ISC, thereby influencing the overall complexity and capacity of the DBM.
\end{enumerate}

\section*{Eleven}
\begin{enumerate}
    \item The analysis conducted by Bansal et al. on the Inherent Structure Capacity of DBMs provides valuable insights but also has notable limitations.
    \item Firstly, the analysis is limited to DBMs with only one or two hidden layers. This restricts the applicability of their results to more complex architectures that may involve additional layers.
    \item Secondly, for DBMs with two hidden layers, the analysis only considers configurations where the first layer is wide and the second layer is narrow. This specific scenario limits generalization to other possible layer configurations.
    \item Additionally, the current methodology does not provide a way to calculate the numerical value of the ISC for arbitrary architectures. This means that the utility of the ISC metric is confined to optimal configurations predefined by their criteria.
    \item Our work aims to overcome these limitations by adopting a different approach, utilizing techniques from various recent studies. This will allow for a more flexible and comprehensive analysis of DBM architectures across a wider range of configurations.
\end{enumerate}

\section*{Twelve}
\begin{enumerate}
    \item As we saw earlier, the energy function \( H_J(\sigma) \) of a DBM with \( L \) hidden layers is defined as a summation over interactions between units across successive layers, from the visible layer indexed as 0 up to layer \( L \).
    \item Each layer's unit count is aggregated into a vector \( N \), which defines the \emph{architecture} of the DBM. This vector includes every layer in the DBM, providing a comprehensive view of its structure.
    \item Assuming the coupling coefficients \( J \) between units are sampled from a Gaussian distribution, this introduces randomness with a mean of 0 and a variance defined by the geometric mean of the unit counts of adjacent layers.
    \item Our objective is to analyze the inherent structures within the configuration space \( S \) of DBMs. To do this, we consider an ensemble of DBMs that share an arbitrary but fixed architecture, \( N \).
    \item We aim to derive an expression for the ISC of this ensemble, defined as \( C_J(N) \), which is the normalized logarithm of the expected number of inherent structures. The normalization is with the number of visible units.
\end{enumerate}

\section*{Thirteen}
\begin{enumerate}
    \item Let us quickly go over simple definitions that will be helpful.
    \item The total spins number, denoted as \( N \), is the sum of all units across every layer in the DBM, giving us a complete count of the network's size.
    \item We define the proportion of units in any layer \( l \) relative to the visible layer (where \( l = 0 \)) as \( \alpha_l = N_l / N_0 \). This ratio helps us understand the scaling of each layer in relation to the input layer.
    \item Next, we define two inter-layer ratios. \( \gamma_l^2 \) is defined as the ratio of the number of units in layer \( l+1 \) to layer \( l \), while \( \nu_l^2 \) is the ratio of units in layer \( l-1 \) to layer \( l \).
    \item These definitions yield some trivial identities which we use the deriving our main result.
\end{enumerate}

\section*{Fourteen}
\begin{enumerate}
    \item This slide visually represents a DBM with three layers and showcases the geometric parameters that describe its structure.
    \item We have a DBM where the total number of spins, \( N \), is 10, arranged across three layers, visually differentiated by two colors representing the state values of \(\sigma_i = \pm 1\).
    \item We have a visible layer with two units, then three hidden layers with three, three, and two units respectively.
    \item On the left, we have the proportion \( \alpha_l \) for each layer relative to the visible layer is noted, with \(\alpha_0 = 1.0\), \(\alpha_1 = 1.5\), and \(\alpha_2 = 1.0\), indicating relative scaling.
    \item On the right, we have the inter-layer ratios such as \( \gamma \) and \( \nu \) describe the relationship between the layers: \( \gamma_0 = \sqrt{3/2} \) and its reciprocal \( \nu_1 = \sqrt{2/3} \) illustrate the connectivity and potential interaction strengths between adjacent layers.
\end{enumerate}

\section*{Fifteen}
\begin{enumerate}
    \item Now we introduce the concept of single-site energy.
    \item Single-site energy for a spin \( i \) in layer \( l \), denoted as \( \lambda_{il} \), is calculated by summing the interaction energies of the spin with all adjacent units in both the layer above and below, adjusted by coupling coefficients \( J \).
    \item This energy effectively represents the potential change in the system's total energy if the spin state \( \sigma_{i,l} \) were to be flipped.
    \item The mathematical expression, you see a sum running over the units in the layer above and the layer below. The here Kronecker delta are used to handle boundary conditions for spins in the top-most and bottom-most layers.
    \item An inherent structure in the context of a DBM is then defined as any configuration where all single-site energies are strictly positive, indicating that every spin is in a local energy minimum relative to its immediate environment.
    \item This definition implies that these configurations are stable against single-spin flips.
\end{enumerate}

\section*{Sixteen}
\begin{enumerate}
    \item We now present the main theorem for calculating the Inherent Structure Capacity (ISC) for a DBM with an arbitrary number of hidden layers.
    \item Consider an ensemble of DBMs with a fixed architecture \(N\) and coupling coefficients drawn from a Gaussian distribution as described before. The ISC for this ensemble, denoted as \(C_J(N)\), is given by the formula show.
    \item Here \(x_l\) and \(y_l\) are auxiliary variables associated with each layer from the visible layer to the last hidden layer. For \(L\) hidden layers there are \(2L\) such variables, and we must obtain the saddle of this expression with respect to those \(2L\) variables. Then we can substitute it in the formula to get a numerical value for the ISC.
    \item This formula improves on previous ISC calculations:
    \item 1. Applying to DBMs with any number of hidden layers, not just limited to one or two.
    \item 2. Accommodating DBMs with arbitrary proportions of units across layers, not just configurations where the first layer is significantly wider than the second.
    \item 3. Providing numerical values for ISC across any architecture, allowing for a more flexible and comprehensive analysis beyond just optimal configurations.
\end{enumerate}

\section*{Seventeen}
\begin{enumerate}
    \item We begin the derivation of inherent structures in a DBM with the area formula.
    \item First thing to notice is that it is a multi-dimensional integral over the space of single-site energies.
    \item In fact, it is an integral over a restricted subspace of the positive value of the single-site energies.
    \item Going inwards, we integrate over all possible spin configurations, where each configuration's contribution is determined by the product of Dirac delta functions.
    \item To compute this integral, we use the integral representation of the delta function, introducing auxiliary variables \( k_{i,l} \) for each spin.
    \item Then after we find the expectation of the number of inherent structures with respect to the Gaussian distribution, we are left with the final equation.
\end{enumerate}

\section*{Eighteen}
\begin{enumerate}
    \item To simplify the complex integrals involved in calculating the Inherent Structure Capacity (ISC) of DBMs, we employ a Hubbard-Stratonovich like integral transformation.
    \item This transformation is applied to the cross-term involving the coupling coefficients \(J_{ij}\), which are integrated over using auxiliary variables \(x\) and \(y\) introduced for each layer.
    \item After substitution and simplification we are left with the final equation.
    \item Notice that each \(k_{il}\) integrals have now all become Gaussian.
\end{enumerate}

\section*{Nineteen}
\begin{enumerate}
    \item The final step in our calculation of the Inherent Structure Capacity involves executing Gaussian integrals. The previous transformations have simplified the integrals over the auxiliary variables \( k_{il} \) into Gaussian forms, which are computationally straightforward to evaluate.
    \item We just plug the Gaussian integral result for each \(k_{il}\) and obtain this final result.
    \item Now notice that each \(\lambda_{il}\) integrals has become Gaussian.
    \item The integration over \( \lambda_{il} \) variables is now straightforward because the integrand is entirely
\end{enumerate}

\section*{Twenty}
\begin{enumerate}
    \item The integral however is over the interval \([0,~\infty)\).
    \item We use the displayed result to perform the \(\lambda_{il}\) integrals after which the formula simplifies significantly.
    \item Notice that the argument of each exponential now has a \(N_{0}\) sitting out front, meaning it is candidate for saddle-point integration, where the maximum of the integrand dominates the sum so we approximate the integral with the maximum of the integrand. The maximum must be found with respect to the auxiliary variables \(x_{il}\) and \(y_{il}\).
\end{enumerate}

\section*{Twenty-one}
\begin{enumerate}
    \item Note that for steepest descent approximation to be applicable, we have to assume \(N_{0} \to \infty\). So we have. After approximating the integral with the maxima of the exponential, we take the logarithm on both sides and divide by \(N_{0}\). Then we can identify the ISC and thus write the final result, which appeared in the theorem we say earlier in the main result slide.
    \item The ISC is defined as the saddle point of a complex expression involving error functions and the geometric parameters of like the proportions and the inter-layer ratios that we defined earlier.
\end{enumerate}

\section*{Twenty-two}
\begin{enumerate}
    \item To find this saddle point, we derive equations by setting the derivatives of \( C_J(N) \) with respect to the auxiliary variables \( x_l \) and \( y_l \) to zero, leading to a system of non-linear equations.
    \item Solving this system yields the values of \( x_l \) and \( y_l \) that maximize \( C_J(N) \), thereby determining the ISC in the limit as \( N_0 \) goes to infinity.
\end{enumerate}

\section*{Twenty-three}
\begin{enumerate}
    \item We saw earlier that as the number of hidden units increases indefinitely, the ISC for RBMs approaches a saturation point. This behavior demonstrates that adding more hidden units beyond a certain threshold does not significantly enhance the model’s capacity to encode diverse data patterns.
    \item For an RBM, the ISC formula we have derived reduces to the form shown. We must solve for a saddle point involving two variables. We numerically solve for the saddle points \(\{x_{\text{max}}, y_{\text{max}}\}\) to determine the maximum value of \(C_J(\alpha_1)\), giving us the ISC as a function of \( \alpha_1 \)
\end{enumerate}

\section*{Twenty-four}
\begin{enumerate}
    \item This slide presents the graphical representation of the Inherent Structure Capacity (ISC) for a Restricted Boltzmann Machine (RBM) with a single hidden layer as a function of the proportion of hidden units to visible units, \( \alpha_1 \).
    \item As the proportion \( \alpha_1 \) increases, the ISC approaches a saturation limit. This graph illustrates that as \( \alpha_1 \) exceeds 1, the increase in ISC slows and eventually plateaus, indicating diminishing returns on model capacity with further increases in hidden units.
    \item The saturation value of the ISC is observed to be around 0.506, which is somewhat lower than the anticipated 0.585.
    \item Further investigation is required to reconcile the difference between the observed saturation point and the expected value from prior studies.
\end{enumerate}

\section*{Twenty-five}
\begin{enumerate}
    \item The \emph{second} key result from \cite{bansal2018using} concerns ISC for DBMs. It said that for small values of \(\alpha_{2}\), \(\mathcal{C}_{J} (\alpha_{1}, \alpha_{2})\) increases linearly with \(\alpha_{2}\).
    \item For an 2 hidden layer DBM, the ISC formula we have derived reduces to the form shown. We must solve for a saddle point involving four variables.
\end{enumerate}

\section*{Twenty-six}
\begin{enumerate}
    \item We saw earlier that as the number of hidden units in the second layer is slowly increase after having added a large number of units to the first hidden layer, the ISC, which had plateaud  for RBMs approaches a saturation point. This behavior demonstrates that adding more hidden units beyond a certain threshold does not significantly enhance the model’s capacity to encode diverse data patterns.
    \item It is shown that for small values of \(\alpha_2\), the ISC linearly increases with \(\alpha_2\). Notice the increase in the value of ISC, starts from saturating value of about 0.506 of the RBM.
\end{enumerate}

\section*{Twenty-seven}
\begin{enumerate}
    \item The \emph{third} key result from \cite{bansal2018using} was concerning network design under a budget. It stated that with a budget of \(c N_{0}^{2}\) parameters, multi-layering is \emph{not} recommended when \(c < 1\) whereas for \(c \geq 1\) multi-layering is recommended.
    \item In other words, the curve \(\alpha_{1} (1 + \alpha_{2}) = c\) separates \emph{realizable} architectures from \emph{non-realizable} ones in the \(\alpha_{1} - \alpha_{2}\) plane.
    \item On plotting a heat-map of \(\mathcal{C}_{J} (\alpha_{1}, \alpha_{2})\) over this plane and superimposing the curve \(\alpha_{1} (1 + \alpha_{2}) = c\) associated with a budget, we recover both these conclusions.
\end{enumerate}

\section*{Twenty-eight}
\begin{enumerate}
    \item This slide examines the impact of tight budget constraints (c < 1) on the inherent structure capacity (ISC) of Deep Boltzmann Machines (DBMs).
    \item The heatmap shown illustrates the ISC values across different configurations of α₁ and α₂, where α₁ and α₂ represent the proportions of units in the first and second hidden layers, respectively.
    \item The curve represented in orange shows the critical boundary for c = 0.5. Networks to the left of this curve are realizable under the given budget, while those to the right are not.
    \item Under tight budget constraints (example shown for c = 0.5), the optimal neatwork design does not benefit from multi-layering. This is visible from the heatmap where higher ISC values are concentrated around α₂ = 0, meaning adding a second layer does not improve capacity.
\end{enumerate}

\section*{Twenty-nine}
\begin{enumerate}
    \item This slide focuses on scenarios where the budget constraint c is greater than or equal to 1, indicating more flexibility in network design for Deep Boltzmann Machines (DBMs).
    \item The heatmap illustrates ISC values across different proportions of units in the first (α₁) and second (α₂) hidden layers for c = 10. The green-to-yellow gradient shows increasing ISC values with specific combinations of α₁ and α₂.
    \item The dotted line represents the curve α₁(1 + α₂) = 10, defining the boundary for feasible network architectures under this budget. Networks below and to the left of this line are viable, while those beyond are not due to exceeding budget constraints.
    \item Under flexible budget conditions, the inherent structure capacity (ISC) is optimized not just by increasing the size of the first hidden layer but also by investing in the second layer, as depicted in the heatmap.
\end{enumerate}

\section*{Thirty}
\begin{enumerate}
    \item The optimum points for maximizing ISC occur where α₁ and α₂ are both non-zero, which supports the thesis that strategically adding layers can be beneficial.
\end{enumerate}

\section*{Thirty-one}
\begin{enumerate}
    \item This slide examines the impact of significantly increasing the proportion of units in the second hidden layer, α₂, beyond the small values previously explored.
    \item The diagrams illustrate how the DBM architecture evolves as α₂ is increased while α₁ remains fixed. Initially, the network starts with a predominant first hidden layer. As α₂ increases, the network approaches a more balanced structure.
    \item However, as α₂ continues to grow, exceeding α₁, it leads to an imbalance where the second hidden layer becomes more dominant. This shift is depicted in the sequence from left to right.
    \item The mathematical model we've derived allows us to explore these scenarios without constraints on the upper limit of α₂, providing a more comprehensive view of ISC behavior across a broader range of network configurations.
\end{enumerate}

\section*{Thirty-two}
\begin{enumerate}
    \item This slide focuses on the optimization of inherent structure capacity (ISC) in Deep Boltzmann Machines with two hidden layers by exploring different budget scheme for the first (α₁) and second (α₂) layer proportions, namely we just ask for \(\alpha_{1} + \alpha_{2}\) to be less than some constant \(c\).
    \item The plot illustrates ISC as a function of α₂ for various fixed values of α₁, all under a total budget where α₁ + α₂ = 10. Each curve represents a different value of α₁, from 5.0 to 10.0, indicating how ISC changes as α₂ is increased while keeping the total layer proportion constant.
    \item You can observe that for smaller values of α₂, ISC increases linearly. As α₂ increases beyond a certain point, however, the rate of of growth of ISC increase diminishes. At least some of this is con-commitant with the network becoming more balanced. Thereafter, the ISC starts to increase steeply, with the network having a wide second hidden layer and a narrow first hidden layer, the exact opposite of the situation we have thus far encoutered.
    \item The circles on each curve mark the total ISC for a given portioning of \(c\) among \(\alpha_{1}\) and \(\alpha_{2}\). We see that prematurely adding a second hidden layer can be sub-optimal. Instead, fully utilizing the capacity of the first hidden layer before expanding to a second layer results in a higher overall ISC.
\end{enumerate}

\section*{Thirty-three}
\begin{enumerate}
    \item We cannot redefine \(\mathcal{N}_{s}\) in \(\mathcal{C} (\boldsymbol{N}) \equiv N_{0}^{-1} \ln \langle \mathcal{N}_{s} \rangle_{J}\) and proceed through the same set of steps.
    \item To see this, suppose \(\lambda_{il} > 0\) for all \(i\) and \(l\). The following holds \emph{unconditionally}: \(\sigma_{il} \to -\sigma_{il} \implies \Delta H_{J} > 0 \quad \forall i,~ j.\).
    \item Contrast this with the case where \(\lambda_{il} > 0\) for all \(i\) and \(l = 0\), but the values for \(l \neq 0\) are indeterminate. In this case we cannot say with certainty whether \(\Delta H_J > 0\).
\end{enumerate}

\section*{Thirty-four}
\begin{enumerate}
    \item We did not present the details of this investigation and the associated challenges due to the time constraint.
\end{enumerate}

\section*{Thirty-five}
\begin{enumerate}
    \item Investigating how the architecture of the DBM affects the previously discussed one-to-many relationship between the modes of \(p(\boldsymbol{\sigma}_{0})\) and \(p(\boldsymbol{\sigma})\).
    \item Improving techniques for sampling the inherent structures of the DBM to make numerical estimation of \(\mathrm{ISC}\) feasible.
    \item Extending the domain of applicability of the \(\mathrm{ISC}\) beyond Gaussian distributed couplings.
    \item Designing DBMs for real-world applications using \(\mathrm{ISC}\) and demonstrating a non-trivial savings in the number of parameters.
\end{enumerate}

\end{document}
