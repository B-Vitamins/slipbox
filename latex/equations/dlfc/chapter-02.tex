\documentclass{article}

% Document Layout and Fonts
\usepackage[margin=0.9in]{geometry}    % Page margins
\usepackage{fontspec}                  % For custom fonts (LuaLaTeX feature)
\usepackage{tgpagella}
\usepackage{mathpazo}
\setmainfont{EB Garamond}              % Main font (EB Garamond)
\usepackage{microtype}                 % Improves text appearance
\usepackage{titlesec}                  % Customize section title fonts

% Right-align section headings
\titleformat{\section}
  {\normalfont\large\scshape\raggedright}  % Right-align and small caps
  {}{0em}{}[]

% Right-align subsection headings and add a line below
\titleformat{\subsection}
  {\normalfont\normalsize\raggedleft}     % Right-align subsections
  {}{0em}{\titlerule[0.5pt]}              % Horizontal line below

% Right-align and italicize subsubsections
\titleformat{\subsubsection}
  {\normalfont\normalsize\itshape\raggedleft} % Right-align and italicize subsubsections
  {}{0em}{}[]

% Math and Science Packages
\usepackage{amsmath, amsfonts, amssymb, mathtools, amsthm, dsfont}

% Math commands and operators
\newcommand{\minus}{\scalebox{0.8}{\(-\)}}
\newcommand{\plus}{\scalebox{0.6}{\(+\)}}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{Tr}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}    % Differential d

% Definitions, theorems, corollaries, and friends
\usepackage[english]{babel}
\usepackage[hidelinks]{hyperref}
\newtheorem{axiom}{Axiom}
\newtheorem{postulate}{Postulate}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem*{remark}{Remark}

\title{\LARGE\scshape\MakeUppercase{Chapter 2: Probabilities}}
\author{\textit{Bishop and Bishop}}
\date{}

\begin{document}

\maketitle

\section{The Rules of Probability}

\subsection{A medical screening example}
No equations

\subsection{The sum and product rules}

\begin{align*}
p\left(X=x_{i}, Y=y_{j}\right)=\frac{n_{i j}}{N} 
\tag{2.1}
\end{align*}

\begin{align*}
p\left(X=x_{i}\right)=\frac{c_{i}}{N}
\tag{2.2}
\end{align*}

\begin{align*}
\sum_{i=1}^{L} p\left(X=x_{i}\right)=1
\tag{2.3}
\end{align*}

\begin{align*}
p\left(X=x_{i}\right)=\sum_{j=1}^{M} p\left(X=x_{i}, Y=y_{j}\right)
\tag{2.4}
\end{align*}

\begin{align*}
p\left(Y=y_{j} \mid X=x_{i}\right)=\frac{n_{i j}}{c_{i}}
\tag{2.5}
\end{align*}

\begin{align*}
\sum_{j=1}^{M} p\left(Y=y_{j} \mid X=x_{i}\right)=1
\tag{2.6}
\end{align*}

\begin{align*}
p\left(X=x_{i}, Y=y_{j}\right) & =p\left(Y=y_{j} \mid X=x_{i}\right) p\left(X=x_{i}\right)
\tag{2.7}
\end{align*}

\begin{align*}
\text{sum rule:} \quad & p(X)=\sum_{Y} p(X, Y)
\tag{2.8}
\end{align*}

\begin{align*}
\text{product rule:} \quad & p(X, Y)=p(Y \mid X) p(X)
\tag{2.9}
\end{align*}

\subsection{Bayes' theorem}

\begin{align*}
p(Y \mid X)=\frac{p(X \mid Y) p(Y)}{p(X)}
\tag{2.10}
\end{align*}

\begin{align*}
p(X)=\sum_{Y} p(X \mid Y) p(Y)
\tag{2.11}
\end{align*}

\subsection{Medical screening revisited}

\begin{align*}
p(C=1)=\frac{1}{100}
\tag{2.12}
\end{align*}

\begin{align*}
p(C=0)=\frac{99}{100}
\tag{2.13}
\end{align*}

\begin{align*}
p(T=1 \mid C=1) = \frac{90}{100}
\tag{2.14}
\end{align*}

\begin{align*}
p(T=0 \mid C=1) = \frac{10}{100}
\tag{2.15}
\end{align*}

\begin{align*}
p(T=1 \mid C=0) = \frac{3}{100}
\tag{2.16}
\end{align*}

\begin{align*}
p(T=0 \mid C=0) = \frac{97}{100}
\tag{2.17}
\end{align*}

\begin{align*}
p(T=1 \mid C=1)+p(T=0 \mid C=1)=1
\tag{2.18}
\end{align*}

\begin{align*}
p(T=1 \mid C=0)+p(T=0 \mid C=0)=1
\tag{2.19}
\end{align*}

\begin{align*}
p(T=1) & =p(T=1 \mid C=0) p(C=0)+p(T=1 \mid C=1) p(C=1) \\
& =\frac{3}{100} \times \frac{99}{100} + \frac{90}{100} \times \frac{1}{100}=\frac{387}{10,000}=0.0387
\tag{2.20}
\end{align*}

\begin{align*}
p(C=1 \mid T=1) & =\frac{p(T=1 \mid C=1) p(C=1)}{p(T=1)} \\
& =\frac{90}{100} \times \frac{1}{100} \times \frac{10,000}{387}=\frac{90}{387} \simeq 0.23
\tag{2.21}
\end{align*}

\begin{align*}
p(C=0 \mid T=1)=1-\frac{90}{387}=\frac{297}{387} \simeq 0.77
\tag{2.22}
\end{align*}

\subsection{Prior and posterior probabilities}
No equations

\subsection{Independent variables}
No equations

\section{Probability Densities}

\begin{align*}
p(x \in(a, b))=\int_{a}^{b} p(x) \mathrm{d} x
\tag{2.23}
\end{align*}

\begin{align*}
p(x) \geqslant 0
\tag{2.24}
\end{align*}

\begin{align*}
\int_{-\infty}^{\infty} p(x) \mathrm{d} x = 1
\tag{2.25}
\end{align*}

\begin{align*}
P(z) = \int_{-\infty}^{z} p(x) \mathrm{d} x
\tag{2.26}
\end{align*}

\begin{align*}
p(\mathbf{x}) \geqslant 0
\tag{2.27}
\end{align*}

\begin{align*}
\int p(\mathbf{x}) \mathrm{d} \mathbf{x} = 1
\tag{2.28}
\end{align*}

\begin{align*}
\text{sum rule:} & p(\mathbf{x}) = \int p(\mathbf{x}, \mathbf{y}) \mathrm{d} \mathbf{y}
\tag{2.29}
\end{align*}

\begin{align*}
\text{product rule:} & p(\mathbf{x}, \mathbf{y}) = p(\mathbf{y} \mid \mathbf{x}) p(\mathbf{x})
\tag{2.30}
\end{align*}

\begin{align*}
p(\mathbf{y} \mid \mathbf{x}) = \frac{p(\mathbf{x} \mid \mathbf{y}) p(\mathbf{y})}{p(\mathbf{x})}
\tag{2.31}
\end{align*}

\begin{align*}
p(\mathbf{x}) = \int p(\mathbf{x} \mid \mathbf{y}) p(\mathbf{y}) \mathrm{d} \mathbf{y}
\tag{2.32}
\end{align*}

\subsection{Example distributions}

\begin{align*}
p(x) = \frac{1}{d - c}, \quad x \in (c, d)
\tag{2.33}
\end{align*}

\begin{align*}
p(x \mid \lambda) = \lambda \exp(-\lambda x), \quad x \geqslant 0
\tag{2.34}
\end{align*}

\begin{align*}
p(x \mid \mu, \gamma) = \frac{1}{2 \gamma} \exp \left( -\frac{|x - \mu|}{\gamma} \right)
\tag{2.35}
\end{align*}

\begin{align*}
p(x \mid \mu) = \delta(x - \mu)
\tag{2.36}
\end{align*}

\begin{align*}
p(x \mid \mathcal{D}) = \frac{1}{N} \sum_{n=1}^{N} \delta\left( x - x_n \right)
\tag{2.37}
\end{align*}

\subsection{Expectations and covariances}

\begin{align*}
\mathbb{E}[f] = \sum_{x} p(x) f(x)
\tag{2.38}
\end{align*}

\begin{align*}
\mathbb{E}[f] = \int p(x) f(x) \mathrm{d} x
\tag{2.39}
\end{align*}

\begin{align*}
\mathbb{E}[f] \simeq \frac{1}{N} \sum_{n=1}^{N} f(x_n)
\tag{2.40}
\end{align*}

\begin{align*}
\mathbb{E}_{x}[f(x, y)]
\tag{2.41}
\end{align*}

\begin{align*}
\mathbb{E}_{x}[f \mid y] = \sum_{x} p(x \mid y) f(x)
\tag{2.42}
\end{align*}

\begin{align*}
\mathbb{E}_{x}[f \mid y] = \int p(x \mid y) f(x) \mathrm{d} x
\tag{2.43}
\end{align*}

\begin{align*}
\operatorname{var}[f] = \mathbb{E}\left[ (f(x) - \mathbb{E}[f(x)])^2 \right]
\tag{2.44}
\end{align*}

\begin{align*}
\operatorname{var}[f] = \mathbb{E}\left[ f(x)^2 \right] - \mathbb{E}[f(x)]^2
\tag{2.45}
\end{align*}

\begin{align*}
\operatorname{var}[x] = \mathbb{E}\left[ x^2 \right] - \mathbb{E}[x]^2
\tag{2.46}
\end{align*}

\begin{align*}
\operatorname{cov}[x, y] = \mathbb{E}_{x, y}\left[ \{x - \mathbb{E}[x]\} \{y - \mathbb{E}[y]\} \right] 
= \mathbb{E}_{x, y}[x y] - \mathbb{E}[x] \mathbb{E}[y]
\tag{2.47}
\end{align*}

\begin{align*}
\operatorname{cov}[\mathbf{x}, \mathbf{y}] = \mathbb{E}_{\mathbf{x}, \mathbf{y}}\left[ \{\mathbf{x} - \mathbb{E}[\mathbf{x}]\} \{\mathbf{y}^{\top} - \mathbb{E}[\mathbf{y}^{\top}]\} \right] 
= \mathbb{E}_{\mathbf{x}, \mathbf{y}}\left[ \mathbf{x} \mathbf{y}^{\top} \right] - \mathbb{E}[\mathbf{x}] \mathbb{E}[\mathbf{y}^{\top}]
\tag{2.48}
\end{align*}

\section{The Gaussian Distribution}

\begin{align*}
\mathcal{N}\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}
\tag{2.49}
\end{align*}

\begin{align*}
\mathcal{N}\left(x \mid \mu, \sigma^{2}\right)>0
\tag{2.50}
\end{align*}

\begin{align*}
\int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) \mathrm{d} x=1
\tag{2.51}
\end{align*}

\subsection{Mean and variance}

\begin{align*}
\mathbb{E}[x] = \int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) x \mathrm{d} x = \mu
\tag{2.52}
\end{align*}

\begin{align*}
\mathbb{E}\left[ x^{2} \right] = \int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) x^{2} \mathrm{d} x = \mu^{2} + \sigma^{2}
\tag{2.53}
\end{align*}

\begin{align*}
\operatorname{var}[x] = \mathbb{E}\left[ x^{2} \right] - \mathbb{E}[x]^{2} = \sigma^{2}
\tag{2.54}
\end{align*}

\subsection{Likelihood function}

\begin{align*}
p\left(\mathbf{x} \mid \mu, \sigma^{2}\right) = \prod_{n=1}^{N} \mathcal{N}\left(x_{n} \mid \mu, \sigma^{2}\right)
\tag{2.55}
\end{align*}

\begin{align*}
\ln p\left(\mathbf{x} \mid \mu, \sigma^{2}\right) = -\frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}\left( x_n - \mu \right)^2 - \frac{N}{2} \ln \sigma^{2} - \frac{N}{2} \ln (2 \pi)
\tag{2.56}
\end{align*}

\begin{align*}
\mu_{\mathrm{ML}} = \frac{1}{N} \sum_{n=1}^{N} x_n
\tag{2.57}
\end{align*}

\begin{align*}
\sigma_{\mathrm{ML}}^{2} = \frac{1}{N} \sum_{n=1}^{N} \left( x_n - \mu_{\mathrm{ML}} \right)^2
\tag{2.58}
\end{align*}

\subsection{Bias of maximum likelihood}

\begin{align*}
\mathbb{E}\left[ \mu_{\mathrm{ML}} \right] = \mu
\tag{2.59}
\end{align*}

\begin{align*}
\mathbb{E}\left[ \sigma_{\mathrm{ML}}^{2} \right] = \left( \frac{N - 1}{N} \right) \sigma^{2}
\tag{2.60}
\end{align*}

\begin{align*}
\widehat{\sigma}^{2} = \frac{1}{N} \sum_{n=1}^{N} \left( x_n - \mu \right)^2
\tag{2.61}
\end{align*}

\begin{align*}
\mathbb{E}\left[ \widehat{\sigma}^{2} \right] = \sigma^{2}
\tag{2.62}
\end{align*}

\begin{align*}
\widetilde{\sigma}^{2} = \frac{N}{N - 1} \sigma_{\mathrm{ML}}^{2} = \frac{1}{N - 1} \sum_{n=1}^{N} \left( x_n - \mu_{\mathrm{ML}} \right)^2
\tag{2.63}
\end{align*}

\subsection{Linear regression}

\begin{align*}
p\left(t \mid x, \mathbf{w}, \sigma^{2}\right) = \mathcal{N}\left( t \mid y(x, \mathbf{w}), \sigma^{2} \right)
\tag{2.64}
\end{align*}

\begin{align*}
p\left( \mathbf{t} \mid \mathbf{x}, \mathbf{w}, \sigma^{2} \right) = \prod_{n=1}^{N} \mathcal{N}\left( t_n \mid y(x_n, \mathbf{w}), \sigma^{2} \right)
\tag{2.65}
\end{align*}

\begin{align*}
\ln p\left( \mathbf{t} \mid \mathbf{x}, \mathbf{w}, \sigma^{2} \right) = -\frac{1}{2 \sigma^{2}} \sum_{n=1}^{N} \left\{ y(x_n, \mathbf{w}) - t_n \right\}^{2} - \frac{N}{2} \ln \sigma^{2} - \frac{N}{2} \ln (2 \pi)
\tag{2.66}
\end{align*}

\begin{align*}
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \left\{ y(x_n, \mathbf{w}) - t_n \right\}^{2}
\tag{2.67}
\end{align*}

\begin{align*}
\sigma_{\mathrm{ML}}^{2} = \frac{1}{N} \sum_{n=1}^{N} \left\{ y(x_n, \mathbf{w}_{\mathrm{ML}}) - t_n \right\}^{2}
\tag{2.68}
\end{align*}

\begin{align*}
p\left( t \mid x, \mathbf{w}_{\mathrm{ML}}, \sigma_{\mathrm{ML}}^{2} \right) = \mathcal{N}\left( t \mid y(x, \mathbf{w}_{\mathrm{ML}}), \sigma_{\mathrm{ML}}^{2} \right)
\tag{2.69}
\end{align*}

\section{Transformation of Densities}

\begin{align*}
\widetilde{f}(y) = f(g(y)) 
\tag{2.70}
\end{align*}

\begin{align*}
p_{y}(y) & = p_{x}(x)\left|\frac{\mathrm{d} x}{\mathrm{~d} y}\right| \\
& = p_{x}(g(y))\left|\frac{\mathrm{d} g}{\mathrm{~d} y}\right|
\tag{2.71}
\end{align*}

\begin{align*}
\tilde{f}^{\prime}(\widehat{y}) = f^{\prime}(g(\widehat{y})) g^{\prime}(\widehat{y}) = 0
\tag{2.72}
\end{align*}

\begin{align*}
p_{y}^{\prime}(y) = s p_{x}^{\prime}(g(y))\left\{g^{\prime}(y)\right\}^{2} + s p_{x}(g(y)) g^{\prime \prime}(y)
\tag{2.73}
\end{align*}

\begin{align*}
x = g(y) = \ln (y) - \ln (1 - y) + 5
\tag{2.74}
\end{align*}

\begin{align*}
y = g^{-1}(x) = \frac{1}{1 + \exp (-x + 5)}
\tag{2.75}
\end{align*}

\subsection{Multivariate distributions}

\begin{align*}
p_{\mathbf{y}}(\mathbf{y}) = p_{\mathbf{x}}(\mathbf{x}) \left| \operatorname{det} \mathbf{J} \right|
\tag{2.76}
\end{align*}

\begin{align*}
\mathbf{J} = \left[
\begin{array}{ccc}
\frac{\partial g_{1}}{\partial y_{1}} & \cdots & \frac{\partial g_{1}}{\partial y_{D}} \\
\vdots & \ddots & \vdots \\
\frac{\partial g_{D}}{\partial y_{1}} & \cdots & \frac{\partial g_{D}}{\partial y_{D}}
\end{array}
\right]
\tag{2.77}
\end{align*}

\begin{align*}
y_{1} = x_{1} + \tanh \left(5 x_{1}\right)
\tag{2.78}
\end{align*}

\begin{align*}
y_{2} = x_{2} + \tanh \left(5 x_{2}\right) + \frac{x_{1}^{3}}{3}
\tag{2.79}
\end{align*}


\section{Information Theory}

\subsection{Entropy}

\begin{align*}
h(x) = -\log_2 p(x) 
\tag{2.80}
\end{align*}

\begin{align*}
\mathrm{H}[x] = -\sum_{x} p(x) \log_2 p(x) 
\tag{2.81}
\end{align*}

\begin{align*}
\mathrm{H}[x] = -\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{8} \log_2 \frac{1}{8} - \frac{1}{16} \log_2 \frac{1}{16} - \frac{4}{64} \log_2 \frac{1}{64} = 2 \text{ bits.}
\end{align*}

\subsection{Physics perspective}

\begin{align*}
W = \frac{N!}{\prod_{i} n_{i}!} 
\tag{2.82}
\end{align*}

\begin{align*}
\mathrm{H} = \frac{1}{N} \ln W = \frac{1}{N} \ln N! - \frac{1}{N} \sum_{i} \ln n_{i}! 
\tag{2.83}
\end{align*}

\begin{align*}
\ln N! \s = N \ln N - N 
\tag{2.84}
\end{align*}

\begin{align*}
\mathrm{H} = - \sum_{i} p_{i} \ln p_{i} 
\tag{2.85}
\end{align*}

\begin{align*}
\mathrm{H}[p] = -\sum_{i} p\left(x_{i}\right) \ln p\left(x_{i}\right) 
\tag{2.86}
\end{align*}

\begin{align*}
\widetilde{\mathrm{H}} = -\sum_{i} p\left(x_{i}\right) \ln p\left(x_{i}\right) + \lambda\left(\sum_{i} p\left(x_{i}\right) - 1\right) 
\tag{2.87}
\end{align*}

\subsection{Differential entropy}

\begin{align*}
\int_{i \Delta}^{(i+1) \Delta} p(x) \mathrm{d} x = p\left(x_{i}\right) \Delta 
\tag{2.89}
\end{align*}

\begin{align*}
\mathrm{H}_{\Delta} = -\sum_{i} p\left(x_{i}\right) \Delta \ln \left(p\left(x_{i}\right) \Delta\right) = -\sum_{i} p\left(x_{i}\right) \Delta \ln p\left(x_{i}\right) - \ln \Delta 
\tag{2.90}
\end{align*}

\begin{align*}
\lim_{\Delta \rightarrow 0}\left\{-\sum_{i} p\left(x_{i}\right) \Delta \ln p\left(x_{i}\right)\right\} = -\int p(x) \ln p(x) \mathrm{d} x 
\tag{2.91}
\end{align*}

\begin{align*}
\mathrm{H}[\mathbf{x}] = -\int p(\mathbf{x}) \ln p(\mathbf{x}) \mathrm{d} \mathbf{x} 
\tag{2.92}
\end{align*}

\subsection{Maximum entropy}

\begin{align*}
\int_{-\infty}^{\infty} p(x) \mathrm{d} x = 1 
\tag{2.93}
\end{align*}

\begin{align*}
\int_{-\infty}^{\infty} x p(x) \mathrm{d} x = \mu 
\tag{2.94}
\end{align*}

\begin{align*}
\int_{-\infty}^{\infty}(x-\mu)^{2} p(x) \mathrm{d} x = \sigma^{2}
\tag{2.95}
\end{align*}

\begin{align*}
p(x) = \exp \left\{-1 + \lambda_{1} + \lambda_{2} x + \lambda_{3}(x - \mu)^{2}\right\} 
\tag{2.97}
\end{align*}

\begin{align*}
p(x) = \frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{(x - \mu)^{2}}{2 \sigma^{2}}\right\} 
\tag{2.98}
\end{align*}

\begin{align*}
\mathrm{H}[x] = \frac{1}{2}\left\{1 + \ln \left(2 \pi \sigma^{2}\right)\right\} 
\tag{2.99}
\end{align*}

\subsection{Kullback-Leibler divergence}

\begin{align*}
\mathrm{KL}(p \| q) & =-\int p(\mathbf{x}) \ln q(\mathbf{x}) \mathrm{d} \mathbf{x} -\left(-\int p(\mathbf{x}) \ln p(\mathbf{x}) \mathrm{d} \mathbf{x}\right) \\
& =-\int p(\mathbf{x}) \ln \left\{\frac{q(\mathbf{x})}{p(\mathbf{x})}\right\} \mathrm{d} \mathbf{x}
\tag{2.100}
\end{align*}

\begin{align*}
f(\lambda a + (1 - \lambda) b) \leqslant \lambda f(a) + (1 - \lambda) f(b) 
\tag{2.101}
\end{align*}

\begin{align*}
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leqslant \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) 
\tag{2.102}
\end{align*}

\begin{align*}
f(\mathbb{E}[x]) \leqslant \mathbb{E}[f(x)]
\tag{2.103}
\end{align*}

\begin{align*}
f\left(\int \mathbf{x} p(\mathbf{x}) \mathrm{d} \mathbf{x}\right) \leqslant \int f(\mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x}
\tag{2.104}
\end{align*}

\begin{align*}
\mathrm{KL}(p \| q) = -\int p(\mathbf{x}) \ln \left\{\frac{q(\mathbf{x})}{p(\mathbf{x})}\right\} \mathrm{d} \mathbf{x} \geqslant -\ln \int q(\mathbf{x}) \mathrm{d} \mathbf{x} = 0
\tag{2.105}
\end{align*}

\begin{align*}
\mathrm{KL}(p \| q) \simeq \frac{1}{N} \sum_{n=1}^{N}\left\{-\ln q\left(\mathbf{x}_{n} \mid \boldsymbol{\theta}\right)+\ln p\left(\mathbf{x}_{n}\right)\right\}
\tag{2.106}
\end{align*}

\subsection{Conditional entropy}

\begin{align*}
\mathrm{H}[\mathbf{y} \mid \mathbf{x}] = -\iint p(\mathbf{y}, \mathbf{x}) \ln p(\mathbf{y} \mid \mathbf{x}) \mathrm{d} \mathbf{y} \mathrm{d} \mathbf{x} 
\tag{2.107}
\end{align*}

\begin{align*}
\mathrm{H}[\mathbf{x}, \mathbf{y}] = \mathrm{H}[\mathbf{y} \mid \mathbf{x}] + \mathrm{H}[\mathbf{x}]
\tag{2.108}
\end{align*}

\subsection{Mutual information}

\begin{align*}
\mathrm{I}[\mathbf{x}, \mathbf{y}] & \equiv \mathrm{KL}(p(\mathbf{x}, \mathbf{y}) \| p(\mathbf{x}) p(\mathbf{y})) \\
& = -\iint p(\mathbf{x}, \mathbf{y}) \ln \left(\frac{p(\mathbf{x}) p(\mathbf{y})}{p(\mathbf{x}, \mathbf{y})}\right) \mathrm{d} \mathbf{x} \mathrm{d} \mathbf{y}
\tag{2.109}
\end{align*}

\begin{align*}
\mathrm{I}[\mathbf{x}, \mathbf{y}] = \mathrm{H}[\mathbf{x}] - \mathrm{H}[\mathbf{x} \mid \mathbf{y}] = \mathrm{H}[\mathbf{y}] - \mathrm{H}[\mathbf{y} \mid \mathbf{x}]
\tag{2.110}
\end{align*}

\section{Bayesian Probabilities}

\subsection{Model parameters}

\begin{align*}
p(\mathbf{w} \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \mathbf{w}) p(\mathbf{w})}{p(\mathcal{D})}
\tag{2.111}
\end{align*}

\begin{align*}
\text{ posterior } \propto \text{ likelihood } \times \text{ prior }
\tag{2.112}
\end{align*}

\begin{align*}
p(\mathcal{D}) = \int p(\mathcal{D} \mid \mathbf{w}) p(\mathbf{w}) \mathrm{d} \mathbf{w}
\tag{2.113}
\end{align*}

\subsection{Regularization}

\begin{align*}
-\ln p(\mathbf{w} \mid \mathcal{D}) = -\ln p(\mathcal{D} \mid \mathbf{w}) - \ln p(\mathbf{w}) + \ln p(\mathcal{D})
\tag{2.114}
\end{align*}

\begin{align*}
p(\mathbf{w} \mid s) = \prod_{i=0}^{M} \mathcal{N}\left(w_{i} \mid 0, s^{2}\right) = \prod_{i=0}^{M} \left(\frac{1}{2 \pi s^{2}}\right)^{1 / 2} \exp \left\{-\frac{w_{i}^{2}}{2 s^{2}}\right\}
\tag{2.115}
\end{align*}

\begin{align*}
-\ln p(\mathbf{w} \mid \mathcal{D}) = -\ln p(\mathcal{D} \mid \mathbf{w}) + \frac{1}{2 s^{2}} \sum_{i=0}^{M} w_{i}^{2} + \text{const.}
\tag{2.116}
\end{align*}

\begin{align*}
E(\mathbf{w}) = \frac{1}{2 \sigma^{2}} \sum_{n=1}^{N} \left\{ y\left(x_{n}, \mathbf{w}\right) - t_{n} \right\}^{2} + \frac{1}{2 s^{2}} \mathbf{w}^{\top} \mathbf{w}
\tag{2.117}
\end{align*}

\subsection{Bayesian machine learning}

\begin{align*}
p(t \mid x, \mathcal{D}) = \int p(t \mid x, \mathbf{w}) p(\mathbf{w} \mid \mathcal{D}) \mathrm{d} \mathbf{w}
\tag{2.118}
\end{align*}

\end{document}
