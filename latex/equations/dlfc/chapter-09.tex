\documentclass{article}

% Document Layout and Fonts
\usepackage[margin=0.9in]{geometry}    % Page margins
\usepackage{fontspec}                  % For custom fonts (LuaLaTeX feature)
\usepackage{tgpagella}
\usepackage{mathpazo}
\setmainfont{EB Garamond}              % Main font (EB Garamond)
\usepackage{microtype}                 % Improves text appearance
\usepackage{titlesec}                  % Customize section title fonts

% Right-align section headings
\titleformat{\section}
  {\normalfont\large\scshape\raggedright}  % Right-align and small caps
  {}{0em}{}[]

% Right-align subsection headings and add a line below
\titleformat{\subsection}
  {\normalfont\normalsize\raggedleft}     % Right-align subsections
  {}{0em}{\titlerule[0.5pt]}              % Horizontal line below

% Right-align and italicize subsubsections
\titleformat{\subsubsection}
  {\normalfont\normalsize\itshape\raggedleft} % Right-align and italicize subsubsections
  {}{0em}{}[]

% Math and Science Packages
\usepackage{amsmath, amsfonts, amssymb, mathtools, amsthm, dsfont}

% Math commands and operators
\newcommand{\minus}{\scalebox{0.8}{\(-\)}}
\newcommand{\plus}{\scalebox{0.6}{\(+\)}}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{Tr}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}    % Differential d

% Definitions, theorems, corollaries, and friends
\usepackage[english]{babel}
\usepackage[hidelinks]{hyperref}
\newtheorem{axiom}{Axiom}
\newtheorem{postulate}{Postulate}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem*{remark}{Remark}

\title{\LARGE\scshape\MakeUppercase{Chapter 9: Regularization}}
\author{\textit{Bishop and Bishop}}
\date{}

\begin{document}

\maketitle

\section{Inductive Bias}

\subsection{Inverse problems}
No equations

\subsection{No free lunch theorem}
No equations

\subsection{Symmetry and invariance}
No equations

\subsection{Equivariance}
\begin{align*}
\mathcal{S}(\mathcal{T}(\mathbf{I})) = \mathcal{T}(\mathcal{S}(\mathbf{I})) 
\tag{9.2}
\end{align*}

\begin{align*}
\mathcal{S}(\mathcal{T}(\mathbf{I})) = \widetilde{\mathcal{T}}(\mathcal{S}(\mathbf{I})) 
\tag{9.3}
\end{align*}

\begin{align*}
\mathcal{C}(\mathcal{T}(\mathbf{I})) = \mathcal{C}(\mathbf{I}) 
\tag{9.4}
\end{align*}

\section{Weight Decay}

\begin{align*}
\nabla \widetilde{E}(\mathbf{w})=\nabla E(\mathbf{w})+\lambda \mathbf{w} 
\tag{9.5}
\end{align*}

\subsection{Consistent regularizers}

\begin{align*}
z_{j}=h\left(\sum_{i} w_{j i} x_{i}+w_{j 0}\right) 
\tag{9.6}
\end{align*}

\begin{align*}
y_{k}=\sum_{j} w_{k j} z_{j}+w_{k 0} 
\tag{9.7}
\end{align*}

\begin{align*}
x_{i} \rightarrow \widetilde{x}_{i}=a x_{i}+b 
\tag{9.8}
\end{align*}

\begin{align*}
w_{j i} \rightarrow \widetilde{w}_{j i}=\frac{1}{a} w_{j i}  
\tag{9.9}
\end{align*}

\begin{align*}
w_{j 0} \rightarrow \widetilde{w}_{j 0}=w_{j 0}-\frac{b}{a} \sum_{i} w_{j i}
\tag{9.10}
\end{align*}

\begin{align*}
y_{k} \rightarrow \widetilde{y}_{k}=c y_{k}+d 
\tag{9.11}
\end{align*}

\begin{align*}
w_{k j} \rightarrow \widetilde{w}_{k j}=c w_{k j}  
\tag{9.12}
\end{align*}

\begin{align*}
w_{k 0} \rightarrow \widetilde{w}_{k 0}=c w_{k 0}+d
\tag{9.13}
\end{align*}

\begin{align*}
\frac{\lambda_{1}}{2} \sum_{w \in \mathcal{W}_{1}} w^{2}+\frac{\lambda_{2}}{2} \sum_{w \in \mathcal{W}_{2}} w^{2} 
\tag{9.14}
\end{align*}

\begin{align*}
p\left(\mathbf{w} \mid \alpha_{1}, \alpha_{2}\right) \propto \exp \left(-\frac{\alpha_{1}}{2} \sum_{w \in \mathcal{W}_{1}} w^{2}-\frac{\alpha_{2}}{2} \sum_{w \in \mathcal{W}_{2}} w^{2}\right)
\tag{9.15}
\end{align*}

\begin{align*}
\Omega(\mathbf{w})=\frac{1}{2} \sum_{k} \alpha_{k}\|\mathbf{w}\|_{k}^{2}
\tag{9.16}
\end{align*}

\begin{align*}
\|\mathbf{w}\|_{k}^{2}=\sum_{j \in \mathcal{W}_{k}} w_{j}^{2}
\tag{9.17}
\end{align*}

\subsection{Generalized weight decay}

\begin{align*}
\Omega(\mathbf{w})=\frac{\lambda}{2} \sum_{j=1}^{M}\left|w_{j}\right|^{q}
\tag{9.18}
\end{align*}

\begin{align*}
E(\mathbf{w})+\frac{\lambda}{2} \sum_{j=1}^{M}\left|w_{j}\right|^{q}
\tag{9.19}
\end{align*}

\begin{align*}
\sum_{j=1}^{M}\left|w_{j}\right|^{q} \leqslant \eta
\tag{9.20}
\end{align*}

\section{Learning Curves}

\subsection{Early stopping}
No equations

\subsection{Double descent}
No equations

\section{Parameter Sharing}

\subsection{Soft weight sharing}

\begin{align*}
p(\mathbf{w})=\prod_{i}\left\{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)\right\} 
\tag{9.21}
\end{align*}

\begin{align*}
\Omega(\mathbf{w})=-\sum_{i} \ln \left(\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)\right) 
\tag{9.22}
\end{align*}

\begin{align*}
\widetilde{E}(\mathbf{w})=E(\mathbf{w})+\lambda \Omega(\mathbf{w}) 
\tag{9.23}
\end{align*}

\begin{align*}
\gamma_{j}\left(w_{i}\right)=\frac{\pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)}{\sum_{k} \pi_{k} \mathcal{N}\left(w_{i} \mid \mu_{k}, \sigma_{k}^{2}\right)} 
\tag{9.24}
\end{align*}

\begin{align*}
\frac{\partial \widetilde{E}}{\partial w_{i}}=\frac{\partial E}{\partial w_{i}}+\lambda \sum_{j} \gamma_{j}\left(w_{i}\right) \frac{\left(w_{i}-\mu_{j}\right)}{\sigma_{j}^{2}} 
\tag{9.25}
\end{align*}

\begin{align*}
\frac{\partial \widetilde{E}}{\partial \mu_{j}}=\lambda \sum_{i} \gamma_{j}\left(w_{i}\right) \frac{\left(\mu_{j}-w_{i}\right)}{\sigma_{j}^{2}} 
\tag{9.26}
\end{align*}

\begin{align*}
\sigma_{j}^{2}=\exp \left(\xi_{j}\right) 
\tag{9.27}
\end{align*}

\begin{align*}
\frac{\partial \widetilde{E}}{\partial \xi}=\frac{\lambda}{2} \sum_{i} \gamma_{j}\left(w_{i}\right)\left(1-\frac{\left(w_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{2}}\right) 
\tag{9.28}
\end{align*}

\begin{align*}
\sum_{j} \pi_{j}=1, \quad 0 \leqslant \pi_{i} \leqslant 1 
\tag{9.29}
\end{align*}

\begin{align*}
\pi_{j}=\frac{\exp \left(\eta_{j}\right)}{\sum_{k=1}^{K} \exp \left(\eta_{k}\right)} 
\tag{9.30}
\end{align*}

\begin{align*}
\frac{\partial \widetilde{E}}{\partial \eta_{j}}=\lambda \sum_{i}\left\{\pi_{j}-\gamma_{j}\left(w_{i}\right)\right\} 
\tag{9.31}
\end{align*}

\section{Residual Connections}

\begin{align*}
\mathbf{z}_{1} & =\mathbf{F}_{1}(\mathbf{x})  \tag{9.32}
\end{align*}

\begin{align*}
\mathbf{z}_{2} & =\mathbf{F}_{2}\left(\mathbf{z}_{1}\right)  \tag{9.33}
\end{align*}

\begin{align*}
\mathbf{y} & =\mathbf{F}_{3}\left(\mathbf{z}_{2}\right) \tag{9.34}
\end{align*}

\begin{align*}
\mathbf{z}_{1} & =\mathbf{F}_{1}(\mathbf{x})+\mathbf{x}  \tag{9.35}
\end{align*}

\begin{align*}
\mathbf{z}_{2} & =\mathbf{F}_{2}\left(\mathbf{z}_{1}\right)+\mathbf{z}_{1}  \tag{9.36}
\end{align*}

\begin{align*}
\mathbf{y} & =\mathbf{F}_{3}\left(\mathbf{z}_{2}\right)+\mathbf{z}_{2} \tag{9.37}
\end{align*}

\begin{align*}
\mathbf{F}_{l}\left(\mathbf{z}_{l-1}\right)=\mathbf{z}_{l}-\mathbf{z}_{l-1} \tag{9.38}
\end{align*}

\begin{align*}
\mathbf{y}=\mathbf{F}_{3}\left(\mathbf{F}_{2}\left(\mathbf{F}_{1}(\mathbf{x})+\mathbf{x}\right)+\mathbf{z}_{1}\right)+\mathbf{z}_{2} \tag{9.39}
\end{align*}

\begin{align*}
\mathbf{y} & =\mathbf{F}_{3}\left(\mathbf{F}_{2}\left(\mathbf{F}_{1}(\mathbf{x})+\mathbf{x}\right)+\mathbf{F}_{1}(\mathbf{x})+\mathbf{x}\right) \\
& \left.+\mathbf{F}_{2}\left(\mathbf{F}_{1}(\mathbf{x})+\mathbf{x}\right)\right) \\
& +\mathbf{F}_{1}(\mathbf{x})+\mathbf{x}
\tag{9.40}
\end{align*}

\begin{align*}
\mathbf{z}_{l}=\mathbf{F}_{l}\left(\mathbf{z}_{l-1}\right)+\mathbf{W} \mathbf{z}_{l-1} \tag{9.41}
\end{align*}

\section{Model Averaging}

\begin{align*}
p(\mathbf{y} \mid \mathbf{x})=\frac{1}{L} \sum_{l=1}^{L} p_{l}(\mathbf{y} \mid \mathbf{x}) \tag{9.42}
\end{align*}

\begin{align*}
y_{\mathrm{COM}}(\mathbf{x})=\frac{1}{M} \sum_{m=1}^{M} y_{m}(\mathbf{x}) \tag{9.43}
\end{align*}

\begin{align*}
y_{m}(\mathbf{x})=h(\mathbf{x})+\epsilon_{m}(\mathbf{x}) \tag{9.44}
\end{align*}

\begin{align*}
\mathbb{E}_{\mathbf{x}}\left[
\left\{y_{m}(\mathbf{x})-h(\mathbf{x})\right\}^{2}\right]
=\mathbb{E}_{\mathbf{x}}\left[
\epsilon_{m}(\mathbf{x})^{2}\right]
\tag{9.45}
\end{align*}

\begin{align*}
E_{\mathrm{AV}}=\frac{1}{M} \sum_{m=1}^{M} \mathbb{E}_{\mathbf{x}}\left[
\epsilon_{m}(\mathbf{x})^{2}\right]
\tag{9.46}
\end{align*}

\begin{align*}
E_{\mathrm{COM}} & =\mathbb{E}_{\mathbf{x}}\left[
\left\{\frac{1}{M} \sum_{m=1}^{M} y_{m}(\mathbf{x})-h(\mathbf{x})\right\}^{2}\right] \\
& =\mathbb{E}_{\mathbf{x}}\left[
\left\{\frac{1}{M} \sum_{m=1}^{M} \epsilon_{m}(\mathbf{x})\right\}^{2}\right]
\tag{9.47}
\end{align*}

\begin{align*}
\mathbb{E}_{\mathbf{x}}\left[
\epsilon_{m}(\mathbf{x})\right]
& =0  \tag{9.48}
\end{align*}

\begin{align*}
\mathbb{E}_{\mathbf{x}}\left[
\epsilon_{m}(\mathbf{x}) \epsilon_{l}(\mathbf{x})\right]
& =0, \quad m \neq l
\tag{9.49}
\end{align*}

\begin{align*}
E_{\mathrm{COM}}=\frac{1}{M} E_{\mathrm{AV}} \tag{9.50}
\end{align*}

\subsection{Dropout}

\begin{align*}
p(\mathbf{y} \mid \mathbf{x})=\sum_{\mathbf{R}} p(\mathbf{R}) p(\mathbf{y} \mid \mathbf{x}, \mathbf{R}) \tag{9.51}
\end{align*}

\end{document}
