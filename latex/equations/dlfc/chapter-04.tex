\documentclass{article}

% Document Layout and Fonts
\usepackage[margin=0.9in]{geometry}    % Page margins
\usepackage{fontspec}                  % For custom fonts (LuaLaTeX feature)
\usepackage{tgpagella}
\usepackage{mathpazo}
\setmainfont{EB Garamond}              % Main font (EB Garamond)
\usepackage{microtype}                 % Improves text appearance
\usepackage{titlesec}                  % Customize section title fonts

% Right-align section headings
\titleformat{\section}
  {\normalfont\large\scshape\raggedright}  % Right-align and small caps
  {}{0em}{}[]

% Right-align subsection headings and add a line below
\titleformat{\subsection}
  {\normalfont\normalsize\raggedleft}     % Right-align subsections
  {}{0em}{\titlerule[0.5pt]}              % Horizontal line below

% Right-align and italicize subsubsections
\titleformat{\subsubsection}
  {\normalfont\normalsize\itshape\raggedleft} % Right-align and italicize subsubsections
  {}{0em}{}[]

% Math and Science Packages
\usepackage{amsmath, amsfonts, amssymb, mathtools, amsthm, dsfont}

% Math commands and operators
\newcommand{\minus}{\scalebox{0.8}{\(-\)}}
\newcommand{\plus}{\scalebox{0.6}{\(+\)}}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{Tr}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}    % Differential d

% Definitions, theorems, corollaries, and friends
\usepackage[english]{babel}
\usepackage[hidelinks]{hyperref}
\newtheorem{axiom}{Axiom}
\newtheorem{postulate}{Postulate}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem*{remark}{Remark}

\title{\LARGE\scshape\MakeUppercase{Chapter 4: Single-layer Networks: Regression}}
\author{\textit{Bishop and Bishop}}
\date{}

\begin{document}

\maketitle

\section{Linear Regression}

\begin{align*}
y(\mathbf{x}, \mathbf{w})=w_{0}+w_{1} x_{1}+\ldots+w_{D} x_{D}
\tag{4.1}
\end{align*}

\subsection{Basis functions}

\begin{align*}
y(\mathbf{x}, \mathbf{w}) = w_0 + \sum_{j=1}^{M-1} w_j \phi_j(\mathbf{x})
\tag{4.2}
\end{align*}

\begin{align*}
y(\mathbf{x}, \mathbf{w}) = \sum_{j=0}^{M-1} w_j \phi_j(\mathbf{x}) = \mathbf{w}^{\top} \boldsymbol{\phi}(\mathbf{x})
\tag{4.3}
\end{align*}

\begin{align*}
\phi_j(x) = \exp \left\{ -\frac{(x - \mu_j)^2}{2 s^2} \right\}
\tag{4.4}
\end{align*}

\begin{align*}
\phi_j(x) = \sigma \left( \frac{x - \mu_j}{s} \right)
\tag{4.5}
\end{align*}

\begin{align*}
\sigma(a) = \frac{1}{1 + \exp(-a)}
\tag{4.6}
\end{align*}

\subsection{Likelihood function}

\begin{align*}
t = y(\mathbf{x}, \mathbf{w}) + \epsilon
\tag{4.7}
\end{align*}

\begin{align*}
p(t \mid \mathbf{x}, \mathbf{w}, \sigma^2) = \mathcal{N}(t \mid y(\mathbf{x}, \mathbf{w}), \sigma^2)
\tag{4.8}
\end{align*}

\begin{align*}
p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \sigma^2) = \prod_{n=1}^{N} \mathcal{N} \left( t_n \mid \mathbf{w}^{\top} \boldsymbol{\phi}(\mathbf{x}_n), \sigma^2 \right)
\tag{4.9}
\end{align*}

\begin{align*}
\ln p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \sigma^2) = -\frac{N}{2} \ln \sigma^2 - \frac{N}{2} \ln (2 \pi) - \frac{1}{\sigma^2} E_D(\mathbf{w})
\tag{4.10}
\end{align*}

\begin{align*}
E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \left\{ t_n - \mathbf{w}^{\top} \boldsymbol{\phi}(\mathbf{x}_n) \right\}^2
\tag{4.11}
\end{align*}

\subsection{Maximum likelihood}

\begin{align*}
\nabla_{\mathbf{w}} \ln p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \sigma^2) = \frac{1}{\sigma^2} \sum_{n=1}^{N} \left\{ t_n - \mathbf{w}^{\top} \boldsymbol{\phi}(\mathbf{x}_n) \right\} \boldsymbol{\phi}(\mathbf{x}_n)^{\top}
\tag{4.12}
\end{align*}

\begin{align*}
0 = \sum_{n=1}^{N} t_n \boldsymbol{\phi}(\mathbf{x}_n)^{\top} - \mathbf{w}^{\top} \left( \sum_{n=1}^{N} \boldsymbol{\phi}(\mathbf{x}_n) \boldsymbol{\phi}(\mathbf{x}_n)^{\top} \right)
\tag{4.13}
\end{align*}

\begin{align*}
\mathbf{w}_{\mathrm{ML}} = \left( \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} \right)^{-1} \boldsymbol{\Phi}^{\top} \mathbf{t}
\tag{4.14}
\end{align*}

\begin{align*}
\boldsymbol{\Phi} = \left( \begin{array}{cccc}
\phi_0(\mathbf{x}_1) & \phi_1(\mathbf{x}_1) & \cdots & \phi_{M-1}(\mathbf{x}_1) \\
\phi_0(\mathbf{x}_2) & \phi_1(\mathbf{x}_2) & \cdots & \phi_{M-1}(\mathbf{x}_2) \\
\vdots & \vdots & \ddots & \vdots \\
\phi_0(\mathbf{x}_N) & \phi_1(\mathbf{x}_N) & \cdots & \phi_{M-1}(\mathbf{x}_N) \\
\end{array} \right)
\tag{4.15}
\end{align*}

\begin{align*}
\boldsymbol{\Phi}^{\dagger} \equiv \left( \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} \right)^{-1} \boldsymbol{\Phi}^{\top}
\tag{4.16}
\end{align*}

\begin{align*}
E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \left\{ t_n - w_0 - \sum_{j=1}^{M-1} w_j \phi_j(\mathbf{x}_n) \right\}^2
\tag{4.17}
\end{align*}

\begin{align*}
w_0 = \bar{t} - \sum_{j=1}^{M-1} w_j \overline{\phi_j}
\tag{4.18}
\end{align*}

\begin{align*}
\bar{t} = \frac{1}{N} \sum_{n=1}^{N} t_n, \quad \overline{\phi_j} = \frac{1}{N} \sum_{n=1}^{N} \phi_j(\mathbf{x}_n)
\tag{4.19}
\end{align*}

\begin{align*}
\sigma_{\mathrm{ML}}^2 = \frac{1}{N} \sum_{n=1}^{N} \left\{ t_n - \mathbf{w}_{\mathrm{ML}}^{\top} \phi(\mathbf{x}_n) \right\}^2
\tag{4.20}
\end{align*}

\subsection{Geometry of least squares}

No equations

\subsection{Sequential learning}

\begin{align*}
\mathbf{w}^{(\tau+1)} = \mathbf{w}^{(\tau)} - \eta \nabla E_n
\tag{4.21}
\end{align*}

\begin{align*}
\mathbf{w}^{(\tau+1)} = \mathbf{w}^{(\tau)} + \eta \left( t_n - \mathbf{w}^{(\tau) \top} \boldsymbol{\phi}_n \right) \boldsymbol{\phi}_n
\tag{4.22}
\end{align*}

\subsection{Regularized least squares}

\begin{align*}
E_D(\mathbf{w}) + \lambda E_W(\mathbf{w})
\tag{4.23}
\end{align*}

\begin{align*}
E_W(\mathbf{w}) = \frac{1}{2} \sum_{j} w_j^2 = \frac{1}{2} \mathbf{w}^{\top} \mathbf{w}
\tag{4.24}
\end{align*}

\begin{align*}
E_D(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^{N} \left\{ t_n - \mathbf{w}^{\top} \boldsymbol{\phi}(\mathbf{x}_n) \right\}^2
\tag{4.25}
\end{align*}

\begin{align*}
\frac{1}{2} \sum_{n=1}^{N} \left\{ t_n - \mathbf{w}^{\top} \boldsymbol{\phi}(\mathbf{x}_n) \right\}^2 + \frac{\lambda}{2} \mathbf{w}^{\top} \mathbf{w}
\tag{4.26}
\end{align*}

\begin{align*}
\mathbf{w} = \left( \lambda \mathbf{I} + \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} \right)^{-1} \boldsymbol{\Phi}^{\top} \mathbf{t}
\tag{4.27}
\end{align*}

\subsection{Multiple outputs}

\begin{align*}
\mathbf{y}(\mathbf{x}, \mathbf{w}) = \mathbf{W}^{\top} \phi(\mathbf{x})
\tag{4.28}
\end{align*}

\begin{align*}
p(\mathbf{t} \mid \mathbf{x}, \mathbf{W}, \sigma^2) = \mathcal{N} \left( \mathbf{t} \mid \mathbf{W}^{\top} \boldsymbol{\phi}(\mathbf{x}), \sigma^2 \mathbf{I} \right)
\tag{4.29}
\end{align*}

\begin{align*}
\ln p(\mathbf{T} \mid \mathbf{X}, \mathbf{W}, \sigma^2) = -\frac{N K}{2} \ln \left( 2 \pi \sigma^2 \right) - \frac{1}{2 \sigma^2} \sum_{n=1}^{N} \left\| \mathbf{t}_n - \mathbf{W}^{\top} \boldsymbol{\phi}(\mathbf{x}_n) \right\|^2
\tag{4.30}
\end{align*}

\begin{align*}
\mathbf{W}_{\mathrm{ML}} = \left( \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} \right)^{-1} \boldsymbol{\Phi}^{\top} \mathbf{T}
\tag{4.31}
\end{align*}

\begin{align*}
\mathbf{w}_k = \left( \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi} \right)^{-1} \boldsymbol{\Phi}^{\top} \mathbf{t}_k = \boldsymbol{\Phi}^{\dagger} \mathbf{t}_k
\tag{4.32}
\end{align*}


\section{Decision theory}

\begin{align*}
p\left(t \mid \mathbf{x}, \mathbf{w}_{\mathrm{ML}}, \sigma_{\mathrm{ML}}^{2}\right) = \mathcal{N}\left( t \mid y\left( \mathbf{x}, \mathbf{w}_{\mathrm{ML}} \right), \sigma_{\mathrm{ML}}^{2} \right)
\tag{4.33}
\end{align*}

\begin{align*}
\mathbb{E}[L] = \iint L(t, f(\mathbf{x})) p(\mathbf{x}, t) \mathrm{d} \mathbf{x} \mathrm{d} t
\tag{4.34}
\end{align*}

\begin{align*}
\mathbb{E}[L] = \iint \{f(\mathbf{x}) - t\}^{2} p(\mathbf{x}, t) \mathrm{d} \mathbf{x} \mathrm{d} t
\tag{4.35}
\end{align*}

\begin{align*}
\frac{\delta \mathbb{E}[L]}{\delta f(\mathbf{x})} = 2 \int \{ f(\mathbf{x}) - t \} p(\mathbf{x}, t) \mathrm{d} t = 0
\tag{4.36}
\end{align*}

\begin{align*}
f^{\star}(\mathbf{x}) = \frac{1}{p(\mathbf{x})} \int t p(\mathbf{x}, t) \mathrm{d} t = \int t p(t \mid \mathbf{x}) \mathrm{d} t = \mathbb{E}_{t}[t \mid \mathbf{x}]
\tag{4.37}
\end{align*}

\begin{align*}
\mathbb{E}[t \mid \mathbf{x}] = \int t p(t \mid \mathbf{x}) \mathrm{d} t = y(\mathbf{x}, \mathbf{w})
\tag{4.38}
\end{align*}

\begin{align*}
\mathbb{E}[L] = \int \{ f(\mathbf{x}) - \mathbb{E}[t \mid \mathbf{x}] \}^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x} + \int \operatorname{var}[t \mid \mathbf{x}] p(\mathbf{x}) \mathrm{d} \mathbf{x}
\tag{4.39}
\end{align*}

\begin{align*}
\mathbb{E}\left[ L_q \right] = \iint |f(\mathbf{x}) - t|^{q} p(\mathbf{x}, t) \mathrm{d} \mathbf{x} \mathrm{d} t
\tag{4.40}
\end{align*}

\section{The Bias-Variance Trade-off}

\begin{align*}
h(\mathbf{x}) = \mathbb{E}[t \mid \mathbf{x}] = \int t p(t \mid \mathbf{x}) \mathrm{d} t
\tag{4.41}
\end{align*}

\begin{align*}
\mathbb{E}[L] = \int \{f(\mathbf{x}) - h(\mathbf{x})\}^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x} + \iint \{h(\mathbf{x}) - t\}^{2} p(\mathbf{x}, t) \mathrm{d} \mathbf{x} \mathrm{d} t
\tag{4.42}
\end{align*}

\begin{align*}
\{f(\mathbf{x} ; \mathcal{D}) - h(\mathbf{x})\}^{2}
\tag{4.43}
\end{align*}

\begin{align*}
& \left\{ f(\mathbf{x} ; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[f(\mathbf{x} ; \mathcal{D})] + \mathbb{E}_{\mathcal{D}}[f(\mathbf{x} ; \mathcal{D})] - h(\mathbf{x}) \right\}^{2} \\
& = \left\{ f(\mathbf{x} ; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[f(\mathbf{x} ; \mathcal{D})] \right\}^{2} + \left\{ \mathbb{E}_{\mathcal{D}}[f(\mathbf{x} ; \mathcal{D})] - h(\mathbf{x}) \right\}^{2} \\
& + 2 \left\{ f(\mathbf{x} ; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[f(\mathbf{x} ; \mathcal{D})] \right\} \left\{ \mathbb{E}_{\mathcal{D}}[f(\mathbf{x} ; \mathcal{D})] - h(\mathbf{x}) \right\}
\tag{4.44}
\end{align*}

\begin{align*}
\mathbb{E}_{\mathcal{D}}\left[ \{f(\mathbf{x} ; \mathcal{D}) - h(\mathbf{x})\}^{2} \right]
= \underbrace{\left\{ \mathbb{E}_{\mathcal{D}}[f(\mathbf{x} ; \mathcal{D})] - h(\mathbf{x}) \right\}^{2}}_{(\text{bias})^{2}} + \underbrace{\mathbb{E}_{\mathcal{D}} \left[ \left\{ f(\mathbf{x} ; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[f(\mathbf{x} ; \mathcal{D})] \right\}^{2} \right]}_{\text{variance}}
\tag{4.45}
\end{align*}

\begin{align*}
\text{expected loss} = (\text{bias})^{2} + \text{variance} + \text{noise}
\tag{4.46}
\end{align*}

\begin{align*}
(\text{bias})^{2} = \int \left\{ \mathbb{E}_{\mathcal{D}}[f(\mathbf{x} ; \mathcal{D})] - h(\mathbf{x}) \right\}^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x}
\tag{4.47}
\end{align*}

\begin{align*}
\text{variance} = \int \mathbb{E}_{\mathcal{D}} \left[ \left\{ f(\mathbf{x} ; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[f(\mathbf{x} ; \mathcal{D})] \right\}^{2} \right] p(\mathbf{x}) \mathrm{d} \mathbf{x}
\tag{4.48}
\end{align*}

\begin{align*}
\text{noise} = \iint \{h(\mathbf{x}) - t\}^{2} p(\mathbf{x}, t) \mathrm{d} \mathbf{x} \mathrm{d} t
\tag{4.49}
\end{align*}

\begin{align*}
\bar{f}(x) = \frac{1}{L} \sum_{l=1}^{L} f^{(l)}(x)
\tag{4.50}
\end{align*}

\begin{align*}
(\text{bias})^{2} & = \frac{1}{N} \sum_{n=1}^{N} \left\{ \bar{f}(x_n) - h(x_n) \right\}^{2}
\tag{4.51}
\end{align*}

\begin{align*}
\text{variance} & = \frac{1}{N} \sum_{n=1}^{N} \frac{1}{L} \sum_{l=1}^{L} \left\{ f^{(l)}(x_n) - \bar{f}(x_n) \right\}^{2}
\tag{4.52}
\end{align*}

\end{document}
