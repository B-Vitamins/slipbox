\documentclass{article}

% Document Layout and Fonts
\usepackage[margin=0.9in]{geometry}    % Page margins
\usepackage{fontspec}                  % For custom fonts (LuaLaTeX feature)
\usepackage{tgpagella}
\usepackage{mathpazo}
\setmainfont{EB Garamond}              % Main font (EB Garamond)
\usepackage{microtype}                 % Improves text appearance
\usepackage{titlesec}                  % Customize section title fonts
\usepackage{algorithm}
\usepackage{algorithmic}

% Right-align section headings
\titleformat{\section}
  {\normalfont\large\scshape\raggedright}  % Right-align and small caps
  {}{0em}{}[]

% Right-align subsection headings and add a line below
\titleformat{\subsection}
  {\normalfont\normalsize\raggedleft}     % Right-align subsections
  {}{0em}{\titlerule[0.5pt]}              % Horizontal line below

% Right-align and italicize subsubsections
\titleformat{\subsubsection}
  {\normalfont\normalsize\itshape\raggedleft} % Right-align and italicize subsubsections
  {}{0em}{}[]

% Math and Science Packages
\usepackage{amsmath, amsfonts, amssymb, mathtools, amsthm, dsfont}

% Math commands and operators
\newcommand{\minus}{\scalebox{0.8}{\(-\)}}
\newcommand{\plus}{\scalebox{0.6}{\(+\)}}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{Tr}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}    % Differential d

% Definitions, theorems, corollaries, and friends
\usepackage[english]{babel}
\usepackage[hidelinks]{hyperref}
\newtheorem{axiom}{Axiom}
\newtheorem{postulate}{Postulate}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem*{remark}{Remark}

\title{\LARGE\scshape\MakeUppercase{Chapter 8: Backpropagation}}
\author{\textit{Bishop and Bishop}}
\date{}

\begin{document}

\maketitle

\section{Evaluation of Gradients}

\subsection{Single-layer networks}

\begin{align*}
E(\mathbf{w}) = \sum_{n=1}^{N} E_{n}(\mathbf{w}) \tag{8.1}
\end{align*}

\begin{align*}
y_{k} = \sum_{i} w_{k i} x_{i} \tag{8.2}
\end{align*}

\begin{align*}
E_{n} = \frac{1}{2} \sum_{k} \left( y_{n k} - t_{n k} \right)^{2} \tag{8.3}
\end{align*}

\begin{align*}
\frac{\partial E_{n}}{\partial w_{j i}} = \left( y_{n j} - t_{n j} \right) x_{n i} \tag{8.4}
\end{align*}

\subsection{General feed-forward networks}

\begin{align*}
a_{j} = \sum_{i} w_{j i} z_{i} \tag{8.5}
\end{align*}

\begin{align*}
z_{j} = h(a_{j}) \tag{8.6}
\end{align*}

\begin{align*}
\frac{\partial E_{n}}{\partial w_{j i}} = \frac{\partial E_{n}}{\partial a_{j}} \frac{\partial a_{j}}{\partial w_{j i}} \tag{8.7}
\end{align*}

\begin{align*}
\delta_{j} \equiv \frac{\partial E_{n}}{\partial a_{j}} \tag{8.8}
\end{align*}

\begin{align*}
\frac{\partial a_{j}}{\partial w_{j i}} = z_{i} \tag{8.9}
\end{align*}

\begin{align*}
\frac{\partial E_{n}}{\partial w_{j i}} = \delta_{j} z_{i} \tag{8.10}
\end{align*}

\begin{align*}
\delta_{k} = y_{k} - t_{k} \tag{8.11}
\end{align*}

\begin{align*}
\delta_{j} = \sum_{k} \frac{\partial E_{n}}{\partial a_{k}} \frac{\partial a_{k}}{\partial a_{j}} \tag{8.12}
\end{align*}

\begin{align*}
\delta_{j} = h^{\prime}(a_{j}) \sum_{k} w_{k j} \delta_{k} \tag{8.13}
\end{align*}

\begin{align*}
\frac{\partial E}{\partial w_{j i}} = \sum_{n} \frac{\partial E_{n}}{\partial w_{j i}} \tag{8.14}
\end{align*}

\subsubsection{Algorithm 8.1: Backpropagation}

\begin{algorithm}
\caption{Backpropagation}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Input vector $x_n$
\STATE \quad Network parameters $w$
\STATE \quad Error function $E_n(w)$ for input $x_n$
\STATE \quad Activation function $h(a)$
\STATE \textbf{Output:} Error function derivatives $\left\{\frac{\partial E_n}{\partial w_{ji}}\right\}$
\STATE \textbf{// Forward propagation}
\FOR{j \textbf{ in} all hidden and output units}
    \STATE $a_j \gets \sum_i w_{ji} z_i$ \COMMENT{// $\{z_i\}$ includes inputs $\{x_i\}$}
    \STATE $z_j \gets h(a_j)$ \COMMENT{// activation function}
\ENDFOR
\STATE \textbf{// Error evaluation}
\FOR{k \textbf{ in} all output units}
    \STATE $\delta_k \gets \frac{\partial E_n}{\partial a_k}$ \COMMENT{// compute errors}
\ENDFOR
\STATE \textbf{// Backward propagation, in reverse order}
\FOR{j \textbf{ in} all hidden units}
    \STATE $\delta_j \gets h'(a_j) \sum_k w_{kj} \delta_k$ \COMMENT{// recursive backward evaluation}
    \STATE $\frac{\partial E_n}{\partial w_{ji}} \gets \delta_j z_i$ \COMMENT{// evaluate derivatives}
\ENDFOR
\RETURN $\left\{\frac{\partial E_n}{\partial w_{ji}}\right\}$
\end{algorithmic}
\end{algorithm}

\subsection{A simple example}

\begin{align*}
h(a) \equiv \tanh(a) \tag{8.15}
\end{align*}

\begin{align*}
h^{\prime}(a) = 1 - h(a)^{2} \tag{8.16}
\end{align*}

\begin{align*}
E_{n} = \frac{1}{2} \sum_{k=1}^{K} \left( y_{k} - t_{k} \right)^{2} \tag{8.17}
\end{align*}

\begin{align*}
a_{j} = \sum_{i=0}^{D} w_{j i}^{(1)} x_{i} \tag{8.18}
\end{align*}

\begin{align*}
z_{j} = \tanh(a_{j}) \tag{8.19}
\end{align*}

\begin{align*}
y_{k} = \sum_{j=0}^{M} w_{k j}^{(2)} z_{j} \tag{8.20}
\end{align*}

\begin{align*}
\delta_{k} = y_{k} - t_{k} \tag{8.21}
\end{align*}

\begin{align*}
\delta_{j} = \left( 1 - z_{j}^{2} \right) \sum_{k=1}^{K} w_{k j}^{(2)} \delta_{k} \tag{8.22}
\end{align*}

\begin{align*}
\frac{\partial E_{n}}{\partial w_{j i}^{(1)}} = \delta_{j} x_{i}, \quad \frac{\partial E_{n}}{\partial w_{k j}^{(2)}} = \delta_{k} z_{j} \tag{8.23}
\end{align*}

\subsection{Numerical differentiation}

\begin{align*}
\frac{\partial E_{n}}{\partial w_{j i}} = \frac{E_{n}(w_{j i} + \epsilon) - E_{n}(w_{j i})}{\epsilon} + \mathcal{O}(\epsilon) \tag{8.24}
\end{align*}

\begin{align*}
\frac{\partial E_{n}}{\partial w_{j i}} = \frac{E_{n}(w_{j i} + \epsilon) - E_{n}(w_{j i} - \epsilon)}{2 \epsilon} + \mathcal{O}(\epsilon^{2}) \tag{8.25}
\end{align*}

\subsection{The Jacobian matrix}

\begin{align*}
J_{k i} \equiv \frac{\partial y_{k}}{\partial x_{i}} \tag{8.26}
\end{align*}

\begin{align*}
\frac{\partial E}{\partial w} = \sum_{k, j} \frac{\partial E}{\partial y_{k}} \frac{\partial y_{k}}{\partial z_{j}} \frac{\partial z_{j}}{\partial w} \tag{8.27}
\end{align*}

\begin{align*}
\Delta y_{k} \simeq \sum_{i} \frac{\partial y_{k}}{\partial x_{i}} \Delta x_{i} \tag{8.28}
\end{align*}

\begin{align*}
J_{k i} = \sum_{j} w_{j i} \frac{\partial y_{k}}{\partial a_{j}} \tag{8.29}
\end{align*}

\begin{align*}
\frac{\partial y_{k}}{\partial a_{j}} = h^{\prime}(a_{j}) \sum_{l} w_{l j} \frac{\partial y_{k}}{\partial a_{l}} \tag{8.30}
\end{align*}

\begin{align*}
\frac{\partial y_{k}}{\partial a_{l}} = \delta_{k l} \tag{8.31}
\end{align*}

\begin{align*}
\frac{\partial y_{k}}{\partial a_{l}} = \delta_{k l} \sigma^{\prime}(a_{l}) \tag{8.33}
\end{align*}

\begin{align*}
\frac{\partial y_{k}}{\partial a_{l}} = \delta_{k l} y_{k} - y_{k} y_{l} \tag{8.34}
\end{align*}

\begin{align*}
\frac{\partial y_{k}}{\partial x_{i}} = \frac{y_{k}(x_{i} + \epsilon) - y_{k}(x_{i} - \epsilon)}{2 \epsilon} + \mathcal{O}(\epsilon^{2}) \tag{8.35}
\end{align*}

\subsection{The Hessian matrix}

\begin{align*}
\frac{\partial^{2} E}{\partial w_{j i} \partial w_{l k}} \tag{8.36}
\end{align*}

\begin{align*}
H_{i j} = \frac{\partial^{2} E}{\partial w_{i} \partial w_{j}} \tag{8.37}
\end{align*}

\begin{align*}
E = \frac{1}{2} \sum_{n=1}^{N} \left( y_{n} - t_{n} \right)^{2} \tag{8.38}
\end{align*}

\begin{align*}
\mathbf{H} = \sum_{n=1}^{N} \nabla y_{n} \left( \nabla y_{n} \right)^{\top} + \sum_{n=1}^{N} \left( y_{n} - t_{n} \right) \nabla \nabla y_{n} \tag{8.39}
\end{align*}

\begin{align*}
\mathbf{H} \simeq \sum_{n=1}^{N} \nabla a_{n} \nabla a_{n}^{\top} \tag{8.40}
\end{align*}

\begin{align*}
\mathbf{H} \simeq \sum_{n=1}^{N} y_{n}(1 - y_{n}) \nabla a_{n} \nabla a_{n}^{\top} \tag{8.41}
\end{align*}

\section{Automatic Differentiation}

\subsection{Forward-mode automatic differentiation}

\begin{align*}
f(x_1, x_2) = x_1 x_2 + \exp(x_1 x_2) - \sin(x_2) \tag{8.49}
\end{align*}

\begin{align*}
v_1 = x_1 \tag{8.50}
\end{align*}

\begin{align*}
v_2 = x_2 \tag{8.51}
\end{align*}

\begin{align*}
v_3 = v_1 v_2 \tag{8.52}
\end{align*}

\begin{align*}
v_4 = \sin(v_2) \tag{8.53}
\end{align*}

\begin{align*}
v_5 = \exp(v_3) \tag{8.54}
\end{align*}

\begin{align*}
v_6 = v_3 - v_4 \tag{8.55}
\end{align*}

\begin{align*}
v_7 = v_5 + v_6 \tag{8.56}
\end{align*}

\begin{align*}
\dot{v}_1 = 1 \tag{8.58}
\end{align*}

\begin{align*}
\dot{v}_2 = 0 \tag{8.59}
\end{align*}

\begin{align*}
\dot{v}_3 = v_1 \dot{v}_2 + \dot{v}_1 v_2 \tag{8.60}
\end{align*}

\begin{align*}
\dot{v}_4 = \dot{v}_2 \cos(v_2) \tag{8.61}
\end{align*}

\begin{align*}
\dot{v}_5 = \dot{v}_3 \exp(v_3) \tag{8.62}
\end{align*}

\begin{align*}
\dot{v}_6 = \dot{v}_3 - \dot{v}_4 \tag{8.63}
\end{align*}

\begin{align*}
\dot{v}_7 = \dot{v}_5 + \dot{v}_6 \tag{8.64}
\end{align*}

\begin{align*}
f_2(x_1, x_2) = (x_1 x_2 - \sin(x_2)) \exp(x_1 x_2) \tag{8.65}
\end{align*}

\begin{align*}
\mathbf{J} =
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_D} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_K}{\partial x_1} & \cdots & \frac{\partial f_K}{\partial x_D}
\end{bmatrix} \tag{8.66}
\end{align*}

\begin{align*}
\mathbf{J} \mathbf{r} = 
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_D} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_K}{\partial x_1} & \cdots & \frac{\partial f_K}{\partial x_D}
\end{bmatrix}
\begin{bmatrix}
r_1 \\
\vdots \\
r_D
\end{bmatrix} \tag{8.67}
\end{align*}

\subsection{Reverse-mode automatic differentiation}

\begin{align*}
\bar{v}_i = \frac{\partial f}{\partial v_i} \tag{8.68}
\end{align*}

\begin{align*}
\bar{v}_i = \sum_{j \in \operatorname{ch}(i)} \bar{v}_j \frac{\partial v_j}{\partial v_i} \tag{8.69}
\end{align*}

\begin{align*}
\bar{v}_7 = 1 \tag{8.70}
\end{align*}

\begin{align*}
\bar{v}_6 = \bar{v}_7 \tag{8.71}
\end{align*}

\begin{align*}
\bar{v}_5 = \bar{v}_7 \tag{8.72}
\end{align*}

\begin{align*}
\bar{v}_4 = -\bar{v}_6 \tag{8.73}
\end{align*}

\begin{align*}
\bar{v}_3 = \bar{v}_5 v_5 + \bar{v}_6 \tag{8.74}
\end{align*}

\begin{align*}
\bar{v}_2 = \bar{v}_3 v_1 + \bar{v}_4 \cos(v_2) \tag{8.75}
\end{align*}

\begin{align*}
\bar{v}_1 = \bar{v}_3 v_2 \tag{8.76}
\end{align*}

\end{document}
