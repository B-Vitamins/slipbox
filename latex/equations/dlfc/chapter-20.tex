\documentclass{article}

% Document Layout and Fonts
\usepackage[margin=0.9in]{geometry}    % Page margins
\usepackage{fontspec}                  % For custom fonts (LuaLaTeX feature)
\usepackage{tgpagella}
\usepackage{mathpazo}
\setmainfont{EB Garamond}              % Main font (EB Garamond)
\usepackage{microtype}                 % Improves text appearance
\usepackage{titlesec}                  % Customize section title fonts
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}

% Right-align section headings
\titleformat{\section}
  {\normalfont\large\scshape\raggedright}  % Right-align and small caps
  {}{0em}{}[]

% Right-align subsection headings and add a line below
\titleformat{\subsection}
  {\normalfont\normalsize\raggedleft}     % Right-align subsections
  {}{0em}{\titlerule[0.5pt]}              % Horizontal line below

% Right-align and italicize subsubsections
\titleformat{\subsubsection}
  {\normalfont\normalsize\itshape\raggedleft} % Right-align and italicize subsubsections
  {}{0em}{}[]

% Math and Science Packages
\usepackage{amsmath, amsfonts, amssymb, mathtools, amsthm, dsfont}

% Math commands and operators
\newcommand{\minus}{\scalebox{0.8}{\(-\)}}
\newcommand{\plus}{\scalebox{0.6}{\(+\)}}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\tr}{Tr}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}    % Differential d

% Definitions, theorems, corollaries, and friends
\usepackage[english]{babel}
\usepackage[hidelinks]{hyperref}
\newtheorem{axiom}{Axiom}
\newtheorem{postulate}{Postulate}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem*{remark}{Remark}

\title{\LARGE\scshape\MakeUppercase{Chapter 20: Diffusion Models}}
\author{\textit{Bishop and Bishop}}
\date{}

\begin{document}

\maketitle

\section{Forward Encoder}

\begin{align*}
\mathbf{z}_{1}=\sqrt{1-\beta_{1}} \mathbf{x}+\sqrt{\beta_{1}} \boldsymbol{\epsilon}_{1} 
\tag{20.1}
\end{align*}

\begin{align*}
q\left(\mathbf{z}_{1} \mid \mathbf{x}\right)=\mathcal{N}\left(\mathbf{z}_{1} \mid \sqrt{1-\beta_{1}} \mathbf{x}, \beta_{1} \mathbf{I}\right)
\tag{20.2}
\end{align*}

\begin{align*}
\mathbf{z}_{t}=\sqrt{1-\beta_{t}} \mathbf{z}_{t-1}+\sqrt{\beta_{t}} \boldsymbol{\epsilon}_{t}
\tag{20.3}
\end{align*}

\begin{align*}
q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}\right)=\mathcal{N}\left(\mathbf{z}_{t} \mid \sqrt{1-\beta_{t}} \mathbf{z}_{t-1}, \beta_{t} \mathbf{I}\right)
\tag{20.4}
\end{align*}

\subsection{Diffusion kernel}

\begin{align*}
q\left(\mathbf{z}_{1}, \ldots, \mathbf{z}_{t} \mid \mathbf{x}\right)=q\left(\mathbf{z}_{1} \mid \mathbf{x}\right) \prod_{\tau=2}^{t} q\left(\mathbf{z}_{\tau} \mid \mathbf{z}_{\tau-1}\right)
\tag{20.5}
\end{align*}

\begin{align*}
q\left(\mathbf{z}_{t} \mid \mathbf{x}\right)=\mathcal{N}\left(\mathbf{z}_{t} \mid \sqrt{\alpha_{t}} \mathbf{x},\left(1-\alpha_{t}\right) \mathbf{I}\right)
\tag{20.6}
\end{align*}

\begin{align*}
\alpha_{t}=\prod_{\tau=1}^{t}\left(1-\beta_{\tau}\right)
\tag{20.7}
\end{align*}

\begin{align*}
\mathbf{z}_{t}=\sqrt{\alpha_{t}} \mathbf{x}+\sqrt{1-\alpha_{t}} \boldsymbol{\epsilon}_{t}
\tag{20.8}
\end{align*}

\begin{align*}
q\left(\mathbf{z}_{T} \mid \mathbf{x}\right)=\mathcal{N}\left(\mathbf{z}_{T} \mid \mathbf{0}, \mathbf{I}\right)
\tag{20.9}
\end{align*}

\begin{align*}
q\left(\mathbf{z}_{T}\right)=\mathcal{N}\left(\mathbf{z}_{T} \mid \mathbf{0}, \mathbf{I}\right)
\tag{20.10}
\end{align*}

\subsection{Conditional distribution}

\begin{align*}
q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}\right)=\frac{q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}\right) q\left(\mathbf{z}_{t-1}\right)}{q\left(\mathbf{z}_{t}\right)}
\tag{20.11}
\end{align*}

\begin{align*}
q\left(\mathbf{z}_{t-1}\right)=\int q\left(\mathbf{z}_{t-1} \mid \mathbf{x}\right) p(\mathbf{x}) \mathrm{d} \mathbf{x}
\tag{20.12}
\end{align*}

\begin{align*}
q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{x}\right)=\frac{q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}, \mathbf{x}\right) q\left(\mathbf{z}_{t-1} \mid \mathbf{x}\right)}{q\left(\mathbf{z}_{t} \mid \mathbf{x}\right)}
\tag{20.13}
\end{align*}

\begin{align*}
q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}, \mathbf{x}\right)=q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}\right)
\tag{20.14}
\end{align*}

\begin{align*}
q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{x}\right)=\mathcal{N}\left(\mathbf{z}_{t-1} \mid \mathbf{m}_{t}\left(\mathbf{x}, \mathbf{z}_{t}\right), \sigma_{t}^{2} \mathbf{I}\right)
\tag{20.15}
\end{align*}

\begin{align*}
\mathbf{m}_{t}\left(\mathbf{x}, \mathbf{z}_{t}\right) & =\frac{\left(1-\alpha_{t-1}\right) \sqrt{1-\beta_{t}} \mathbf{z}_{t}+\sqrt{\alpha_{t-1}} \beta_{t} \mathbf{x}}{1-\alpha_{t}} 
\tag{20.16}
\end{align*}

\begin{align*}
\sigma_{t}^{2} & =\frac{\beta_{t}\left(1-\alpha_{t-1}\right)}{1-\alpha_{t}}
\tag{20.17}
\end{align*}

\section{Reverse Decoder}

\begin{align*}
p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right)=\mathcal{N}\left(\mathbf{z}_{t-1} \mid \boldsymbol{\mu}\left(\mathbf{z}_{t}, \mathbf{w}, t\right), \beta_{t} \mathbf{I}\right) 
\tag{20.18}
\end{align*}

\begin{align*}
p\left(\mathbf{x}, \mathbf{z}_{1}, \ldots, \mathbf{z}_{T} \mid \mathbf{w}\right)=p\left(\mathbf{z}_{T}\right)\left\{\prod_{t=2}^{T} p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right)\right\} p\left(\mathbf{x} \mid \mathbf{z}_{1}, \mathbf{w}\right) 
\tag{20.19}
\end{align*}

\subsection{Training the decoder}

\begin{align*}
p(\mathbf{x} \mid \mathbf{w})=\int \cdots \int p\left(\mathbf{x}, \mathbf{z}_{1}, \ldots, \mathbf{z}_{T} \mid \mathbf{w}\right) \mathrm{d} \mathbf{z}_{1} \ldots \mathrm{d} \mathbf{z}_{T} 
\tag{20.20}
\end{align*}

\subsection{Evidence lower bound}

\begin{align*}
\ln p(\mathbf{x} \mid \mathbf{w})=\mathcal{L}(\mathbf{w})+\operatorname{KL}(q(\mathbf{z}) \| p(\mathbf{z} \mid \mathbf{x}, \mathbf{w})) 
\tag{20.21}
\end{align*}

\begin{align*}
\mathcal{L}(\mathbf{w})=\int q(\mathbf{z}) \ln \left\{\frac{p(\mathbf{x}, \mathbf{z} \mid \mathbf{w})}{q(\mathbf{z})}\right\} \mathrm{d} \mathbf{z} 
\tag{20.22}
\end{align*}

\begin{align*}
\mathrm{KL}(f(\mathbf{z}) \| g(\mathbf{z}))=-\int f(\mathbf{z}) \ln \left\{\frac{g(\mathbf{z})}{f(\mathbf{z})}\right\} \mathrm{d} \mathbf{z} 
\tag{20.23}
\end{align*}

\begin{align*}
p(\mathbf{x}, \mathbf{z} \mid \mathbf{w})=p(\mathbf{z} \mid \mathbf{x}, \mathbf{w}) p(\mathbf{x} \mid \mathbf{w}) 
\tag{20.24}
\end{align*}

\begin{align*}
\ln p(\mathbf{x} \mid \mathbf{w}) \geqslant \mathcal{L}(\mathbf{w}) 
\tag{20.25}
\end{align*}

\begin{align*}
\mathcal{L}(\mathbf{w}) &= \mathbb{E}_{q}\left[\ln \frac{p\left(\mathbf{z}_{T}\right)\left\{\prod_{t=2}^{T} p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right)\right\} p\left(\mathbf{x} \mid \mathbf{z}_{1}, \mathbf{w}\right)}{q\left(\mathbf{z}_{1} \mid \mathbf{x}\right) \prod_{t=2}^{T} q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}, \mathbf{x}\right)}\right] \\
&=\mathbb{E}_{q}\left[\ln p\left(\mathbf{z}_{T}\right)+\sum_{t=2}^{T} \ln \frac{p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right)}{q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}, \mathbf{x}\right)}-\ln q\left(\mathbf{z}_{1} \mid \mathbf{x}\right)+\ln p\left(\mathbf{x} \mid \mathbf{z}_{1}, \mathbf{w}\right)\right]
\tag{20.26}
\end{align*}

\begin{align*}
\mathbb{E}_{q}[
\cdot]
\equiv \int \cdots \int q\left(\mathbf{z}_{1} \mid \mathbf{x}\right) \prod_{t=2}^{T} q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}\right)[
\cdot]
\mathrm{d} \mathbf{z}_{1} \ldots \mathrm{d} \mathbf{z}_{T} 
\tag{20.27}
\end{align*}

\begin{align*}
\mathbb{E}_{q}\left[\ln p\left(\mathbf{x} \mid \mathbf{z}_{1}, \mathbf{w}\right)\right]
\simeq \sum_{l=1}^{L} \ln p\left(\mathbf{x} \mid \mathbf{z}_{1}^{(l)}, \mathbf{w}\right)
\tag{20.28}
\end{align*}

\subsection{Rewriting the ELBO}

\begin{align*}
q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}, \mathbf{x}\right)=\frac{q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{x}\right) q\left(\mathbf{z}_{t} \mid \mathbf{x}\right)}{q\left(\mathbf{z}_{t-1} \mid \mathbf{x}\right)} 
\tag{20.29}
\end{align*}

\begin{align*}
\ln \frac{p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right)}{q\left(\mathbf{z}_{t} \mid \mathbf{z}_{t-1}, \mathbf{x}\right)}=\ln \frac{p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right)}{q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{x}\right)}+\ln \frac{q\left(\mathbf{z}_{t-1} \mid \mathbf{x}\right)}{q\left(\mathbf{z}_{t} \mid \mathbf{x}\right)} 
\tag{20.30}
\end{align*}

\begin{align*}
\mathcal{L}(\mathbf{w})=\mathbb{E}_{q}\left[
\sum_{t=2}^{T} \ln \frac{p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right)}{q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{x}\right)}+\ln p\left(\mathbf{x} \mid \mathbf{z}_{1}, \mathbf{w}\right)\right]
\tag{20.31}
\end{align*}

\begin{align*}
\mathcal{L}(\mathbf{w})= & \underbrace{\int q\left(\mathbf{z}_{1} \mid \mathbf{x}\right) \ln p\left(\mathbf{x} \mid \mathbf{z}_{1}, \mathbf{w}\right) \mathrm{d} \mathbf{z}_{1}}_{\text{reconstruction term }} \\
& -\underbrace{\sum_{t=2}^{T} \int \mathrm{KL}\left(q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{x}\right) \| p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right)\right) q\left(\mathbf{z}_{t} \mid \mathbf{x}\right) \mathrm{d} \mathbf{z}_{t}}_{\text{consistency terms }}
\tag{20.32}
\end{align*}

\begin{align*}
\mathrm{KL}\left(q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{x}\right) \| p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right)\right)
= \frac{1}{2 \beta_{t}}\left\|\mathbf{m}_{t}\left(\mathbf{x}, \mathbf{z}_{t}\right)-\boldsymbol{\mu}\left(\mathbf{z}_{t}, \mathbf{w}, t\right)\right\|^{2}+\mathrm{const}
\tag{20.33}
\end{align*}

\subsection{Predicting the noise}

\begin{align*}
\mathbf{x}=\frac{1}{\sqrt{\alpha_{t}}} \mathbf{z}_{t}-\frac{\sqrt{1-\alpha_{t}}}{\sqrt{\alpha_{t}}} \boldsymbol{\epsilon}_{t} 
\tag{20.34}
\end{align*}

\begin{align*}
\mathbf{m}_{t}\left(\mathbf{x}, \mathbf{z}_{t}\right)=\frac{1}{\sqrt{1-\beta_{t}}}\left\{\mathbf{z}_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}} \boldsymbol{\epsilon}_{t}\right\} 
\tag{20.35}
\end{align*}

\begin{align*}
\boldsymbol{\mu}\left(\mathbf{z}_{t}, \mathbf{w}, t\right)=\frac{1}{\sqrt{1-\beta_{t}}}\left\{\mathbf{z}_{t}-\frac{\beta_{t}}{\sqrt{1-\alpha_{t}}} \mathbf{g}\left(\mathbf{z}_{t}, \mathbf{w}, t\right)\right\} 
\tag{20.36}
\end{align*}

\begin{align*}
\mathrm{KL}\left(q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{x}\right) \| p\left(\mathbf{z}_{t-1} \mid \mathbf{z}_{t}, \mathbf{w}\right) \right) 
& =\frac{\beta_{t}}{2\left(1-\alpha_{t}\right)\left(1-\beta_{t}\right)}\left\|\mathbf{g}\left(\mathbf{z}_{t}, \mathbf{w}, t\right)-\boldsymbol{\epsilon}_{t}\right\|^{2}+\text{ const } \\
& =\frac{\beta_{t}}{2\left(1-\alpha_{t}\right)\left(1-\beta_{t}\right)}\left\|\mathbf{g}\left(\sqrt{\alpha_{t}} \mathbf{x}+\sqrt{1-\alpha_{t}} \boldsymbol{\epsilon}_{t}, \mathbf{w}, t\right)-\boldsymbol{\epsilon}_{t}\right\|^{2}+\text{ const }
\tag{20.37}
\end{align*}

\begin{align*}
\ln p\left(\mathbf{x} \mid \mathbf{z}_{1}, \mathbf{w}\right)=-\frac{1}{2 \beta_{1}}\left\|\mathbf{x}-\boldsymbol{\mu}\left(\mathbf{z}_{1}, \mathbf{w}, 1\right)\right\|^{2}+\text{ const. } 
\tag{20.38}
\end{align*}

\begin{align*}
\ln p\left(\mathbf{x} \mid \mathbf{z}_{1}, \mathbf{w}\right)=-\frac{1}{2\left(1-\beta_{t}\right)}\left\|\mathbf{g}\left(\mathbf{z}_{1}, \mathbf{w}, 1\right)-\boldsymbol{\epsilon}_{1}\right\|^{2}+\text{ const. } 
\tag{20.39}
\end{align*}

\begin{align*}
\mathcal{L}(\mathbf{w})=-\sum_{t=1}^{T}\left\|\mathbf{g}\left(\sqrt{\alpha_{t}} \mathbf{x}+\sqrt{1-\alpha_{t}} \boldsymbol{\epsilon}_{t}, \mathbf{w}, t\right)-\boldsymbol{\epsilon}_{t}\right\|^{2} 
\tag{20.40}
\end{align*}

\subsection{Generating new samples}

\begin{align*}
\mathbf{z}_{t-1}=\boldsymbol{\mu}\left(\mathbf{z}_{t}, \mathbf{w}, t\right)+\sqrt{\beta_{t}} \boldsymbol{\epsilon}, \quad \text{ where } \boldsymbol{\epsilon} \sim \mathcal{N}(\boldsymbol{\epsilon} \mid \mathbf{0}, \mathbf{I})
\tag{20.41}
\end{align*}

\subsubsection{Algorithm 20.1: Training a denoising diffusion probabilistic model}

\begin{algorithm}[H]
\caption{Training a Denoising Diffusion Probabilistic Model}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Training data $D = \{x_n\}$
\STATE \quad Noise schedule $\{\beta_1, \ldots, \beta_T\}$
\STATE \textbf{Output:} Network parameters $w$
\FOR{$t \in \{1, \ldots, T\}$}
    \STATE $\alpha_t \gets \prod_{\tau=1}^{t} (1 - \beta_\tau)$ \COMMENT{// Calculate alphas from betas}
\ENDFOR
\REPEAT
    \STATE $x \sim D$ \COMMENT{// Sample a data point}
    \STATE $t \sim \{1, \ldots, T\}$ \COMMENT{// Sample a point along the Markov chain}
    \STATE $\epsilon \sim \mathcal{N}(0, I)$ \COMMENT{// Sample a noise vector}
    \STATE $z_t \gets \sqrt{\alpha_t} x + \sqrt{1 - \alpha_t} \epsilon$ \COMMENT{// Evaluate noisy latent variable}
    \STATE $L(w) \gets \|g(z_t, w, t) - \epsilon\|^2$ \COMMENT{// Compute loss term}
    \STATE \text{Take optimizer step}
\UNTIL converged
\RETURN $w$
\end{algorithmic}
\end{algorithm}

\subsubsection{Algorithm 20.2: Sampling from a denoising diffusion probabilistic model}

\begin{algorithm}[H]
\caption{Sampling from a Denoising Diffusion Probabilistic Model}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Trained denoising network $g(z, w, t)$
\STATE \quad Noise schedule $\{\beta_1, \ldots, \beta_T\}$
\STATE \textbf{Output:} Sample vector $x$ in data space
\STATE $z_T \sim \mathcal{N}(z|0, I)$ \COMMENT{// Sample from final latent space}
\FOR{$t \in T, \ldots, 2$}
    \STATE $\alpha_t \gets \prod_{\tau=1}^{t} (1 - \beta_\tau)$ \COMMENT{// Calculate alpha}
    \STATE $\mu(z_t, w, t) \gets \frac{1}{\sqrt{1 - \beta_t}} \left(z_t - \frac{\beta_t}{\sqrt{1 - \alpha_t}} g(z_t, w, t)\right)$ \COMMENT{// Evaluate network output}
    \STATE $\epsilon \sim \mathcal{N}(0, I)$ \COMMENT{// Sample a noise vector}
    \STATE $z_{t-1} \gets \mu(z_t, w, t) + \sqrt{\beta_t} \epsilon$ \COMMENT{// Add scaled noise}
\ENDFOR
\STATE $x \gets \frac{1}{\sqrt{1 - \beta_1}} \left(z_1 - \frac{\beta_1}{\sqrt{1 - \alpha_1}} g(z_1, w, t)\right)$ \COMMENT{// Final denoising step}
\RETURN $x$
\end{algorithmic}
\end{algorithm}

\section{Score Matching}

\begin{align*}
\mathbf{s}(\mathbf{x})=\nabla_{\mathbf{x}} \ln p(\mathbf{x}) 
\tag{20.42}
\end{align*}

\subsection{Score loss function}

\begin{align*}
J(\mathbf{w})=\frac{1}{2} \int\left\|\mathbf{s}(\mathbf{x}, \mathbf{w})-\nabla_{\mathbf{x}} \ln p(\mathbf{x})\right\|^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x} 
\tag{20.43}
\end{align*}

\subsection{Modified score loss}

\begin{align*}
p_{\mathcal{D}}(\mathbf{x})=\frac{1}{N} \sum_{n=1}^{N} \delta\left(\mathbf{x}-\mathbf{x}_{n}\right) 
\tag{20.44}
\end{align*}

\begin{align*}
\delta(\mathbf{x}) & =0, \quad \mathbf{x} \neq \mathbf{0}  
\tag{20.45}\\
\int \delta(\mathbf{x}) \mathrm{d} \mathbf{x} & =1
\tag{20.46}
\end{align*}

\begin{align*}
q_{\sigma}(\mathbf{z})=\int q(\mathbf{z} \mid \mathbf{x}, \sigma) p(\mathbf{x}) \mathrm{d} \mathbf{x} 
\tag{20.47}
\end{align*}

\begin{align*}
q(\mathbf{z} \mid \mathbf{x}, \sigma)=\mathcal{N}\left(\mathbf{z} \mid \mathbf{x}, \sigma^{2} \mathbf{I}\right) 
\tag{20.48}
\end{align*}

\begin{align*}
J(\mathbf{w})=\frac{1}{2} \int\left\|\mathbf{s}(\mathbf{z}, \mathbf{w})-\nabla_{\mathbf{z}} \ln q_{\sigma}(\mathbf{z})\right\|^{2} q_{\sigma}(\mathbf{z}) \mathrm{d} \mathbf{z} 
\tag{20.49}
\end{align*}

\begin{align*}
J(\mathbf{w})=\frac{1}{2} \iint\left\|\mathbf{s}(\mathbf{z}, \mathbf{w})-\nabla_{\mathbf{z}} \ln q(\mathbf{z} \mid \mathbf{x}, \sigma)\right\|^{2} q(\mathbf{z} \mid \mathbf{x}, \sigma) p(\mathbf{x}) \mathrm{d} \mathbf{z} \mathrm{d} \mathbf{x}+\text{ const. } 
\tag{20.50}
\end{align*}

\begin{align*}
J(\mathbf{w})=\frac{1}{2 N} \sum_{n=1}^{N} \int\left\|\mathbf{s}(\mathbf{z}, \mathbf{w})-\nabla_{\mathbf{z}} \ln q\left(\mathbf{z} \mid \mathbf{x}_{n}, \sigma\right)\right\|^{2} q\left(\mathbf{z} \mid \mathbf{x}_{n}, \sigma\right) \mathrm{d} \mathbf{z}+\text{ const. } 
\tag{20.51}
\end{align*}

\begin{align*}
\nabla_{\mathbf{z}} \ln q(\mathbf{z} \mid \mathbf{x}, \sigma)=-\frac{1}{\sigma} \boldsymbol{\epsilon} 
\tag{20.52}
\end{align*}

\begin{align*}
\nabla_{\mathbf{z}} \ln q(\mathbf{z} \mid \mathbf{x}, \sigma)=-\frac{1}{\sqrt{1-\alpha_{t}}} \boldsymbol{\epsilon} 
\tag{20.53}
\end{align*}

\subsection{Noise variance}

\begin{align*}
\frac{1}{2} \sum_{i=1}^{L} \lambda(i) \int\left\|\mathbf{s}\left(\mathbf{z}, \mathbf{w}, \sigma_{i}^{2}\right)-\nabla_{\mathbf{z}} \ln q\left(\mathbf{z} \mid \mathbf{x}_{n}, \sigma_{i}\right)\right\|^{2} q\left(\mathbf{z} \mid \mathbf{x}_{n}, \sigma_{i}\right) \mathrm{d} \mathbf{z} 
\tag{20.54}
\end{align*}

\subsection{Stochastic differential equations}

\begin{align*}
\mathrm{d} \mathbf{z}=\underbrace{\mathbf{f}(\mathbf{z}, t) \mathrm{d} t}_{\text{drift }}+\underbrace{g(t) \mathrm{d} \mathbf{v}}_{\text{diffusion }} 
\tag{20.55}
\end{align*}

\begin{align*}
\mathrm{d} \mathbf{z}=\left\{\mathbf{f}(\mathbf{z}, t)-g^{2}(t) \nabla_{\mathbf{z}} \ln p(\mathbf{z})\right\} \mathrm{d} t+g(t) \mathrm{d} \mathbf{v} 
\tag{20.56}
\end{align*}

\begin{align*}
\frac{\mathrm{d} \mathbf{z}}{\mathrm{d} t}=\mathbf{f}(\mathbf{z}, t)-\frac{1}{2} g^{2}(t) \nabla_{\mathbf{z}} \ln p(\mathbf{z}) 
\tag{20.57}
\end{align*}

\section{Guided Diffusion}

\subsection{Classifier guidance}

\begin{align*}
\nabla_{\mathbf{x}} \ln p(\mathbf{x} \mid \mathbf{c}) & =\nabla_{\mathbf{x}} \ln \left\{\frac{p(\mathbf{c} \mid \mathbf{x}) p(\mathbf{x})}{p(\mathbf{c})}\right\} \\
& =\nabla_{\mathbf{x}} \ln p(\mathbf{x})+\nabla_{\mathbf{x}} \ln p(\mathbf{c} \mid \mathbf{x}) 
\tag{20.58}
\end{align*}

\begin{align*}
\operatorname{score}(\mathbf{x}, \mathbf{c}, \lambda)=\nabla_{\mathbf{x}} \ln p(\mathbf{x})+\lambda \nabla_{\mathbf{x}} \ln p(\mathbf{c} \mid \mathbf{x}) 
\tag{20.59}
\end{align*}

\subsection{Classifier-free guidance}

\begin{align*}
\operatorname{score}(\mathbf{x}, \mathbf{c}, \lambda)=\lambda \nabla_{\mathbf{x}} \ln p(\mathbf{x} \mid \mathbf{c})+(1-\lambda) \nabla_{\mathbf{x}} \ln p(\mathbf{x}) 
\tag{20.60}
\end{align*}

\end{document}
